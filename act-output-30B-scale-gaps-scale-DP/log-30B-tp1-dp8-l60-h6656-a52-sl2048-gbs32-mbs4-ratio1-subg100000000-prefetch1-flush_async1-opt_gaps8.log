[2024-03-29 16:04:13,497] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 16:04:16,611] [INFO] [runner.py:463:main] Using IP address of 10.140.58.110 for node x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-29 16:04:16,614] [INFO] [runner.py:557:main] deepspeed_env file = ./.deepspeed_env
[2024-03-29 16:04:16,614] [INFO] [multinode_runner.py:80:get_cmd] Running on the following workers: x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov,x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-29 16:04:16,614] [INFO] [runner.py:568:main] cmd = pdsh -S -f 1024 -w x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov,x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov export PYTHONUSERBASE=/home/am6429/.local/polaris/conda/2023-10-04; export PYTHONPATH=/home/am6429/dl-io/Megatron-DeepSpeed; export PATH=/home/am6429/.conda/envs/dspeed_env/bin:/soft/datascience/conda/2023-10-04/mconda3/condabin:/soft/compilers/cudatoolkit/cuda-11.8.0/bin:/soft/buildtools/cmake/cmake-3.23.2/cmake-3.23.2-linux-x86_64/bin:/opt/cray/pe/gcc/11.2.0/bin:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/include:/opt/cray/pe/pals/1.2.11/bin:/opt/cray/pe/craype/2.7.20/bin:/opt/cray/libfabric/1.15.2.0/bin:/home/am6429/.conda/envs/dspeed_env/bin:/opt/cray/pe/perftools/23.03.0/bin:/opt/cray/pe/papi/7.0.0.1/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/am6429/.local/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/am6429/veloc-build/include:/home/am6429/veloc-build/bin:/soft/datascience/conda/2023-01-10/mconda3/include:/opt/cray/pe/bin:/soft/datascience/conda/2023-01-10/mconda3/include; export LD_LIBRARY_PATH=/usr/lib64/:/soft/compilers/cudatoolkit/cuda-11.8.0/lib64:/opt/cray/pe/gcc/11.2.0/snos/lib64:/soft/compilers/cudatoolkit/cuda-11.8.0/extras/CUPTI/lib64:/soft/libraries/trt/TensorRT-8.5.3.1.Linux.x86_64-gnu.cuda-11.8.cudnn8.6/lib:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/lib:/soft/libraries/cudnn/cudnn-11-linux-x64-v8.6.0.163/lib:/opt/cray/libfabric/1.15.2.0/lib64:/usr/lib64:/opt/cray/pe/papi/7.0.0.1/lib64:/home/am6429/veloc-build/lib:/home/am6429/veloc-build/lib64:/home/am6429/nvcomp/lib:/soft/datascience/conda/2023-01-10/mconda3/lib:/soft/datascience/conda/2023-01-10/mconda3/lib/; export http_proxy=http://proxy.alcf.anl.gov:3128; export https_proxy=http://proxy.alcf.anl.gov:3128; export CC=gcc; export CXX=g++; export IBV_FORK_SAFE=1; export CFLAGS=-I/soft/datascience/conda/2023-01-10/mconda3/include/; export LDFLAGS=-L/soft/datascience/conda/2023-01-10/mconda3/lib/; export CUDA_DEVICE_MAX_CONNECTIONS=1; export TORCHSNAPSHOT_PER_RANK_MEMORY_BUDGET_BYTES=34359738368; export _DEFAULT_MAX_PER_RANK_IO_CONCURRENCY=1; export _MAX_PER_RANK_IO_CONCURRENCY=1; export NSYS_REPORT_DIR=/home/am6429/dl-io/dl-io-outputs/act-output-30B//rep-30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch1-flush_async1-opt_gaps8-%n;  cd /home/am6429/dl-io/Megatron-DeepSpeed; /home/am6429/.conda/envs/dspeed_env/bin/python -u -m deepspeed.launcher.launch --world_info=eyJ4MzEwNGMwczFiMG4wLmhzbi5jbS5wb2xhcmlzLmFsY2YuYW5sLmdvdiI6IFswLCAxLCAyLCAzXSwgIngzMTA0YzBzMjViMG4wLmhzbi5jbS5wb2xhcmlzLmFsY2YuYW5sLmdvdiI6IFswLCAxLCAyLCAzXX0= --node_rank=%n --master_addr=10.140.58.110 --master_port=29700 /home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py --tensor-model-parallel-size 1 --num-layers 60 --hidden-size 6656 --num-attention-heads 52 --micro-batch-size 4 --global-batch-size 32 --seq-length 2048 --max-position-embeddings 2048 --train-iters 10 --save /local/scratch/llama2/zero3-tp1}_dp8 --data-path /home/am6429/dl-io/datasets/meg-gpt2_text_document --vocab-file /home/am6429/dl-io/datasets/gpt2-vocab.json --merge-file /home/am6429/dl-io/datasets/gpt2-merges.txt --data-impl mmap --tokenizer-type GPTSentencePieceTokenizer --tokenizer-model /home/am6429/dl-io/datasets/tokenizer.model --split 949,50,1 --distributed-backend nccl --lr 3e-4 --lr-decay-style cosine --min-lr 3e-5 --weight-decay 0.1 --clip-grad 1 --lr-warmup-iters 1 --optimizer adam --adam-beta1 0.9 --adam-beta2 0.95 --log-interval 1 --save-interval 1000 --eval-interval 1000 --eval-iters 0 --bf16 --no-query-key-layer-scaling --attention-dropout 0 --hidden-dropout 0 --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization rmsnorm --disable-bias-linear --num-key-value-heads 4 --deepspeed --exit-interval 20 --deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json --zero-stage=3 --no-pipeline-parallel --cpu-optimizer --checkpoint-activations --deepspeed-activation-checkpointing
x3104c0s25b0n0: [2024-03-29 16:04:18,599] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 16:04:18,631] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 16:04:20,500] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3104c0s1b0n0: [2024-03-29 16:04:20,500] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=0
x3104c0s1b0n0: [2024-03-29 16:04:20,500] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3104c0s1b0n0: [2024-03-29 16:04:20,500] [INFO] [launch.py:163:main] dist_world_size=8
x3104c0s1b0n0: [2024-03-29 16:04:20,500] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3104c0s1b0n0: [2024-03-29 16:04:20,501] [INFO] [launch.py:253:main] process 63743 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s1b0n0: [2024-03-29 16:04:20,502] [INFO] [launch.py:253:main] process 63744 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s1b0n0: [2024-03-29 16:04:20,502] [INFO] [launch.py:253:main] process 63745 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s1b0n0: [2024-03-29 16:04:20,503] [INFO] [launch.py:253:main] process 63746 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s25b0n0: [2024-03-29 16:04:20,589] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3104c0s25b0n0: [2024-03-29 16:04:20,589] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=1
x3104c0s25b0n0: [2024-03-29 16:04:20,589] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3104c0s25b0n0: [2024-03-29 16:04:20,589] [INFO] [launch.py:163:main] dist_world_size=8
x3104c0s25b0n0: [2024-03-29 16:04:20,589] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3104c0s25b0n0: [2024-03-29 16:04:20,590] [INFO] [launch.py:253:main] process 25014 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s25b0n0: [2024-03-29 16:04:20,590] [INFO] [launch.py:253:main] process 25015 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s25b0n0: [2024-03-29 16:04:20,591] [INFO] [launch.py:253:main] process 25016 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s25b0n0: [2024-03-29 16:04:20,591] [INFO] [launch.py:253:main] process 25017 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s25b0n0: [2024-03-29 16:04:22,257] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s25b0n0: [2024-03-29 16:04:22,262] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s25b0n0: [2024-03-29 16:04:22,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 16:04:22,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s25b0n0: [2024-03-29 16:04:22,264] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 16:04:22,292] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 16:04:22,340] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 16:04:22,343] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s25b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s25b0n0:       meet the required dependencies to JIT install the op.
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: JIT compiled ops requires ninja
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s25b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s25b0n0:       meet the required dependencies to JIT install the op.
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: JIT compiled ops requires ninja
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s25b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s25b0n0:       meet the required dependencies to JIT install the op.
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: JIT compiled ops requires ninja
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s25b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s25b0n0:       meet the required dependencies to JIT install the op.
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: JIT compiled ops requires ninja
x3104c0s25b0n0: ninja .................. [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: op name ................ installed .. compatible
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s25b0n0: ninja .................. [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: op name ................ installed .. compatible
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s25b0n0: ninja .................. [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: op name ................ installed .. compatible
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s25b0n0: ninja .................. [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: op name ................ installed .. compatible
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s25b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s25b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s25b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s25b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s25b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: quantizer .............. cutlass_ops[93m[NO][0m .......  ............[92m[OKAY][0m 
x3104c0s25b0n0: [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: ragged_opstransformer_inference .............  ..[93m[NO][0m  [93m[NO][0m.......  .......[92m[OKAY][0m 
x3104c0s25b0n0: [92m[OKAY][0m
x3104c0s25b0n0: random_ltd quantizer.............  ..............[93m[NO][0m  [93m[NO][0m.......  .......[92m[OKAY][0m 
x3104c0s25b0n0: [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s25b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s25b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s25b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s25b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s25b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s25b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed general environment info:
x3104c0s25b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s25b0n0: ragged_ops torch version.............  ....................[93m[NO][0m  .......2.0.1+cu118 
x3104c0s25b0n0: [92m[OKAY][0mdeepspeed install path
x3104c0s25b0n0:  ........... random_ltd['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed'] 
x3104c0s25b0n0: .............deepspeed info  [93m[NO][0m...................  .......0.13.3+8074cd62, 8074cd62, hybrid_opt_offload 
x3104c0s25b0n0: [92m[OKAY][0m
x3104c0s25b0n0: torch cuda version ............... 11.8
x3104c0s25b0n0: torch hip version ................ None
x3104c0s25b0n0: nvcc version ..................... 11.8
x3104c0s25b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s25b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s25b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s25b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s25b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed general environment info:
x3104c0s25b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s25b0n0: torch version .................... 2.0.1+cu118
x3104c0s25b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s25b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s25b0n0: torch cuda version ............... 11.8
x3104c0s25b0n0: torch hip version ................ None
x3104c0s25b0n0: nvcc version ..................... 11.8
x3104c0s25b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s25b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s25b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed general environment info:
x3104c0s25b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s25b0n0: torch version .................... 2.0.1+cu118
x3104c0s25b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s25b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s25b0n0: torch cuda version ............... 11.8
x3104c0s25b0n0: torch hip version ................ None
x3104c0s25b0n0: nvcc version ..................... 11.8
x3104c0s25b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s25b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s25b0n0: DeepSpeed general environment info:
x3104c0s25b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s25b0n0: torch version .................... 2.0.1+cu118
x3104c0s25b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s25b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s25b0n0: torch cuda version ............... 11.8
x3104c0s25b0n0: torch hip version ................ None
x3104c0s25b0n0: nvcc version ..................... 11.8
x3104c0s25b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s25b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s25b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s25b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s25b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s25b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s25b0n0: [2024-03-29 16:04:24,716] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s25b0n0: [2024-03-29 16:04:24,717] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s25b0n0: [2024-03-29 16:04:24,717] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s25b0n0: [2024-03-29 16:04:24,718] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s1b0n0:       meet the required dependencies to JIT install the op.
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: JIT compiled ops requires ninja
x3104c0s1b0n0: ninja .................. [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: op name ................ installed .. compatible
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s1b0n0:       meet the required dependencies to JIT install the op.
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: JIT compiled ops requires ninja
x3104c0s1b0n0: ninja .................. [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: op name ................ installed .. compatible
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s1b0n0:       meet the required dependencies to JIT install the op.
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: JIT compiled ops requires ninja
x3104c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ninja .................. [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: op name ................ installed .. compatible
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed general environment info:
x3104c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s1b0n0: torch version .................... 2.0.1+cu118
x3104c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s1b0n0: torch cuda version ............... 11.8
x3104c0s1b0n0: torch hip version ................ None
x3104c0s1b0n0: nvcc version ..................... 11.8
x3104c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s1b0n0:       meet the required dependencies to JIT install the op.
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: JIT compiled ops requires ninja
x3104c0s1b0n0: ninja .................. [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: op name ................ installed .. compatible
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [2024-03-29 16:04:25,153] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed general environment info:
x3104c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s1b0n0: torch version .................... 2.0.1+cu118
x3104c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s1b0n0: torch cuda version ............... 11.8
x3104c0s1b0n0: torch hip version ................ None
x3104c0s1b0n0: nvcc version ..................... 11.8
x3104c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: INFO: overriding default arguments for tokenizer_type:GPTSentencePieceTokenizer                    with tokenizer_type:GPT2BPETokenizer
x3104c0s1b0n0: using world size: 8, data-parallel-size: 8, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
x3104c0s1b0n0: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
x3104c0s1b0n0: using torch.bfloat16 for parameters ...
x3104c0s1b0n0: ------------------------ arguments ------------------------
x3104c0s1b0n0:   accumulate_allreduce_grads_in_fp32 .............. True
x3104c0s1b0n0:   adam_beta1 ...................................... 0.9
x3104c0s1b0n0:   adam_beta2 ...................................... 0.95
x3104c0s1b0n0:   adam_eps ........................................ 1e-08
x3104c0s1b0n0:   add_bias_linear ................................. False
x3104c0s1b0n0:   add_position_embedding .......................... False
x3104c0s1b0n0:   adlr_autoresume ................................. False
x3104c0s1b0n0:   adlr_autoresume_interval ........................ 1000
x3104c0s1b0n0:   aml_data_download_path .......................... None
x3104c0s1b0n0:   apply_layernorm_1p .............................. False
x3104c0s1b0n0:   apply_query_key_layer_scaling ................... False
x3104c0s1b0n0:   apply_residual_connection_post_layernorm ........ False
x3104c0s1b0n0:   async_tensor_model_parallel_allreduce ........... False
x3104c0s1b0n0:   attention_dropout ............................... 0.0
x3104c0s1b0n0:   attention_softmax_in_fp32 ....................... False
x3104c0s1b0n0:   barrier_with_L1_time ............................ True
x3104c0s1b0n0:   bert_binary_head ................................ True
x3104c0s1b0n0:   bert_embedder_type .............................. megatron
x3104c0s1b0n0:   bert_load ....................................... None
x3104c0s1b0n0:   bf16 ............................................ True
x3104c0s1b0n0:   bias_dropout_fusion ............................. True
x3104c0s1b0n0:   bias_gelu_fusion ................................ False
x3104c0s1b0n0:   biencoder_projection_dim ........................ 0
x3104c0s1b0n0:   biencoder_shared_query_context_model ............ False
x3104c0s1b0n0:   block_data_path ................................. None
x3104c0s1b0n0:   checkpoint_activations .......................... True
x3104c0s1b0n0:   checkpoint_in_cpu ............................... False
x3104c0s1b0n0:   checkpoint_num_layers ........................... 1
x3104c0s1b0n0:   classes_fraction ................................ 1.0
x3104c0s1b0n0:   clip_grad ....................................... 1.0
x3104c0s1b0n0:   compression_training ............................ False
x3104c0s1b0n0:   consumed_train_samples .......................... 0
x3104c0s1b0n0:   consumed_train_tokens ........................... 0
x3104c0s1b0n0:   consumed_valid_samples .......................... 0
x3104c0s1b0n0:   contigious_checkpointing ........................ False
x3104c0s1b0n0:   cpu_optimizer ................................... True
x3104c0s1b0n0:   cpu_torch_adam .................................. False
x3104c0s1b0n0:   create_moe_param_group .......................... False
x3104c0s1b0n0:   curriculum_learning_legacy ...................... False
x3104c0s1b0n0:   data_cache_path ................................. None
x3104c0s1b0n0:   data_efficiency_curriculum_learning ............. False
x3104c0s1b0n0:   data_impl ....................................... mmap
x3104c0s1b0n0:   data_parallel_random_init ....................... False
x3104c0s1b0n0:   data_parallel_size .............................. 8
x3104c0s1b0n0:   data_path ....................................... ['/home/am6429/dl-io/datasets/meg-gpt2_text_document']
x3104c0s1b0n0:   data_per_class_fraction ......................... 1.0
x3104c0s1b0n0:   data_sharding ................................... True
x3104c0s1b0n0:   dataloader_type ................................. single
x3104c0s1b0n0:   DDP_impl ........................................ local
x3104c0s1b0n0:   decoder_num_layers .............................. None
x3104c0s1b0n0:   decoder_seq_length .............................. None
x3104c0s1b0n0:   deepscale ....................................... False
x3104c0s1b0n0:   deepscale_config ................................ None
x3104c0s1b0n0:   deepspeed ....................................... True
x3104c0s1b0n0:   deepspeed_activation_checkpointing .............. True
x3104c0s1b0n0:   deepspeed_config ................................ /home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json
x3104c0s1b0n0:   dino_bottleneck_size ............................ 256
x3104c0s1b0n0:   dino_freeze_last_layer .......................... 1
x3104c0s1b0n0:   dino_head_hidden_size ........................... 2048
x3104c0s1b0n0:   dino_local_crops_number ......................... 10
x3104c0s1b0n0:   dino_local_img_size ............................. 96
x3104c0s1b0n0:   dino_norm_last_layer ............................ False
x3104c0s1b0n0:   dino_teacher_temp ............................... 0.07
x3104c0s1b0n0:   dino_warmup_teacher_temp ........................ 0.04
x3104c0s1b0n0:   dino_warmup_teacher_temp_epochs ................. 30
x3104c0s1b0n0:   distribute_checkpointed_activations ............. False
x3104c0s1b0n0:   distribute_saved_activations .................... False
x3104c0s1b0n0:   distributed_backend ............................. nccl
x3104c0s1b0n0:   distributed_timeout_minutes ..................... 10
x3104c0s1b0n0:   ds_inference .................................... False
x3104c0s1b0n0:   ds_pipeline_enabled ............................. False
x3104c0s1b0n0:   ds_sequence_parallel_size ....................... 1
x3104c0s1b0n0:   embedding_path .................................. None
x3104c0s1b0n0:   embedding_weights_in_fp32 ....................... False
x3104c0s1b0n0:   empty_unused_memory_level ....................... 0
x3104c0s1b0n0:   enable_expert_tensor_parallelism ................ False
x3104c0s1b0n0:   encoder_num_layers .............................. 60
x3104c0s1b0n0:   encoder_seq_length .............................. 2048
x3104c0s1b0n0:   end_weight_decay ................................ 0.1
x3104c0s1b0n0:   eod_mask_loss ................................... False
x3104c0s1b0n0:   eval_interval ................................... 1000
x3104c0s1b0n0:   eval_iters ...................................... 0
x3104c0s1b0n0:   evidence_data_path .............................. None
x3104c0s1b0n0:   exit_duration_in_mins ........................... None
x3104c0s1b0n0:   exit_interval ................................... 20
x3104c0s1b0n0:   exit_on_missing_checkpoint ...................... False
x3104c0s1b0n0:   exit_signal_handler ............................. False
x3104c0s1b0n0:   expert_interval ................................. 2
x3104c0s1b0n0:   ffn_hidden_size ................................. 17728
x3104c0s1b0n0:   finetune ........................................ False
x3104c0s1b0n0:   force_ds_sequence_parallel ...................... False
x3104c0s1b0n0:   fp16 ............................................ False
x3104c0s1b0n0:   fp16_lm_cross_entropy ........................... False
x3104c0s1b0n0:   fp32_residual_connection ........................ False
x3104c0s1b0n0:   fp8_amax_compute_algo ........................... most_recent
x3104c0s1b0n0:   fp8_amax_history_len ............................ 1
x3104c0s1b0n0:   fp8_e4m3 ........................................ False
x3104c0s1b0n0:   fp8_hybrid ...................................... False
x3104c0s1b0n0:   fp8_interval .................................... 1
x3104c0s1b0n0:   fp8_margin ...................................... 0
x3104c0s1b0n0:   fp8_wgrad ....................................... True
x3104c0s1b0n0:   global_batch_size ............................... 32
x3104c0s1b0n0:   gradient_accumulation_fusion .................... True
x3104c0s1b0n0:   head_lr_mult .................................... 1.0
x3104c0s1b0n0:   hidden_dropout .................................. 0.0
x3104c0s1b0n0:   hidden_size ..................................... 6656
x3104c0s1b0n0:   hidden_size_teacher ............................. None
x3104c0s1b0n0:   hysteresis ...................................... 2
x3104c0s1b0n0:   ict_head_size ................................... None
x3104c0s1b0n0:   ict_load ........................................ None
x3104c0s1b0n0:   img_h ........................................... 224
x3104c0s1b0n0:   img_w ........................................... 224
x3104c0s1b0n0:   indexer_batch_size .............................. 128
x3104c0s1b0n0:   indexer_log_interval ............................ 1000
x3104c0s1b0n0:   inference ....................................... False
x3104c0s1b0n0:   inference_batch_times_seqlen_threshold .......... 512
x3104c0s1b0n0:   init_method_std ................................. 0.02
x3104c0s1b0n0:   init_method_xavier_uniform ...................... False
x3104c0s1b0n0:   initial_loss_scale .............................. 4294967296
x3104c0s1b0n0:   iter_per_epoch .................................. 1250
x3104c0s1b0n0:   kd .............................................. False
x3104c0s1b0n0:   kd_alpha_ce ..................................... 1
x3104c0s1b0n0:   kd_beta_ce ...................................... 1
x3104c0s1b0n0:   kd_temp ......................................... 1.0
x3104c0s1b0n0:   kv_channels ..................................... 128
x3104c0s1b0n0:   layernorm_epsilon ............................... 1e-05
x3104c0s1b0n0:   lazy_mpu_init ................................... None
x3104c0s1b0n0:   load ............................................ None
x3104c0s1b0n0:   load_teacher .................................... None
x3104c0s1b0n0:   local_rank ...................................... 0
x3104c0s1b0n0:   log_batch_size_to_tensorboard ................... False
x3104c0s1b0n0:   log_interval .................................... 1
x3104c0s1b0n0:   log_learning_rate_to_tensorboard ................ True
x3104c0s1b0n0:   log_loss_scale_to_tensorboard ................... True
x3104c0s1b0n0:   log_memory_to_tensorboard ....................... False
x3104c0s1b0n0:   log_num_zeros_in_grad ........................... False
x3104c0s1b0n0:   log_optimizer_states_to_tensorboard ............. False
x3104c0s1b0n0:   log_params_norm ................................. False
x3104c0s1b0n0:   log_timers_to_tensorboard ....................... False
x3104c0s1b0n0:   log_validation_ppl_to_tensorboard ............... False
x3104c0s1b0n0:   log_world_size_to_tensorboard ................... False
x3104c0s1b0n0:   loss_scale ...................................... None
x3104c0s1b0n0:   loss_scale_window ............................... 1000
x3104c0s1b0n0:   lr .............................................. 0.0003
x3104c0s1b0n0:   lr_decay_iters .................................. None
x3104c0s1b0n0:   lr_decay_samples ................................ None
x3104c0s1b0n0:   lr_decay_style .................................. cosine
x3104c0s1b0n0:   lr_decay_tokens ................................. None
x3104c0s1b0n0:   lr_warmup_fraction .............................. None
x3104c0s1b0n0:   lr_warmup_iters ................................. 1
x3104c0s1b0n0:   lr_warmup_samples ............................... 0
x3104c0s1b0n0:   lr_warmup_tokens ................................ None
x3104c0s1b0n0:   make_vocab_size_divisible_by .................... 128
x3104c0s1b0n0:   mask_factor ..................................... 1.0
x3104c0s1b0n0:   mask_prob ....................................... 0.15
x3104c0s1b0n0:   mask_type ....................................... random
x3104c0s1b0n0:   masked_softmax_fusion ........................... True
x3104c0s1b0n0:   max_position_embeddings ......................... 2048
x3104c0s1b0n0:   max_tokens_to_oom ............................... 12000
x3104c0s1b0n0:   memory_centric_tiled_linear ..................... False
x3104c0s1b0n0:   merge_file ...................................... /home/am6429/dl-io/datasets/gpt2-merges.txt
x3104c0s1b0n0:   micro_batch_size ................................ 4
x3104c0s1b0n0:   min_loss_scale .................................. 1.0
x3104c0s1b0n0:   min_lr .......................................... 3e-05
x3104c0s1b0n0:   mlp_type ........................................ standard
x3104c0s1b0n0:   mmap_warmup ..................................... False
x3104c0s1b0n0:   moe_eval_capacity_factor ........................ 1.0
x3104c0s1b0n0:   moe_expert_parallel_size ........................ 1
x3104c0s1b0n0:   moe_loss_coeff .................................. 0.1
x3104c0s1b0n0:   moe_min_capacity ................................ 4
x3104c0s1b0n0:   moe_token_dropping .............................. True
x3104c0s1b0n0:   moe_train_capacity_factor ....................... 1.0
x3104c0s1b0n0:   mos ............................................. False
x3104c0s1b0n0:   no_load_lr_state ................................ False
x3104c0s1b0n0:   no_load_optim ................................... None
x3104c0s1b0n0:   no_load_rng ..................................... None
x3104c0s1b0n0:   no_persist_layer_norm ........................... False
x3104c0s1b0n0:   no_pipeline_parallel ............................ True
x3104c0s1b0n0:   no_save_optim ................................... None
x3104c0s1b0n0:   no_save_rng ..................................... None
x3104c0s1b0n0:   normalization ................................... rmsnorm
x3104c0s1b0n0:   num_attention_heads ............................. 52
x3104c0s1b0n0:   num_attention_heads_teacher ..................... None
x3104c0s1b0n0:   num_channels .................................... 3
x3104c0s1b0n0:   num_classes ..................................... 1000
x3104c0s1b0n0:   num_experts ..................................... [1]
x3104c0s1b0n0:   num_experts_switch .............................. None
x3104c0s1b0n0:   num_experts_teacher ............................. [1]
x3104c0s1b0n0:   num_key_value_heads ............................. 4
x3104c0s1b0n0:   num_layers ...................................... 60
x3104c0s1b0n0:   num_layers_per_virtual_pipeline_stage ........... None
x3104c0s1b0n0:   num_layers_teacher .............................. None
x3104c0s1b0n0:   num_workers ..................................... 2
x3104c0s1b0n0:   onnx_safe ....................................... None
x3104c0s1b0n0:   openai_gelu ..................................... False
x3104c0s1b0n0:   optimizer ....................................... adam
x3104c0s1b0n0:   output_bert_embeddings .......................... False
x3104c0s1b0n0:   overlap_p2p_comm ................................ False
x3104c0s1b0n0:   override_opt_param_scheduler .................... False
x3104c0s1b0n0:   params_dtype .................................... torch.bfloat16
x3104c0s1b0n0:   partition_activations ........................... False
x3104c0s1b0n0:   patch_dim ....................................... 16
x3104c0s1b0n0:   perform_initialization .......................... True
x3104c0s1b0n0:   pipeline_model_parallel_size .................... 1
x3104c0s1b0n0:   pipeline_model_parallel_split_rank .............. None
x3104c0s1b0n0:   profile_backward ................................ False
x3104c0s1b0n0:   query_in_block_prob ............................. 0.1
x3104c0s1b0n0:   rampup_batch_size ............................... None
x3104c0s1b0n0:   random_ltd ...................................... False
x3104c0s1b0n0:   rank ............................................ 0
x3104c0s1b0n0:   recompute_granularity ........................... None
x3104c0s1b0n0:   recompute_method ................................ None
x3104c0s1b0n0:   recompute_num_layers ............................ 1
x3104c0s1b0n0:   remote_device ................................... none
x3104c0s1b0n0:   reset_attention_mask ............................ False
x3104c0s1b0n0:   reset_iteration ................................. False
x3104c0s1b0n0:   reset_position_ids .............................. False
x3104c0s1b0n0:   retriever_report_topk_accuracies ................ []
x3104c0s1b0n0:   retriever_score_scaling ......................... False
x3104c0s1b0n0:   retriever_seq_length ............................ 256
x3104c0s1b0n0:   retro_add_retriever ............................. False
x3104c0s1b0n0:   retro_cyclic_train_iters ........................ None
x3104c0s1b0n0:   retro_encoder_attention_dropout ................. 0.1
x3104c0s1b0n0:   retro_encoder_hidden_dropout .................... 0.1
x3104c0s1b0n0:   retro_encoder_layers ............................ 2
x3104c0s1b0n0:   retro_num_neighbors ............................. 2
x3104c0s1b0n0:   retro_num_retrieved_chunks ...................... 2
x3104c0s1b0n0:   retro_return_doc_ids ............................ False
x3104c0s1b0n0:   retro_workdir ................................... None
x3104c0s1b0n0:   return_data_index ............................... False
x3104c0s1b0n0:   rotary_percent .................................. 1.0
x3104c0s1b0n0:   sample_rate ..................................... 1.0
x3104c0s1b0n0:   save ............................................ /local/scratch/llama2/zero3-tp1}_dp8
x3104c0s1b0n0:   save_interval ................................... 1000
x3104c0s1b0n0:   scatter_gather_tensors_in_pipeline .............. True
x3104c0s1b0n0:   scattered_embeddings ............................ False
x3104c0s1b0n0:   seed ............................................ 1234
x3104c0s1b0n0:   seq_length ...................................... 2048
x3104c0s1b0n0:   sequence_parallel ............................... False
x3104c0s1b0n0:   sgd_momentum .................................... 0.9
x3104c0s1b0n0:   short_seq_prob .................................. 0.1
x3104c0s1b0n0:   skip_train ...................................... False
x3104c0s1b0n0:   split ........................................... 949,50,1
x3104c0s1b0n0:   split_transformers .............................. False
x3104c0s1b0n0:   squared_relu .................................... False
x3104c0s1b0n0:   standalone_embedding_stage ...................... False
x3104c0s1b0n0:   start_weight_decay .............................. 0.1
x3104c0s1b0n0:   swiglu .......................................... True
x3104c0s1b0n0:   swin_backbone_type .............................. tiny
x3104c0s1b0n0:   synchronize_each_layer .......................... False
x3104c0s1b0n0:   tensor_model_parallel_size ...................... 1
x3104c0s1b0n0:   tensorboard_dir ................................. None
x3104c0s1b0n0:   tensorboard_log_interval ........................ 1
x3104c0s1b0n0:   tensorboard_queue_size .......................... 1000
x3104c0s1b0n0:   test_data_path .................................. None
x3104c0s1b0n0:   tile_factor ..................................... 1
x3104c0s1b0n0:   timing_log_level ................................ 0
x3104c0s1b0n0:   timing_log_option ............................... minmax
x3104c0s1b0n0:   titles_data_path ................................ None
x3104c0s1b0n0:   tokenizer_model ................................. /home/am6429/dl-io/datasets/tokenizer.model
x3104c0s1b0n0:   tokenizer_type .................................. GPT2BPETokenizer
x3104c0s1b0n0:   topk ............................................ 1
x3104c0s1b0n0:   train_data_exact_num_epochs ..................... None
x3104c0s1b0n0:   train_data_path ................................. None
x3104c0s1b0n0:   train_desc_path ................................. None
x3104c0s1b0n0:   train_doc_idx_path .............................. None
x3104c0s1b0n0:   train_idx_path .................................. None
x3104c0s1b0n0:   train_iters ..................................... 10
x3104c0s1b0n0:   train_sample_idx_path ........................... None
x3104c0s1b0n0:   train_samples ................................... None
x3104c0s1b0n0:   train_shuffle_idx_path .......................... None
x3104c0s1b0n0:   train_tokens .................................... None
x3104c0s1b0n0:   transformer_impl ................................ local
x3104c0s1b0n0:   transformer_pipeline_model_parallel_size ........ 1
x3104c0s1b0n0:   untie_embeddings_and_output_weights ............. True
x3104c0s1b0n0:   use_checkpoint_args ............................. False
x3104c0s1b0n0:   use_checkpoint_opt_param_scheduler .............. False
x3104c0s1b0n0:   use_contiguous_buffers_in_local_ddp ............. True
x3104c0s1b0n0:   use_cpu_initialization .......................... None
x3104c0s1b0n0:   use_dataset_only ................................ False
x3104c0s1b0n0:   use_distributed_optimizer ....................... False
x3104c0s1b0n0:   use_flash_attn .................................. False
x3104c0s1b0n0:   use_flash_attn_triton ........................... False
x3104c0s1b0n0:   use_flash_attn_v1 ............................... False
x3104c0s1b0n0:   use_flash_attn_v2 ............................... False
x3104c0s1b0n0:   use_one_sent_docs ............................... False
x3104c0s1b0n0:   use_pin_memory .................................. False
x3104c0s1b0n0:   use_ring_exchange_p2p ........................... False
x3104c0s1b0n0:   use_rotary_position_embeddings .................. True
x3104c0s1b0n0:   use_tutel ....................................... False
x3104c0s1b0n0:   valid_data_path ................................. None
x3104c0s1b0n0:   variable_seq_lengths ............................ False
x3104c0s1b0n0:   virtual_pipeline_model_parallel_size ............ None
x3104c0s1b0n0:   vision_backbone_type ............................ vit
x3104c0s1b0n0:   vision_pretraining .............................. False
x3104c0s1b0n0:   vision_pretraining_type ......................... classify
x3104c0s1b0n0:   vocab_extra_ids ................................. 0
x3104c0s1b0n0:   vocab_file ...................................... /home/am6429/dl-io/datasets/gpt2-vocab.json
x3104c0s1b0n0:   vocab_size ...................................... None
x3104c0s1b0n0:   weight_decay .................................... 0.1
x3104c0s1b0n0:   weight_decay_incr_style ......................... constant
x3104c0s1b0n0:   world_size ...................................... 8
x3104c0s1b0n0:   zero_allgather_bucket_size ...................... 0.0
x3104c0s1b0n0:   zero_contigious_gradients ....................... False
x3104c0s1b0n0:   zero_reduce_bucket_size ......................... 0.0
x3104c0s1b0n0:   zero_reduce_scatter ............................. False
x3104c0s1b0n0:   zero_stage ...................................... 3
x3104c0s1b0n0: -------------------- end of arguments ---------------------
x3104c0s1b0n0: setting number of micro-batches to constant 1
x3104c0s1b0n0: > building GPT2BPETokenizer tokenizer ...
x3104c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed general environment info:
x3104c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s1b0n0: torch version .................... 2.0.1+cu118
x3104c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s1b0n0: torch cuda version ............... 11.8
x3104c0s1b0n0: torch hip version ................ None
x3104c0s1b0n0: nvcc version ..................... 11.8
x3104c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s1b0n0:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
x3104c0s1b0n0: > initializing torch distributed ...
x3104c0s1b0n0: [2024-03-29 16:04:25,238] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0: [2024-03-29 16:04:25,238] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
x3104c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: [2024-03-29 16:04:25,281] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed general environment info:
x3104c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s1b0n0: torch version .................... 2.0.1+cu118
x3104c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s1b0n0: torch cuda version ............... 11.8
x3104c0s1b0n0: torch hip version ................ None
x3104c0s1b0n0: nvcc version ..................... 11.8
x3104c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s1b0n0: [2024-03-29 16:04:25,353] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0: > initialized tensor model parallel with size 1
x3104c0s1b0n0: > initialized pipeline model parallel with size 1
x3104c0s1b0n0: > setting random seeds to 1234 ...
x3104c0s1b0n0: [2024-03-29 16:04:26,607] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
x3104c0s1b0n0: > compiling dataset index builder ...
x3104c0s1b0n0: make: Entering directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3104c0s1b0n0: make: Nothing to be done for 'default'.
x3104c0s1b0n0: make: Leaving directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3104c0s1b0n0: >>> done with dataset index builder. Compilation time: 0.084 seconds
x3104c0s1b0n0: > compiling and loading fused kernels ...
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: >>> done with compiling and loading fused kernels. Compilation time: 3.196 seconds
x3104c0s25b0n0: <<<<<<<<<<< 5
x3104c0s25b0n0: <<<<<<<<<<< 6
x3104c0s25b0n0: <<<<<<<<<<< 4
x3104c0s25b0n0: <<<<<<<<<<< 7
x3104c0s1b0n0: initialize_megatron took 5.487253665924072
x3104c0s1b0n0: <<<<<<<<<<< 0
x3104c0s1b0n0: <<<<<<<<<<< 2
x3104c0s1b0n0: <<<<<<<<<<< 1
x3104c0s1b0n0: <<<<<<<<<<< 3
x3104c0s1b0n0: time to initialize megatron (seconds): 6.292
x3104c0s1b0n0: [after megatron is initialized] datetime: 2024-03-29 16:04:30 
x3104c0s1b0n0: get_accelerator and all_reduce  took 0.0031633377075195312
x3104c0s1b0n0: building GPT model ...
x3104c0s1b0n0: [2024-03-29 16:04:30,752] [INFO] [utils.py:800:see_memory_usage] Before Building Model
x3104c0s1b0n0: [2024-03-29 16:04:30,752] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 4.62 GB         CA 0.0 GB         Max_CA 5 GB 
x3104c0s1b0n0: [2024-03-29 16:04:30,753] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 17.86 GB, percent = 3.5%
x3104c0s1b0n0: [2024-03-29 16:04:37,468] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 423, num_elems = 27.64B
x3104c0s1b0n0: [2024-03-29 16:04:37,532] [INFO] [utils.py:800:see_memory_usage] After Building Model
x3104c0s1b0n0: [2024-03-29 16:04:37,532] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 7.07 GB         CA 22.24 GB         Max_CA 38 GB 
x3104c0s1b0n0: [2024-03-29 16:04:37,533] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 18.28 GB, percent = 3.6%
x3104c0s1b0n0:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 27635239424
x3104c0s25b0n0: ninja: no work to do.
x3104c0s25b0n0: Time to load cpu_adam op: 2.2865099906921387 seconds
x3104c0s25b0n0: ninja: no work to do.
x3104c0s25b0n0: Time to load cpu_adam op: 2.278465986251831 seconds
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: Time to load cpu_adam op: 2.527905225753784 seconds
x3104c0s1b0n0: Time to load cpu_adam op: 2.4374561309814453 seconds
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: Time to load cpu_adam op: 2.464930772781372 seconds
x3104c0s25b0n0: Time to load cpu_adam op: 2.3537566661834717 seconds
x3104c0s25b0n0: Time to load cpu_adam op: 2.315446138381958 seconds
x3104c0s1b0n0: Time to load cpu_adam op: 2.5398359298706055 seconds
x3104c0s25b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s25b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s25b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s25b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: > learning rate decay style: cosine
x3104c0s1b0n0: DeepSpeed is enabled.
x3104c0s1b0n0: [2024-03-29 16:04:42,050] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.3+8074cd62, git-hash=8074cd62, git-branch=hybrid_opt_offload
x3104c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: [2024-03-29 16:04:42,120] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After args sanity test
x3104c0s1b0n0: [2024-03-29 16:04:42,121] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.24 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 16:04:42,121] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.47 GB, percent = 5.1%
x3104c0s25b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s25b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s25b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s25b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: [2024-03-29 16:04:42,175] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure distributed model
x3104c0s1b0n0: [2024-03-29 16:04:42,176] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.24 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 16:04:42,176] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.64 GB, percent = 5.1%
x3104c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: [2024-03-29 16:04:42,237] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After configure distributed model
x3104c0s1b0n0: [2024-03-29 16:04:42,237] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.24 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 16:04:42,237] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.71 GB, percent = 5.1%
x3104c0s1b0n0: [2024-03-29 16:04:42,237] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
x3104c0s1b0n0: [2024-03-29 16:04:42,290] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After setting model parameters
x3104c0s1b0n0: [2024-03-29 16:04:42,290] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.24 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 16:04:42,291] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.71 GB, percent = 5.1%
x3104c0s1b0n0: [2024-03-29 16:04:42,344] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure optimizer
x3104c0s1b0n0: [2024-03-29 16:04:42,345] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.24 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 16:04:42,345] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.71 GB, percent = 5.1%
x3104c0s1b0n0: [2024-03-29 16:04:42,345] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
x3104c0s1b0n0: [2024-03-29 16:04:42,345] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
x3104c0s1b0n0: [2024-03-29 16:04:42,364] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
x3104c0s1b0n0: [2024-03-29 16:04:42,364] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
x3104c0s1b0n0: [2024-03-29 16:04:42,364] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
x3104c0s1b0n0: [2024-03-29 16:04:42,364] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
x3104c0s1b0n0: [2024-03-29 16:04:42,415] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
x3104c0s1b0n0: [2024-03-29 16:04:42,416] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.24 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 16:04:42,416] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.71 GB, percent = 5.1%
x3104c0s1b0n0: [2024-03-29 16:04:42,418] [INFO] [stage3.py:137:__init__] Reduce bucket size 500,000,000
x3104c0s1b0n0: [2024-03-29 16:04:42,418] [INFO] [stage3.py:138:__init__] Prefetch bucket size 50,000,000
x3104c0s1b0n0: [2024-03-29 16:04:42,471] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
x3104c0s1b0n0: [2024-03-29 16:04:42,471] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.24 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 16:04:42,471] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.71 GB, percent = 5.1%
x3104c0s1b0n0: Parameter Offload: Total persistent parameters: 805376 in 121 params
x3104c0s1b0n0: [2024-03-29 16:04:42,548] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
x3104c0s1b0n0: [2024-03-29 16:04:42,549] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.24 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 16:04:42,549] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.71 GB, percent = 5.1%
x3104c0s1b0n0: [2024-03-29 16:04:42,604] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
x3104c0s1b0n0: [2024-03-29 16:04:42,605] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.24 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 16:04:42,605] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.71 GB, percent = 5.1%
x3104c0s1b0n0: [2024-03-29 16:04:42,606] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s1b0n0: [2024-03-29 16:04:42,606] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s1b0n0: [2024-03-29 16:04:42,606] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s1b0n0: [2024-03-29 16:04:42,606] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s25b0n0: [2024-03-29 16:04:42,606] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s25b0n0: [2024-03-29 16:04:42,606] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s25b0n0: [2024-03-29 16:04:42,606] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s25b0n0: [2024-03-29 16:04:42,606] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s1b0n0: [2024-03-29 16:04:42,606] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s1b0n0: [2024-03-29 16:04:42,606] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s1b0n0: [2024-03-29 16:04:42,606] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s1b0n0: [2024-03-29 16:04:42,606] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s25b0n0: [2024-03-29 16:04:42,606] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s25b0n0: [2024-03-29 16:04:42,606] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s25b0n0: [2024-03-29 16:04:42,606] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s25b0n0: [2024-03-29 16:04:42,606] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s1b0n0: [2024-03-29 16:04:42,606] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,606] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,606] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,606] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,607] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,608] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,609] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,610] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s25b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s25b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s25b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s1b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s1b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s1b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s1b0n0: [2024-03-29 16:04:42,611] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s1b0n0: [2024-03-29 16:04:44,908] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 31
x3104c0s1b0n0: [2024-03-29 16:04:44,909] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.44 GB         CA 6.44 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 16:04:44,909] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 45.05 GB, percent = 9.0%
x3104c0s1b0n0: [2024-03-29 16:04:45,001] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
x3104c0s1b0n0: [2024-03-29 16:04:45,002] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3104c0s1b0n0: [2024-03-29 16:04:45,002] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 45.05 GB, percent = 9.0%
x3104c0s1b0n0: [2024-03-29 16:05:04,210] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
x3104c0s1b0n0: [2024-03-29 16:05:04,210] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3104c0s1b0n0: [2024-03-29 16:05:04,210] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 209.69 GB, percent = 41.7%
x3104c0s1b0n0: [2024-03-29 16:05:05,369] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
x3104c0s1b0n0: [2024-03-29 16:05:05,370] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3104c0s1b0n0: [2024-03-29 16:05:05,370] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 219.16 GB, percent = 43.6%
x3104c0s1b0n0: [2024-03-29 16:05:11,911] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | init_optimizer_state: 6528.82
x3104c0s1b0n0: [2024-03-29 16:05:11,981] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
x3104c0s1b0n0: [2024-03-29 16:05:11,982] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3104c0s1b0n0: [2024-03-29 16:05:11,982] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 279.16 GB, percent = 55.5%
x3104c0s1b0n0: [2024-03-29 16:05:11,982] [INFO] [stage3.py:520:_setup_for_real_optimizer] optimizer state initialized
x3104c0s1b0n0: [2024-03-29 16:05:19,862] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
x3104c0s1b0n0: [2024-03-29 16:05:19,862] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 8.61 GB         CA 22.75 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 16:05:19,862] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.85 GB, percent = 62.0%
x3104c0s1b0n0: [2024-03-29 16:05:19,863] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
x3104c0s1b0n0: [2024-03-29 16:05:19,931] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure LR scheduler
x3104c0s1b0n0: [2024-03-29 16:05:19,931] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 16:05:19,932] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.85 GB, percent = 62.0%
x3104c0s1b0n0: [2024-03-29 16:05:19,932] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
x3104c0s1b0n0: [2024-03-29 16:05:19,932] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7fd4bd510250>
x3104c0s1b0n0: [2024-03-29 16:05:19,932] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 16:05:19,997] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before rewriting optimizer step
x3104c0s1b0n0: [2024-03-29 16:05:19,998] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 16:05:19,998] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.85 GB, percent = 62.0%
x3104c0s1b0n0: [2024-03-29 16:05:20,064] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure checkpointing
x3104c0s1b0n0: [2024-03-29 16:05:20,064] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 16:05:20,064] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.86 GB, percent = 62.0%
x3104c0s1b0n0: [2024-03-29 16:05:20,065] [INFO] [config.py:998:print] DeepSpeedEngine configuration:
x3104c0s1b0n0: [2024-03-29 16:05:20,065] [INFO] [config.py:1002:print]   activation_checkpointing_config  {
x3104c0s1b0n0:     "partition_activations": false, 
x3104c0s1b0n0:     "contiguous_memory_optimization": false, 
x3104c0s1b0n0:     "cpu_checkpointing": false, 
x3104c0s1b0n0:     "number_checkpoints": null, 
x3104c0s1b0n0:     "synchronize_checkpoint_boundary": false, 
x3104c0s1b0n0:     "profile": false
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 16:05:20,065] [INFO] [config.py:1002:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
x3104c0s1b0n0: [2024-03-29 16:05:20,065] [INFO] [config.py:1002:print]   amp_enabled .................. False
x3104c0s1b0n0: [2024-03-29 16:05:20,065] [INFO] [config.py:1002:print]   amp_params ................... False
x3104c0s1b0n0: [2024-03-29 16:05:20,065] [INFO] [config.py:1002:print]   autotuning_config ............ {
x3104c0s1b0n0:     "enabled": false, 
x3104c0s1b0n0:     "start_step": null, 
x3104c0s1b0n0:     "end_step": null, 
x3104c0s1b0n0:     "metric_path": null, 
x3104c0s1b0n0:     "arg_mappings": null, 
x3104c0s1b0n0:     "metric": "throughput", 
x3104c0s1b0n0:     "model_info": null, 
x3104c0s1b0n0:     "results_dir": "autotuning_results", 
x3104c0s1b0n0:     "exps_dir": "autotuning_exps", 
x3104c0s1b0n0:     "overwrite": true, 
x3104c0s1b0n0:     "fast": true, 
x3104c0s1b0n0:     "start_profile_step": 3, 
x3104c0s1b0n0:     "end_profile_step": 5, 
x3104c0s1b0n0:     "tuner_type": "gridsearch", 
x3104c0s1b0n0:     "tuner_early_stopping": 5, 
x3104c0s1b0n0:     "tuner_num_trials": 50, 
x3104c0s1b0n0:     "model_info_path": null, 
x3104c0s1b0n0:     "mp_size": 1, 
x3104c0s1b0n0:     "max_train_batch_size": null, 
x3104c0s1b0n0:     "min_train_batch_size": 1, 
x3104c0s1b0n0:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
x3104c0s1b0n0:     "min_train_micro_batch_size_per_gpu": 1, 
x3104c0s1b0n0:     "num_tuning_micro_batch_sizes": 3
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 16:05:20,065] [INFO] [config.py:1002:print]   bfloat16_enabled ............. True
x3104c0s1b0n0: [2024-03-29 16:05:20,065] [INFO] [config.py:1002:print]   bfloat16_immediate_grad_update  False
x3104c0s1b0n0: [2024-03-29 16:05:20,065] [INFO] [config.py:1002:print]   checkpoint_parallel_write_pipeline  False
x3104c0s1b0n0: [2024-03-29 16:05:20,065] [INFO] [config.py:1002:print]   checkpoint_tag_validation_enabled  True
x3104c0s1b0n0: [2024-03-29 16:05:20,065] [INFO] [config.py:1002:print]   checkpoint_tag_validation_fail  False
x3104c0s1b0n0: [2024-03-29 16:05:20,065] [INFO] [config.py:1002:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd4bd510bb0>
x3104c0s1b0n0: [2024-03-29 16:05:20,065] [INFO] [config.py:1002:print]   communication_data_type ...... None
x3104c0s1b0n0: [2024-03-29 16:05:20,065] [INFO] [config.py:1002:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
x3104c0s1b0n0: [2024-03-29 16:05:20,065] [INFO] [config.py:1002:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
x3104c0s1b0n0: [2024-03-29 16:05:20,065] [INFO] [config.py:1002:print]   curriculum_enabled_legacy .... False
x3104c0s1b0n0: [2024-03-29 16:05:20,065] [INFO] [config.py:1002:print]   curriculum_params_legacy ..... False
x3104c0s1b0n0: [2024-03-29 16:05:20,065] [INFO] [config.py:1002:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
x3104c0s1b0n0: [2024-03-29 16:05:20,065] [INFO] [config.py:1002:print]   data_efficiency_enabled ...... False
x3104c0s1b0n0: [2024-03-29 16:05:20,065] [INFO] [config.py:1002:print]   dataloader_drop_last ......... False
x3104c0s1b0n0: [2024-03-29 16:05:20,065] [INFO] [config.py:1002:print]   datastates_config ............ {
x3104c0s1b0n0:     "enabled": null, 
x3104c0s1b0n0:     "config": {
x3104c0s1b0n0:     }
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 16:05:20,065] [INFO] [config.py:1002:print]   disable_allgather ............ False
x3104c0s1b0n0: [2024-03-29 16:05:20,065] [INFO] [config.py:1002:print]   dump_state ................... False
x3104c0s1b0n0: [2024-03-29 16:05:20,065] [INFO] [config.py:1002:print]   dynamic_loss_scale_args ...... None
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   eigenvalue_enabled ........... False
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   eigenvalue_gas_boundary_resolution  1
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   eigenvalue_layer_name ........ bert.encoder.layer
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   eigenvalue_layer_num ......... 0
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   eigenvalue_max_iter .......... 100
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   eigenvalue_stability ......... 1e-06
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   eigenvalue_tol ............... 0.01
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   eigenvalue_verbose ........... False
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   elasticity_enabled ........... False
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   flops_profiler_config ........ {
x3104c0s1b0n0:     "enabled": false, 
x3104c0s1b0n0:     "recompute_fwd_factor": 0.0, 
x3104c0s1b0n0:     "profile_step": 1, 
x3104c0s1b0n0:     "module_depth": -1, 
x3104c0s1b0n0:     "top_modules": 1, 
x3104c0s1b0n0:     "detailed": true, 
x3104c0s1b0n0:     "output_file": null
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   fp16_auto_cast ............... None
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   fp16_enabled ................. False
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   fp16_master_weights_and_gradients  False
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   global_rank .................. 0
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   grad_accum_dtype ............. bf16
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   gradient_accumulation_steps .. 1
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   gradient_clipping ............ 0.0
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   gradient_predivide_factor .... 1.0
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   graph_harvesting ............. False
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   initial_dynamic_scale ........ 1
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   load_universal_checkpoint .... False
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   loss_scale ................... 1.0
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   memory_breakdown ............. True
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   mics_hierarchial_params_gather  False
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   mics_shard_size .............. -1
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   nebula_config ................ {
x3104c0s1b0n0:     "enabled": false, 
x3104c0s1b0n0:     "persistent_storage_path": null, 
x3104c0s1b0n0:     "persistent_time_interval": 100, 
x3104c0s1b0n0:     "num_of_version_in_retention": 2, 
x3104c0s1b0n0:     "enable_nebula_load": true, 
x3104c0s1b0n0:     "load_path": null
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   optimizer_legacy_fusion ...... False
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   optimizer_name ............... None
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   optimizer_params ............. None
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   pld_enabled .................. False
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   pld_params ................... False
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   prescale_gradients ........... False
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   scheduler_name ............... None
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   scheduler_params ............. None
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   seq_parallel_communication_data_type  torch.float32
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   sparse_attention ............. None
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   sparse_gradients_enabled ..... False
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   steps_per_print .............. 1
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   train_batch_size ............. 32
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   train_micro_batch_size_per_gpu  4
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   use_data_before_expert_parallel_  False
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   use_node_local_storage ....... False
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   wall_clock_breakdown ......... True
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   weight_quantization_config ... None
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   world_size ................... 8
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   zero_allow_untested_optimizer  False
x3104c0s1b0n0: [2024-03-29 16:05:20,066] [INFO] [config.py:1002:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0, prefetch_optimizer=True, part_grads_async=True, prefetch_optimizer_gap=8) sub_group_size=100000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
x3104c0s1b0n0: [2024-03-29 16:05:20,067] [INFO] [config.py:1002:print]   zero_enabled ................. True
x3104c0s1b0n0: [2024-03-29 16:05:20,067] [INFO] [config.py:1002:print]   zero_force_ds_cpu_optimizer .. True
x3104c0s1b0n0: [2024-03-29 16:05:20,067] [INFO] [config.py:1002:print]   zero_optimization_stage ...... 3
x3104c0s1b0n0: [2024-03-29 16:05:20,067] [INFO] [config.py:988:print_user_config]   json = {
x3104c0s1b0n0:     "train_batch_size": 32, 
x3104c0s1b0n0:     "train_micro_batch_size_per_gpu": 4, 
x3104c0s1b0n0:     "steps_per_print": 1, 
x3104c0s1b0n0:     "zero_optimization": {
x3104c0s1b0n0:         "stage": 3, 
x3104c0s1b0n0:         "offload_optimizer": {
x3104c0s1b0n0:             "device": "cpu", 
x3104c0s1b0n0:             "ratio": 1, 
x3104c0s1b0n0:             "pin_memory": true, 
x3104c0s1b0n0:             "prefetch_optimizer": 1, 
x3104c0s1b0n0:             "part_grads_async": 1, 
x3104c0s1b0n0:             "prefetch_optimizer_gap": 8
x3104c0s1b0n0:         }, 
x3104c0s1b0n0:         "sub_group_size": 1.000000e+08
x3104c0s1b0n0:     }, 
x3104c0s1b0n0:     "bf16": {
x3104c0s1b0n0:         "enabled": true
x3104c0s1b0n0:     }, 
x3104c0s1b0n0:     "data_types": {
x3104c0s1b0n0:         "grad_accum_dtype": "bf16"
x3104c0s1b0n0:     }, 
x3104c0s1b0n0:     "wall_clock_breakdown": true, 
x3104c0s1b0n0:     "memory_breakdown": true, 
x3104c0s1b0n0:     "flops_profiler": {
x3104c0s1b0n0:         "enabled": false
x3104c0s1b0n0:     }
x3104c0s1b0n0: }
x3104c0s1b0n0: <TIMER:model-and-optimizer-setup,50.46414279937744>
x3104c0s25b0n0: <TIMER:model-and-optimizer-setup,50.4646213054657>
x3104c0s25b0n0: <TIMER:model-and-optimizer-setup,50.46535086631775>
x3104c0s1b0n0: <TIMER:model-and-optimizer-setup,50.465551137924194>
x3104c0s1b0n0: <TIMER:model-and-optimizer-setup,50.46562337875366>
x3104c0s25b0n0: <TIMER:model-and-optimizer-setup,50.46607303619385>
x3104c0s1b0n0: <TIMER:model-and-optimizer-setup,50.466158628463745>
x3104c0s25b0n0: <TIMER:model-and-optimizer-setup,50.46641945838928>
x3104c0s1b0n0: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-29 16:05:21 
x3104c0s1b0n0: > building train, validation, and test datasets ...
x3104c0s1b0n0:  > datasets target sizes (minimum size):
x3104c0s1b0n0:     train:      320
x3104c0s1b0n0:     validation: 0
x3104c0s1b0n0:     test:       0
x3104c0s1b0n0: > building train, validation, and test datasets for GPT ...
x3104c0s1b0n0: Single data path provided for train, valid & test
x3104c0s1b0n0:  > building dataset index ...
x3104c0s1b0n0:     reading sizes...
x3104c0s1b0n0:     reading pointers...
x3104c0s1b0n0:     reading document index...
x3104c0s1b0n0:     creating numpy buffer of mmap...
x3104c0s1b0n0:     creating memory view of numpy buffer...
x3104c0s1b0n0:  > finished creating indexed dataset in 0.002146 seconds
x3104c0s1b0n0:     number of documents: 79000
x3104c0s1b0n0:  > dataset split:
x3104c0s1b0n0:     train:
x3104c0s1b0n0:      document indices in [0, 74971) total of 74971 documents
x3104c0s1b0n0:     validation:
x3104c0s1b0n0:      document indices in [74971, 78921) total of 3950 documents
x3104c0s1b0n0:     test:
x3104c0s1b0n0:      document indices in [78921, 79000) total of 79 documents
x3104c0s1b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_doc_idx.npy
x3104c0s1b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_sample_idx.npy
x3104c0s1b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_shuffle_idx.npy
x3104c0s1b0n0:     loaded indexed file in 0.002 seconds
x3104c0s1b0n0:     total number of samples: 108448
x3104c0s1b0n0:     total number of epochs: 1
x3104c0s1b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_doc_idx.npy
x3104c0s1b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_sample_idx.npy
x3104c0s1b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_shuffle_idx.npy
x3104c0s1b0n0:     loaded indexed file in 0.003 seconds
x3104c0s1b0n0:     total number of samples: 5792
x3104c0s1b0n0:     total number of epochs: 1
x3104c0s1b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_doc_idx.npy
x3104c0s1b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_sample_idx.npy
x3104c0s1b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_shuffle_idx.npy
x3104c0s1b0n0:     loaded indexed file in 0.004 seconds
x3104c0s1b0n0:     total number of samples: 185
x3104c0s1b0n0:     total number of epochs: 1
x3104c0s1b0n0: > finished creating GPT datasets ...
x3104c0s25b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5039939880371094>
x3104c0s25b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5090742111206055>
x3104c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5239245891571045>
x3104c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5309677124023438>
x3104c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5372262001037598>
x3104c0s25b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5535180568695068>
x3104c0s25b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5545039176940918>
x3104c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.6473102569580078>
x3104c0s1b0n0: [after dataloaders are built] datetime: 2024-03-29 16:05:21 
x3104c0s1b0n0: done with setup ...
x3104c0s25b0n0: (min, max) time across ranks (ms):
x3104c0s25b0n0:     model-and-optimizer-setup ......................: (50464.14, 50466.42)
x3104c0s25b0n0:     train/valid/test-data-iterators-setup ..........: (503.99, 647.31)
x3104c0s1b0n0: training ...
x3104c0s1b0n0: [before training begins] datetime: 2024-03-29 16:05:21 
x3104c0s1b0n0: [before the start of training step] datetime: 2024-03-29 16:05:21 
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 16:05:22,003] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 16:05:22,003] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 7.45 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 16:05:22,004] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.2 GB, percent = 62.4%
x3104c0s1b0n0: [2024-03-29 16:05:22,126] [INFO] [checkpointing.py:539:forward] Activation Checkpointing Information
x3104c0s1b0n0: [2024-03-29 16:05:22,126] [INFO] [checkpointing.py:540:forward] ----Partition Activations False, CPU CHECKPOINTING False
x3104c0s1b0n0: [2024-03-29 16:05:22,126] [INFO] [checkpointing.py:541:forward] ----contiguous Memory Checkpointing False with 60 total layers
x3104c0s1b0n0: [2024-03-29 16:05:22,126] [INFO] [checkpointing.py:543:forward] ----Synchronization False
x3104c0s1b0n0: [2024-03-29 16:05:22,126] [INFO] [checkpointing.py:544:forward] ----Profiling time in checkpointing False
x3104c0s1b0n0: [2024-03-29 16:05:29,997] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 16:05:29,997] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 19.76 GB         CA 22.95 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 16:05:29,998] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.37 GB, percent = 62.5%
x3104c0s1b0n0: [2024-03-29 16:05:30,165] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 16:05:30,165] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 16.84 GB         CA 16.91 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 16:05:30,166] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.37 GB, percent = 62.5%
x3104c0s1b0n0: [2024-03-29 16:05:53,942] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 16:05:53,943] [INFO] [utils.py:801:see_memory_usage] MA 10.88 GB         Max_MA 20.43 GB         CA 11.08 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:05:53,943] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.37 GB, percent = 62.5%
x3104c0s1b0n0: [2024-03-29 16:05:54,021] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 16:05:54,022] [INFO] [utils.py:801:see_memory_usage] MA 10.88 GB         Max_MA 10.88 GB         CA 11.08 GB         Max_CA 11 GB 
x3104c0s1b0n0: [2024-03-29 16:05:54,022] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.41 GB, percent = 62.5%
x3104c0s25b0n0: [2024-03-29 16:06:01,045] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.95135498046875
x3104c0s25b0n0: [2024-03-29 16:06:01,066] [INFO] [stage3.py:2251:step] Full outer step loop took 6.972382545471191
x3104c0s1b0n0: [2024-03-29 16:06:01,156] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.061727285385132
x3104c0s1b0n0: [2024-03-29 16:06:01,171] [INFO] [stage3.py:2251:step] Full outer step loop took 7.077659606933594
x3104c0s25b0n0: [2024-03-29 16:06:01,190] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.096794605255127
x3104c0s25b0n0: [2024-03-29 16:06:01,215] [INFO] [stage3.py:2251:step] Full outer step loop took 7.121765851974487
x3104c0s1b0n0: [2024-03-29 16:06:01,310] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.2160327434539795
x3104c0s1b0n0: [2024-03-29 16:06:01,348] [INFO] [stage3.py:2251:step] Full outer step loop took 7.254287004470825
x3104c0s1b0n0: [2024-03-29 16:06:01,352] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.258469343185425
x3104c0s1b0n0: [2024-03-29 16:06:01,376] [INFO] [stage3.py:2251:step] Full outer step loop took 7.282643795013428
x3104c0s1b0n0: [2024-03-29 16:06:01,391] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.297094821929932
x3104c0s1b0n0: [2024-03-29 16:06:01,400] [INFO] [stage3.py:2251:step] Full outer step loop took 7.306295394897461
x3104c0s25b0n0: [2024-03-29 16:06:01,474] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.379905462265015
x3104c0s25b0n0: [2024-03-29 16:06:01,490] [INFO] [stage3.py:2251:step] Full outer step loop took 7.39644718170166
x3104c0s25b0n0: [2024-03-29 16:06:01,533] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.439507484436035
x3104c0s25b0n0: [2024-03-29 16:06:01,542] [INFO] [stage3.py:2251:step] Full outer step loop took 7.448678255081177
x3104c0s25b0n0: [2024-03-29 16:06:01,551] [INFO] [stage3.py:2277:step] End to end step took 7.457842588424683
x3104c0s1b0n0: [2024-03-29 16:06:01,551] [INFO] [stage3.py:2277:step] End to end step took 7.457662343978882
x3104c0s1b0n0: [2024-03-29 16:06:01,551] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 7283.21
x3104c0s1b0n0: [2024-03-29 16:06:01,552] [WARNING] [stage3.py:2267:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3104c0s1b0n0: [2024-03-29 16:06:01,552] [INFO] [stage3.py:2277:step] End to end step took 7.458093643188477
x3104c0s25b0n0: [2024-03-29 16:06:01,552] [INFO] [stage3.py:2277:step] End to end step took 7.45841383934021
x3104c0s25b0n0: [2024-03-29 16:06:01,552] [INFO] [stage3.py:2277:step] End to end step took 7.458277940750122
x3104c0s25b0n0: [2024-03-29 16:06:01,552] [INFO] [stage3.py:2277:step] End to end step took 7.458435535430908
x3104c0s1b0n0: [2024-03-29 16:06:01,552] [INFO] [stage3.py:2277:step] End to end step took 7.458453416824341
x3104c0s1b0n0: [2024-03-29 16:06:01,552] [INFO] [stage3.py:2277:step] End to end step took 7.458409547805786
x3104c0s1b0n0: [2024-03-29 16:06:01,552] [INFO] [logging.py:96:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 16:06:01,552] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 8061.37 | bwd_microstep: 23622.80 | bwd_inner_microstep: 23530.14 | bwd_allreduce_microstep: 92.55 | step_microstep: 7530.37
x3104c0s1b0n0: [2024-03-29 16:06:01,552] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 8061.37 | bwd: 23622.79 | bwd_inner: 23530.14 | bwd_allreduce: 92.56 | step: 7530.37
x3104c0s1b0n0: [2024-03-29 16:06:01,661] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 16:06:01,662] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 11.94 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 16:06:01,662] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.58 GB, percent = 81.0%
x3104c0s1b0n0: <TIMER:interval-time,39.849491119384766><TIMER:interval-time,39.84949970245361>
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,39.849504470825195>
x3104c0s1b0n0: <TIMER:interval-time,39.849546670913696>
x3104c0s25b0n0: <TIMER:interval-time,39.84953427314758>
x3104c0s25b0n0: <TIMER:interval-time,39.84953999519348><TIMER:interval-time,39.84953856468201>
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,39.84954595565796>
x3104c0s25b0n0:  elapsed_time 39.849546 | consumed samples:           32 | consumed tokens:        65536 | elapsed time per iteration (ms): 39849.5 | learning rate: 3.000E-04 | global batch size:    32 | lm loss: 1.215015E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.803 | TFLOPs: 55.56 |
x3104c0s1b0n0: [Rank 0] (after 1 iterations) memory (MB) | allocated: 10510.36328125 | max allocated: 10510.36376953125 | reserved: 10586.0 | max reserved: 10586.0
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 16:06:01,803] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 16:06:01,804] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 16:06:01,804] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.66 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:06:08,310] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 16:06:08,310] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:06:08,310] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.67 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:06:08,398] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 16:06:08,398] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:06:08,398] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.67 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:06:25,802] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 16:06:25,803] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 23.77 GB         CA 12.34 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 16:06:25,803] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.68 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:06:25,878] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 16:06:25,878] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 12.14 GB         CA 12.34 GB         Max_CA 12 GB 
x3104c0s1b0n0: [2024-03-29 16:06:25,878] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.68 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:06:30,870] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.968118190765381
x3104c0s1b0n0: [2024-03-29 16:06:30,887] [INFO] [stage3.py:2251:step] Full outer step loop took 4.98499059677124
x3104c0s25b0n0: [2024-03-29 16:06:30,906] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.003695011138916
x3104c0s1b0n0: [2024-03-29 16:06:30,910] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.008053302764893
x3104c0s1b0n0: [2024-03-29 16:06:30,918] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.015313386917114
x3104c0s1b0n0: [2024-03-29 16:06:30,924] [INFO] [stage3.py:2251:step] Full outer step loop took 5.021874904632568
x3104c0s25b0n0: [2024-03-29 16:06:30,925] [INFO] [stage3.py:2251:step] Full outer step loop took 5.022130966186523
x3104c0s25b0n0: [2024-03-29 16:06:30,926] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.023658037185669
x3104c0s1b0n0: [2024-03-29 16:06:30,932] [INFO] [stage3.py:2251:step] Full outer step loop took 5.029542684555054
x3104c0s1b0n0: [2024-03-29 16:06:30,952] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.04944634437561
x3104c0s25b0n0: [2024-03-29 16:06:30,959] [INFO] [stage3.py:2251:step] Full outer step loop took 5.056882858276367
x3104c0s1b0n0: [2024-03-29 16:06:30,961] [INFO] [stage3.py:2251:step] Full outer step loop took 5.058592081069946
x3104c0s25b0n0: [2024-03-29 16:06:31,024] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.121317625045776
x3104c0s25b0n0: [2024-03-29 16:06:31,024] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.121267557144165
x3104c0s25b0n0: [2024-03-29 16:06:31,033] [INFO] [stage3.py:2251:step] Full outer step loop took 5.130491733551025
x3104c0s25b0n0: [2024-03-29 16:06:31,033] [INFO] [stage3.py:2251:step] Full outer step loop took 5.13051438331604
x3104c0s1b0n0: [2024-03-29 16:06:31,042] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 5058.73
x3104c0s1b0n0: [2024-03-29 16:06:31,042] [INFO] [stage3.py:2277:step] End to end step took 5.139979124069214
x3104c0s1b0n0: [2024-03-29 16:06:31,042] [INFO] [stage3.py:2277:step] End to end step took 5.140147924423218
x3104c0s1b0n0: [2024-03-29 16:06:31,042] [INFO] [stage3.py:2277:step] End to end step took 5.140133857727051
x3104c0s25b0n0: [2024-03-29 16:06:31,042] [INFO] [stage3.py:2277:step] End to end step took 5.140230894088745
x3104c0s25b0n0: [2024-03-29 16:06:31,042] [INFO] [stage3.py:2277:step] End to end step took 5.140223026275635
x3104c0s1b0n0: [2024-03-29 16:06:31,043] [INFO] [logging.py:96:log_dist] [Rank 0] step=2, skipped=0, lr=[0.0002918585038060976], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 16:06:31,043] [INFO] [stage3.py:2277:step] End to end step took 5.140387296676636
x3104c0s25b0n0: [2024-03-29 16:06:31,043] [INFO] [stage3.py:2277:step] End to end step took 5.140460252761841
x3104c0s1b0n0: [2024-03-29 16:06:31,043] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6459.67 | bwd_microstep: 17230.08 | bwd_inner_microstep: 17143.36 | bwd_allreduce_microstep: 86.65 | step_microstep: 5164.30
x3104c0s1b0n0: [2024-03-29 16:06:31,043] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6459.66 | bwd: 17230.07 | bwd_inner: 17143.36 | bwd_allreduce: 86.66 | step: 5164.30
x3104c0s25b0n0: [2024-03-29 16:06:31,043] [INFO] [stage3.py:2277:step] End to end step took 5.140823125839233
x3104c0s1b0n0: [2024-03-29 16:06:31,137] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 16:06:31,138] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.14 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 16:06:31,138] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.68 GB, percent = 81.0%
x3104c0s1b0n0: <TIMER:interval-time,29.47559642791748>
x3104c0s1b0n0: <TIMER:interval-time,29.47560453414917><TIMER:interval-time,29.475611448287964>
x3104c0s1b0n0: <TIMER:interval-time,29.475607872009277>
x3104c0s1b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,29.475618362426758>
x3104c0s25b0n0: <TIMER:interval-time,29.475620985031128><TIMER:interval-time,29.475624084472656>
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,29.475733518600464>
x3104c0s25b0n0:  elapsed_time 29.475621 | consumed samples:           64 | consumed tokens:       131072 | elapsed time per iteration (ms): 29475.6 | learning rate: 2.919E-04 | global batch size:    32 | lm loss: 1.082593E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.086 | TFLOPs: 75.12 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 16:06:31,293] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 16:06:31,294] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 16:06:31,294] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.68 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:06:37,938] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 16:06:37,939] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:06:37,939] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.68 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:06:38,030] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 16:06:38,031] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:06:38,031] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.68 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:06:55,346] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 16:06:55,347] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 23.77 GB         CA 12.34 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 16:06:55,347] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.69 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:06:55,416] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 16:06:55,417] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 12.14 GB         CA 12.34 GB         Max_CA 12 GB 
x3104c0s1b0n0: [2024-03-29 16:06:55,417] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.69 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:07:00,336] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.895791292190552
x3104c0s1b0n0: [2024-03-29 16:07:00,346] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.90545654296875
x3104c0s1b0n0: [2024-03-29 16:07:00,350] [INFO] [stage3.py:2251:step] Full outer step loop took 4.909501552581787
x3104c0s1b0n0: [2024-03-29 16:07:00,366] [INFO] [stage3.py:2251:step] Full outer step loop took 4.925435781478882
x3104c0s1b0n0: [2024-03-29 16:07:00,445] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.004284858703613
x3104c0s1b0n0: [2024-03-29 16:07:00,448] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.007632732391357
x3104c0s1b0n0: [2024-03-29 16:07:00,455] [INFO] [stage3.py:2251:step] Full outer step loop took 5.014405965805054
x3104c0s1b0n0: [2024-03-29 16:07:00,457] [INFO] [stage3.py:2251:step] Full outer step loop took 5.016869783401489
x3104c0s25b0n0: [2024-03-29 16:07:00,462] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.021079778671265
x3104c0s25b0n0: [2024-03-29 16:07:00,472] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.031269311904907
x3104c0s25b0n0: [2024-03-29 16:07:00,490] [INFO] [stage3.py:2251:step] Full outer step loop took 5.049284219741821
x3104c0s25b0n0: [2024-03-29 16:07:00,491] [INFO] [stage3.py:2251:step] Full outer step loop took 5.051098108291626
x3104c0s25b0n0: [2024-03-29 16:07:00,496] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.055809259414673
x3104c0s25b0n0: [2024-03-29 16:07:00,527] [INFO] [stage3.py:2251:step] Full outer step loop took 5.08624005317688
x3104c0s25b0n0: [2024-03-29 16:07:00,590] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.149342775344849
x3104c0s25b0n0: [2024-03-29 16:07:00,599] [INFO] [stage3.py:2251:step] Full outer step loop took 5.15911340713501
x3104c0s1b0n0: [2024-03-29 16:07:00,609] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 5017.02
x3104c0s1b0n0: [2024-03-29 16:07:00,609] [INFO] [stage3.py:2277:step] End to end step took 5.168973922729492
x3104c0s25b0n0: [2024-03-29 16:07:00,609] [INFO] [stage3.py:2277:step] End to end step took 5.169170141220093
x3104c0s1b0n0: [2024-03-29 16:07:00,610] [INFO] [logging.py:96:log_dist] [Rank 0] step=3, skipped=0, lr=[0.00026841599982106197], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 16:07:00,610] [INFO] [stage3.py:2277:step] End to end step took 5.169365644454956
x3104c0s25b0n0: [2024-03-29 16:07:00,610] [INFO] [stage3.py:2277:step] End to end step took 5.169356346130371
x3104c0s1b0n0: [2024-03-29 16:07:00,610] [INFO] [timer.py:260:stop] epoch=0/micro_step=3/global_step=3, RunningAvgSamplesPerSec=1.0916179588361972, CurrSamplesPerSec=1.0916179588361972, MemAllocated=10.26GB, MaxMemAllocated=12.14GB
x3104c0s1b0n0: [2024-03-29 16:07:00,610] [INFO] [stage3.py:2277:step] End to end step took 5.169480562210083
x3104c0s25b0n0: [2024-03-29 16:07:00,610] [INFO] [stage3.py:2277:step] End to end step took 5.169480323791504
x3104c0s25b0n0: [2024-03-29 16:07:00,610] [INFO] [stage3.py:2277:step] End to end step took 5.169612646102905
x3104c0s1b0n0: [2024-03-29 16:07:00,610] [INFO] [stage3.py:2277:step] End to end step took 5.169705867767334
x3104c0s1b0n0: [2024-03-29 16:07:00,610] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6609.74 | bwd_microstep: 17093.80 | bwd_inner_microstep: 17010.40 | bwd_allreduce_microstep: 83.32 | step_microstep: 5192.93
x3104c0s1b0n0: [2024-03-29 16:07:00,610] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6609.72 | bwd: 17093.80 | bwd_inner: 17010.40 | bwd_allreduce: 83.34 | step: 5192.93
x3104c0s1b0n0: [2024-03-29 16:07:00,705] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 16:07:00,706] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.14 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 16:07:00,706] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.69 GB, percent = 81.0%
x3104c0s25b0n0: <TIMER:interval-time,29.567598819732666><TIMER:interval-time,29.567599296569824>
x3104c0s25b0n0: <TIMER:interval-time,29.5676007270813>
x3104c0s1b0n0: <TIMER:interval-time,29.567574739456177><TIMER:interval-time,29.56758403778076>
x3104c0s25b0n0: 
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,29.567588567733765><TIMER:interval-time,29.567588567733765>
x3104c0s1b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,29.56769895553589>
x3104c0s25b0n0:  elapsed_time 29.567601 | consumed samples:           96 | consumed tokens:       196608 | elapsed time per iteration (ms): 29567.6 | learning rate: 2.684E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.082 | TFLOPs: 74.88 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 16:07:00,840] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 16:07:00,840] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 16:07:00,840] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.69 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:07:06,785] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 16:07:06,786] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:07:06,786] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.69 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:07:06,885] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 16:07:06,885] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:07:06,885] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.69 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:07:23,067] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 16:07:23,067] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 23.77 GB         CA 12.34 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 16:07:23,068] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.69 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:07:23,139] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 16:07:23,139] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 12.14 GB         CA 12.34 GB         Max_CA 12 GB 
x3104c0s1b0n0: [2024-03-29 16:07:23,139] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.69 GB, percent = 81.0%
x3104c0s25b0n0: [2024-03-29 16:07:28,090] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.926790952682495
x3104c0s25b0n0: [2024-03-29 16:07:28,123] [INFO] [stage3.py:2251:step] Full outer step loop took 4.960513353347778
x3104c0s1b0n0: [2024-03-29 16:07:28,123] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.960580587387085
x3104c0s1b0n0: [2024-03-29 16:07:28,138] [INFO] [stage3.py:2251:step] Full outer step loop took 4.975449323654175
x3104c0s1b0n0: [2024-03-29 16:07:28,156] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.992726802825928
x3104c0s1b0n0: [2024-03-29 16:07:28,203] [INFO] [stage3.py:2251:step] Full outer step loop took 5.0397279262542725
x3104c0s1b0n0: [2024-03-29 16:07:28,209] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.046584367752075
x3104c0s1b0n0: [2024-03-29 16:07:28,210] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.046936511993408
x3104c0s1b0n0: [2024-03-29 16:07:28,219] [INFO] [stage3.py:2251:step] Full outer step loop took 5.055785894393921
x3104c0s1b0n0: [2024-03-29 16:07:28,220] [INFO] [stage3.py:2251:step] Full outer step loop took 5.056779384613037
x3104c0s25b0n0: [2024-03-29 16:07:28,343] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.179900407791138
x3104c0s25b0n0: [2024-03-29 16:07:28,355] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.192244529724121
x3104c0s25b0n0: [2024-03-29 16:07:28,360] [INFO] [stage3.py:2251:step] Full outer step loop took 5.197341680526733
x3104c0s25b0n0: [2024-03-29 16:07:28,367] [INFO] [stage3.py:2251:step] Full outer step loop took 5.203937530517578
x3104c0s25b0n0: [2024-03-29 16:07:28,400] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.237283706665039
x3104c0s25b0n0: [2024-03-29 16:07:28,409] [INFO] [stage3.py:2251:step] Full outer step loop took 5.2464025020599365
x3104c0s1b0n0: [2024-03-29 16:07:28,419] [INFO] [stage3.py:2277:step] End to end step took 5.255800247192383
x3104c0s1b0n0: [2024-03-29 16:07:28,419] [INFO] [stage3.py:2277:step] End to end step took 5.255694627761841
x3104c0s25b0n0: [2024-03-29 16:07:28,419] [INFO] [stage3.py:2277:step] End to end step took 5.255895614624023
x3104c0s1b0n0: [2024-03-29 16:07:28,419] [INFO] [stage3.py:2277:step] End to end step took 5.255890369415283
x3104c0s25b0n0: [2024-03-29 16:07:28,419] [INFO] [stage3.py:2277:step] End to end step took 5.2561469078063965
x3104c0s25b0n0: [2024-03-29 16:07:28,419] [INFO] [stage3.py:2277:step] End to end step took 5.2562172412872314
x3104c0s1b0n0: [2024-03-29 16:07:28,419] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4975.87
x3104c0s25b0n0: [2024-03-29 16:07:28,419] [INFO] [stage3.py:2277:step] End to end step took 5.256420612335205
x3104c0s1b0n0: [2024-03-29 16:07:28,419] [INFO] [stage3.py:2277:step] End to end step took 5.256581544876099
x3104c0s1b0n0: [2024-03-29 16:07:28,420] [INFO] [logging.py:96:log_dist] [Rank 0] step=4, skipped=0, lr=[0.00023249999999999996], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 16:07:28,420] [INFO] [timer.py:260:stop] epoch=0/micro_step=4/global_step=4, RunningAvgSamplesPerSec=1.124919114687751, CurrSamplesPerSec=1.1603159889062853, MemAllocated=10.26GB, MaxMemAllocated=12.14GB
x3104c0s1b0n0: [2024-03-29 16:07:28,420] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 5903.83 | bwd_microstep: 16014.03 | bwd_inner_microstep: 15937.11 | bwd_allreduce_microstep: 76.85 | step_microstep: 5280.80
x3104c0s1b0n0: [2024-03-29 16:07:28,421] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 5903.82 | bwd: 16014.03 | bwd_inner: 15937.10 | bwd_allreduce: 76.87 | step: 5280.80
x3104c0s1b0n0: [2024-03-29 16:07:28,510] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 16:07:28,510] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.14 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 16:07:28,511] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.69 GB, percent = 81.0%
x3104c0s1b0n0: <TIMER:interval-time,27.80415368080139><TIMER:interval-time,27.804155588150024><TIMER:interval-time,27.804161310195923><TIMER:interval-time,27.804158926010132>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,27.804171323776245><TIMER:interval-time,27.80418634414673>
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,27.804194688796997>
x3104c0s25b0n0: <TIMER:interval-time,27.804301738739014>
x3104c0s25b0n0:  elapsed_time 27.804195 | consumed samples:          128 | consumed tokens:       262144 | elapsed time per iteration (ms): 27804.2 | learning rate: 2.325E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.151 | TFLOPs: 79.63 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 16:07:28,601] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 16:07:28,601] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 16:07:28,602] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.69 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:07:34,939] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 16:07:34,940] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:07:34,940] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.69 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:07:35,023] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 16:07:35,023] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:07:35,023] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.69 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:07:51,161] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 16:07:51,162] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 23.77 GB         CA 12.34 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 16:07:51,162] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.7 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:07:51,232] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 16:07:51,232] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 12.14 GB         CA 12.34 GB         Max_CA 12 GB 
x3104c0s1b0n0: [2024-03-29 16:07:51,232] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.7 GB, percent = 81.0%
x3104c0s25b0n0: [2024-03-29 16:07:56,035] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.779235601425171
x3104c0s25b0n0: [2024-03-29 16:07:56,100] [INFO] [stage3.py:2251:step] Full outer step loop took 4.844498634338379
x3104c0s1b0n0: [2024-03-29 16:07:56,101] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.845390558242798
x3104c0s1b0n0: [2024-03-29 16:07:56,129] [INFO] [stage3.py:2251:step] Full outer step loop took 4.873306035995483
x3104c0s1b0n0: [2024-03-29 16:07:56,137] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.880753993988037
x3104c0s1b0n0: [2024-03-29 16:07:56,172] [INFO] [stage3.py:2251:step] Full outer step loop took 4.9157726764678955
x3104c0s1b0n0: [2024-03-29 16:07:56,247] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.991316795349121
x3104c0s1b0n0: [2024-03-29 16:07:56,252] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.99647068977356
x3104c0s1b0n0: [2024-03-29 16:07:56,259] [INFO] [stage3.py:2251:step] Full outer step loop took 5.003386974334717
x3104c0s1b0n0: [2024-03-29 16:07:56,261] [INFO] [stage3.py:2251:step] Full outer step loop took 5.0056962966918945
x3104c0s25b0n0: [2024-03-29 16:07:56,335] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.079688310623169
x3104c0s25b0n0: [2024-03-29 16:07:56,378] [INFO] [stage3.py:2251:step] Full outer step loop took 5.12240195274353
x3104c0s25b0n0: [2024-03-29 16:07:56,384] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.128031253814697
x3104c0s25b0n0: [2024-03-29 16:07:56,393] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.1370978355407715
x3104c0s25b0n0: [2024-03-29 16:07:56,394] [INFO] [stage3.py:2251:step] Full outer step loop took 5.138660192489624
x3104c0s25b0n0: [2024-03-29 16:07:56,402] [INFO] [stage3.py:2251:step] Full outer step loop took 5.146265268325806
x3104c0s1b0n0: [2024-03-29 16:07:56,411] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 5005.83
x3104c0s1b0n0: [2024-03-29 16:07:56,411] [INFO] [stage3.py:2277:step] End to end step took 5.155581951141357
x3104c0s1b0n0: [2024-03-29 16:07:56,411] [INFO] [stage3.py:2277:step] End to end step took 5.155447244644165
x3104c0s25b0n0: [2024-03-29 16:07:56,411] [INFO] [stage3.py:2277:step] End to end step took 5.155555248260498
x3104c0s25b0n0: [2024-03-29 16:07:56,411] [INFO] [stage3.py:2277:step] End to end step took 5.155654191970825
x3104c0s25b0n0: [2024-03-29 16:07:56,411] [INFO] [stage3.py:2277:step] End to end step took 5.155718564987183
x3104c0s1b0n0: [2024-03-29 16:07:56,411] [INFO] [logging.py:96:log_dist] [Rank 0] step=5, skipped=0, lr=[0.0001884425039850356], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 16:07:56,411] [INFO] [stage3.py:2277:step] End to end step took 5.155747890472412
x3104c0s1b0n0: [2024-03-29 16:07:56,412] [INFO] [stage3.py:2277:step] End to end step took 5.155939340591431
x3104c0s25b0n0: [2024-03-29 16:07:56,412] [INFO] [stage3.py:2277:step] End to end step took 5.156037330627441
x3104c0s1b0n0: [2024-03-29 16:07:56,412] [INFO] [timer.py:260:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=1.133386493754586, CurrSamplesPerSec=1.1507095056095509, MemAllocated=10.26GB, MaxMemAllocated=12.14GB
x3104c0s1b0n0: [2024-03-29 16:07:56,412] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6300.25 | bwd_microstep: 15978.63 | bwd_inner_microstep: 15900.51 | bwd_allreduce_microstep: 78.05 | step_microstep: 5179.48
x3104c0s1b0n0: [2024-03-29 16:07:56,412] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6300.23 | bwd: 15978.63 | bwd_inner: 15900.50 | bwd_allreduce: 78.06 | step: 5179.48
x3104c0s1b0n0: [2024-03-29 16:07:56,503] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 16:07:56,503] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.14 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 16:07:56,503] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.7 GB, percent = 81.0%
x3104c0s1b0n0: <TIMER:interval-time,27.99252200126648><TIMER:interval-time,27.99252200126648>
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,27.99252462387085>
x3104c0s1b0n0: <TIMER:interval-time,27.992531538009644>
x3104c0s25b0n0: <TIMER:interval-time,27.992560386657715>
x3104c0s25b0n0: <TIMER:interval-time,27.99256682395935><TIMER:interval-time,27.992567777633667>
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,27.992661714553833>
x3104c0s25b0n0:  elapsed_time 27.992568 | consumed samples:          160 | consumed tokens:       327680 | elapsed time per iteration (ms): 27992.6 | learning rate: 1.884E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.143 | TFLOPs: 79.10 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 16:07:56,655] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 16:07:56,655] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 16:07:56,656] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.7 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:08:02,964] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 16:08:02,965] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:08:02,965] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.7 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:08:03,048] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 16:08:03,048] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:08:03,048] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.7 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:08:19,152] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 16:08:19,153] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 23.77 GB         CA 12.34 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 16:08:19,153] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.7 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:08:19,227] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 16:08:19,228] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 12.14 GB         CA 12.34 GB         Max_CA 12 GB 
x3104c0s1b0n0: [2024-03-29 16:08:19,228] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.7 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:08:24,186] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.934359550476074
x3104c0s1b0n0: [2024-03-29 16:08:24,199] [INFO] [stage3.py:2251:step] Full outer step loop took 4.947707653045654
x3104c0s1b0n0: [2024-03-29 16:08:24,200] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.948868751525879
x3104c0s1b0n0: [2024-03-29 16:08:24,211] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.959707975387573
x3104c0s1b0n0: [2024-03-29 16:08:24,212] [INFO] [stage3.py:2251:step] Full outer step loop took 4.9611170291900635
x3104c0s1b0n0: [2024-03-29 16:08:24,231] [INFO] [stage3.py:2251:step] Full outer step loop took 4.979480028152466
x3104c0s1b0n0: [2024-03-29 16:08:24,246] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.994768142700195
x3104c0s1b0n0: [2024-03-29 16:08:24,255] [INFO] [stage3.py:2251:step] Full outer step loop took 5.003972768783569
x3104c0s25b0n0: [2024-03-29 16:08:24,283] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.03100323677063
x3104c0s25b0n0: [2024-03-29 16:08:24,286] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.034713268280029
x3104c0s25b0n0: [2024-03-29 16:08:24,303] [INFO] [stage3.py:2251:step] Full outer step loop took 5.05210542678833
x3104c0s25b0n0: [2024-03-29 16:08:24,356] [INFO] [stage3.py:2251:step] Full outer step loop took 5.104698419570923
x3104c0s25b0n0: [2024-03-29 16:08:24,452] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.200274705886841
x3104c0s25b0n0: [2024-03-29 16:08:24,466] [INFO] [stage3.py:2251:step] Full outer step loop took 5.214276313781738
x3104c0s25b0n0: [2024-03-29 16:08:24,504] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.2523980140686035
x3104c0s25b0n0: [2024-03-29 16:08:24,513] [INFO] [stage3.py:2251:step] Full outer step loop took 5.2615296840667725
x3104c0s25b0n0: [2024-03-29 16:08:24,522] [INFO] [stage3.py:2277:step] End to end step took 5.270910024642944
x3104c0s1b0n0: [2024-03-29 16:08:24,522] [INFO] [stage3.py:2277:step] End to end step took 5.271056413650513
x3104c0s1b0n0: [2024-03-29 16:08:24,522] [INFO] [stage3.py:2277:step] End to end step took 5.271099328994751
x3104c0s25b0n0: [2024-03-29 16:08:24,523] [INFO] [stage3.py:2277:step] End to end step took 5.271160364151001
x3104c0s1b0n0: [2024-03-29 16:08:24,523] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 5004.18
x3104c0s25b0n0: [2024-03-29 16:08:24,523] [INFO] [stage3.py:2277:step] End to end step took 5.271363019943237
x3104c0s25b0n0: [2024-03-29 16:08:24,523] [INFO] [stage3.py:2277:step] End to end step took 5.271456718444824
x3104c0s1b0n0: [2024-03-29 16:08:24,523] [INFO] [stage3.py:2277:step] End to end step took 5.271447658538818
x3104c0s1b0n0: [2024-03-29 16:08:24,523] [INFO] [stage3.py:2277:step] End to end step took 5.2714855670928955
x3104c0s1b0n0: [2024-03-29 16:08:24,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=6, skipped=0, lr=[0.0001415574960149644], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 16:08:24,524] [INFO] [timer.py:260:stop] epoch=0/micro_step=6/global_step=6, RunningAvgSamplesPerSec=1.137082449211242, CurrSamplesPerSec=1.1483163738912274, MemAllocated=10.26GB, MaxMemAllocated=12.14GB
x3104c0s1b0n0: [2024-03-29 16:08:24,524] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6263.03 | bwd_microstep: 15935.33 | bwd_inner_microstep: 15857.50 | bwd_allreduce_microstep: 77.76 | step_microstep: 5295.86
x3104c0s1b0n0: [2024-03-29 16:08:24,524] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6263.01 | bwd: 15935.32 | bwd_inner: 15857.49 | bwd_allreduce: 77.77 | step: 5295.86
x3104c0s1b0n0: [2024-03-29 16:08:24,618] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 16:08:24,619] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.14 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 16:08:24,619] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.7 GB, percent = 81.0%
x3104c0s25b0n0: <TIMER:interval-time,28.114891290664673><TIMER:interval-time,28.114896059036255>
x3104c0s25b0n0: <TIMER:interval-time,28.11489725112915>
x3104c0s1b0n0: <TIMER:interval-time,28.114858150482178><TIMER:interval-time,28.114861249923706><TIMER:interval-time,28.114856004714966>
x3104c0s25b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,28.11486291885376>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,28.115008115768433>
x3104c0s25b0n0:  elapsed_time 28.114896 | consumed samples:          192 | consumed tokens:       393216 | elapsed time per iteration (ms): 28114.9 | learning rate: 1.416E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.138 | TFLOPs: 78.75 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 16:08:24,749] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 16:08:24,749] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 16:08:24,749] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.7 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:08:31,129] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 16:08:31,130] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:08:31,130] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.7 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:08:31,211] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 16:08:31,211] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:08:31,211] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.7 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:08:48,480] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 16:08:48,481] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 23.77 GB         CA 12.34 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 16:08:48,481] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.71 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:08:48,553] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 16:08:48,554] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 12.14 GB         CA 12.34 GB         Max_CA 12 GB 
x3104c0s1b0n0: [2024-03-29 16:08:48,554] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.71 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:08:53,449] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.871453046798706
x3104c0s1b0n0: [2024-03-29 16:08:53,474] [INFO] [stage3.py:2251:step] Full outer step loop took 4.896761417388916
x3104c0s1b0n0: [2024-03-29 16:08:53,575] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.997128248214722
x3104c0s1b0n0: [2024-03-29 16:08:53,590] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.012729644775391
x3104c0s1b0n0: [2024-03-29 16:08:53,598] [INFO] [stage3.py:2251:step] Full outer step loop took 5.021167516708374
x3104c0s1b0n0: [2024-03-29 16:08:53,601] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.0242767333984375
x3104c0s1b0n0: [2024-03-29 16:08:53,605] [INFO] [stage3.py:2251:step] Full outer step loop took 5.027794122695923
x3104c0s1b0n0: [2024-03-29 16:08:53,611] [INFO] [stage3.py:2251:step] Full outer step loop took 5.0334792137146
x3104c0s25b0n0: [2024-03-29 16:08:53,671] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.093509197235107
x3104c0s25b0n0: [2024-03-29 16:08:53,677] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.0995283126831055
x3104c0s25b0n0: [2024-03-29 16:08:53,701] [INFO] [stage3.py:2251:step] Full outer step loop took 5.123969316482544
x3104c0s25b0n0: [2024-03-29 16:08:53,708] [INFO] [stage3.py:2251:step] Full outer step loop took 5.130513668060303
x3104c0s25b0n0: [2024-03-29 16:08:53,776] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.198334217071533
x3104c0s25b0n0: [2024-03-29 16:08:53,795] [INFO] [stage3.py:2251:step] Full outer step loop took 5.21770715713501
x3104c0s25b0n0: [2024-03-29 16:08:53,821] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.244293212890625
x3104c0s25b0n0: [2024-03-29 16:08:53,830] [INFO] [stage3.py:2251:step] Full outer step loop took 5.253415107727051
x3104c0s1b0n0: [2024-03-29 16:08:53,840] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 5033.62
x3104c0s1b0n0: [2024-03-29 16:08:53,840] [INFO] [stage3.py:2277:step] End to end step took 5.262923240661621
x3104c0s1b0n0: [2024-03-29 16:08:53,840] [INFO] [stage3.py:2277:step] End to end step took 5.262903928756714
x3104c0s25b0n0: [2024-03-29 16:08:53,840] [INFO] [stage3.py:2277:step] End to end step took 5.263047456741333
x3104c0s1b0n0: [2024-03-29 16:08:53,840] [INFO] [stage3.py:2277:step] End to end step took 5.263007402420044
x3104c0s25b0n0: [2024-03-29 16:08:53,840] [INFO] [stage3.py:2277:step] End to end step took 5.262994766235352
x3104c0s1b0n0: [2024-03-29 16:08:53,840] [INFO] [logging.py:96:log_dist] [Rank 0] step=7, skipped=0, lr=[9.750000000000001e-05], mom=[(0.9, 0.95)]
x3104c0s25b0n0: [2024-03-29 16:08:53,840] [INFO] [stage3.py:2277:step] End to end step took 5.263378381729126
x3104c0s1b0n0: [2024-03-29 16:08:53,841] [INFO] [timer.py:260:stop] epoch=0/micro_step=7/global_step=7, RunningAvgSamplesPerSec=1.1294749275867415, CurrSamplesPerSec=1.1000362546024638, MemAllocated=10.26GB, MaxMemAllocated=12.14GB
x3104c0s1b0n0: [2024-03-29 16:08:53,841] [INFO] [stage3.py:2277:step] End to end step took 5.26334547996521
x3104c0s25b0n0: [2024-03-29 16:08:53,841] [INFO] [stage3.py:2277:step] End to end step took 5.263524293899536
x3104c0s1b0n0: [2024-03-29 16:08:53,841] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6339.56 | bwd_microstep: 17101.73 | bwd_inner_microstep: 17017.88 | bwd_allreduce_microstep: 83.78 | step_microstep: 5286.86
x3104c0s1b0n0: [2024-03-29 16:08:53,841] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6339.55 | bwd: 17101.73 | bwd_inner: 17017.88 | bwd_allreduce: 83.79 | step: 5286.86
x3104c0s1b0n0: [2024-03-29 16:08:53,929] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 16:08:53,930] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.14 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 16:08:53,930] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.71 GB, percent = 81.0%
x3104c0s1b0n0: <TIMER:interval-time,29.310606956481934><TIMER:interval-time,29.310598373413086><TIMER:interval-time,29.310609817504883>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,29.310614824295044>
x3104c0s25b0n0: <TIMER:interval-time,29.3106210231781><TIMER:interval-time,29.310606241226196>
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,29.310627937316895><TIMER:interval-time,29.310627222061157>
x3104c0s25b0n0: 
x3104c0s25b0n0:  elapsed_time 29.310628 | consumed samples:          224 | consumed tokens:       458752 | elapsed time per iteration (ms): 29310.6 | learning rate: 9.750E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.092 | TFLOPs: 75.54 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            x3104c0s1b0n0: [2024-03-29 16:10:12,296] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 16:10:12,296] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 23.77 GB         CA 12.34 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 16:10:12,297] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.71 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:10:12,367] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 16:10:12,368] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 12.14 GB         CA 12.34 GB         Max_CA 12 GB 
x3104c0s1b0n0: [2024-03-29 16:10:12,368] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.71 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:10:17,271] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.878645658493042
x3104c0s1b0n0: [2024-03-29 16:10:17,302] [INFO] [stage3.py:2251:step] Full outer step loop took 4.910343885421753
x3104c0s1b0n0: [2024-03-29 16:10:17,426] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.034245252609253
x3104c0s1b0n0: [2024-03-29 16:10:17,440] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.048364639282227
x3104c0s1b0n0: [2024-03-29 16:10:17,445] [INFO] [stage3.py:2251:step] Full outer step loop took 5.052694082260132
x3104c0s1b0n0: [2024-03-29 16:10:17,446] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.053743600845337
x3104c0s1b0n0: [2024-03-29 16:10:17,451] [INFO] [stage3.py:2251:step] Full outer step loop took 5.058659076690674
x3104c0s1b0n0: [2024-03-29 16:10:17,455] [INFO] [stage3.py:2251:step] Full outer step loop took 5.06303334236145
x3104c0s25b0n0: [2024-03-29 16:10:17,504] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.111251592636108
x3104c0s25b0n0: [2024-03-29 16:10:17,520] [INFO] [stage3.py:2251:step] Full outer step loop took 5.128320693969727
x3104c0s25b0n0: [2024-03-29 16:10:17,556] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.162461042404175
x3104c0s25b0n0: [2024-03-29 16:10:17,556] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.162959575653076
x3104c0s25b0n0: [2024-03-29 16:10:17,592] [INFO] [stage3.py:2251:step] Full outer step loop took 5.199754953384399
x3104c0s25b0n0: [2024-03-29 16:10:17,592] [INFO] [stage3.py:2251:step] Full outer step loop took 5.1997458934783936
x3104c0s25b0n0: [2024-03-29 16:10:17,595] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.2028772830963135
x3104c0s25b0n0: [2024-03-29 16:10:17,604] [INFO] [stage3.py:2251:step] Full outer step loop took 5.212013006210327
x3104c0s1b0n0: [2024-03-29 16:10:17,613] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 5063.18
x3104c0s25b0n0: [2024-03-29 16:10:17,613] [INFO] [stage3.py:2277:step] End to end step took 5.221326589584351
x3104c0s1b0n0: [2024-03-29 16:10:17,613] [INFO] [stage3.py:2277:step] End to end step took 5.221361398696899
x3104c0s1b0n0: [2024-03-29 16:10:17,613] [INFO] [stage3.py:2277:step] End to end step took 5.221247911453247
x3104c0s1b0n0: [2024-03-29 16:10:17,613] [INFO] [stage3.py:2277:step] End to end step took 5.221417188644409
x3104c0s25b0n0: [2024-03-29 16:10:17,613] [INFO] [stage3.py:2277:step] End to end step took 5.221377372741699
x3104c0s25b0n0: [2024-03-29 16:10:17,613] [INFO] [stage3.py:2277:step] End to end step took 5.221472978591919
x3104c0s25b0n0: [2024-03-29 16:10:17,614] [INFO] [stage3.py:2277:step] End to end step took 5.221601963043213
x3104c0s1b0n0: [2024-03-29 16:10:17,614] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[3e-05], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 16:10:17,614] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=1.1390381495491813, CurrSamplesPerSec=1.16185675192556, MemAllocated=10.26GB, MaxMemAllocated=12.14GB
x3104c0s1b0n0: [2024-03-29 16:10:17,614] [INFO] [stage3.py:2277:step] End to end step took 5.222016096115112
x3104c0s1b0n0: [2024-03-29 16:10:17,614] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 5914.67 | bwd_microstep: 16038.39 | bwd_inner_microstep: 15964.04 | bwd_allreduce_microstep: 74.28 | step_microstep: 5245.79
x3104c0s1b0n0: [2024-03-29 16:10:17,614] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 5914.65 | bwd: 16038.38 | bwd_inner: 15964.03 | bwd_allreduce: 74.29 | step: 5245.79
x3104c0s1b0n0: [2024-03-29 16:10:17,712] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 16:10:17,713] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.14 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 16:10:17,713] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.71 GB, percent = 81.0%
x3104c0s25b0n0: <TIMER:interval-time,27.780088186264038><TIMER:interval-time,27.780089855194092><TIMER:interval-time,27.780089855194092>
x3104c0s1b0n0: <TIMER:interval-time,27.78005290031433><TIMER:interval-time,27.780057907104492><TIMER:interval-time,27.7800555229187>
x3104c0s25b0n0: 
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,27.7800612449646>
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,27.780094146728516>
x3104c0s25b0n0:  elapsed_time 27.780094 | consumed samples:          320 | consumed tokens:       655360 | elapsed time per iteration (ms): 27780.1 | learning rate: 3.000E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.152 | TFLOPs: 79.70 |
x3104c0s25b0n0: <<<only_train:295.90867495536804>>>
x3104c0s25b0n0: <<<only_train:295.9087407588959>>>
x3104c0s25b0n0: <<<only_train:295.9087588787079>>>
x3104c0s25b0n0: <<<only_train:295.90858697891235>>>
x3104c0s1b0n0: <<<only_train:295.90863943099976>>>
x3104c0s1b0n0: <<<only_train:295.90853786468506>>>
x3104c0s1b0n0: <<<only_train:295.9080741405487>>>
x3104c0s1b0n0: <<<only_train:295.90869188308716>>>
x3104c0s1b0n0: [after training ends] datetime: 2024-03-29 16:10:17 
x3104c0s25b0n0: <<<full_time:295.9088325500488>>><<<full_time:295.90902400016785>>>
x3104c0s25b0n0: 
x3104c0s25b0n0: <<<full_time:295.90903902053833>>>
x3104c0s25b0n0: <<<full_time:295.90907073020935>>>
x3104c0s1b0n0: <<<full_time:295.90830969810486>>>
x3104c0s1b0n0: <<<full_time:295.9089004993439>>>
x3104c0s1b0n0: <<<full_time:295.90892028808594>>>
x3104c0s1b0n0: <<<full_time:295.9088089466095>>>
x3104c0s1b0n0: [2024-03-29 16:10:27,011] [INFO] [launch.py:348:main] Process 63745 exits successfully.
x3104c0s25b0n0: [2024-03-29 16:10:27,124] [INFO] [launch.py:348:main] Process 25014 exits successfully.
x3104c0s1b0n0: [2024-03-29 16:10:35,020] [INFO] [launch.py:348:main] Process 63743 exits successfully.
x3104c0s1b0n0: [2024-03-29 16:10:35,020] [INFO] [launch.py:348:main] Process 63746 exits successfully.
x3104c0s1b0n0: [2024-03-29 16:10:35,020] [INFO] [launch.py:348:main] Process 63744 exits successfully.
x3104c0s25b0n0: [2024-03-29 16:10:35,132] [INFO] [launch.py:348:main] Process 25016 exits successfully.
x3104c0s25b0n0: [2024-03-29 16:10:35,132] [INFO] [launch.py:348:main] Process 25017 exits successfully.
x3104c0s25b0n0: [2024-03-29 16:10:35,132] [INFO] [launch.py:348:main] Process 25015 exits successfully.

[2024-03-29 15:57:22,730] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 15:57:26,012] [INFO] [runner.py:463:main] Using IP address of 10.140.58.110 for node x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-29 15:57:26,014] [INFO] [runner.py:557:main] deepspeed_env file = ./.deepspeed_env
[2024-03-29 15:57:26,015] [INFO] [multinode_runner.py:80:get_cmd] Running on the following workers: x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov,x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-29 15:57:26,015] [INFO] [runner.py:568:main] cmd = pdsh -S -f 1024 -w x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov,x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov export PYTHONUSERBASE=/home/am6429/.local/polaris/conda/2023-10-04; export PYTHONPATH=/home/am6429/dl-io/Megatron-DeepSpeed; export PATH=/home/am6429/.conda/envs/dspeed_env/bin:/soft/datascience/conda/2023-10-04/mconda3/condabin:/soft/compilers/cudatoolkit/cuda-11.8.0/bin:/soft/buildtools/cmake/cmake-3.23.2/cmake-3.23.2-linux-x86_64/bin:/opt/cray/pe/gcc/11.2.0/bin:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/include:/opt/cray/pe/pals/1.2.11/bin:/opt/cray/pe/craype/2.7.20/bin:/opt/cray/libfabric/1.15.2.0/bin:/home/am6429/.conda/envs/dspeed_env/bin:/opt/cray/pe/perftools/23.03.0/bin:/opt/cray/pe/papi/7.0.0.1/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/am6429/.local/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/am6429/veloc-build/include:/home/am6429/veloc-build/bin:/soft/datascience/conda/2023-01-10/mconda3/include:/opt/cray/pe/bin:/soft/datascience/conda/2023-01-10/mconda3/include; export LD_LIBRARY_PATH=/usr/lib64/:/soft/compilers/cudatoolkit/cuda-11.8.0/lib64:/opt/cray/pe/gcc/11.2.0/snos/lib64:/soft/compilers/cudatoolkit/cuda-11.8.0/extras/CUPTI/lib64:/soft/libraries/trt/TensorRT-8.5.3.1.Linux.x86_64-gnu.cuda-11.8.cudnn8.6/lib:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/lib:/soft/libraries/cudnn/cudnn-11-linux-x64-v8.6.0.163/lib:/opt/cray/libfabric/1.15.2.0/lib64:/usr/lib64:/opt/cray/pe/papi/7.0.0.1/lib64:/home/am6429/veloc-build/lib:/home/am6429/veloc-build/lib64:/home/am6429/nvcomp/lib:/soft/datascience/conda/2023-01-10/mconda3/lib:/soft/datascience/conda/2023-01-10/mconda3/lib/; export http_proxy=http://proxy.alcf.anl.gov:3128; export https_proxy=http://proxy.alcf.anl.gov:3128; export CC=gcc; export CXX=g++; export IBV_FORK_SAFE=1; export CFLAGS=-I/soft/datascience/conda/2023-01-10/mconda3/include/; export LDFLAGS=-L/soft/datascience/conda/2023-01-10/mconda3/lib/; export CUDA_DEVICE_MAX_CONNECTIONS=1; export TORCHSNAPSHOT_PER_RANK_MEMORY_BUDGET_BYTES=34359738368; export _DEFAULT_MAX_PER_RANK_IO_CONCURRENCY=1; export _MAX_PER_RANK_IO_CONCURRENCY=1; export NSYS_REPORT_DIR=/home/am6429/dl-io/dl-io-outputs/act-output-30B//rep-30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch1-flush_async1-opt_gaps6-%n;  cd /home/am6429/dl-io/Megatron-DeepSpeed; /home/am6429/.conda/envs/dspeed_env/bin/python -u -m deepspeed.launcher.launch --world_info=eyJ4MzEwNGMwczFiMG4wLmhzbi5jbS5wb2xhcmlzLmFsY2YuYW5sLmdvdiI6IFswLCAxLCAyLCAzXSwgIngzMTA0YzBzMjViMG4wLmhzbi5jbS5wb2xhcmlzLmFsY2YuYW5sLmdvdiI6IFswLCAxLCAyLCAzXX0= --node_rank=%n --master_addr=10.140.58.110 --master_port=29700 /home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py --tensor-model-parallel-size 1 --num-layers 60 --hidden-size 6656 --num-attention-heads 52 --micro-batch-size 4 --global-batch-size 32 --seq-length 2048 --max-position-embeddings 2048 --train-iters 10 --save /local/scratch/llama2/zero3-tp1}_dp8 --data-path /home/am6429/dl-io/datasets/meg-gpt2_text_document --vocab-file /home/am6429/dl-io/datasets/gpt2-vocab.json --merge-file /home/am6429/dl-io/datasets/gpt2-merges.txt --data-impl mmap --tokenizer-type GPTSentencePieceTokenizer --tokenizer-model /home/am6429/dl-io/datasets/tokenizer.model --split 949,50,1 --distributed-backend nccl --lr 3e-4 --lr-decay-style cosine --min-lr 3e-5 --weight-decay 0.1 --clip-grad 1 --lr-warmup-iters 1 --optimizer adam --adam-beta1 0.9 --adam-beta2 0.95 --log-interval 1 --save-interval 1000 --eval-interval 1000 --eval-iters 0 --bf16 --no-query-key-layer-scaling --attention-dropout 0 --hidden-dropout 0 --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization rmsnorm --disable-bias-linear --num-key-value-heads 4 --deepspeed --exit-interval 20 --deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json --zero-stage=3 --no-pipeline-parallel --cpu-optimizer --checkpoint-activations --deepspeed-activation-checkpointing
x3104c0s1b0n0: [2024-03-29 15:57:28,038] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 15:57:29,900] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3104c0s1b0n0: [2024-03-29 15:57:29,900] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=0
x3104c0s1b0n0: [2024-03-29 15:57:29,900] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3104c0s1b0n0: [2024-03-29 15:57:29,900] [INFO] [launch.py:163:main] dist_world_size=8
x3104c0s1b0n0: [2024-03-29 15:57:29,900] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3104c0s1b0n0: [2024-03-29 15:57:29,901] [INFO] [launch.py:253:main] process 60298 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s1b0n0: [2024-03-29 15:57:29,901] [INFO] [launch.py:253:main] process 60299 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s1b0n0: [2024-03-29 15:57:29,902] [INFO] [launch.py:253:main] process 60300 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s1b0n0: [2024-03-29 15:57:29,902] [INFO] [launch.py:253:main] process 60301 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s1b0n0: [2024-03-29 15:57:31,667] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 15:57:31,725] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 15:57:31,730] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 15:57:31,745] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s1b0n0:       meet the required dependencies to JIT install the op.
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: JIT compiled ops requires ninja
x3104c0s1b0n0: ninja .................. [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: op name ................ installed .. compatible
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed general environment info:
x3104c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s1b0n0: torch version .................... 2.0.1+cu118
x3104c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s1b0n0: torch cuda version ............... 11.8
x3104c0s1b0n0: torch hip version ................ None
x3104c0s1b0n0: nvcc version ..................... 11.8
x3104c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s1b0n0: [2024-03-29 15:57:34,445] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s1b0n0:       meet the required dependencies to JIT install the op.
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: JIT compiled ops requires ninja
x3104c0s1b0n0: ninja .................. [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: op name ................ installed .. compatible
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s1b0n0:       meet the required dependencies to JIT install the op.
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: JIT compiled ops requires ninja
x3104c0s1b0n0: ninja .................. [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: op name ................ installed .. compatible
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s1b0n0:       meet the required dependencies to JIT install the op.
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: JIT compiled ops requires ninja
x3104c0s1b0n0: ninja .................. [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: op name ................ installed .. compatible
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed general environment info:
x3104c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s1b0n0: torch version .................... 2.0.1+cu118
x3104c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s1b0n0: torch cuda version ............... 11.8
x3104c0s1b0n0: torch hip version ................ None
x3104c0s1b0n0: nvcc version ..................... 11.8
x3104c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: DeepSpeed general environment info:
x3104c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s1b0n0: torch version .................... 2.0.1+cu118
x3104c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s1b0n0: torch cuda version ............... 11.8
x3104c0s1b0n0: torch hip version ................ None
x3104c0s1b0n0: nvcc version ..................... 11.8
x3104c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed general environment info:
x3104c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s1b0n0: torch version .................... 2.0.1+cu118
x3104c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s1b0n0: torch cuda version ............... 11.8
x3104c0s1b0n0: torch hip version ................ None
x3104c0s1b0n0: nvcc version ..................... 11.8
x3104c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s1b0n0: [2024-03-29 15:57:34,677] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s1b0n0: INFO: overriding default arguments for tokenizer_type:GPTSentencePieceTokenizer                    with tokenizer_type:GPT2BPETokenizer
x3104c0s1b0n0: using world size: 8, data-parallel-size: 8, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
x3104c0s1b0n0: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
x3104c0s1b0n0: using torch.bfloat16 for parameters ...
x3104c0s1b0n0: ------------------------ arguments ------------------------
x3104c0s1b0n0:   accumulate_allreduce_grads_in_fp32 .............. True
x3104c0s1b0n0:   adam_beta1 ...................................... 0.9
x3104c0s1b0n0:   adam_beta2 ...................................... 0.95
x3104c0s1b0n0:   adam_eps ........................................ 1e-08
x3104c0s1b0n0:   add_bias_linear ................................. False
x3104c0s1b0n0:   add_position_embedding .......................... False
x3104c0s1b0n0:   adlr_autoresume ................................. False
x3104c0s1b0n0:   adlr_autoresume_interval ........................ 1000
x3104c0s1b0n0:   aml_data_download_path .......................... None
x3104c0s1b0n0:   apply_layernorm_1p .............................. False
x3104c0s1b0n0:   apply_query_key_layer_scaling ................... False
x3104c0s1b0n0:   apply_residual_connection_post_layernorm ........ False
x3104c0s1b0n0:   async_tensor_model_parallel_allreduce ........... False
x3104c0s1b0n0:   attention_dropout ............................... 0.0
x3104c0s1b0n0:   attention_softmax_in_fp32 ....................... False
x3104c0s1b0n0:   barrier_with_L1_time ............................ True
x3104c0s1b0n0:   bert_binary_head ................................ True
x3104c0s1b0n0:   bert_embedder_type .............................. megatron
x3104c0s1b0n0:   bert_load ....................................... None
x3104c0s1b0n0:   bf16 ............................................ True
x3104c0s1b0n0:   bias_dropout_fusion ............................. True
x3104c0s1b0n0:   bias_gelu_fusion ................................ False
x3104c0s1b0n0:   biencoder_projection_dim ........................ 0
x3104c0s1b0n0:   biencoder_shared_query_context_model ............ False
x3104c0s1b0n0:   block_data_path ................................. None
x3104c0s1b0n0:   checkpoint_activations .......................... True
x3104c0s1b0n0:   checkpoint_in_cpu ............................... False
x3104c0s1b0n0:   checkpoint_num_layers ........................... 1
x3104c0s1b0n0:   classes_fraction ................................ 1.0
x3104c0s1b0n0:   clip_grad ....................................... 1.0
x3104c0s1b0n0:   compression_training ............................ False
x3104c0s1b0n0:   consumed_train_samples .......................... 0
x3104c0s1b0n0:   consumed_train_tokens ........................... 0
x3104c0s1b0n0:   consumed_valid_samples .......................... 0
x3104c0s1b0n0:   contigious_checkpointing ........................ False
x3104c0s1b0n0:   cpu_optimizer ................................... True
x3104c0s1b0n0:   cpu_torch_adam .................................. False
x3104c0s1b0n0:   create_moe_param_group .......................... False
x3104c0s1b0n0:   curriculum_learning_legacy ...................... False
x3104c0s1b0n0:   data_cache_path ................................. None
x3104c0s1b0n0:   data_efficiency_curriculum_learning ............. False
x3104c0s1b0n0:   data_impl ....................................... mmap
x3104c0s1b0n0:   data_parallel_random_init ....................... False
x3104c0s1b0n0:   data_parallel_size .............................. 8
x3104c0s1b0n0:   data_path ....................................... ['/home/am6429/dl-io/datasets/meg-gpt2_text_document']
x3104c0s1b0n0:   data_per_class_fraction ......................... 1.0
x3104c0s1b0n0:   data_sharding ................................... True
x3104c0s1b0n0:   dataloader_type ................................. single
x3104c0s1b0n0:   DDP_impl ........................................ local
x3104c0s1b0n0:   decoder_num_layers .............................. None
x3104c0s1b0n0:   decoder_seq_length .............................. None
x3104c0s1b0n0:   deepscale ....................................... False
x3104c0s1b0n0:   deepscale_config ................................ None
x3104c0s1b0n0:   deepspeed ....................................... True
x3104c0s1b0n0:   deepspeed_activation_checkpointing .............. True
x3104c0s1b0n0:   deepspeed_config ................................ /home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json
x3104c0s1b0n0:   dino_bottleneck_size ............................ 256
x3104c0s1b0n0:   dino_freeze_last_layer .......................... 1
x3104c0s1b0n0:   dino_head_hidden_size ........................... 2048
x3104c0s1b0n0:   dino_local_crops_number ......................... 10
x3104c0s1b0n0:   dino_local_img_size ............................. 96
x3104c0s1b0n0:   dino_norm_last_layer ............................ False
x3104c0s1b0n0:   dino_teacher_temp ............................... 0.07
x3104c0s1b0n0:   dino_warmup_teacher_temp ........................ 0.04
x3104c0s1b0n0:   dino_warmup_teacher_temp_epochs ................. 30
x3104c0s1b0n0:   distribute_checkpointed_activations ............. False
x3104c0s1b0n0:   distribute_saved_activations .................... False
x3104c0s1b0n0:   distributed_backend ............................. nccl
x3104c0s1b0n0:   distributed_timeout_minutes ..................... 10
x3104c0s1b0n0:   ds_inference .................................... False
x3104c0s1b0n0:   ds_pipeline_enabled ............................. False
x3104c0s1b0n0:   ds_sequence_parallel_size ....................... 1
x3104c0s1b0n0:   embedding_path .................................. None
x3104c0s1b0n0:   embedding_weights_in_fp32 ....................... False
x3104c0s1b0n0:   empty_unused_memory_level ....................... 0
x3104c0s1b0n0:   enable_expert_tensor_parallelism ................ False
x3104c0s1b0n0:   encoder_num_layers .............................. 60
x3104c0s1b0n0:   encoder_seq_length .............................. 2048
x3104c0s1b0n0:   end_weight_decay ................................ 0.1
x3104c0s1b0n0:   eod_mask_loss ................................... False
x3104c0s1b0n0:   eval_interval ................................... 1000
x3104c0s1b0n0:   eval_iters ...................................... 0
x3104c0s1b0n0:   evidence_data_path .............................. None
x3104c0s1b0n0:   exit_duration_in_mins ........................... None
x3104c0s1b0n0:   exit_interval ................................... 20
x3104c0s1b0n0:   exit_on_missing_checkpoint ...................... False
x3104c0s1b0n0:   exit_signal_handler ............................. False
x3104c0s1b0n0:   expert_interval ................................. 2
x3104c0s1b0n0:   ffn_hidden_size ................................. 17728
x3104c0s1b0n0:   finetune ........................................ False
x3104c0s1b0n0:   force_ds_sequence_parallel ...................... False
x3104c0s1b0n0:   fp16 ............................................ False
x3104c0s1b0n0:   fp16_lm_cross_entropy ........................... False
x3104c0s1b0n0:   fp32_residual_connection ........................ False
x3104c0s1b0n0:   fp8_amax_compute_algo ........................... most_recent
x3104c0s1b0n0:   fp8_amax_history_len ............................ 1
x3104c0s1b0n0:   fp8_e4m3 ........................................ False
x3104c0s1b0n0:   fp8_hybrid ...................................... False
x3104c0s1b0n0:   fp8_interval .................................... 1
x3104c0s1b0n0:   fp8_margin ...................................... 0
x3104c0s1b0n0:   fp8_wgrad ....................................... True
x3104c0s1b0n0:   global_batch_size ............................... 32
x3104c0s1b0n0:   gradient_accumulation_fusion .................... True
x3104c0s1b0n0:   head_lr_mult .................................... 1.0
x3104c0s1b0n0:   hidden_dropout .................................. 0.0
x3104c0s1b0n0:   hidden_size ..................................... 6656
x3104c0s1b0n0:   hidden_size_teacher ............................. None
x3104c0s1b0n0:   hysteresis ...................................... 2
x3104c0s1b0n0:   ict_head_size ................................... None
x3104c0s1b0n0:   ict_load ........................................ None
x3104c0s1b0n0:   img_h ........................................... 224
x3104c0s1b0n0:   img_w ........................................... 224
x3104c0s1b0n0:   indexer_batch_size .............................. 128
x3104c0s1b0n0:   indexer_log_interval ............................ 1000
x3104c0s1b0n0:   inference ....................................... False
x3104c0s1b0n0:   inference_batch_times_seqlen_threshold .......... 512
x3104c0s1b0n0:   init_method_std ................................. 0.02
x3104c0s1b0n0:   init_method_xavier_uniform ...................... False
x3104c0s1b0n0:   initial_loss_scale .............................. 4294967296
x3104c0s1b0n0:   iter_per_epoch .................................. 1250
x3104c0s1b0n0:   kd .............................................. False
x3104c0s1b0n0:   kd_alpha_ce ..................................... 1
x3104c0s1b0n0:   kd_beta_ce ...................................... 1
x3104c0s1b0n0:   kd_temp ......................................... 1.0
x3104c0s1b0n0:   kv_channels ..................................... 128
x3104c0s1b0n0:   layernorm_epsilon ............................... 1e-05
x3104c0s1b0n0:   lazy_mpu_init ................................... None
x3104c0s1b0n0:   load ............................................ None
x3104c0s1b0n0:   load_teacher .................................... None
x3104c0s1b0n0:   local_rank ...................................... 0
x3104c0s1b0n0:   log_batch_size_to_tensorboard ................... False
x3104c0s1b0n0:   log_interval .................................... 1
x3104c0s1b0n0:   log_learning_rate_to_tensorboard ................ True
x3104c0s1b0n0:   log_loss_scale_to_tensorboard ................... True
x3104c0s1b0n0:   log_memory_to_tensorboard ....................... False
x3104c0s1b0n0:   log_num_zeros_in_grad ........................... False
x3104c0s1b0n0:   log_optimizer_states_to_tensorboard ............. False
x3104c0s1b0n0:   log_params_norm ................................. False
x3104c0s1b0n0:   log_timers_to_tensorboard ....................... False
x3104c0s1b0n0:   log_validation_ppl_to_tensorboard ............... False
x3104c0s1b0n0:   log_world_size_to_tensorboard ................... False
x3104c0s1b0n0:   loss_scale ...................................... None
x3104c0s1b0n0:   loss_scale_window ............................... 1000
x3104c0s1b0n0:   lr .............................................. 0.0003
x3104c0s1b0n0:   lr_decay_iters .................................. None
x3104c0s1b0n0:   lr_decay_samples ................................ None
x3104c0s1b0n0:   lr_decay_style .................................. cosine
x3104c0s1b0n0:   lr_decay_tokens ................................. None
x3104c0s1b0n0:   lr_warmup_fraction .............................. None
x3104c0s1b0n0:   lr_warmup_iters ................................. 1
x3104c0s1b0n0:   lr_warmup_samples ............................... 0
x3104c0s1b0n0:   lr_warmup_tokens ................................ None
x3104c0s1b0n0:   make_vocab_size_divisible_by .................... 128
x3104c0s1b0n0:   mask_factor ..................................... 1.0
x3104c0s1b0n0:   mask_prob ....................................... 0.15
x3104c0s1b0n0:   mask_type ....................................... random
x3104c0s1b0n0:   masked_softmax_fusion ........................... True
x3104c0s1b0n0:   max_position_embeddings ......................... 2048
x3104c0s1b0n0:   max_tokens_to_oom ............................... 12000
x3104c0s1b0n0:   memory_centric_tiled_linear ..................... False
x3104c0s1b0n0:   merge_file ...................................... /home/am6429/dl-io/datasets/gpt2-merges.txt
x3104c0s1b0n0:   micro_batch_size ................................ 4
x3104c0s1b0n0:   min_loss_scale .................................. 1.0
x3104c0s1b0n0:   min_lr .......................................... 3e-05
x3104c0s1b0n0:   mlp_type ........................................ standard
x3104c0s1b0n0:   mmap_warmup ..................................... False
x3104c0s1b0n0:   moe_eval_capacity_factor ........................ 1.0
x3104c0s1b0n0:   moe_expert_parallel_size ........................ 1
x3104c0s1b0n0:   moe_loss_coeff .................................. 0.1
x3104c0s1b0n0:   moe_min_capacity ................................ 4
x3104c0s1b0n0:   moe_token_dropping .............................. True
x3104c0s1b0n0:   moe_train_capacity_factor ....................... 1.0
x3104c0s1b0n0:   mos ............................................. False
x3104c0s1b0n0:   no_load_lr_state ................................ False
x3104c0s1b0n0:   no_load_optim ................................... None
x3104c0s1b0n0:   no_load_rng ..................................... None
x3104c0s1b0n0:   no_persist_layer_norm ........................... False
x3104c0s1b0n0:   no_pipeline_parallel ............................ True
x3104c0s1b0n0:   no_save_optim ................................... None
x3104c0s1b0n0:   no_save_rng ..................................... None
x3104c0s1b0n0:   normalization ................................... rmsnorm
x3104c0s1b0n0:   num_attention_heads ............................. 52
x3104c0s1b0n0:   num_attention_heads_teacher ..................... None
x3104c0s1b0n0:   num_channels .................................... 3
x3104c0s1b0n0:   num_classes ..................................... 1000
x3104c0s1b0n0:   num_experts ..................................... [1]
x3104c0s1b0n0:   num_experts_switch .............................. None
x3104c0s1b0n0:   num_experts_teacher ............................. [1]
x3104c0s1b0n0:   num_key_value_heads ............................. 4
x3104c0s1b0n0:   num_layers ...................................... 60
x3104c0s1b0n0:   num_layers_per_virtual_pipeline_stage ........... None
x3104c0s1b0n0:   num_layers_teacher .............................. None
x3104c0s1b0n0:   num_workers ..................................... 2
x3104c0s1b0n0:   onnx_safe ....................................... None
x3104c0s1b0n0:   openai_gelu ..................................... False
x3104c0s1b0n0:   optimizer ....................................... adam
x3104c0s1b0n0:   output_bert_embeddings .......................... False
x3104c0s1b0n0:   overlap_p2p_comm ................................ False
x3104c0s1b0n0:   override_opt_param_scheduler .................... False
x3104c0s1b0n0:   params_dtype .................................... torch.bfloat16
x3104c0s1b0n0:   partition_activations ........................... False
x3104c0s1b0n0:   patch_dim ....................................... 16
x3104c0s1b0n0:   perform_initialization .......................... True
x3104c0s1b0n0:   pipeline_model_parallel_size .................... 1
x3104c0s1b0n0:   pipeline_model_parallel_split_rank .............. None
x3104c0s1b0n0:   profile_backward ................................ False
x3104c0s1b0n0:   query_in_block_prob ............................. 0.1
x3104c0s1b0n0:   rampup_batch_size ............................... None
x3104c0s1b0n0:   random_ltd ...................................... False
x3104c0s1b0n0:   rank ............................................ 0
x3104c0s1b0n0:   recompute_granularity ........................... None
x3104c0s1b0n0:   recompute_method ................................ None
x3104c0s1b0n0:   recompute_num_layers ............................ 1
x3104c0s1b0n0:   remote_device ................................... none
x3104c0s1b0n0:   reset_attention_mask ............................ False
x3104c0s1b0n0:   reset_iteration ................................. False
x3104c0s1b0n0:   reset_position_ids .............................. False
x3104c0s1b0n0:   retriever_report_topk_accuracies ................ []
x3104c0s1b0n0:   retriever_score_scaling ......................... False
x3104c0s1b0n0:   retriever_seq_length ............................ 256
x3104c0s1b0n0:   retro_add_retriever ............................. False
x3104c0s1b0n0:   retro_cyclic_train_iters ........................ None
x3104c0s1b0n0:   retro_encoder_attention_dropout ................. 0.1
x3104c0s1b0n0:   retro_encoder_hidden_dropout .................... 0.1
x3104c0s1b0n0:   retro_encoder_layers ............................ 2
x3104c0s1b0n0:   retro_num_neighbors ............................. 2
x3104c0s1b0n0:   retro_num_retrieved_chunks ...................... 2
x3104c0s1b0n0:   retro_return_doc_ids ............................ False
x3104c0s1b0n0:   retro_workdir ................................... None
x3104c0s1b0n0:   return_data_index ............................... False
x3104c0s1b0n0:   rotary_percent .................................. 1.0
x3104c0s1b0n0:   sample_rate ..................................... 1.0
x3104c0s1b0n0:   save ............................................ /local/scratch/llama2/zero3-tp1}_dp8
x3104c0s1b0n0:   save_interval ................................... 1000
x3104c0s1b0n0:   scatter_gather_tensors_in_pipeline .............. True
x3104c0s1b0n0:   scattered_embeddings ............................ False
x3104c0s1b0n0:   seed ............................................ 1234
x3104c0s1b0n0:   seq_length ...................................... 2048
x3104c0s1b0n0:   sequence_parallel ............................... False
x3104c0s1b0n0:   sgd_momentum .................................... 0.9
x3104c0s1b0n0:   short_seq_prob .................................. 0.1
x3104c0s1b0n0:   skip_train ...................................... False
x3104c0s1b0n0:   split ........................................... 949,50,1
x3104c0s1b0n0:   split_transformers .............................. False
x3104c0s1b0n0:   squared_relu .................................... False
x3104c0s1b0n0:   standalone_embedding_stage ...................... False
x3104c0s1b0n0:   start_weight_decay .............................. 0.1
x3104c0s1b0n0:   swiglu .......................................... True
x3104c0s1b0n0:   swin_backbone_type .............................. tiny
x3104c0s1b0n0:   synchronize_each_layer .......................... False
x3104c0s1b0n0:   tensor_model_parallel_size ...................... 1
x3104c0s1b0n0:   tensorboard_dir ................................. None
x3104c0s1b0n0:   tensorboard_log_interval ........................ 1
x3104c0s1b0n0:   tensorboard_queue_size .......................... 1000
x3104c0s1b0n0:   test_data_path .................................. None
x3104c0s1b0n0:   tile_factor ..................................... 1
x3104c0s1b0n0:   timing_log_level ................................ 0
x3104c0s1b0n0:   timing_log_option ............................... minmax
x3104c0s1b0n0:   titles_data_path ................................ None
x3104c0s1b0n0:   tokenizer_model ................................. /home/am6429/dl-io/datasets/tokenizer.model
x3104c0s1b0n0:   tokenizer_type .................................. GPT2BPETokenizer
x3104c0s1b0n0:   topk ............................................ 1
x3104c0s1b0n0:   train_data_exact_num_epochs ..................... None
x3104c0s1b0n0:   train_data_path ................................. None
x3104c0s1b0n0:   train_desc_path ................................. None
x3104c0s1b0n0:   train_doc_idx_path .............................. None
x3104c0s1b0n0:   train_idx_path .................................. None
x3104c0s1b0n0:   train_iters ..................................... 10
x3104c0s1b0n0:   train_sample_idx_path ........................... None
x3104c0s1b0n0:   train_samples ................................... None
x3104c0s1b0n0:   train_shuffle_idx_path .......................... None
x3104c0s1b0n0:   train_tokens .................................... None
x3104c0s1b0n0:   transformer_impl ................................ local
x3104c0s1b0n0:   transformer_pipeline_model_parallel_size ........ 1
x3104c0s1b0n0:   untie_embeddings_and_output_weights ............. True
x3104c0s1b0n0:   use_checkpoint_args ............................. False
x3104c0s1b0n0:   use_checkpoint_opt_param_scheduler .............. False
x3104c0s1b0n0:   use_contiguous_buffers_in_local_ddp ............. True
x3104c0s1b0n0:   use_cpu_initialization .......................... None
x3104c0s1b0n0:   use_dataset_only ................................ False
x3104c0s1b0n0:   use_distributed_optimizer ....................... False
x3104c0s1b0n0:   use_flash_attn .................................. False
x3104c0s1b0n0:   use_flash_attn_triton ........................... False
x3104c0s1b0n0:   use_flash_attn_v1 ............................... False
x3104c0s1b0n0:   use_flash_attn_v2 ............................... False
x3104c0s1b0n0:   use_one_sent_docs ............................... False
x3104c0s1b0n0:   use_pin_memory .................................. False
x3104c0s1b0n0:   use_ring_exchange_p2p ........................... False
x3104c0s1b0n0:   use_rotary_position_embeddings .................. True
x3104c0s1b0n0:   use_tutel ....................................... False
x3104c0s1b0n0:   valid_data_path ................................. None
x3104c0s1b0n0:   variable_seq_lengths ............................ False
x3104c0s1b0n0:   virtual_pipeline_model_parallel_size ............ None
x3104c0s1b0n0:   vision_backbone_type ............................ vit
x3104c0s1b0n0:   vision_pretraining .............................. False
x3104c0s1b0n0:   vision_pretraining_type ......................... classify
x3104c0s1b0n0:   vocab_extra_ids ................................. 0
x3104c0s1b0n0:   vocab_file ...................................... /home/am6429/dl-io/datasets/gpt2-vocab.json
x3104c0s1b0n0:   vocab_size ...................................... None
x3104c0s1b0n0:   weight_decay .................................... 0.1
x3104c0s1b0n0:   weight_decay_incr_style ......................... constant
x3104c0s1b0n0:   world_size ...................................... 8
x3104c0s1b0n0:   zero_allgather_bucket_size ...................... 0.0
x3104c0s1b0n0:   zero_contigious_gradients ....................... False
x3104c0s1b0n0:   zero_reduce_bucket_size ......................... 0.0
x3104c0s1b0n0:   zero_reduce_scatter ............................. False
x3104c0s1b0n0:   zero_stage ...................................... 3
x3104c0s1b0n0: -------------------- end of arguments ---------------------
x3104c0s1b0n0: setting number of micro-batches to constant 1
x3104c0s1b0n0: > building GPT2BPETokenizer tokenizer ...
x3104c0s1b0n0: [2024-03-29 15:57:34,721] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
x3104c0s1b0n0: > initializing torch distributed ...
x3104c0s1b0n0: [2024-03-29 15:57:34,736] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0: [2024-03-29 15:57:34,736] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
x3104c0s25b0n0: [2024-03-29 15:57:36,892] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s25b0n0: [2024-03-29 15:57:41,307] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3104c0s25b0n0: [2024-03-29 15:57:41,307] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=1
x3104c0s25b0n0: [2024-03-29 15:57:41,307] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3104c0s25b0n0: [2024-03-29 15:57:41,307] [INFO] [launch.py:163:main] dist_world_size=8
x3104c0s25b0n0: [2024-03-29 15:57:41,307] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3104c0s25b0n0: [2024-03-29 15:57:41,308] [INFO] [launch.py:253:main] process 22121 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s25b0n0: [2024-03-29 15:57:41,308] [INFO] [launch.py:253:main] process 22122 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s25b0n0: [2024-03-29 15:57:41,309] [INFO] [launch.py:253:main] process 22123 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s25b0n0: [2024-03-29 15:57:41,309] [INFO] [launch.py:253:main] process 22124 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s25b0n0: [2024-03-29 15:57:43,463] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s25b0n0: [2024-03-29 15:57:43,468] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s25b0n0: [2024-03-29 15:57:43,469] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s25b0n0: [2024-03-29 15:57:43,470] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s25b0n0: ----------------------------------------------------------------------------------------------------
x3104c0s25b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s25b0n0: 
x3104c0s25b0n0: DeepSpeed C++/CUDA extension op report--------------------------------------------------
x3104c0s25b0n0: 
x3104c0s25b0n0: ----------------------------------------------------------------------------------------------------NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s25b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s25b0n0:       meet the required dependencies to JIT install the op.
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s25b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s25b0n0:       meet the required dependencies to JIT install the op.DeepSpeed C++/CUDA extension op report--------------------------------------------------
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0: ----------------------------------------------------------------------------------------------------JIT compiled ops requires ninja
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0: JIT compiled ops requires ninjaNOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s25b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s25b0n0:       meet the required dependencies to JIT install the op.
x3104c0s25b0n0: 
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: JIT compiled ops requires ninja
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s25b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s25b0n0:       meet the required dependencies to JIT install the op.
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: JIT compiled ops requires ninja
x3104c0s25b0n0: ninja .................. ninjaninja[92m[OKAY][0mninja 
x3104c0s25b0n0:  .................. ..................  --------------------------------------------------..................[92m[OKAY][0m
x3104c0s25b0n0: [92m[OKAY][0m 
x3104c0s25b0n0: 
x3104c0s25b0n0: op name[92m[OKAY][0m---------------------------------------------------------------------------------------------------- 
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0: ................op name-------------------------------------------------- installedop name  
x3104c0s25b0n0: ................ ................op name..    installedinstalledcompatible 
x3104c0s25b0n0: ................ ..-------------------------------------------------- .. installed
x3104c0s25b0n0:  compatiblecompatible 
x3104c0s25b0n0: 
x3104c0s25b0n0: ..-------------------------------------------------- --------------------------------------------------async_copier
x3104c0s25b0n0: 
x3104c0s25b0n0: compatible 
x3104c0s25b0n0: ........... --------------------------------------------------[92m[YES][0m
x3104c0s25b0n0:  async_copierasync_copier ...... ......................  async_copier[92m[OKAY][0m[92m[YES][0m  
x3104c0s25b0n0: ........... [92m[YES][0m ...... [92m[YES][0m ...... [92m[OKAY][0m ......
x3104c0s25b0n0: [92m[OKAY][0m [92m[OKAY][0m
x3104c0s25b0n0: 
x3104c0s25b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: async_iocpu_lion  ..............................  [93m[NO][0m[93m[NO][0m  ..............  [92m[OKAY][0m[92m[OKAY][0m
x3104c0s25b0n0: 
x3104c0s25b0n0: fused_adam [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH.............
x3104c0s25b0n0:  [93m[NO][0mevoformer_attn  ................  [92m[OKAY][0m[93m[NO][0m
x3104c0s25b0n0:  ....... [93m[NO][0mcpu_adam
x3104c0s25b0n0:  ...............fused_lamb  [93m[NO][0m.............  .......[93m[NO][0m  [92m[OKAY][0m.......
x3104c0s25b0n0:  [92m[OKAY][0m
x3104c0s25b0n0: cpu_adagrad fused_lion............  .............[93m[NO][0m  [93m[NO][0m.......  .......[92m[OKAY][0m 
x3104c0s25b0n0: [92m[OKAY][0m
x3104c0s25b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATHasync_io
x3104c0s25b0n0:  evoformer_attn...............  [93m[NO][0m.........  .......[93m[NO][0m  [92m[OKAY][0m.......
x3104c0s25b0n0:  [93m[NO][0m
x3104c0s25b0n0: fused_adamfused_lamb  ..........................  [93m[NO][0m[93m[NO][0m .......  .......[92m[OKAY][0m 
x3104c0s25b0n0: [92m[OKAY][0m
x3104c0s25b0n0: cpu_adamfused_lion  ............... .............[93m[NO][0m  [93m[NO][0m.......  .......[92m[OKAY][0m 
x3104c0s25b0n0: [92m[OKAY][0m
x3104c0s25b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s25b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s25b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s25b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s25b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s25b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s25b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: DeepSpeed general environment info:
x3104c0s25b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s25b0n0: torch version .................... 2.0.1+cu118
x3104c0s25b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s25b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s25b0n0: torch cuda version ............... 11.8
x3104c0s25b0n0: torch hip version ................ None
x3104c0s25b0n0: nvcc version ..................... 11.8
x3104c0s25b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s25b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s25b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: DeepSpeed general environment info:
x3104c0s25b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s25b0n0: torch version .................... 2.0.1+cu118
x3104c0s25b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s25b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s25b0n0: torch cuda version ............... 11.8
x3104c0s25b0n0: torch hip version ................ None
x3104c0s25b0n0: nvcc version ..................... 11.8
x3104c0s25b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s25b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s25b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s25b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s25b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s25b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s25b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed general environment info:
x3104c0s25b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s25b0n0: torch version .................... 2.0.1+cu118
x3104c0s25b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s25b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offloadDeepSpeed general environment info:
x3104c0s25b0n0: torch cuda version
x3104c0s25b0n0:  ............... torch install path11.8
x3104c0s25b0n0:  torch hip version...............  ................ None
x3104c0s25b0n0: ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']nvcc version
x3104c0s25b0n0:  ..................... torch version11.8 
x3104c0s25b0n0: ....................deepspeed wheel compiled w.  2.0.1+cu118......
x3104c0s25b0n0:  torch 2.0, cuda 11.8deepspeed install path
x3104c0s25b0n0:  shared memory (/dev/shm) size...........  .... 251.61 GB['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s25b0n0: 
x3104c0s25b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s25b0n0: torch cuda version ............... 11.8
x3104c0s25b0n0: torch hip version ................ None
x3104c0s25b0n0: nvcc version ..................... 11.8
x3104c0s25b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s25b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s25b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s25b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s25b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s25b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s25b0n0: [2024-03-29 15:57:47,036] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s25b0n0: [2024-03-29 15:57:47,036] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s25b0n0: [2024-03-29 15:57:47,036] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s25b0n0: [2024-03-29 15:57:47,036] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0: > initialized tensor model parallel with size 1
x3104c0s1b0n0: > initialized pipeline model parallel with size 1
x3104c0s1b0n0: > setting random seeds to 1234 ...
x3104c0s1b0n0: [2024-03-29 15:57:47,502] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
x3104c0s1b0n0: > compiling dataset index builder ...
x3104c0s1b0n0: make: Entering directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3104c0s1b0n0: make: Nothing to be done for 'default'.
x3104c0s1b0n0: make: Leaving directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3104c0s1b0n0: >>> done with dataset index builder. Compilation time: 0.085 seconds
x3104c0s1b0n0: > compiling and loading fused kernels ...
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: >>> done with compiling and loading fused kernels. Compilation time: 3.969 seconds
x3104c0s1b0n0: <<<<<<<<<<< 1
x3104c0s1b0n0: <<<<<<<<<<< 2
x3104c0s1b0n0: initialize_megatron took 17.65894365310669
x3104c0s1b0n0: <<<<<<<<<<< 0
x3104c0s1b0n0: <<<<<<<<<<< 3
x3104c0s25b0n0: <<<<<<<<<<< 5
x3104c0s25b0n0: <<<<<<<<<<< 7
x3104c0s25b0n0: <<<<<<<<<<< 6
x3104c0s25b0n0: <<<<<<<<<<< 4
x3104c0s1b0n0: time to initialize megatron (seconds): 18.987
x3104c0s1b0n0: [after megatron is initialized] datetime: 2024-03-29 15:57:53 
x3104c0s1b0n0: get_accelerator and all_reduce  took 0.7660136222839355
x3104c0s1b0n0: building GPT model ...
x3104c0s1b0n0: [2024-03-29 15:57:53,183] [INFO] [utils.py:800:see_memory_usage] Before Building Model
x3104c0s1b0n0: [2024-03-29 15:57:53,184] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 4.62 GB         CA 0.0 GB         Max_CA 5 GB 
x3104c0s1b0n0: [2024-03-29 15:57:53,184] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 17.85 GB, percent = 3.5%
x3104c0s1b0n0: [2024-03-29 15:57:59,934] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 423, num_elems = 27.64B
x3104c0s1b0n0: [2024-03-29 15:57:59,996] [INFO] [utils.py:800:see_memory_usage] After Building Model
x3104c0s1b0n0: [2024-03-29 15:57:59,997] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 7.07 GB         CA 22.31 GB         Max_CA 38 GB 
x3104c0s1b0n0: [2024-03-29 15:57:59,997] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 18.26 GB, percent = 3.6%
x3104c0s1b0n0:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 27635239424
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: Time to load cpu_adam op: 2.446131944656372 seconds
x3104c0s25b0n0: ninja: no work to do.
x3104c0s25b0n0: Time to load cpu_adam op: 2.296816825866699 seconds
x3104c0s25b0n0: ninja: no work to do.
x3104c0s25b0n0: Time to load cpu_adam op: 2.2834506034851074 seconds
x3104c0s25b0n0: ninja: no work to do.
x3104c0s25b0n0: Time to load cpu_adam op: 2.294057607650757 seconds
x3104c0s1b0n0: Time to load cpu_adam op: 2.4801015853881836 seconds
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: Time to load cpu_adam op: 2.4682960510253906 seconds
x3104c0s25b0n0: Time to load cpu_adam op: 2.441697597503662 seconds
x3104c0s1b0n0: Time to load cpu_adam op: 2.639706611633301 seconds
x3104c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: > learning rate decay style: cosine
x3104c0s1b0n0: DeepSpeed is enabled.
x3104c0s1b0n0: [2024-03-29 15:58:04,398] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.3+8074cd62, git-hash=8074cd62, git-branch=hybrid_opt_offload
x3104c0s1b0n0: [2024-03-29 15:58:04,469] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After args sanity test
x3104c0s1b0n0: [2024-03-29 15:58:04,470] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.31 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:58:04,470] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 23.77 GB, percent = 4.7%
x3104c0s1b0n0: [2024-03-29 15:58:04,528] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure distributed model
x3104c0s1b0n0: [2024-03-29 15:58:04,529] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.31 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:58:04,529] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 24.1 GB, percent = 4.8%
x3104c0s1b0n0: [2024-03-29 15:58:04,590] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After configure distributed model
x3104c0s1b0n0: [2024-03-29 15:58:04,591] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.31 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:58:04,591] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 24.37 GB, percent = 4.8%
x3104c0s1b0n0: [2024-03-29 15:58:04,591] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
x3104c0s1b0n0: [2024-03-29 15:58:04,645] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After setting model parameters
x3104c0s1b0n0: [2024-03-29 15:58:04,645] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.31 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:58:04,645] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 24.64 GB, percent = 4.9%
x3104c0s1b0n0: [2024-03-29 15:58:04,699] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure optimizer
x3104c0s1b0n0: [2024-03-29 15:58:04,700] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.31 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:58:04,700] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 24.89 GB, percent = 4.9%
x3104c0s1b0n0: [2024-03-29 15:58:04,701] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
x3104c0s1b0n0: [2024-03-29 15:58:04,701] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
x3104c0s1b0n0: [2024-03-29 15:58:04,718] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
x3104c0s1b0n0: [2024-03-29 15:58:04,718] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
x3104c0s1b0n0: [2024-03-29 15:58:04,719] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
x3104c0s1b0n0: [2024-03-29 15:58:04,719] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
x3104c0s1b0n0: [2024-03-29 15:58:04,772] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
x3104c0s1b0n0: [2024-03-29 15:58:04,772] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.31 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:58:04,773] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.25 GB, percent = 5.0%
x3104c0s1b0n0: [2024-03-29 15:58:04,774] [INFO] [stage3.py:137:__init__] Reduce bucket size 500,000,000
x3104c0s1b0n0: [2024-03-29 15:58:04,774] [INFO] [stage3.py:138:__init__] Prefetch bucket size 50,000,000
x3104c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: [2024-03-29 15:58:04,828] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
x3104c0s1b0n0: [2024-03-29 15:58:04,829] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.31 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:58:04,829] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.48 GB, percent = 5.1%
x3104c0s1b0n0: Parameter Offload: Total persistent parameters: 805376 in 121 params
x3104c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: [2024-03-29 15:58:04,905] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
x3104c0s1b0n0: [2024-03-29 15:58:04,906] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.31 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:58:04,906] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.7 GB, percent = 5.1%
x3104c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: [2024-03-29 15:58:04,959] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
x3104c0s1b0n0: [2024-03-29 15:58:04,960] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.31 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:58:04,960] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.71 GB, percent = 5.1%
x3104c0s25b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s25b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s25b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s25b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s25b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s25b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s25b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s25b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s25b0n0: [2024-03-29 15:58:05,354] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s1b0n0: [2024-03-29 15:58:05,354] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s25b0n0: [2024-03-29 15:58:05,354] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s25b0n0: [2024-03-29 15:58:05,354] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s25b0n0: [2024-03-29 15:58:05,354] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s1b0n0: [2024-03-29 15:58:05,354] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s1b0n0: [2024-03-29 15:58:05,354] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s1b0n0: [2024-03-29 15:58:05,354] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s1b0n0: [2024-03-29 15:58:05,354] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s1b0n0: [2024-03-29 15:58:05,354] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s1b0n0: [2024-03-29 15:58:05,354] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s1b0n0: [2024-03-29 15:58:05,354] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s25b0n0: [2024-03-29 15:58:05,354] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s25b0n0: [2024-03-29 15:58:05,354] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s25b0n0: [2024-03-29 15:58:05,354] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s25b0n0: [2024-03-29 15:58:05,354] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s1b0n0: [2024-03-29 15:58:05,354] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,354] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,354] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,354] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,354] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,354] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,354] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,354] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,355] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,356] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,357] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,358] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s25b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s25b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s25b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s1b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s1b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s1b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s1b0n0: [2024-03-29 15:58:05,359] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s1b0n0: [2024-03-29 15:58:07,733] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 31
x3104c0s1b0n0: [2024-03-29 15:58:07,734] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.44 GB         CA 6.44 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:58:07,734] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 45.05 GB, percent = 9.0%
x3104c0s1b0n0: [2024-03-29 15:58:07,895] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
x3104c0s1b0n0: [2024-03-29 15:58:07,896] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3104c0s1b0n0: [2024-03-29 15:58:07,896] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 45.05 GB, percent = 9.0%
x3104c0s1b0n0: [2024-03-29 15:58:26,912] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
x3104c0s1b0n0: [2024-03-29 15:58:26,913] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3104c0s1b0n0: [2024-03-29 15:58:26,913] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 210.17 GB, percent = 41.8%
x3104c0s1b0n0: [2024-03-29 15:58:27,923] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
x3104c0s1b0n0: [2024-03-29 15:58:27,923] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3104c0s1b0n0: [2024-03-29 15:58:27,923] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 219.45 GB, percent = 43.6%
x3104c0s1b0n0: [2024-03-29 15:58:34,635] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | init_optimizer_state: 6698.09
x3104c0s1b0n0: [2024-03-29 15:58:34,708] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
x3104c0s1b0n0: [2024-03-29 15:58:34,708] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3104c0s1b0n0: [2024-03-29 15:58:34,708] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 279.19 GB, percent = 55.5%
x3104c0s1b0n0: [2024-03-29 15:58:34,737] [INFO] [stage3.py:520:_setup_for_real_optimizer] optimizer state initialized
x3104c0s1b0n0: [2024-03-29 15:58:43,268] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
x3104c0s1b0n0: [2024-03-29 15:58:43,268] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 8.61 GB         CA 22.75 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 15:58:43,268] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.88 GB, percent = 62.0%
x3104c0s1b0n0: [2024-03-29 15:58:43,268] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
x3104c0s1b0n0: [2024-03-29 15:58:43,331] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure LR scheduler
x3104c0s1b0n0: [2024-03-29 15:58:43,331] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 15:58:43,331] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.88 GB, percent = 62.0%
x3104c0s1b0n0: [2024-03-29 15:58:43,331] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
x3104c0s1b0n0: [2024-03-29 15:58:43,331] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7f01bd1a0280>
x3104c0s1b0n0: [2024-03-29 15:58:43,331] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 15:58:43,391] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before rewriting optimizer step
x3104c0s1b0n0: [2024-03-29 15:58:43,391] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 15:58:43,391] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.87 GB, percent = 62.0%
x3104c0s1b0n0: [2024-03-29 15:58:43,452] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure checkpointing
x3104c0s1b0n0: [2024-03-29 15:58:43,453] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 15:58:43,453] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.89 GB, percent = 62.0%
x3104c0s1b0n0: [2024-03-29 15:58:43,453] [INFO] [config.py:998:print] DeepSpeedEngine configuration:
x3104c0s1b0n0: [2024-03-29 15:58:43,453] [INFO] [config.py:1002:print]   activation_checkpointing_config  {
x3104c0s1b0n0:     "partition_activations": false, 
x3104c0s1b0n0:     "contiguous_memory_optimization": false, 
x3104c0s1b0n0:     "cpu_checkpointing": false, 
x3104c0s1b0n0:     "number_checkpoints": null, 
x3104c0s1b0n0:     "synchronize_checkpoint_boundary": false, 
x3104c0s1b0n0:     "profile": false
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 15:58:43,453] [INFO] [config.py:1002:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
x3104c0s1b0n0: [2024-03-29 15:58:43,453] [INFO] [config.py:1002:print]   amp_enabled .................. False
x3104c0s1b0n0: [2024-03-29 15:58:43,453] [INFO] [config.py:1002:print]   amp_params ................... False
x3104c0s1b0n0: [2024-03-29 15:58:43,453] [INFO] [config.py:1002:print]   autotuning_config ............ {
x3104c0s1b0n0:     "enabled": false, 
x3104c0s1b0n0:     "start_step": null, 
x3104c0s1b0n0:     "end_step": null, 
x3104c0s1b0n0:     "metric_path": null, 
x3104c0s1b0n0:     "arg_mappings": null, 
x3104c0s1b0n0:     "metric": "throughput", 
x3104c0s1b0n0:     "model_info": null, 
x3104c0s1b0n0:     "results_dir": "autotuning_results", 
x3104c0s1b0n0:     "exps_dir": "autotuning_exps", 
x3104c0s1b0n0:     "overwrite": true, 
x3104c0s1b0n0:     "fast": true, 
x3104c0s1b0n0:     "start_profile_step": 3, 
x3104c0s1b0n0:     "end_profile_step": 5, 
x3104c0s1b0n0:     "tuner_type": "gridsearch", 
x3104c0s1b0n0:     "tuner_early_stopping": 5, 
x3104c0s1b0n0:     "tuner_num_trials": 50, 
x3104c0s1b0n0:     "model_info_path": null, 
x3104c0s1b0n0:     "mp_size": 1, 
x3104c0s1b0n0:     "max_train_batch_size": null, 
x3104c0s1b0n0:     "min_train_batch_size": 1, 
x3104c0s1b0n0:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
x3104c0s1b0n0:     "min_train_micro_batch_size_per_gpu": 1, 
x3104c0s1b0n0:     "num_tuning_micro_batch_sizes": 3
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 15:58:43,453] [INFO] [config.py:1002:print]   bfloat16_enabled ............. True
x3104c0s1b0n0: [2024-03-29 15:58:43,453] [INFO] [config.py:1002:print]   bfloat16_immediate_grad_update  False
x3104c0s1b0n0: [2024-03-29 15:58:43,453] [INFO] [config.py:1002:print]   checkpoint_parallel_write_pipeline  False
x3104c0s1b0n0: [2024-03-29 15:58:43,453] [INFO] [config.py:1002:print]   checkpoint_tag_validation_enabled  True
x3104c0s1b0n0: [2024-03-29 15:58:43,453] [INFO] [config.py:1002:print]   checkpoint_tag_validation_fail  False
x3104c0s1b0n0: [2024-03-29 15:58:43,453] [INFO] [config.py:1002:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f01bd1a0be0>
x3104c0s1b0n0: [2024-03-29 15:58:43,453] [INFO] [config.py:1002:print]   communication_data_type ...... None
x3104c0s1b0n0: [2024-03-29 15:58:43,453] [INFO] [config.py:1002:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
x3104c0s1b0n0: [2024-03-29 15:58:43,453] [INFO] [config.py:1002:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
x3104c0s1b0n0: [2024-03-29 15:58:43,453] [INFO] [config.py:1002:print]   curriculum_enabled_legacy .... False
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   curriculum_params_legacy ..... False
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   data_efficiency_enabled ...... False
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   dataloader_drop_last ......... False
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   datastates_config ............ {
x3104c0s1b0n0:     "enabled": null, 
x3104c0s1b0n0:     "config": {
x3104c0s1b0n0:     }
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   disable_allgather ............ False
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   dump_state ................... False
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   dynamic_loss_scale_args ...... None
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   eigenvalue_enabled ........... False
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   eigenvalue_gas_boundary_resolution  1
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   eigenvalue_layer_name ........ bert.encoder.layer
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   eigenvalue_layer_num ......... 0
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   eigenvalue_max_iter .......... 100
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   eigenvalue_stability ......... 1e-06
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   eigenvalue_tol ............... 0.01
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   eigenvalue_verbose ........... False
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   elasticity_enabled ........... False
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   flops_profiler_config ........ {
x3104c0s1b0n0:     "enabled": false, 
x3104c0s1b0n0:     "recompute_fwd_factor": 0.0, 
x3104c0s1b0n0:     "profile_step": 1, 
x3104c0s1b0n0:     "module_depth": -1, 
x3104c0s1b0n0:     "top_modules": 1, 
x3104c0s1b0n0:     "detailed": true, 
x3104c0s1b0n0:     "output_file": null
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   fp16_auto_cast ............... None
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   fp16_enabled ................. False
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   fp16_master_weights_and_gradients  False
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   global_rank .................. 0
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   grad_accum_dtype ............. bf16
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   gradient_accumulation_steps .. 1
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   gradient_clipping ............ 0.0
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   gradient_predivide_factor .... 1.0
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   graph_harvesting ............. False
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   initial_dynamic_scale ........ 1
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   load_universal_checkpoint .... False
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   loss_scale ................... 1.0
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   memory_breakdown ............. True
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   mics_hierarchial_params_gather  False
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   mics_shard_size .............. -1
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   nebula_config ................ {
x3104c0s1b0n0:     "enabled": false, 
x3104c0s1b0n0:     "persistent_storage_path": null, 
x3104c0s1b0n0:     "persistent_time_interval": 100, 
x3104c0s1b0n0:     "num_of_version_in_retention": 2, 
x3104c0s1b0n0:     "enable_nebula_load": true, 
x3104c0s1b0n0:     "load_path": null
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   optimizer_legacy_fusion ...... False
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   optimizer_name ............... None
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   optimizer_params ............. None
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   pld_enabled .................. False
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   pld_params ................... False
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   prescale_gradients ........... False
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   scheduler_name ............... None
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   scheduler_params ............. None
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   seq_parallel_communication_data_type  torch.float32
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   sparse_attention ............. None
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   sparse_gradients_enabled ..... False
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   steps_per_print .............. 1
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   train_batch_size ............. 32
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   train_micro_batch_size_per_gpu  4
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   use_data_before_expert_parallel_  False
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   use_node_local_storage ....... False
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   wall_clock_breakdown ......... True
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   weight_quantization_config ... None
x3104c0s1b0n0: [2024-03-29 15:58:43,454] [INFO] [config.py:1002:print]   world_size ................... 8
x3104c0s1b0n0: [2024-03-29 15:58:43,455] [INFO] [config.py:1002:print]   zero_allow_untested_optimizer  False
x3104c0s1b0n0: [2024-03-29 15:58:43,455] [INFO] [config.py:1002:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0, prefetch_optimizer=True, part_grads_async=True, prefetch_optimizer_gap=6) sub_group_size=100000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
x3104c0s1b0n0: [2024-03-29 15:58:43,455] [INFO] [config.py:1002:print]   zero_enabled ................. True
x3104c0s1b0n0: [2024-03-29 15:58:43,455] [INFO] [config.py:1002:print]   zero_force_ds_cpu_optimizer .. True
x3104c0s1b0n0: [2024-03-29 15:58:43,455] [INFO] [config.py:1002:print]   zero_optimization_stage ...... 3
x3104c0s1b0n0: [2024-03-29 15:58:43,455] [INFO] [config.py:988:print_user_config]   json = {
x3104c0s1b0n0:     "train_batch_size": 32, 
x3104c0s1b0n0:     "train_micro_batch_size_per_gpu": 4, 
x3104c0s1b0n0:     "steps_per_print": 1, 
x3104c0s1b0n0:     "zero_optimization": {
x3104c0s1b0n0:         "stage": 3, 
x3104c0s1b0n0:         "offload_optimizer": {
x3104c0s1b0n0:             "device": "cpu", 
x3104c0s1b0n0:             "ratio": 1, 
x3104c0s1b0n0:             "pin_memory": true, 
x3104c0s1b0n0:             "prefetch_optimizer": 1, 
x3104c0s1b0n0:             "part_grads_async": 1, 
x3104c0s1b0n0:             "prefetch_optimizer_gap": 6
x3104c0s1b0n0:         }, 
x3104c0s1b0n0:         "sub_group_size": 1.000000e+08
x3104c0s1b0n0:     }, 
x3104c0s1b0n0:     "bf16": {
x3104c0s1b0n0:         "enabled": true
x3104c0s1b0n0:     }, 
x3104c0s1b0n0:     "data_types": {
x3104c0s1b0n0:         "grad_accum_dtype": "bf16"
x3104c0s1b0n0:     }, 
x3104c0s1b0n0:     "wall_clock_breakdown": true, 
x3104c0s1b0n0:     "memory_breakdown": true, 
x3104c0s1b0n0:     "flops_profiler": {
x3104c0s1b0n0:         "enabled": false
x3104c0s1b0n0:     }
x3104c0s1b0n0: }
x3104c0s25b0n0: <TIMER:model-and-optimizer-setup,51.7336802482605>
x3104c0s25b0n0: <TIMER:model-and-optimizer-setup,51.7359082698822>
x3104c0s25b0n0: <TIMER:model-and-optimizer-setup,51.73593616485596>
x3104c0s1b0n0: <TIMER:model-and-optimizer-setup,51.735878705978394>
x3104c0s1b0n0: <TIMER:model-and-optimizer-setup,51.73592019081116>
x3104c0s25b0n0: <TIMER:model-and-optimizer-setup,51.73605298995972>
x3104c0s1b0n0: <TIMER:model-and-optimizer-setup,51.737507820129395>
x3104c0s1b0n0: <TIMER:model-and-optimizer-setup,51.7375910282135>
x3104c0s1b0n0: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-29 15:58:44 
x3104c0s1b0n0: > building train, validation, and test datasets ...
x3104c0s1b0n0:  > datasets target sizes (minimum size):
x3104c0s1b0n0:     train:      320
x3104c0s1b0n0:     validation: 0
x3104c0s1b0n0:     test:       0
x3104c0s1b0n0: > building train, validation, and test datasets for GPT ...
x3104c0s1b0n0: Single data path provided for train, valid & test
x3104c0s1b0n0:  > building dataset index ...
x3104c0s1b0n0:     reading sizes...
x3104c0s1b0n0:     reading pointers...
x3104c0s1b0n0:     reading document index...
x3104c0s1b0n0:     creating numpy buffer of mmap...
x3104c0s1b0n0:     creating memory view of numpy buffer...
x3104c0s1b0n0:  > finished creating indexed dataset in 0.002352 seconds
x3104c0s1b0n0:     number of documents: 79000
x3104c0s1b0n0:  > dataset split:
x3104c0s1b0n0:     train:
x3104c0s1b0n0:      document indices in [0, 74971) total of 74971 documents
x3104c0s1b0n0:     validation:
x3104c0s1b0n0:      document indices in [74971, 78921) total of 3950 documents
x3104c0s1b0n0:     test:
x3104c0s1b0n0:      document indices in [78921, 79000) total of 79 documents
x3104c0s1b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_doc_idx.npy
x3104c0s1b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_sample_idx.npy
x3104c0s1b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_shuffle_idx.npy
x3104c0s1b0n0:     loaded indexed file in 0.002 seconds
x3104c0s1b0n0:     total number of samples: 108448
x3104c0s1b0n0:     total number of epochs: 1
x3104c0s1b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_doc_idx.npy
x3104c0s1b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_sample_idx.npy
x3104c0s1b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_shuffle_idx.npy
x3104c0s1b0n0:     loaded indexed file in 0.004 seconds
x3104c0s1b0n0:     total number of samples: 5792
x3104c0s1b0n0:     total number of epochs: 1
x3104c0s1b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_doc_idx.npy
x3104c0s1b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_sample_idx.npy
x3104c0s1b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_shuffle_idx.npy
x3104c0s1b0n0:     loaded indexed file in 0.004 seconds
x3104c0s1b0n0:     total number of samples: 185
x3104c0s1b0n0:     total number of epochs: 1
x3104c0s1b0n0: > finished creating GPT datasets ...
x3104c0s25b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5342309474945068>
x3104c0s25b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5358500480651855>
x3104c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5365252494812012>
x3104c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5437312126159668>
x3104c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5477135181427002>
x3104c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5658102035522461>
x3104c0s25b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5727548599243164>
x3104c0s25b0n0: <TIMER:train/valid/test-data-iterators-setup,0.574455738067627>
x3104c0s1b0n0: [after dataloaders are built] datetime: 2024-03-29 15:58:45 
x3104c0s1b0n0: done with setup ...
x3104c0s1b0n0: training ...
x3104c0s25b0n0: (min, max) time across ranks (ms):
x3104c0s25b0n0:     model-and-optimizer-setup ......................: (51733.68, 51737.59)
x3104c0s25b0n0:     train/valid/test-data-iterators-setup ..........: (534.23, 574.46)
x3104c0s1b0n0: [before training begins] datetime: 2024-03-29 15:58:45 
x3104c0s1b0n0: [before the start of training step] datetime: 2024-03-29 15:58:45 
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:58:45,663] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:58:45,663] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 7.45 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 15:58:45,664] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.37 GB, percent = 62.5%
x3104c0s1b0n0: [2024-03-29 15:58:45,797] [INFO] [checkpointing.py:539:forward] Activation Checkpointing Information
x3104c0s1b0n0: [2024-03-29 15:58:45,797] [INFO] [checkpointing.py:540:forward] ----Partition Activations False, CPU CHECKPOINTING False
x3104c0s1b0n0: [2024-03-29 15:58:45,797] [INFO] [checkpointing.py:541:forward] ----contiguous Memory Checkpointing False with 60 total layers
x3104c0s1b0n0: [2024-03-29 15:58:45,797] [INFO] [checkpointing.py:543:forward] ----Synchronization False
x3104c0s1b0n0: [2024-03-29 15:58:45,797] [INFO] [checkpointing.py:544:forward] ----Profiling time in checkpointing False
x3104c0s1b0n0: [2024-03-29 15:58:54,448] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:58:54,449] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 19.76 GB         CA 22.95 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 15:58:54,449] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.51 GB, percent = 62.5%
x3104c0s1b0n0: [2024-03-29 15:58:54,625] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:58:54,625] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 16.84 GB         CA 16.91 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 15:58:54,625] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.51 GB, percent = 62.5%
x3104c0s1b0n0: [2024-03-29 15:59:20,115] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:59:20,116] [INFO] [utils.py:801:see_memory_usage] MA 11.3 GB         Max_MA 20.43 GB         CA 11.53 GB         Max_CA 28 GB 
x3104c0s1b0n0: [2024-03-29 15:59:20,116] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.53 GB, percent = 62.5%
x3104c0s1b0n0: [2024-03-29 15:59:20,196] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:59:20,197] [INFO] [utils.py:801:see_memory_usage] MA 11.3 GB         Max_MA 11.3 GB         CA 11.53 GB         Max_CA 12 GB 
x3104c0s1b0n0: [2024-03-29 15:59:20,197] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.65 GB, percent = 62.5%
x3104c0s1b0n0: [2024-03-29 15:59:27,026] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.762542486190796
x3104c0s1b0n0: [2024-03-29 15:59:27,038] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.77666163444519
x3104c0s1b0n0: [2024-03-29 15:59:27,042] [INFO] [stage3.py:2251:step] Full outer step loop took 6.780609607696533
x3104c0s1b0n0: [2024-03-29 15:59:27,069] [INFO] [stage3.py:2251:step] Full outer step loop took 6.807427406311035
x3104c0s1b0n0: [2024-03-29 15:59:27,072] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.810438632965088
x3104c0s1b0n0: [2024-03-29 15:59:27,082] [INFO] [stage3.py:2251:step] Full outer step loop took 6.820511102676392
x3104c0s1b0n0: [2024-03-29 15:59:27,084] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.822593688964844
x3104c0s1b0n0: [2024-03-29 15:59:27,093] [INFO] [stage3.py:2251:step] Full outer step loop took 6.83172345161438
x3104c0s25b0n0: [2024-03-29 15:59:27,171] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.9091432094573975
x3104c0s25b0n0: [2024-03-29 15:59:27,183] [INFO] [stage3.py:2251:step] Full outer step loop took 6.921587944030762
x3104c0s25b0n0: [2024-03-29 15:59:27,253] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.991139650344849
x3104c0s25b0n0: [2024-03-29 15:59:27,285] [INFO] [stage3.py:2251:step] Full outer step loop took 7.023089408874512
x3104c0s25b0n0: [2024-03-29 15:59:27,420] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.158498525619507
x3104c0s25b0n0: [2024-03-29 15:59:27,420] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.158935070037842
x3104c0s25b0n0: [2024-03-29 15:59:27,429] [INFO] [stage3.py:2251:step] Full outer step loop took 7.167906284332275
x3104c0s25b0n0: [2024-03-29 15:59:27,430] [INFO] [stage3.py:2251:step] Full outer step loop took 7.1688079833984375
x3104c0s25b0n0: [2024-03-29 15:59:27,440] [INFO] [stage3.py:2277:step] End to end step took 7.1782917976379395
x3104c0s25b0n0: [2024-03-29 15:59:27,440] [INFO] [stage3.py:2277:step] End to end step took 7.178526163101196
x3104c0s1b0n0: [2024-03-29 15:59:27,439] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6808.70
x3104c0s1b0n0: [2024-03-29 15:59:27,440] [INFO] [stage3.py:2277:step] End to end step took 7.178298711776733
x3104c0s1b0n0: [2024-03-29 15:59:27,440] [INFO] [stage3.py:2277:step] End to end step took 7.178362131118774
x3104c0s1b0n0: [2024-03-29 15:59:27,440] [WARNING] [stage3.py:2267:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3104c0s1b0n0: [2024-03-29 15:59:27,440] [INFO] [stage3.py:2277:step] End to end step took 7.178338527679443
x3104c0s1b0n0: [2024-03-29 15:59:27,440] [INFO] [stage3.py:2277:step] End to end step took 7.17836332321167
x3104c0s1b0n0: [2024-03-29 15:59:27,440] [INFO] [logging.py:96:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
x3104c0s25b0n0: [2024-03-29 15:59:27,440] [INFO] [stage3.py:2277:step] End to end step took 7.178755044937134
x3104c0s25b0n0: [2024-03-29 15:59:27,440] [INFO] [stage3.py:2277:step] End to end step took 7.178873777389526
x3104c0s1b0n0: [2024-03-29 15:59:27,440] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 8860.23 | bwd_microstep: 25345.22 | bwd_inner_microstep: 25258.17 | bwd_allreduce_microstep: 86.96 | step_microstep: 7243.10
x3104c0s1b0n0: [2024-03-29 15:59:27,441] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 8860.23 | bwd: 25345.22 | bwd_inner: 25258.17 | bwd_allreduce: 86.96 | step: 7243.10
x3104c0s1b0n0: [2024-03-29 15:59:27,535] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:59:27,535] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.36 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:59:27,536] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.03 GB, percent = 79.7%
x3104c0s25b0n0: <TIMER:interval-time,42.09088850021362><TIMER:interval-time,42.09088587760925><TIMER:interval-time,42.09088683128357>
x3104c0s25b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,42.09084987640381><TIMER:interval-time,42.09076905250549><TIMER:interval-time,42.09085202217102>
x3104c0s25b0n0: <TIMER:interval-time,42.09089207649231>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,42.09086489677429>
x3104c0s25b0n0: 
x3104c0s25b0n0:  elapsed_time 42.090887 | consumed samples:           32 | consumed tokens:        65536 | elapsed time per iteration (ms): 42090.9 | learning rate: 3.000E-04 | global batch size:    32 | lm loss: 1.215015E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.760 | TFLOPs: 52.60 |
x3104c0s1b0n0: [Rank 0] (after 1 iterations) memory (MB) | allocated: 10510.36328125 | max allocated: 10510.36376953125 | reserved: 10586.0 | max reserved: 10586.0
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:59:27,671] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:59:27,672] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 15:59:27,672] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.12 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 15:59:34,620] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:59:34,621] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:59:34,621] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.12 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 15:59:34,718] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:59:34,719] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:59:34,719] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.12 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 15:59:51,910] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:59:51,911] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 23.87 GB         CA 12.79 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 15:59:51,911] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.12 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 15:59:51,986] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:59:51,986] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 12.56 GB         CA 12.79 GB         Max_CA 13 GB 
x3104c0s1b0n0: [2024-03-29 15:59:51,986] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.12 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 15:59:56,699] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.68896222114563
x3104c0s1b0n0: [2024-03-29 15:59:56,709] [INFO] [stage3.py:2251:step] Full outer step loop took 4.698554039001465
x3104c0s1b0n0: [2024-03-29 15:59:56,807] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.7962117195129395
x3104c0s1b0n0: [2024-03-29 15:59:56,820] [INFO] [stage3.py:2251:step] Full outer step loop took 4.810130834579468
x3104c0s1b0n0: [2024-03-29 15:59:56,834] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.824025630950928
x3104c0s1b0n0: [2024-03-29 15:59:56,843] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.832836389541626
x3104c0s1b0n0: [2024-03-29 15:59:56,845] [INFO] [stage3.py:2251:step] Full outer step loop took 4.834550857543945
x3104c0s1b0n0: [2024-03-29 15:59:56,852] [INFO] [stage3.py:2251:step] Full outer step loop took 4.841902017593384
x3104c0s25b0n0: [2024-03-29 15:59:56,945] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.9348039627075195
x3104c0s25b0n0: [2024-03-29 15:59:56,963] [INFO] [stage3.py:2251:step] Full outer step loop took 4.95277214050293
x3104c0s25b0n0: [2024-03-29 15:59:57,045] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.0345234870910645
x3104c0s25b0n0: [2024-03-29 15:59:57,054] [INFO] [stage3.py:2251:step] Full outer step loop took 5.044111013412476
x3104c0s25b0n0: [2024-03-29 15:59:57,056] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.046038627624512
x3104c0s25b0n0: [2024-03-29 15:59:57,056] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.046183824539185
x3104c0s25b0n0: [2024-03-29 15:59:57,066] [INFO] [stage3.py:2251:step] Full outer step loop took 5.055557012557983
x3104c0s25b0n0: [2024-03-29 15:59:57,067] [INFO] [stage3.py:2251:step] Full outer step loop took 5.05659818649292
x3104c0s1b0n0: [2024-03-29 15:59:57,077] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4842.04
x3104c0s25b0n0: [2024-03-29 15:59:57,077] [INFO] [stage3.py:2277:step] End to end step took 5.0665833950042725
x3104c0s1b0n0: [2024-03-29 15:59:57,077] [INFO] [stage3.py:2277:step] End to end step took 5.066671371459961
x3104c0s1b0n0: [2024-03-29 15:59:57,077] [INFO] [stage3.py:2277:step] End to end step took 5.066718816757202
x3104c0s1b0n0: [2024-03-29 15:59:57,077] [INFO] [stage3.py:2277:step] End to end step took 5.066672086715698
x3104c0s1b0n0: [2024-03-29 15:59:57,077] [INFO] [logging.py:96:log_dist] [Rank 0] step=2, skipped=0, lr=[0.0002918585038060976], mom=[(0.9, 0.95)]
x3104c0s25b0n0: [2024-03-29 15:59:57,077] [INFO] [stage3.py:2277:step] End to end step took 5.066839933395386
x3104c0s25b0n0: [2024-03-29 15:59:57,077] [INFO] [stage3.py:2277:step] End to end step took 5.0670740604400635
x3104c0s1b0n0: [2024-03-29 15:59:57,077] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6910.73 | bwd_microstep: 17023.92 | bwd_inner_microstep: 16937.04 | bwd_allreduce_microstep: 86.79 | step_microstep: 5090.73
x3104c0s1b0n0: [2024-03-29 15:59:57,077] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6910.71 | bwd: 17023.91 | bwd_inner: 16937.03 | bwd_allreduce: 86.80 | step: 5090.73
x3104c0s1b0n0: [2024-03-29 15:59:57,077] [INFO] [stage3.py:2277:step] End to end step took 5.0673744678497314
x3104c0s25b0n0: [2024-03-29 15:59:57,078] [INFO] [stage3.py:2277:step] End to end step took 5.067310571670532
x3104c0s1b0n0: [2024-03-29 15:59:57,169] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:59:57,170] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.56 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:59:57,170] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.13 GB, percent = 79.7%
x3104c0s25b0n0: <TIMER:interval-time,29.634398221969604><TIMER:interval-time,29.634400129318237>
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,29.634501695632935>
x3104c0s25b0n0: <TIMER:interval-time,29.634519815444946>
x3104c0s1b0n0: <TIMER:interval-time,29.63432240486145><TIMER:interval-time,29.6343252658844><TIMER:interval-time,29.634326934814453>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,29.63433003425598>
x3104c0s25b0n0:  elapsed_time 29.634398 | consumed samples:           64 | consumed tokens:       131072 | elapsed time per iteration (ms): 29634.4 | learning rate: 2.919E-04 | global batch size:    32 | lm loss: 1.082593E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.080 | TFLOPs: 74.71 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:59:57,294] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:59:57,294] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 15:59:57,294] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:00:03,663] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 16:00:03,664] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:00:03,664] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:00:03,744] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 16:00:03,745] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:00:03,745] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.16 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:00:20,888] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 16:00:20,889] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 23.87 GB         CA 12.79 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 16:00:20,889] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:00:20,961] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 16:00:20,962] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 12.56 GB         CA 12.79 GB         Max_CA 13 GB 
x3104c0s1b0n0: [2024-03-29 16:00:20,962] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:00:25,633] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.647387504577637
x3104c0s1b0n0: [2024-03-29 16:00:25,643] [INFO] [stage3.py:2251:step] Full outer step loop took 4.657520294189453
x3104c0s1b0n0: [2024-03-29 16:00:25,801] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.8156585693359375
x3104c0s1b0n0: [2024-03-29 16:00:25,802] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.816296339035034
x3104c0s1b0n0: [2024-03-29 16:00:25,803] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.816768169403076
x3104c0s1b0n0: [2024-03-29 16:00:25,811] [INFO] [stage3.py:2251:step] Full outer step loop took 4.824807405471802
x3104c0s1b0n0: [2024-03-29 16:00:25,811] [INFO] [stage3.py:2251:step] Full outer step loop took 4.825449466705322
x3104c0s1b0n0: [2024-03-29 16:00:25,812] [INFO] [stage3.py:2251:step] Full outer step loop took 4.825969457626343
x3104c0s25b0n0: [2024-03-29 16:00:25,872] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.886356830596924
x3104c0s25b0n0: [2024-03-29 16:00:25,881] [INFO] [stage3.py:2251:step] Full outer step loop took 4.895445346832275
x3104c0s25b0n0: [2024-03-29 16:00:26,029] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.042285919189453
x3104c0s25b0n0: [2024-03-29 16:00:26,032] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.0462470054626465
x3104c0s25b0n0: [2024-03-29 16:00:26,037] [INFO] [stage3.py:2251:step] Full outer step loop took 5.051337718963623
x3104c0s25b0n0: [2024-03-29 16:00:26,038] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.051675081253052
x3104c0s25b0n0: [2024-03-29 16:00:26,041] [INFO] [stage3.py:2251:step] Full outer step loop took 5.055577754974365
x3104c0s25b0n0: [2024-03-29 16:00:26,047] [INFO] [stage3.py:2251:step] Full outer step loop took 5.060977935791016
x3104c0s1b0n0: [2024-03-29 16:00:26,056] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4826.12
x3104c0s25b0n0: [2024-03-29 16:00:26,056] [INFO] [stage3.py:2277:step] End to end step took 5.070547580718994
x3104c0s25b0n0: [2024-03-29 16:00:26,056] [INFO] [stage3.py:2277:step] End to end step took 5.070628643035889
x3104c0s25b0n0: [2024-03-29 16:00:26,056] [INFO] [stage3.py:2277:step] End to end step took 5.070655107498169
x3104c0s1b0n0: [2024-03-29 16:00:26,056] [INFO] [stage3.py:2277:step] End to end step took 5.070619583129883
x3104c0s1b0n0: [2024-03-29 16:00:26,056] [INFO] [stage3.py:2277:step] End to end step took 5.070492267608643
x3104c0s1b0n0: [2024-03-29 16:00:26,057] [INFO] [logging.py:96:log_dist] [Rank 0] step=3, skipped=0, lr=[0.00026841599982106197], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 16:00:26,057] [INFO] [stage3.py:2277:step] End to end step took 5.0709686279296875
x3104c0s25b0n0: [2024-03-29 16:00:26,057] [INFO] [stage3.py:2277:step] End to end step took 5.0712268352508545
x3104c0s1b0n0: [2024-03-29 16:00:26,057] [INFO] [timer.py:260:stop] epoch=0/micro_step=3/global_step=3, RunningAvgSamplesPerSec=1.1125999467929153, CurrSamplesPerSec=1.1125999467929153, MemAllocated=10.26GB, MaxMemAllocated=12.56GB
x3104c0s1b0n0: [2024-03-29 16:00:26,057] [INFO] [stage3.py:2277:step] End to end step took 5.071220874786377
x3104c0s1b0n0: [2024-03-29 16:00:26,057] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6333.76 | bwd_microstep: 16978.20 | bwd_inner_microstep: 16894.90 | bwd_allreduce_microstep: 83.23 | step_microstep: 5094.83
x3104c0s1b0n0: [2024-03-29 16:00:26,057] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6333.75 | bwd: 16978.20 | bwd_inner: 16894.90 | bwd_allreduce: 83.24 | step: 5094.83
x3104c0s1b0n0: [2024-03-29 16:00:26,154] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 16:00:26,154] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.56 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 16:00:26,155] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: <TIMER:interval-time,28.984074354171753>
x3104c0s1b0n0: <TIMER:interval-time,28.98408031463623><TIMER:interval-time,28.9840829372406>
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,28.984099626541138>
x3104c0s25b0n0: <TIMER:interval-time,28.984110116958618><TIMER:interval-time,28.984097957611084><TIMER:interval-time,28.98410177230835>
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,28.98411750793457>
x3104c0s25b0n0:  elapsed_time 28.984118 | consumed samples:           96 | consumed tokens:       196608 | elapsed time per iteration (ms): 28984.1 | learning rate: 2.684E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.104 | TFLOPs: 76.39 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 16:00:26,293] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 16:00:26,294] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 16:00:26,294] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.19 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:00:32,694] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 16:00:32,694] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:00:32,694] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.19 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:00:32,777] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 16:00:32,777] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:00:32,777] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.19 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:00:49,892] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 16:00:49,892] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 23.87 GB         CA 12.79 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 16:00:49,893] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.19 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:00:49,967] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 16:00:49,968] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 12.56 GB         CA 12.79 GB         Max_CA 13 GB 
x3104c0s1b0n0: [2024-03-29 16:00:49,968] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.19 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:00:54,714] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.722301006317139
x3104c0s1b0n0: [2024-03-29 16:00:54,723] [INFO] [stage3.py:2251:step] Full outer step loop took 4.731382131576538
x3104c0s1b0n0: [2024-03-29 16:00:54,817] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.824841737747192
x3104c0s1b0n0: [2024-03-29 16:00:54,855] [INFO] [stage3.py:2251:step] Full outer step loop took 4.862985372543335
x3104c0s1b0n0: [2024-03-29 16:00:54,863] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.871703386306763
x3104c0s1b0n0: [2024-03-29 16:00:54,863] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.871717929840088
x3104c0s1b0n0: [2024-03-29 16:00:54,872] [INFO] [stage3.py:2251:step] Full outer step loop took 4.880792140960693
x3104c0s1b0n0: [2024-03-29 16:00:54,872] [INFO] [stage3.py:2251:step] Full outer step loop took 4.880785942077637
x3104c0s25b0n0: [2024-03-29 16:00:55,036] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.044071197509766
x3104c0s25b0n0: [2024-03-29 16:00:55,059] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.067708730697632
x3104c0s25b0n0: [2024-03-29 16:00:55,060] [INFO] [stage3.py:2251:step] Full outer step loop took 5.068010091781616
x3104c0s25b0n0: [2024-03-29 16:00:55,065] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.073398113250732
x3104c0s25b0n0: [2024-03-29 16:00:55,066] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.074187994003296
x3104c0s25b0n0: [2024-03-29 16:00:55,069] [INFO] [stage3.py:2251:step] Full outer step loop took 5.077387094497681
x3104c0s25b0n0: [2024-03-29 16:00:55,074] [INFO] [stage3.py:2251:step] Full outer step loop took 5.082474708557129
x3104c0s25b0n0: [2024-03-29 16:00:55,075] [INFO] [stage3.py:2251:step] Full outer step loop took 5.083256006240845
x3104c0s1b0n0: [2024-03-29 16:00:55,084] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4880.93
x3104c0s1b0n0: [2024-03-29 16:00:55,084] [INFO] [stage3.py:2277:step] End to end step took 5.0925843715667725
x3104c0s25b0n0: [2024-03-29 16:00:55,084] [INFO] [stage3.py:2277:step] End to end step took 5.092705249786377
x3104c0s1b0n0: [2024-03-29 16:00:55,084] [INFO] [stage3.py:2277:step] End to end step took 5.092725992202759
x3104c0s1b0n0: [2024-03-29 16:00:55,084] [INFO] [stage3.py:2277:step] End to end step took 5.092663288116455
x3104c0s25b0n0: [2024-03-29 16:00:55,084] [INFO] [stage3.py:2277:step] End to end step took 5.092738628387451
x3104c0s25b0n0: [2024-03-29 16:00:55,084] [INFO] [stage3.py:2277:step] End to end step took 5.092763185501099
x3104c0s25b0n0: [2024-03-29 16:00:55,084] [INFO] [stage3.py:2277:step] End to end step took 5.092825651168823
x3104c0s1b0n0: [2024-03-29 16:00:55,085] [INFO] [logging.py:96:log_dist] [Rank 0] step=4, skipped=0, lr=[0.00023249999999999996], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 16:00:55,085] [INFO] [stage3.py:2277:step] End to end step took 5.092986583709717
x3104c0s1b0n0: [2024-03-29 16:00:55,085] [INFO] [timer.py:260:stop] epoch=0/micro_step=4/global_step=4, RunningAvgSamplesPerSec=1.11205454054534, CurrSamplesPerSec=1.1115096687616133, MemAllocated=10.26GB, MaxMemAllocated=12.56GB
x3104c0s1b0n0: [2024-03-29 16:00:55,085] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6360.75 | bwd_microstep: 16945.51 | bwd_inner_microstep: 16861.36 | bwd_allreduce_microstep: 84.09 | step_microstep: 5116.77
x3104c0s1b0n0: [2024-03-29 16:00:55,085] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6360.74 | bwd: 16945.51 | bwd_inner: 16861.35 | bwd_allreduce: 84.10 | step: 5116.77
x3104c0s1b0n0: [2024-03-29 16:00:55,195] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 16:00:55,196] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.56 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 16:00:55,196] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.2 GB, percent = 79.7%
x3104c0s1b0n0: <TIMER:interval-time,29.04145836830139><TIMER:interval-time,29.041451692581177><TIMER:interval-time,29.04145836830139>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,29.0415096282959><TIMER:interval-time,29.041512489318848><TIMER:interval-time,29.04151487350464>
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,29.041513681411743>
x3104c0s25b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,29.04152250289917>
x3104c0s25b0n0:  elapsed_time 29.041515 | consumed samples:          128 | consumed tokens:       262144 | elapsed time per iteration (ms): 29041.5 | learning rate: 2.325E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.102 | TFLOPs: 76.24 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 16:00:55,311] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 16:00:55,312] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 16:00:55,312] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.19 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:01:01,717] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 16:01:01,718] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:01:01,718] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.18 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:01:01,796] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 16:01:01,796] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:01:01,797] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.18 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:01:18,879] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 16:01:18,880] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 23.87 GB         CA 12.79 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 16:01:18,880] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.18 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:01:18,951] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 16:01:18,951] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 12.56 GB         CA 12.79 GB         Max_CA 13 GB 
x3104c0s1b0n0: [2024-03-29 16:01:18,952] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.18 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:01:23,709] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.7335498332977295
x3104c0s1b0n0: [2024-03-29 16:01:23,727] [INFO] [stage3.py:2251:step] Full outer step loop took 4.751850843429565
x3104c0s25b0n0: [2024-03-29 16:01:23,737] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.762058973312378
x3104c0s25b0n0: [2024-03-29 16:01:23,757] [INFO] [stage3.py:2251:step] Full outer step loop took 4.781646013259888
x3104c0s25b0n0: [2024-03-29 16:01:23,800] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.824873924255371
x3104c0s1b0n0: [2024-03-29 16:01:23,815] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.83959436416626
x3104c0s25b0n0: [2024-03-29 16:01:23,814] [INFO] [stage3.py:2251:step] Full outer step loop took 4.839139699935913
x3104c0s1b0n0: [2024-03-29 16:01:23,829] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.853739261627197
x3104c0s1b0n0: [2024-03-29 16:01:23,830] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.854496240615845
x3104c0s1b0n0: [2024-03-29 16:01:23,830] [INFO] [stage3.py:2251:step] Full outer step loop took 4.8549604415893555
x3104c0s1b0n0: [2024-03-29 16:01:23,838] [INFO] [stage3.py:2251:step] Full outer step loop took 4.86282753944397
x3104c0s1b0n0: [2024-03-29 16:01:23,839] [INFO] [stage3.py:2251:step] Full outer step loop took 4.863555192947388
x3104c0s25b0n0: [2024-03-29 16:01:23,894] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.919100284576416
x3104c0s25b0n0: [2024-03-29 16:01:23,900] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.924475908279419
x3104c0s25b0n0: [2024-03-29 16:01:23,904] [INFO] [stage3.py:2251:step] Full outer step loop took 4.928468942642212
x3104c0s25b0n0: [2024-03-29 16:01:23,909] [INFO] [stage3.py:2251:step] Full outer step loop took 4.93352198600769
x3104c0s1b0n0: [2024-03-29 16:01:23,918] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4863.69
x3104c0s1b0n0: [2024-03-29 16:01:23,918] [INFO] [stage3.py:2277:step] End to end step took 4.943065881729126
x3104c0s25b0n0: [2024-03-29 16:01:23,918] [INFO] [stage3.py:2277:step] End to end step took 4.943222522735596
x3104c0s1b0n0: [2024-03-29 16:01:23,918] [INFO] [stage3.py:2277:step] End to end step took 4.943242311477661
x3104c0s1b0n0: [2024-03-29 16:01:23,918] [INFO] [stage3.py:2277:step] End to end step took 4.943267822265625
x3104c0s25b0n0: [2024-03-29 16:01:23,918] [INFO] [stage3.py:2277:step] End to end step took 4.943292856216431
x3104c0s1b0n0: [2024-03-29 16:01:23,919] [INFO] [logging.py:96:log_dist] [Rank 0] step=5, skipped=0, lr=[0.0001884425039850356], mom=[(0.9, 0.95)]
x3104c0s25b0n0: [2024-03-29 16:01:23,919] [INFO] [stage3.py:2277:step] End to end step took 4.943657636642456
x3104c0s25b0n0: [2024-03-29 16:01:23,919] [INFO] [stage3.py:2277:step] End to end step took 4.943646192550659
x3104c0s1b0n0: [2024-03-29 16:01:23,919] [INFO] [stage3.py:2277:step] End to end step took 4.9435484409332275
x3104c0s1b0n0: [2024-03-29 16:01:23,919] [INFO] [timer.py:260:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=1.1142500025106055, CurrSamplesPerSec=1.118667035739133, MemAllocated=10.26GB, MaxMemAllocated=12.56GB
x3104c0s1b0n0: [2024-03-29 16:01:23,919] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6367.28 | bwd_microstep: 16913.67 | bwd_inner_microstep: 16829.08 | bwd_allreduce_microstep: 84.51 | step_microstep: 4967.24
x3104c0s1b0n0: [2024-03-29 16:01:23,919] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6367.27 | bwd: 16913.66 | bwd_inner: 16829.08 | bwd_allreduce: 84.53 | step: 4967.25
x3104c0s1b0n0: [2024-03-29 16:01:24,013] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 16:01:24,013] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.56 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 16:01:24,013] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.18 GB, percent = 79.7%
x3104c0s1b0n0: <TIMER:interval-time,28.81692910194397><TIMER:interval-time,28.816920042037964><TIMER:interval-time,28.81693172454834>
x3104c0s1b0n0: <TIMER:interval-time,28.81693458557129>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,28.816924571990967><TIMER:interval-time,28.81692862510681><TIMER:interval-time,28.81692910194397>
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,28.817057371139526>
x3104c0s25b0n0:  elapsed_time 28.816929 | consumed samples:          160 | consumed tokens:       327680 | elapsed time per iteration (ms): 28816.9 | learning rate: 1.884E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.110 | TFLOPs: 76.83 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 16:01:24,157] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 16:01:24,157] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 16:01:24,157] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:01:30,768] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 16:01:30,768] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:01:30,769] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:01:30,847] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 16:01:30,848] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:01:30,848] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:01:48,050] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 16:01:48,051] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 23.87 GB         CA 12.79 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 16:01:48,051] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:01:48,124] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 16:01:48,125] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 12.56 GB         CA 12.79 GB         Max_CA 13 GB 
x3104c0s1b0n0: [2024-03-29 16:01:48,125] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:01:52,834] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.685370922088623
x3104c0s1b0n0: [2024-03-29 16:01:52,842] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.693673610687256
x3104c0s1b0n0: [2024-03-29 16:01:52,845] [INFO] [stage3.py:2251:step] Full outer step loop took 4.696399927139282
x3104c0s1b0n0: [2024-03-29 16:01:52,852] [INFO] [stage3.py:2251:step] Full outer step loop took 4.703227758407593
x3104c0s1b0n0: [2024-03-29 16:01:52,946] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.7968363761901855
x3104c0s1b0n0: [2024-03-29 16:01:52,946] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.797220230102539
x3104c0s1b0n0: [2024-03-29 16:01:52,955] [INFO] [stage3.py:2251:step] Full outer step loop took 4.8059234619140625
x3104c0s1b0n0: [2024-03-29 16:01:52,955] [INFO] [stage3.py:2251:step] Full outer step loop took 4.806284427642822
x3104c0s25b0n0: [2024-03-29 16:01:52,974] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.8251965045928955
x3104c0s25b0n0: [2024-03-29 16:01:52,985] [INFO] [stage3.py:2251:step] Full outer step loop took 4.836056709289551
x3104c0s25b0n0: [2024-03-29 16:01:53,111] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.962535619735718
x3104c0s25b0n0: [2024-03-29 16:01:53,116] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.966736555099487
x3104c0s25b0n0: [2024-03-29 16:01:53,125] [INFO] [stage3.py:2251:step] Full outer step loop took 4.975826978683472
x3104c0s25b0n0: [2024-03-29 16:01:53,126] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.976898193359375
x3104c0s25b0n0: [2024-03-29 16:01:53,130] [INFO] [stage3.py:2251:step] Full outer step loop took 4.981022119522095
x3104c0s25b0n0: [2024-03-29 16:01:53,135] [INFO] [stage3.py:2251:step] Full outer step loop took 4.9859397411346436
x3104c0s1b0n0: [2024-03-29 16:01:53,144] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4806.43
x3104c0s1b0n0: [2024-03-29 16:01:53,144] [INFO] [stage3.py:2277:step] End to end step took 4.995048522949219
x3104c0s25b0n0: [2024-03-29 16:01:53,144] [INFO] [stage3.py:2277:step] End to end step took 4.995281934738159
x3104c0s25b0n0: [2024-03-29 16:01:53,144] [INFO] [stage3.py:2277:step] End to end step took 4.995332956314087
x3104c0s25b0n0: [2024-03-29 16:01:53,144] [INFO] [stage3.py:2277:step] End to end step took 4.9953529834747314
x3104c0s1b0n0: [2024-03-29 16:01:53,144] [INFO] [logging.py:96:log_dist] [Rank 0] step=6, skipped=0, lr=[0.0001415574960149644], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 16:01:53,144] [INFO] [stage3.py:2277:step] End to end step took 4.995638608932495
x3104c0s1b0n0: [2024-03-29 16:01:53,145] [INFO] [timer.py:260:stop] epoch=0/micro_step=6/global_step=6, RunningAvgSamplesPerSec=1.1116661363758813, CurrSamplesPerSec=1.1039859428345546, MemAllocated=10.26GB, MaxMemAllocated=12.56GB
x3104c0s25b0n0: [2024-03-29 16:01:53,144] [INFO] [stage3.py:2277:step] End to end step took 4.995722770690918
x3104c0s1b0n0: [2024-03-29 16:01:53,145] [INFO] [stage3.py:2277:step] End to end step took 4.995766639709473
x3104c0s1b0n0: [2024-03-29 16:01:53,145] [INFO] [stage3.py:2277:step] End to end step took 4.9958295822143555
x3104c0s1b0n0: [2024-03-29 16:01:53,145] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6573.61 | bwd_microstep: 17037.62 | bwd_inner_microstep: 16952.44 | bwd_allreduce_microstep: 85.11 | step_microstep: 5019.57
x3104c0s1b0n0: [2024-03-29 16:01:53,145] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6573.60 | bwd: 17037.62 | bwd_inner: 16952.43 | bwd_allreduce: 85.13 | step: 5019.57
x3104c0s1b0n0: [2024-03-29 16:01:53,238] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 16:01:53,239] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.56 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 16:01:53,239] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: <TIMER:interval-time,29.22480058670044><TIMER:interval-time,29.224801301956177>
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,29.224807500839233>
x3104c0s1b0n0: <TIMER:interval-time,29.224815607070923>
x3104c0s25b0n0: <TIMER:interval-time,29.2248432636261><TIMER:interval-time,29.224830389022827>
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,29.224852323532104>
x3104c0s25b0n0: <TIMER:interval-time,29.22497057914734>
x3104c0s25b0n0:  elapsed_time 29.224971 | consumed samples:          192 | consumed tokens:       393216 | elapsed time per iteration (ms): 29225.0 | learning rate: 1.416E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.095 | TFLOPs: 75.76 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 16:01:53,394] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 16:01:53,395] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 16:01:53,395] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.18 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:01:59,775] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 16:01:59,775] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:01:59,775] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:01:59,857] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 16:01:59,858] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:01:59,858] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:02:16,960] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 16:02:16,961] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 23.87 GB         CA 12.79 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 16:02:16,961] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:02:17,032] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 16:02:17,033] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 12.56 GB         CA 12.79 GB         Max_CA 13 GB 
x3104c0s1b0n0: [2024-03-29 16:02:17,033] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:02:21,757] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.700536012649536
x3104c0s1b0n0: [2024-03-29 16:02:21,797] [INFO] [stage3.py:2251:step] Full outer step loop took 4.740030288696289
x3104c0s1b0n0: [2024-03-29 16:02:21,820] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.763195276260376
x3104c0s1b0n0: [2024-03-29 16:02:21,829] [INFO] [stage3.py:2251:step] Full outer step loop took 4.772337198257446
x3104c0s1b0n0: [2024-03-29 16:02:21,907] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.850136041641235
x3104c0s1b0n0: [2024-03-29 16:02:21,918] [INFO] [stage3.py:2251:step] Full outer step loop took 4.861518383026123
x3104c0s1b0n0: [2024-03-29 16:02:21,944] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.887359619140625
x3104c0s1b0n0: [2024-03-29 16:02:21,953] [INFO] [stage3.py:2251:step] Full outer step loop took 4.8964619636535645
x3104c0s25b0n0: [2024-03-29 16:02:21,968] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.912097215652466
x3104c0s25b0n0: [2024-03-29 16:02:21,978] [INFO] [stage3.py:2251:step] Full outer step loop took 4.921231269836426
x3104c0s25b0n0: [2024-03-29 16:02:22,100] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.0441296100616455
x3104c0s25b0n0: [2024-03-29 16:02:22,138] [INFO] [stage3.py:2251:step] Full outer step loop took 5.081793785095215
x3104c0s25b0n0: [2024-03-29 16:02:22,185] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.128449440002441
x3104c0s25b0n0: [2024-03-29 16:02:22,185] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.128797292709351
x3104c0s25b0n0: [2024-03-29 16:02:22,194] [INFO] [stage3.py:2251:step] Full outer step loop took 5.137876033782959
x3104c0s25b0n0: [2024-03-29 16:02:22,194] [INFO] [stage3.py:2251:step] Full outer step loop took 5.1380932331085205
x3104c0s25b0n0: [2024-03-29 16:02:22,204] [INFO] [stage3.py:2277:step] End to end step took 5.14741587638855
x3104c0s25b0n0: [2024-03-29 16:02:22,204] [INFO] [stage3.py:2277:step] End to end step took 5.1475372314453125
x3104c0s1b0n0: [2024-03-29 16:02:22,204] [INFO] [stage3.py:2277:step] End to end step took 5.147532224655151
x3104c0s1b0n0: [2024-03-29 16:02:22,204] [INFO] [stage3.py:2277:step] End to end step took 5.1478047370910645
x3104c0s25b0n0: [2024-03-29 16:02:22,204] [INFO] [stage3.py:2277:step] End to end step took 5.147865295410156
x3104c0s25b0n0: [2024-03-29 16:02:22,204] [INFO] [stage3.py:2277:step] End to end step took 5.148013353347778
x3104c0s1b0n0: [2024-03-29 16:02:22,204] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4862.87
x3104c0s1b0n0: [2024-03-29 16:02:22,204] [INFO] [stage3.py:2277:step] End to end step took 5.147985935211182
x3104c0s1b0n0: [2024-03-29 16:02:22,205] [INFO] [stage3.py:2277:step] End to end step took 5.148091077804565
x3104c0s1b0n0: [2024-03-29 16:02:22,205] [INFO] [logging.py:96:log_dist] [Rank 0] step=7, skipped=0, lr=[9.750000000000001e-05], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 16:02:22,205] [INFO] [timer.py:260:stop] epoch=0/micro_step=7/global_step=7, RunningAvgSamplesPerSec=1.1114869203666113, CurrSamplesPerSec=1.1107706337993049, MemAllocated=10.26GB, MaxMemAllocated=12.56GB
x3104c0s1b0n0: [2024-03-29 16:02:22,205] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6337.02 | bwd_microstep: 16938.45 | bwd_inner_microstep: 16852.19 | bwd_allreduce_microstep: 86.20 | step_microstep: 5172.42
x3104c0s1b0n0: [2024-03-29 16:02:22,206] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6337.01 | bwd: 16938.45 | bwd_inner: 16852.18 | bwd_allreduce: 86.21 | step: 5172.42
x3104c0s1b0n0: [2024-03-29 16:02:22,312] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 16:02:22,313] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.56 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 16:02:22,313] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: <TIMER:interval-time,29.073784112930298><TIMER:interval-time,29.073790550231934><TIMER:interval-time,29.07378077507019>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,29.073791980743408>
x3104c0s25b0n0: <TIMER:interval-time,29.07382345199585><TIMER:interval-time,29.07380509376526>
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,29.073939085006714><TIMER:interval-time,29.073941469192505>
x3104c0s25b0n0: 
x3104c0s25b0n0:  elapsed_time 29.073805 | consumed samples:          224 | consumed tokens:       458752 | elapsed time per iteration (ms): 29073.8 | learning rate: 9.750E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.101 | TFLOPs: 76.15 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 16:02:22,438] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 16:02:22,439] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 16:02:22,439] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.18 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:02:28,877] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 16:02:28,878] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:02:28,878] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:02:28,963] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 16:02:28,964] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:02:28,964] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:02:46,111] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 16:02:46,111] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 23.87 GB         CA 12.79 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 16:02:46,112] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:02:46,184] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 16:02:46,185] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 12.56 GB         CA 12.79 GB         Max_CA 13 GB 
x3104c0s1b0n0: [2024-03-29 16:02:46,185] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:02:51,012] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.804218292236328
x3104c0s1b0n0: [2024-03-29 16:02:51,044] [INFO] [stage3.py:2251:step] Full outer step loop took 4.836010932922363
x3104c0s1b0n0: [2024-03-29 16:02:51,065] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.856657981872559
x3104c0s1b0n0: [2024-03-29 16:02:51,082] [INFO] [stage3.py:2251:step] Full outer step loop took 4.873811960220337
x3104c0s1b0n0: [2024-03-29 16:02:51,089] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.880711793899536
x3104c0s1b0n0: [2024-03-29 16:02:51,100] [INFO] [stage3.py:2251:step] Full outer step loop took 4.891702175140381
x3104c0s1b0n0: [2024-03-29 16:02:51,134] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.92593240737915
x3104c0s1b0n0: [2024-03-29 16:02:51,143] [INFO] [stage3.py:2251:step] Full outer step loop took 4.935007572174072
x3104c0s25b0n0: [2024-03-29 16:02:51,239] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.031256437301636
x3104c0s25b0n0: [2024-03-29 16:02:51,242] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.034447431564331
x3104c0s25b0n0: [2024-03-29 16:02:51,247] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.039390325546265
x3104c0s25b0n0: [2024-03-29 16:02:51,249] [INFO] [stage3.py:2251:step] Full outer step loop took 5.040745258331299
x3104c0s25b0n0: [2024-03-29 16:02:51,249] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.040998458862305
x3104c0s25b0n0: [2024-03-29 16:02:51,252] [INFO] [stage3.py:2251:step] Full outer step loop took 5.044035911560059
x3104c0s25b0n0: [2024-03-29 16:02:51,257] [INFO] [stage3.py:2251:step] Full outer step loop took 5.048480272293091
x3104c0s25b0n0: [2024-03-29 16:02:51,258] [INFO] [stage3.py:2251:step] Full outer step loop took 5.05011510848999
x3104c0s1b0n0: [2024-03-29 16:02:51,267] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4935.18
x3104c0s25b0n0: [2024-03-29 16:02:51,267] [INFO] [stage3.py:2277:step] End to end step took 5.059439659118652
x3104c0s1b0n0: [2024-03-29 16:02:51,268] [INFO] [stage3.py:2277:step] End to end step took 5.059439420700073
x3104c0s25b0n0: [2024-03-29 16:02:51,268] [INFO] [stage3.py:2277:step] End to end step took 5.0595924854278564
x3104c0s25b0n0: [2024-03-29 16:02:51,268] [INFO] [stage3.py:2277:step] End to end step took 5.059606313705444
x3104c0s25b0n0: [2024-03-29 16:02:51,268] [INFO] [stage3.py:2277:step] End to end step took 5.059654951095581
x3104c0s1b0n0: [2024-03-29 16:02:51,268] [INFO] [stage3.py:2277:step] End to end step took 5.059626340866089
x3104c0s1b0n0: [2024-03-29 16:02:51,268] [INFO] [logging.py:96:log_dist] [Rank 0] step=8, skipped=0, lr=[6.158400017893797e-05], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 16:02:51,268] [INFO] [stage3.py:2277:step] End to end step took 5.060006380081177
x3104c0s1b0n0: [2024-03-29 16:02:51,268] [INFO] [timer.py:260:stop] epoch=0/micro_step=8/global_step=8, RunningAvgSamplesPerSec=1.1112445142878262, CurrSamplesPerSec=1.1100340681689729, MemAllocated=10.26GB, MaxMemAllocated=12.56GB
x3104c0s1b0n0: [2024-03-29 16:02:51,268] [INFO] [stage3.py:2277:step] End to end step took 5.060189247131348
x3104c0s1b0n0: [2024-03-29 16:02:51,268] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6389.07 | bwd_microstep: 16983.84 | bwd_inner_microstep: 16901.36 | bwd_allreduce_microstep: 82.41 | step_microstep: 5083.40
x3104c0s1b0n0: [2024-03-29 16:02:51,269] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6389.05 | bwd: 16983.84 | bwd_inner: 16901.36 | bwd_allreduce: 82.43 | step: 5083.41
x3104c0s1b0n0: [2024-03-29 16:02:51,366] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 16:02:51,367] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.56 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 16:02:51,367] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: <TIMER:interval-time,29.05329394340515><TIMER:interval-time,29.053292512893677><TIMER:interval-time,29.053282976150513>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,29.053296327590942><TIMER:interval-time,29.05331015586853>
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,29.053303480148315>
x3104c0s1b0n0: <TIMER:interval-time,29.05338430404663>
x3104c0s25b0n0: <TIMER:interval-time,29.05343008041382>
x3104c0s25b0n0:  elapsed_time 29.053310 | consumed samples:          256 | consumed tokens:       524288 | elapsed time per iteration (ms): 29053.3 | learning rate: 6.158E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.101 | TFLOPs: 76.21 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 16:02:51,512] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 16:02:51,513] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 16:02:51,513] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.18 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:02:57,937] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 16:02:57,937] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:02:57,937] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:02:58,017] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 16:02:58,018] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:02:58,018] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:03:15,409] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 16:03:15,410] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 23.87 GB         CA 12.79 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 16:03:15,410] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:03:15,484] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 16:03:15,484] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 12.56 GB         CA 12.79 GB         Max_CA 13 GB 
x3104c0s1b0n0: [2024-03-29 16:03:15,484] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:03:20,249] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.7412896156311035
x3104c0s1b0n0: [2024-03-29 16:03:20,263] [INFO] [stage3.py:2251:step] Full outer step loop took 4.755420923233032
x3104c0s1b0n0: [2024-03-29 16:03:20,268] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.76001501083374
x3104c0s1b0n0: [2024-03-29 16:03:20,277] [INFO] [stage3.py:2251:step] Full outer step loop took 4.769112586975098
x3104c0s1b0n0: [2024-03-29 16:03:20,351] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.843807935714722
x3104c0s1b0n0: [2024-03-29 16:03:20,352] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.844712257385254
x3104c0s1b0n0: [2024-03-29 16:03:20,361] [INFO] [stage3.py:2251:step] Full outer step loop took 4.852908134460449
x3104c0s1b0n0: [2024-03-29 16:03:20,361] [INFO] [stage3.py:2251:step] Full outer step loop took 4.853774309158325
x3104c0s25b0n0: [2024-03-29 16:03:20,573] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.065305233001709
x3104c0s25b0n0: [2024-03-29 16:03:20,582] [INFO] [stage3.py:2251:step] Full outer step loop took 5.074754238128662
x3104c0s25b0n0: [2024-03-29 16:03:20,589] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.081048250198364
x3104c0s25b0n0: [2024-03-29 16:03:20,589] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.081041574478149
x3104c0s25b0n0: [2024-03-29 16:03:20,594] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.0859715938568115
x3104c0s25b0n0: [2024-03-29 16:03:20,598] [INFO] [stage3.py:2251:step] Full outer step loop took 5.090610980987549
x3104c0s25b0n0: [2024-03-29 16:03:20,598] [INFO] [stage3.py:2251:step] Full outer step loop took 5.090740203857422
x3104c0s25b0n0: [2024-03-29 16:03:20,603] [INFO] [stage3.py:2251:step] Full outer step loop took 5.095020294189453
x3104c0s1b0n0: [2024-03-29 16:03:20,612] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4853.08
x3104c0s25b0n0: [2024-03-29 16:03:20,612] [INFO] [stage3.py:2277:step] End to end step took 5.104149580001831
x3104c0s25b0n0: [2024-03-29 16:03:20,612] [INFO] [stage3.py:2277:step] End to end step took 5.104315757751465
x3104c0s1b0n0: [2024-03-29 16:03:20,612] [INFO] [stage3.py:2277:step] End to end step took 5.104352951049805
x3104c0s1b0n0: [2024-03-29 16:03:20,612] [INFO] [stage3.py:2277:step] End to end step took 5.10447883605957
x3104c0s25b0n0: [2024-03-29 16:03:20,612] [INFO] [stage3.py:2277:step] End to end step took 5.104424476623535
x3104c0s1b0n0: [2024-03-29 16:03:20,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=9, skipped=0, lr=[3.814149619390238e-05], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 16:03:20,613] [INFO] [stage3.py:2277:step] End to end step took 5.104884147644043
x3104c0s1b0n0: [2024-03-29 16:03:20,613] [INFO] [stage3.py:2277:step] End to end step took 5.104825019836426
x3104c0s25b0n0: [2024-03-29 16:03:20,613] [INFO] [stage3.py:2277:step] End to end step took 5.105008125305176
x3104c0s1b0n0: [2024-03-29 16:03:20,613] [INFO] [timer.py:260:stop] epoch=0/micro_step=9/global_step=9, RunningAvgSamplesPerSec=1.1095826955881496, CurrSamplesPerSec=1.0997152326788395, MemAllocated=10.26GB, MaxMemAllocated=12.56GB
x3104c0s1b0n0: [2024-03-29 16:03:20,613] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6385.36 | bwd_microstep: 17222.13 | bwd_inner_microstep: 17138.64 | bwd_allreduce_microstep: 83.43 | step_microstep: 5128.40
x3104c0s1b0n0: [2024-03-29 16:03:20,613] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6385.35 | bwd: 17222.13 | bwd_inner: 17138.63 | bwd_allreduce: 83.44 | step: 5128.40
x3104c0s1b0n0: [2024-03-29 16:03:20,711] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 16:03:20,712] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.56 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 16:03:20,712] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: <TIMER:interval-time,29.34475302696228>
x3104c0s1b0n0: <TIMER:interval-time,29.3447687625885>
x3104c0s1b0n0: <TIMER:interval-time,29.344767093658447><TIMER:interval-time,29.3447687625885>
x3104c0s1b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,29.344762325286865><TIMER:interval-time,29.344766855239868><TIMER:interval-time,29.344751596450806>
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,29.344778299331665>
x3104c0s25b0n0:  elapsed_time 29.344778 | consumed samples:          288 | consumed tokens:       589824 | elapsed time per iteration (ms): 29344.8 | learning rate: 3.814E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.090 | TFLOPs: 75.45 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 16:03:20,858] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 16:03:20,859] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 16:03:20,859] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:03:27,525] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 16:03:27,526] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:03:27,526] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:03:27,609] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 16:03:27,610] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:03:27,610] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:03:45,220] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 16:03:45,221] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 23.87 GB         CA 12.79 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 16:03:45,221] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:03:45,296] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 16:03:45,297] [INFO] [utils.py:801:see_memory_usage] MA 12.56 GB         Max_MA 12.56 GB         CA 12.79 GB         Max_CA 13 GB 
x3104c0s1b0n0: [2024-03-29 16:03:45,297] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: [2024-03-29 16:03:50,066] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.745177268981934
x3104c0s1b0n0: [2024-03-29 16:03:50,076] [INFO] [stage3.py:2251:step] Full outer step loop took 4.7543556690216064
x3104c0s1b0n0: [2024-03-29 16:03:50,077] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.755419492721558
x3104c0s1b0n0: [2024-03-29 16:03:50,086] [INFO] [stage3.py:2251:step] Full outer step loop took 4.764587640762329
x3104c0s1b0n0: [2024-03-29 16:03:50,179] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.857513189315796
x3104c0s1b0n0: [2024-03-29 16:03:50,179] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.857846021652222
x3104c0s1b0n0: [2024-03-29 16:03:50,188] [INFO] [stage3.py:2251:step] Full outer step loop took 4.866580247879028
x3104c0s1b0n0: [2024-03-29 16:03:50,188] [INFO] [stage3.py:2251:step] Full outer step loop took 4.8669352531433105
x3104c0s25b0n0: [2024-03-29 16:03:50,200] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.87883186340332
x3104c0s25b0n0: [2024-03-29 16:03:50,230] [INFO] [stage3.py:2251:step] Full outer step loop took 4.908622741699219
x3104c0s25b0n0: [2024-03-29 16:03:50,238] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.916441202163696
x3104c0s25b0n0: [2024-03-29 16:03:50,250] [INFO] [stage3.py:2251:step] Full outer step loop took 4.928367614746094
x3104c0s25b0n0: [2024-03-29 16:03:50,332] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.0108911991119385
x3104c0s25b0n0: [2024-03-29 16:03:50,338] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.016441822052002
x3104c0s25b0n0: [2024-03-29 16:03:50,343] [INFO] [stage3.py:2251:step] Full outer step loop took 5.021692276000977
x3104c0s25b0n0: [2024-03-29 16:03:50,347] [INFO] [stage3.py:2251:step] Full outer step loop took 5.025481700897217
x3104c0s1b0n0: [2024-03-29 16:03:50,356] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4866.71
x3104c0s1b0n0: [2024-03-29 16:03:50,356] [INFO] [stage3.py:2277:step] End to end step took 5.034909009933472
x3104c0s1b0n0: [2024-03-29 16:03:50,356] [INFO] [stage3.py:2277:step] End to end step took 5.034988880157471
x3104c0s25b0n0: [2024-03-29 16:03:50,356] [INFO] [stage3.py:2277:step] End to end step took 5.03496241569519
x3104c0s25b0n0: [2024-03-29 16:03:50,356] [INFO] [stage3.py:2277:step] End to end step took 5.034987688064575
x3104c0s1b0n0: [2024-03-29 16:03:50,356] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[3e-05], mom=[(0.9, 0.95)]
x3104c0s25b0n0: [2024-03-29 16:03:50,357] [INFO] [stage3.py:2277:step] End to end step took 5.035327911376953
x3104c0s1b0n0: [2024-03-29 16:03:50,357] [INFO] [stage3.py:2277:step] End to end step took 5.03547215461731
x3104c0s1b0n0: [2024-03-29 16:03:50,357] [INFO] [stage3.py:2277:step] End to end step took 5.0354323387146
x3104c0s1b0n0: [2024-03-29 16:03:50,357] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=1.1064335953120121, CurrSamplesPerSec=1.0848806406998706, MemAllocated=10.26GB, MaxMemAllocated=12.56GB
x3104c0s25b0n0: [2024-03-29 16:03:50,357] [INFO] [stage3.py:2277:step] End to end step took 5.035552501678467
x3104c0s1b0n0: [2024-03-29 16:03:50,357] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6624.73 | bwd_microstep: 17441.62 | bwd_inner_microstep: 17355.88 | bwd_allreduce_microstep: 85.68 | step_microstep: 5059.59
x3104c0s1b0n0: [2024-03-29 16:03:50,357] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6624.72 | bwd: 17441.62 | bwd_inner: 17355.87 | bwd_allreduce: 85.69 | step: 5059.60
x3104c0s1b0n0: [2024-03-29 16:03:50,449] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 16:03:50,450] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.56 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 16:03:50,450] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.17 GB, percent = 79.7%
x3104c0s1b0n0: <TIMER:interval-time,29.738009929656982><TIMER:interval-time,29.738018035888672>
x3104c0s1b0n0: <TIMER:interval-time,29.73801612854004>
x3104c0s1b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,29.738036155700684><TIMER:interval-time,29.738040924072266><TIMER:interval-time,29.738040685653687><TIMER:interval-time,29.73804545402527>
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,29.738126039505005>
x3104c0s25b0n0:  elapsed_time 29.738041 | consumed samples:          320 | consumed tokens:       655360 | elapsed time per iteration (ms): 29738.0 | learning rate: 3.000E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.076 | TFLOPs: 74.45 |
x3104c0s25b0n0: <<<only_train:305.0136649608612>>>
x3104c0s25b0n0: <<<only_train:305.013708114624>>>
x3104c0s25b0n0: <<<only_train:305.0135281085968>>>
x3104c0s25b0n0: <<<only_train:305.0137069225311>>>
x3104c0s1b0n0: <<<only_train:305.01626110076904>>>
x3104c0s1b0n0: <<<only_train:305.0163118839264>>>
x3104c0s1b0n0: <<<only_train:305.01627802848816>>>
x3104c0s1b0n0: <<<only_train:305.0163142681122>>>
x3104c0s1b0n0: [after training ends] datetime: 2024-03-29 16:03:50 
x3104c0s1b0n0: <<<full_time:305.0168857574463>>><<<full_time:305.0168843269348>>>
x3104c0s1b0n0: 
x3104c0s1b0n0: <<<full_time:305.0168709754944>>>
x3104c0s1b0n0: <<<full_time:305.01693177223206>>>
x3104c0s25b0n0: <<<full_time:305.0144073963165>>><<<full_time:305.01437759399414>>>
x3104c0s25b0n0: 
x3104c0s25b0n0: <<<full_time:305.0143883228302>>>
x3104c0s25b0n0: <<<full_time:305.01422667503357>>>
x3104c0s1b0n0: [2024-03-29 16:03:59,393] [INFO] [launch.py:348:main] Process 60300 exits successfully.
x3104c0s25b0n0: [2024-03-29 16:03:59,795] [INFO] [launch.py:348:main] Process 22124 exits successfully.
x3104c0s1b0n0: [2024-03-29 16:04:07,402] [INFO] [launch.py:348:main] Process 60299 exits successfully.
x3104c0s1b0n0: [2024-03-29 16:04:07,402] [INFO] [launch.py:348:main] Process 60298 exits successfully.
x3104c0s1b0n0: [2024-03-29 16:04:07,402] [INFO] [launch.py:348:main] Process 60301 exits successfully.
x3104c0s25b0n0: [2024-03-29 16:04:07,804] [INFO] [launch.py:348:main] Process 22123 exits successfully.
x3104c0s25b0n0: [2024-03-29 16:04:07,804] [INFO] [launch.py:348:main] Process 22121 exits successfully.
x3104c0s25b0n0: [2024-03-29 16:04:07,804] [INFO] [launch.py:348:main] Process 22122 exits successfully.

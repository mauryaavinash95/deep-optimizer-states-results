[2024-03-29 16:43:18,740] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 16:43:21,889] [INFO] [runner.py:463:main] Using IP address of 10.140.57.107 for node x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-29 16:43:21,891] [INFO] [runner.py:557:main] deepspeed_env file = ./.deepspeed_env
[2024-03-29 16:43:21,891] [INFO] [multinode_runner.py:80:get_cmd] Running on the following workers: x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov,x3006c0s1b1n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-29 16:43:21,891] [INFO] [runner.py:568:main] cmd = pdsh -S -f 1024 -w x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov,x3006c0s1b1n0.hsn.cm.polaris.alcf.anl.gov export PYTHONUSERBASE=/home/am6429/.local/polaris/conda/2023-10-04; export PYTHONPATH=/home/am6429/dl-io/Megatron-DeepSpeed; export PATH=/home/am6429/.conda/envs/dspeed_env/bin:/soft/datascience/conda/2023-10-04/mconda3/condabin:/soft/compilers/cudatoolkit/cuda-11.8.0/bin:/soft/buildtools/cmake/cmake-3.23.2/cmake-3.23.2-linux-x86_64/bin:/opt/cray/pe/gcc/11.2.0/bin:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/include:/opt/cray/pe/pals/1.2.11/bin:/opt/cray/pe/craype/2.7.20/bin:/opt/cray/libfabric/1.15.2.0/bin:/home/am6429/.conda/envs/dspeed_env/bin:/opt/cray/pe/perftools/23.03.0/bin:/opt/cray/pe/papi/7.0.0.1/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/am6429/.local/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/am6429/veloc-build/include:/home/am6429/veloc-build/bin:/soft/datascience/conda/2023-01-10/mconda3/include:/opt/cray/pe/bin:/soft/datascience/conda/2023-01-10/mconda3/include; export LD_LIBRARY_PATH=/usr/lib64/:/soft/compilers/cudatoolkit/cuda-11.8.0/lib64:/opt/cray/pe/gcc/11.2.0/snos/lib64:/soft/compilers/cudatoolkit/cuda-11.8.0/extras/CUPTI/lib64:/soft/libraries/trt/TensorRT-8.5.3.1.Linux.x86_64-gnu.cuda-11.8.cudnn8.6/lib:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/lib:/soft/libraries/cudnn/cudnn-11-linux-x64-v8.6.0.163/lib:/opt/cray/libfabric/1.15.2.0/lib64:/usr/lib64:/opt/cray/pe/papi/7.0.0.1/lib64:/home/am6429/veloc-build/lib:/home/am6429/veloc-build/lib64:/home/am6429/nvcomp/lib:/soft/datascience/conda/2023-01-10/mconda3/lib:/soft/datascience/conda/2023-01-10/mconda3/lib/; export http_proxy=http://proxy.alcf.anl.gov:3128; export https_proxy=http://proxy.alcf.anl.gov:3128; export CC=gcc; export CXX=g++; export IBV_FORK_SAFE=1; export CFLAGS=-I/soft/datascience/conda/2023-01-10/mconda3/include/; export LDFLAGS=-L/soft/datascience/conda/2023-01-10/mconda3/lib/; export CUDA_DEVICE_MAX_CONNECTIONS=1; export TORCHSNAPSHOT_PER_RANK_MEMORY_BUDGET_BYTES=34359738368; export _DEFAULT_MAX_PER_RANK_IO_CONCURRENCY=1; export _MAX_PER_RANK_IO_CONCURRENCY=1; export NSYS_REPORT_DIR=/home/am6429/dl-io/dl-io-outputs/act-output-30B//rep-30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch0-flush_async0-opt_gaps0-%n;  cd /home/am6429/dl-io/Megatron-DeepSpeed; /home/am6429/.conda/envs/dspeed_env/bin/python -u -m deepspeed.launcher.launch --world_info=eyJ4MzAwNmMwczE5YjFuMC5oc24uY20ucG9sYXJpcy5hbGNmLmFubC5nb3YiOiBbMCwgMSwgMiwgM10sICJ4MzAwNmMwczFiMW4wLmhzbi5jbS5wb2xhcmlzLmFsY2YuYW5sLmdvdiI6IFswLCAxLCAyLCAzXX0= --node_rank=%n --master_addr=10.140.57.107 --master_port=29700 /home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py --tensor-model-parallel-size 1 --num-layers 60 --hidden-size 6656 --num-attention-heads 52 --micro-batch-size 4 --global-batch-size 32 --seq-length 2048 --max-position-embeddings 2048 --train-iters 10 --save /local/scratch/llama2/zero3-tp1}_dp8 --data-path /home/am6429/dl-io/datasets/meg-gpt2_text_document --vocab-file /home/am6429/dl-io/datasets/gpt2-vocab.json --merge-file /home/am6429/dl-io/datasets/gpt2-merges.txt --data-impl mmap --tokenizer-type GPTSentencePieceTokenizer --tokenizer-model /home/am6429/dl-io/datasets/tokenizer.model --split 949,50,1 --distributed-backend nccl --lr 3e-4 --lr-decay-style cosine --min-lr 3e-5 --weight-decay 0.1 --clip-grad 1 --lr-warmup-iters 1 --optimizer adam --adam-beta1 0.9 --adam-beta2 0.95 --log-interval 1 --save-interval 1000 --eval-interval 1000 --eval-iters 0 --bf16 --no-query-key-layer-scaling --attention-dropout 0 --hidden-dropout 0 --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization rmsnorm --disable-bias-linear --num-key-value-heads 4 --deepspeed --exit-interval 20 --deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch0-flush_async0-opt_gaps0.json --zero-stage=3 --no-pipeline-parallel --cpu-optimizer --checkpoint-activations --deepspeed-activation-checkpointing
x3006c0s19b1n0: [2024-03-29 16:43:23,923] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b1n0: [2024-03-29 16:43:23,951] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-29 16:43:25,771] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3006c0s19b1n0: [2024-03-29 16:43:25,771] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=0
x3006c0s19b1n0: [2024-03-29 16:43:25,771] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b1n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3006c0s19b1n0: [2024-03-29 16:43:25,771] [INFO] [launch.py:163:main] dist_world_size=8
x3006c0s19b1n0: [2024-03-29 16:43:25,771] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3006c0s19b1n0: [2024-03-29 16:43:25,772] [INFO] [launch.py:253:main] process 46291 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch0-flush_async0-opt_gaps0.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-29 16:43:25,772] [INFO] [launch.py:253:main] process 46292 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch0-flush_async0-opt_gaps0.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-29 16:43:25,773] [INFO] [launch.py:253:main] process 46293 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch0-flush_async0-opt_gaps0.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-29 16:43:25,773] [INFO] [launch.py:253:main] process 46294 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch0-flush_async0-opt_gaps0.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b1n0: [2024-03-29 16:43:26,339] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3006c0s1b1n0: [2024-03-29 16:43:26,339] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=1
x3006c0s1b1n0: [2024-03-29 16:43:26,339] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b1n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3006c0s1b1n0: [2024-03-29 16:43:26,339] [INFO] [launch.py:163:main] dist_world_size=8
x3006c0s1b1n0: [2024-03-29 16:43:26,339] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3006c0s1b1n0: [2024-03-29 16:43:26,339] [INFO] [launch.py:253:main] process 64410 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch0-flush_async0-opt_gaps0.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b1n0: [2024-03-29 16:43:26,340] [INFO] [launch.py:253:main] process 64411 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch0-flush_async0-opt_gaps0.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b1n0: [2024-03-29 16:43:26,341] [INFO] [launch.py:253:main] process 64412 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch0-flush_async0-opt_gaps0.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b1n0: [2024-03-29 16:43:26,341] [INFO] [launch.py:253:main] process 64413 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch0-flush_async0-opt_gaps0.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-29 16:43:27,576] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-29 16:43:27,579] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-29 16:43:27,581] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-29 16:43:27,584] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b1n0: [2024-03-29 16:43:28,112] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b1n0: [2024-03-29 16:43:28,125] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b1n0: [2024-03-29 16:43:28,128] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b1n0: [2024-03-29 16:43:28,170] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: INFO: overriding default arguments for tokenizer_type:GPTSentencePieceTokenizer                    with tokenizer_type:GPT2BPETokenizer
x3006c0s19b1n0: using world size: 8, data-parallel-size: 8, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
x3006c0s19b1n0: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
x3006c0s19b1n0: using torch.bfloat16 for parameters ...
x3006c0s19b1n0: ------------------------ arguments ------------------------
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0:   accumulate_allreduce_grads_in_fp32 .............. True
x3006c0s19b1n0:   adam_beta1 ...................................... 0.9
x3006c0s19b1n0:   adam_beta2 ...................................... 0.95
x3006c0s19b1n0:   adam_eps ........................................ 1e-08
x3006c0s19b1n0:   add_bias_linear ................................. False
x3006c0s19b1n0:   add_position_embedding .......................... False
x3006c0s19b1n0:   adlr_autoresume ................................. False
x3006c0s19b1n0:   adlr_autoresume_interval ........................ 1000
x3006c0s19b1n0:   aml_data_download_path .......................... None
x3006c0s19b1n0:   apply_layernorm_1p .............................. False
x3006c0s19b1n0:   apply_query_key_layer_scaling ................... False
x3006c0s19b1n0:   apply_residual_connection_post_layernorm ........ False
x3006c0s19b1n0:   async_tensor_model_parallel_allreduce ........... False
x3006c0s19b1n0:   attention_dropout ............................... 0.0
x3006c0s19b1n0:   attention_softmax_in_fp32 ....................... False
x3006c0s19b1n0:   barrier_with_L1_time ............................ True
x3006c0s19b1n0:   bert_binary_head ................................ True
x3006c0s19b1n0:   bert_embedder_type .............................. megatron
x3006c0s19b1n0:   bert_load ....................................... None
x3006c0s19b1n0:   bf16 ............................................ True
x3006c0s19b1n0:   bias_dropout_fusion ............................. True
x3006c0s19b1n0:   bias_gelu_fusion ................................ False
x3006c0s19b1n0:   biencoder_projection_dim ........................ 0
x3006c0s19b1n0:   biencoder_shared_query_context_model ............ False
x3006c0s19b1n0:   block_data_path ................................. None
x3006c0s19b1n0:   checkpoint_activations .......................... True
x3006c0s19b1n0:   checkpoint_in_cpu ............................... False
x3006c0s19b1n0:   checkpoint_num_layers ........................... 1
x3006c0s19b1n0:   classes_fraction ................................ 1.0
x3006c0s19b1n0:   clip_grad ....................................... 1.0
x3006c0s19b1n0:   compression_training ............................ False
x3006c0s19b1n0:   consumed_train_samples .......................... 0
x3006c0s19b1n0:   consumed_train_tokens ........................... 0
x3006c0s19b1n0:   consumed_valid_samples .......................... 0
x3006c0s19b1n0:   contigious_checkpointing ........................ False
x3006c0s19b1n0:   cpu_optimizer ................................... True
x3006c0s19b1n0:   cpu_torch_adam .................................. False
x3006c0s19b1n0:   create_moe_param_group .......................... False
x3006c0s19b1n0:   curriculum_learning_legacy ...................... False
x3006c0s19b1n0:   data_cache_path ................................. None
x3006c0s19b1n0:   data_efficiency_curriculum_learning ............. False
x3006c0s19b1n0:   data_impl ....................................... mmap
x3006c0s19b1n0:   data_parallel_random_init ....................... False
x3006c0s19b1n0:   data_parallel_size .............................. 8
x3006c0s19b1n0:   data_path ....................................... ['/home/am6429/dl-io/datasets/meg-gpt2_text_document']
x3006c0s19b1n0:   data_per_class_fraction ......................... 1.0
x3006c0s19b1n0:   data_sharding ................................... True
x3006c0s19b1n0:   dataloader_type ................................. single
x3006c0s19b1n0:   DDP_impl ........................................ local
x3006c0s19b1n0:   decoder_num_layers .............................. None
x3006c0s19b1n0:   decoder_seq_length .............................. None
x3006c0s19b1n0:   deepscale ....................................... False
x3006c0s19b1n0:   deepscale_config ................................ None
x3006c0s19b1n0:   deepspeed ....................................... True
x3006c0s19b1n0:   deepspeed_activation_checkpointing .............. True
x3006c0s19b1n0:   deepspeed_config ................................ /home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch0-flush_async0-opt_gaps0.json
x3006c0s19b1n0:   dino_bottleneck_size ............................ 256
x3006c0s19b1n0:   dino_freeze_last_layer .......................... 1
x3006c0s19b1n0:   dino_head_hidden_size ........................... 2048
x3006c0s19b1n0:   dino_local_crops_number ......................... 10
x3006c0s19b1n0:   dino_local_img_size ............................. 96
x3006c0s19b1n0:   dino_norm_last_layer ............................ False
x3006c0s19b1n0:   dino_teacher_temp ............................... 0.07
x3006c0s19b1n0:   dino_warmup_teacher_temp ........................ 0.04
x3006c0s19b1n0:   dino_warmup_teacher_temp_epochs ................. 30
x3006c0s19b1n0:   distribute_checkpointed_activations ............. False
x3006c0s19b1n0:   distribute_saved_activations .................... False
x3006c0s19b1n0:   distributed_backend ............................. nccl
x3006c0s19b1n0:   distributed_timeout_minutes ..................... 10
x3006c0s19b1n0:   ds_inference .................................... False
x3006c0s19b1n0:   ds_pipeline_enabled ............................. False
x3006c0s19b1n0:   ds_sequence_parallel_size ....................... 1
x3006c0s19b1n0:   embedding_path .................................. None
x3006c0s19b1n0:   embedding_weights_in_fp32 ....................... False
x3006c0s19b1n0:   empty_unused_memory_level ....................... 0
x3006c0s19b1n0:   enable_expert_tensor_parallelism ................ False
x3006c0s19b1n0:   encoder_num_layers .............................. 60
x3006c0s19b1n0:   encoder_seq_length .............................. 2048
x3006c0s19b1n0:   end_weight_decay ................................ 0.1
x3006c0s19b1n0:   eod_mask_loss ................................... False
x3006c0s19b1n0:   eval_interval ................................... 1000
x3006c0s19b1n0:   eval_iters ...................................... 0
x3006c0s19b1n0:   evidence_data_path .............................. None
x3006c0s19b1n0:   exit_duration_in_mins ........................... None
x3006c0s19b1n0:   exit_interval ................................... 20
x3006c0s19b1n0:   exit_on_missing_checkpoint ...................... False
x3006c0s19b1n0:   exit_signal_handler ............................. False
x3006c0s19b1n0:   expert_interval ................................. 2
x3006c0s19b1n0:   ffn_hidden_size ................................. 17728
x3006c0s19b1n0:   finetune ........................................ False
x3006c0s19b1n0:   force_ds_sequence_parallel ...................... False
x3006c0s19b1n0:   fp16 ............................................ False
x3006c0s19b1n0:   fp16_lm_cross_entropy ........................... False
x3006c0s19b1n0:   fp32_residual_connection ........................ False
x3006c0s19b1n0:   fp8_amax_compute_algo ........................... most_recent
x3006c0s19b1n0:   fp8_amax_history_len ............................ 1
x3006c0s19b1n0:   fp8_e4m3 ........................................ False
x3006c0s19b1n0:   fp8_hybrid ...................................... False
x3006c0s19b1n0:   fp8_interval .................................... 1
x3006c0s19b1n0:   fp8_margin ...................................... 0
x3006c0s19b1n0:   fp8_wgrad ....................................... True
x3006c0s19b1n0:   global_batch_size ............................... 32
x3006c0s19b1n0:   gradient_accumulation_fusion .................... True
x3006c0s19b1n0:   head_lr_mult .................................... 1.0
x3006c0s19b1n0:   hidden_dropout .................................. 0.0
x3006c0s19b1n0:   hidden_size ..................................... 6656
x3006c0s19b1n0:   hidden_size_teacher ............................. None
x3006c0s19b1n0:   hysteresis ...................................... 2
x3006c0s19b1n0:   ict_head_size ................................... None
x3006c0s19b1n0:   ict_load ........................................ None
x3006c0s19b1n0:   img_h ........................................... 224
x3006c0s19b1n0:   img_w ........................................... 224
x3006c0s19b1n0:   indexer_batch_size .............................. 128
x3006c0s19b1n0:   indexer_log_interval ............................ 1000
x3006c0s19b1n0:   inference ....................................... False
x3006c0s19b1n0:   inference_batch_times_seqlen_threshold .......... 512
x3006c0s19b1n0:   init_method_std ................................. 0.02
x3006c0s19b1n0:   init_method_xavier_uniform ...................... False
x3006c0s19b1n0:   initial_loss_scale .............................. 4294967296
x3006c0s19b1n0:   iter_per_epoch .................................. 1250
x3006c0s19b1n0:   kd .............................................. False
x3006c0s19b1n0:   kd_alpha_ce ..................................... 1
x3006c0s19b1n0:   kd_beta_ce ...................................... 1
x3006c0s19b1n0:   kd_temp ......................................... 1.0
x3006c0s19b1n0:   kv_channels ..................................... 128
x3006c0s19b1n0:   layernorm_epsilon ............................... 1e-05
x3006c0s19b1n0:   lazy_mpu_init ................................... None
x3006c0s19b1n0:   load ............................................ None
x3006c0s19b1n0:   load_teacher .................................... None
x3006c0s19b1n0:   local_rank ...................................... 0
x3006c0s19b1n0:   log_batch_size_to_tensorboard ................... False
x3006c0s19b1n0:   log_interval .................................... 1
x3006c0s19b1n0:   log_learning_rate_to_tensorboard ................ True
x3006c0s19b1n0:   log_loss_scale_to_tensorboard ................... True
x3006c0s19b1n0:   log_memory_to_tensorboard ....................... False
x3006c0s19b1n0:   log_num_zeros_in_grad ........................... False
x3006c0s19b1n0:   log_optimizer_states_to_tensorboard ............. False
x3006c0s19b1n0:   log_params_norm ................................. False
x3006c0s19b1n0:   log_timers_to_tensorboard ....................... False
x3006c0s19b1n0:   log_validation_ppl_to_tensorboard ............... False
x3006c0s19b1n0:   log_world_size_to_tensorboard ................... False
x3006c0s19b1n0:   loss_scale ...................................... None
x3006c0s19b1n0:   loss_scale_window ............................... 1000
x3006c0s19b1n0:   lr .............................................. 0.0003
x3006c0s19b1n0:   lr_decay_iters .................................. None
x3006c0s19b1n0:   lr_decay_samples ................................ None
x3006c0s19b1n0:   lr_decay_style .................................. cosine
x3006c0s19b1n0:   lr_decay_tokens ................................. None
x3006c0s19b1n0:   lr_warmup_fraction .............................. None
x3006c0s19b1n0:   lr_warmup_iters ................................. 1
x3006c0s19b1n0:   lr_warmup_samples ............................... 0
x3006c0s19b1n0:   lr_warmup_tokens ................................ None
x3006c0s19b1n0:   make_vocab_size_divisible_by .................... 128
x3006c0s19b1n0:   mask_factor ..................................... 1.0
x3006c0s19b1n0:   mask_prob ....................................... 0.15
x3006c0s19b1n0:   mask_type ....................................... random
x3006c0s19b1n0:   masked_softmax_fusion ........................... True
x3006c0s19b1n0:   max_position_embeddings ......................... 2048
x3006c0s19b1n0:   max_tokens_to_oom ............................... 12000
x3006c0s19b1n0:   memory_centric_tiled_linear ..................... False
x3006c0s19b1n0:   merge_file ...................................... /home/am6429/dl-io/datasets/gpt2-merges.txt
x3006c0s19b1n0:   micro_batch_size ................................ 4
x3006c0s19b1n0:   min_loss_scale .................................. 1.0
x3006c0s19b1n0:   min_lr .......................................... 3e-05
x3006c0s19b1n0:   mlp_type ........................................ standard
x3006c0s19b1n0:   mmap_warmup ..................................... False
x3006c0s19b1n0:   moe_eval_capacity_factor ........................ 1.0
x3006c0s19b1n0:   moe_expert_parallel_size ........................ 1
x3006c0s19b1n0:   moe_loss_coeff .................................. 0.1
x3006c0s19b1n0:   moe_min_capacity ................................ 4
x3006c0s19b1n0:   moe_token_dropping .............................. True
x3006c0s19b1n0:   moe_train_capacity_factor ....................... 1.0
x3006c0s19b1n0:   mos ............................................. False
x3006c0s19b1n0:   no_load_lr_state ................................ False
x3006c0s19b1n0:   no_load_optim ................................... None
x3006c0s19b1n0:   no_load_rng ..................................... None
x3006c0s19b1n0:   no_persist_layer_norm ........................... False
x3006c0s19b1n0:   no_pipeline_parallel ............................ True
x3006c0s19b1n0:   no_save_optim ................................... None
x3006c0s19b1n0:   no_save_rng ..................................... None
x3006c0s19b1n0:   normalization ................................... rmsnorm
x3006c0s19b1n0:   num_attention_heads ............................. 52
x3006c0s19b1n0:   num_attention_heads_teacher ..................... None
x3006c0s19b1n0:   num_channels .................................... 3
x3006c0s19b1n0:   num_classes ..................................... 1000
x3006c0s19b1n0:   num_experts ..................................... [1]
x3006c0s19b1n0:   num_experts_switch .............................. None
x3006c0s19b1n0:   num_experts_teacher ............................. [1]
x3006c0s19b1n0:   num_key_value_heads ............................. 4
x3006c0s19b1n0:   num_layers ...................................... 60
x3006c0s19b1n0:   num_layers_per_virtual_pipeline_stage ........... None
x3006c0s19b1n0:   num_layers_teacher .............................. None
x3006c0s19b1n0:   num_workers ..................................... 2
x3006c0s19b1n0:   onnx_safe ....................................... None
x3006c0s19b1n0:   openai_gelu ..................................... False
x3006c0s19b1n0:   optimizer ....................................... adam
x3006c0s19b1n0:   output_bert_embeddings .......................... False
x3006c0s19b1n0:   overlap_p2p_comm ................................ False
x3006c0s19b1n0:   override_opt_param_scheduler .................... False
x3006c0s19b1n0:   params_dtype .................................... torch.bfloat16
x3006c0s19b1n0:   partition_activations ........................... False
x3006c0s19b1n0:   patch_dim ....................................... 16
x3006c0s19b1n0:   perform_initialization .......................... True
x3006c0s19b1n0:   pipeline_model_parallel_size .................... 1
x3006c0s19b1n0:   pipeline_model_parallel_split_rank .............. None
x3006c0s19b1n0:   profile_backward ................................ False
x3006c0s19b1n0:   query_in_block_prob ............................. 0.1
x3006c0s19b1n0:   rampup_batch_size ............................... None
x3006c0s19b1n0:   random_ltd ...................................... False
x3006c0s19b1n0:   rank ............................................ 0
x3006c0s19b1n0:   recompute_granularity ........................... None
x3006c0s19b1n0:   recompute_method ................................ None
x3006c0s19b1n0:   recompute_num_layers ............................ 1
x3006c0s19b1n0:   remote_device ................................... none
x3006c0s19b1n0:   reset_attention_mask ............................ False
x3006c0s19b1n0:   reset_iteration ................................. False
x3006c0s19b1n0:   reset_position_ids .............................. False
x3006c0s19b1n0:   retriever_report_topk_accuracies ................ []
x3006c0s19b1n0:   retriever_score_scaling ......................... False
x3006c0s19b1n0:   retriever_seq_length ............................ 256
x3006c0s19b1n0:   retro_add_retriever ............................. False
x3006c0s19b1n0:   retro_cyclic_train_iters ........................ None
x3006c0s19b1n0:   retro_encoder_attention_dropout ................. 0.1
x3006c0s19b1n0:   retro_encoder_hidden_dropout .................... 0.1
x3006c0s19b1n0:   retro_encoder_layers ............................ 2
x3006c0s19b1n0:   retro_num_neighbors ............................. 2
x3006c0s19b1n0:   retro_num_retrieved_chunks ...................... 2
x3006c0s19b1n0:   retro_return_doc_ids ............................ False
x3006c0s19b1n0:   retro_workdir ................................... None
x3006c0s19b1n0:   return_data_index ............................... False
x3006c0s19b1n0:   rotary_percent .................................. 1.0
x3006c0s19b1n0:   sample_rate ..................................... 1.0
x3006c0s19b1n0:   save ............................................ /local/scratch/llama2/zero3-tp1}_dp8
x3006c0s19b1n0:   save_interval ................................... 1000
x3006c0s19b1n0:   scatter_gather_tensors_in_pipeline .............. True
x3006c0s19b1n0:   scattered_embeddings ............................ False
x3006c0s19b1n0:   seed ............................................ 1234
x3006c0s19b1n0:   seq_length ...................................... 2048
x3006c0s19b1n0:   sequence_parallel ............................... False
x3006c0s19b1n0:   sgd_momentum .................................... 0.9
x3006c0s19b1n0:   short_seq_prob .................................. 0.1
x3006c0s19b1n0:   skip_train ...................................... False
x3006c0s19b1n0:   split ........................................... 949,50,1
x3006c0s19b1n0:   split_transformers .............................. False
x3006c0s19b1n0:   squared_relu .................................... False
x3006c0s19b1n0:   standalone_embedding_stage ...................... False
x3006c0s19b1n0:   start_weight_decay .............................. 0.1
x3006c0s19b1n0:   swiglu .......................................... True
x3006c0s19b1n0:   swin_backbone_type .............................. tiny
x3006c0s19b1n0:   synchronize_each_layer .......................... False
x3006c0s19b1n0:   tensor_model_parallel_size ...................... 1
x3006c0s19b1n0:   tensorboard_dir ................................. None
x3006c0s19b1n0:   tensorboard_log_interval ........................ 1
x3006c0s19b1n0:   tensorboard_queue_size .......................... 1000
x3006c0s19b1n0:   test_data_path .................................. None
x3006c0s19b1n0:   tile_factor ..................................... 1
x3006c0s19b1n0:   timing_log_level ................................ 0
x3006c0s19b1n0:   timing_log_option ............................... minmax
x3006c0s19b1n0:   titles_data_path ................................ None
x3006c0s19b1n0:   tokenizer_model ................................. /home/am6429/dl-io/datasets/tokenizer.model
x3006c0s19b1n0:   tokenizer_type .................................. GPT2BPETokenizer
x3006c0s19b1n0:   topk ............................................ 1
x3006c0s19b1n0:   train_data_exact_num_epochs ..................... None
x3006c0s19b1n0:   train_data_path ................................. None
x3006c0s19b1n0:   train_desc_path ................................. None
x3006c0s19b1n0:   train_doc_idx_path .............................. None
x3006c0s19b1n0:   train_idx_path .................................. None
x3006c0s19b1n0:   train_iters ..................................... 10
x3006c0s19b1n0:   train_sample_idx_path ........................... None
x3006c0s19b1n0:   train_samples ................................... None
x3006c0s19b1n0:   train_shuffle_idx_path .......................... None
x3006c0s19b1n0:   train_tokens .................................... None
x3006c0s19b1n0:   transformer_impl ................................ local
x3006c0s19b1n0:   transformer_pipeline_model_parallel_size ........ 1
x3006c0s19b1n0:   untie_embeddings_and_output_weights ............. True
x3006c0s19b1n0:   use_checkpoint_args ............................. False
x3006c0s19b1n0:   use_checkpoint_opt_param_scheduler .............. False
x3006c0s19b1n0:   use_contiguous_buffers_in_local_ddp ............. True
x3006c0s19b1n0:   use_cpu_initialization .......................... None
x3006c0s19b1n0:   use_dataset_only ................................ False
x3006c0s19b1n0:   use_distributed_optimizer ....................... False
x3006c0s19b1n0:   use_flash_attn .................................. False
x3006c0s19b1n0:   use_flash_attn_triton ........................... False
x3006c0s19b1n0:   use_flash_attn_v1 ............................... False
x3006c0s19b1n0:   use_flash_attn_v2 ............................... False
x3006c0s19b1n0:   use_one_sent_docs ............................... False
x3006c0s19b1n0:   use_pin_memory .................................. False
x3006c0s19b1n0:   use_ring_exchange_p2p ........................... False
x3006c0s19b1n0:   use_rotary_position_embeddings .................. True
x3006c0s19b1n0:   use_tutel ....................................... False
x3006c0s19b1n0:   valid_data_path ................................. None
x3006c0s19b1n0:   variable_seq_lengths ............................ False
x3006c0s19b1n0:   virtual_pipeline_model_parallel_size ............ None
x3006c0s19b1n0:   vision_backbone_type ............................ vit
x3006c0s19b1n0:   vision_pretraining .............................. False
x3006c0s19b1n0:   vision_pretraining_type ......................... classify
x3006c0s19b1n0:   vocab_extra_ids ................................. 0
x3006c0s19b1n0:   vocab_file ...................................... /home/am6429/dl-io/datasets/gpt2-vocab.json
x3006c0s19b1n0:   vocab_size ...................................... None
x3006c0s19b1n0:   weight_decay .................................... 0.1
x3006c0s19b1n0:   weight_decay_incr_style ......................... constant
x3006c0s19b1n0:   world_size ...................................... 8
x3006c0s19b1n0:   zero_allgather_bucket_size ...................... 0.0
x3006c0s19b1n0:   zero_contigious_gradients ....................... False
x3006c0s19b1n0:   zero_reduce_bucket_size ......................... 0.0
x3006c0s19b1n0:   zero_reduce_scatter ............................. False
x3006c0s19b1n0:   zero_stage ...................................... 3
x3006c0s19b1n0: -------------------- end of arguments ---------------------
x3006c0s19b1n0: setting number of micro-batches to constant 1
x3006c0s19b1n0: > building GPT2BPETokenizer tokenizer ...
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
x3006c0s19b1n0: > initializing torch distributed ...
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: [2024-03-29 16:43:30,577] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: [2024-03-29 16:43:30,577] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: [2024-03-29 16:43:30,604] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: [2024-03-29 16:43:30,620] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: [2024-03-29 16:43:30,622] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b1n0:       meet the required dependencies to JIT install the op.
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: JIT compiled ops requires ninja
x3006c0s1b1n0: ninja .................. [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: op name ................ installed .. compatible
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b1n0:       meet the required dependencies to JIT install the op.
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: JIT compiled ops requires ninja
x3006c0s1b1n0: ninja .................. [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: op name ................ installed .. compatible
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b1n0:       meet the required dependencies to JIT install the op.
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: JIT compiled ops requires ninja
x3006c0s1b1n0: ninja .................. [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: op name ................ installed .. compatible
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b1n0:       meet the required dependencies to JIT install the op.
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: JIT compiled ops requires ninja
x3006c0s1b1n0: ninja .................. [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: op name ................ installed .. compatible
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: DeepSpeed general environment info:
x3006c0s1b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b1n0: torch version .................... 2.0.1+cu118
x3006c0s1b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b1n0: torch cuda version ............... 11.8
x3006c0s1b1n0: torch hip version ................ None
x3006c0s1b1n0: nvcc version ..................... 11.8
x3006c0s1b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: DeepSpeed general environment info:
x3006c0s1b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b1n0: torch version .................... 2.0.1+cu118
x3006c0s1b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b1n0: torch cuda version ............... 11.8
x3006c0s1b1n0: torch hip version ................ None
x3006c0s1b1n0: nvcc version ..................... 11.8
x3006c0s1b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b1n0: [2024-03-29 16:43:31,067] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b1n0: DeepSpeed general environment info:
x3006c0s1b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b1n0: torch version .................... 2.0.1+cu118
x3006c0s1b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b1n0: torch cuda version ............... 11.8
x3006c0s1b1n0: torch hip version ................ None
x3006c0s1b1n0: nvcc version ..................... 11.8
x3006c0s1b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: DeepSpeed general environment info:
x3006c0s1b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b1n0: torch version .................... 2.0.1+cu118
x3006c0s1b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b1n0: torch cuda version ............... 11.8
x3006c0s1b1n0: torch hip version ................ None
x3006c0s1b1n0: nvcc version ..................... 11.8
x3006c0s1b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b1n0: [2024-03-29 16:43:31,133] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b1n0: [2024-03-29 16:43:31,141] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b1n0: [2024-03-29 16:43:31,144] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: > initialized tensor model parallel with size 1
x3006c0s19b1n0: > initialized pipeline model parallel with size 1
x3006c0s19b1n0: > setting random seeds to 1234 ...
x3006c0s19b1n0: [2024-03-29 16:43:31,611] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
x3006c0s19b1n0: > compiling dataset index builder ...
x3006c0s19b1n0: make: Entering directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3006c0s19b1n0: make: Nothing to be done for 'default'.
x3006c0s19b1n0: make: Leaving directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3006c0s19b1n0: >>> done with dataset index builder. Compilation time: 0.087 seconds
x3006c0s19b1n0: > compiling and loading fused kernels ...
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: >>> done with compiling and loading fused kernels. Compilation time: 3.322 seconds
x3006c0s19b1n0: <<<<<<<<<<< 3
x3006c0s19b1n0: <<<<<<<<<<< 1
x3006c0s19b1n0: initialize_megatron took 5.280820846557617
x3006c0s19b1n0: <<<<<<<<<<< 0
x3006c0s1b1n0: <<<<<<<<<<< 6
x3006c0s19b1n0: <<<<<<<<<<< 2
x3006c0s1b1n0: <<<<<<<<<<< 5
x3006c0s1b1n0: <<<<<<<<<<< 7
x3006c0s1b1n0: <<<<<<<<<<< 4
x3006c0s19b1n0: time to initialize megatron (seconds): 5.579
x3006c0s19b1n0: [after megatron is initialized] datetime: 2024-03-29 16:43:35 
x3006c0s19b1n0: get_accelerator and all_reduce  took 0.007509946823120117
x3006c0s19b1n0: building GPT model ...
x3006c0s19b1n0: [2024-03-29 16:43:35,885] [INFO] [utils.py:800:see_memory_usage] Before Building Model
x3006c0s19b1n0: [2024-03-29 16:43:35,886] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 4.62 GB         CA 0.0 GB         Max_CA 5 GB 
x3006c0s19b1n0: [2024-03-29 16:43:35,886] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 22.95 GB, percent = 4.6%
x3006c0s19b1n0: [2024-03-29 16:43:42,685] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 423, num_elems = 27.64B
x3006c0s19b1n0: [2024-03-29 16:43:42,748] [INFO] [utils.py:800:see_memory_usage] After Building Model
x3006c0s19b1n0: [2024-03-29 16:43:42,748] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 7.07 GB         CA 22.31 GB         Max_CA 38 GB 
x3006c0s19b1n0: [2024-03-29 16:43:42,748] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 23.36 GB, percent = 4.6%
x3006c0s19b1n0:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 27635239424
x3006c0s1b1n0: ninja: no work to do.
x3006c0s1b1n0: Time to load cpu_adam op: 2.58764910697937 seconds
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: Time to load cpu_adam op: 2.4495797157287598 seconds
x3006c0s19b1n0: Time to load cpu_adam op: 2.4846243858337402 seconds
x3006c0s19b1n0: Time to load cpu_adam op: 2.48932147026062 seconds
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: Time to load cpu_adam op: 2.452469825744629 seconds
x3006c0s1b1n0: ninja: no work to do.
x3006c0s1b1n0: Time to load cpu_adam op: 2.438199996948242 seconds
x3006c0s1b1n0: ninja: no work to do.
x3006c0s1b1n0: Time to load cpu_adam op: 2.438274383544922 seconds
x3006c0s1b1n0: Time to load cpu_adam op: 2.5606865882873535 seconds
x3006c0s1b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: > learning rate decay style: cosine
x3006c0s19b1n0: DeepSpeed is enabled.
x3006c0s19b1n0: [2024-03-29 16:43:47,211] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.3+8074cd62, git-hash=8074cd62, git-branch=hybrid_opt_offload
x3006c0s19b1n0: [2024-03-29 16:43:47,280] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After args sanity test
x3006c0s19b1n0: [2024-03-29 16:43:47,281] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.31 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:43:47,281] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.71 GB, percent = 6.1%
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-29 16:43:47,336] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure distributed model
x3006c0s19b1n0: [2024-03-29 16:43:47,336] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.31 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:43:47,337] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.79 GB, percent = 6.1%
x3006c0s19b1n0: [2024-03-29 16:43:47,396] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After configure distributed model
x3006c0s19b1n0: [2024-03-29 16:43:47,396] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.31 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:43:47,396] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.79 GB, percent = 6.1%
x3006c0s19b1n0: [2024-03-29 16:43:47,396] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
x3006c0s19b1n0: [2024-03-29 16:43:47,447] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After setting model parameters
x3006c0s19b1n0: [2024-03-29 16:43:47,447] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.31 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:43:47,447] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.79 GB, percent = 6.1%
x3006c0s19b1n0: [2024-03-29 16:43:47,499] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure optimizer
x3006c0s19b1n0: [2024-03-29 16:43:47,499] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.31 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:43:47,499] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.79 GB, percent = 6.1%
x3006c0s19b1n0: [2024-03-29 16:43:47,500] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
x3006c0s19b1n0: [2024-03-29 16:43:47,500] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
x3006c0s1b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-29 16:43:47,517] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
x3006c0s19b1n0: [2024-03-29 16:43:47,517] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
x3006c0s19b1n0: [2024-03-29 16:43:47,517] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
x3006c0s19b1n0: [2024-03-29 16:43:47,517] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
x3006c0s19b1n0: [2024-03-29 16:43:47,567] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
x3006c0s19b1n0: [2024-03-29 16:43:47,568] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.31 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:43:47,568] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.79 GB, percent = 6.1%
x3006c0s19b1n0: [2024-03-29 16:43:47,570] [INFO] [stage3.py:137:__init__] Reduce bucket size 500,000,000
x3006c0s19b1n0: [2024-03-29 16:43:47,570] [INFO] [stage3.py:138:__init__] Prefetch bucket size 50,000,000
x3006c0s1b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-29 16:43:47,621] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
x3006c0s19b1n0: [2024-03-29 16:43:47,622] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.31 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:43:47,622] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.79 GB, percent = 6.1%
x3006c0s1b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: Parameter Offload: Total persistent parameters: 805376 in 121 params
x3006c0s19b1n0: [2024-03-29 16:43:47,697] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
x3006c0s19b1n0: [2024-03-29 16:43:47,698] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.31 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:43:47,698] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.79 GB, percent = 6.1%
x3006c0s19b1n0: [2024-03-29 16:43:47,752] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
x3006c0s19b1n0: [2024-03-29 16:43:47,752] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.31 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:43:47,752] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.79 GB, percent = 6.1%
x3006c0s19b1n0: [2024-03-29 16:43:47,753] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-29 16:43:47,753] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-29 16:43:47,753] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-29 16:43:47,753] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b1n0: [2024-03-29 16:43:47,753] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b1n0: [2024-03-29 16:43:47,753] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b1n0: [2024-03-29 16:43:47,753] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b1n0: [2024-03-29 16:43:47,753] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-29 16:43:47,753] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-29 16:43:47,753] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-29 16:43:47,753] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,754] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:43:47,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:43:47,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-29 16:43:47,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-29 16:43:47,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-29 16:43:47,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b1n0: [2024-03-29 16:43:47,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b1n0: [2024-03-29 16:43:47,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b1n0: [2024-03-29 16:43:47,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b1n0: [2024-03-29 16:43:47,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-29 16:43:49,787] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 31
x3006c0s19b1n0: [2024-03-29 16:43:49,788] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.44 GB         CA 6.44 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:43:49,788] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 50.13 GB, percent = 10.0%
x3006c0s19b1n0: [2024-03-29 16:43:49,848] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
x3006c0s19b1n0: [2024-03-29 16:43:49,849] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-29 16:43:49,849] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 50.14 GB, percent = 10.0%
x3006c0s19b1n0: [2024-03-29 16:43:52,242] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
x3006c0s19b1n0: [2024-03-29 16:43:52,243] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-29 16:43:52,243] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 58.64 GB, percent = 11.7%
x3006c0s19b1n0: [2024-03-29 16:43:54,485] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
x3006c0s19b1n0: [2024-03-29 16:43:54,485] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-29 16:43:54,485] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 85.6 GB, percent = 17.0%
x3006c0s19b1n0: [2024-03-29 16:44:01,337] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | init_optimizer_state: 6835.41
x3006c0s19b1n0: [2024-03-29 16:44:01,407] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
x3006c0s19b1n0: [2024-03-29 16:44:01,408] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-29 16:44:01,408] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 145.78 GB, percent = 29.0%
x3006c0s19b1n0: [2024-03-29 16:44:01,408] [INFO] [stage3.py:520:_setup_for_real_optimizer] optimizer state initialized
x3006c0s19b1n0: [2024-03-29 16:44:09,854] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
x3006c0s19b1n0: [2024-03-29 16:44:09,855] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 8.61 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-29 16:44:09,855] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 178.47 GB, percent = 35.5%
x3006c0s19b1n0: [2024-03-29 16:44:09,855] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
x3006c0s19b1n0: [2024-03-29 16:44:09,920] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure LR scheduler
x3006c0s19b1n0: [2024-03-29 16:44:09,921] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-29 16:44:09,921] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 178.48 GB, percent = 35.5%
x3006c0s19b1n0: [2024-03-29 16:44:09,921] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
x3006c0s19b1n0: [2024-03-29 16:44:09,921] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7f8127ef41f0>
x3006c0s19b1n0: [2024-03-29 16:44:09,921] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-29 16:44:09,985] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before rewriting optimizer step
x3006c0s19b1n0: [2024-03-29 16:44:09,985] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-29 16:44:09,985] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 178.48 GB, percent = 35.5%
x3006c0s19b1n0: [2024-03-29 16:44:10,049] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure checkpointing
x3006c0s19b1n0: [2024-03-29 16:44:10,049] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-29 16:44:10,049] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 178.48 GB, percent = 35.5%
x3006c0s19b1n0: [2024-03-29 16:44:10,050] [INFO] [config.py:998:print] DeepSpeedEngine configuration:
x3006c0s19b1n0: [2024-03-29 16:44:10,050] [INFO] [config.py:1002:print]   activation_checkpointing_config  {
x3006c0s19b1n0:     "partition_activations": false, 
x3006c0s19b1n0:     "contiguous_memory_optimization": false, 
x3006c0s19b1n0:     "cpu_checkpointing": false, 
x3006c0s19b1n0:     "number_checkpoints": null, 
x3006c0s19b1n0:     "synchronize_checkpoint_boundary": false, 
x3006c0s19b1n0:     "profile": false
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-29 16:44:10,050] [INFO] [config.py:1002:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
x3006c0s19b1n0: [2024-03-29 16:44:10,050] [INFO] [config.py:1002:print]   amp_enabled .................. False
x3006c0s19b1n0: [2024-03-29 16:44:10,050] [INFO] [config.py:1002:print]   amp_params ................... False
x3006c0s19b1n0: [2024-03-29 16:44:10,050] [INFO] [config.py:1002:print]   autotuning_config ............ {
x3006c0s19b1n0:     "enabled": false, 
x3006c0s19b1n0:     "start_step": null, 
x3006c0s19b1n0:     "end_step": null, 
x3006c0s19b1n0:     "metric_path": null, 
x3006c0s19b1n0:     "arg_mappings": null, 
x3006c0s19b1n0:     "metric": "throughput", 
x3006c0s19b1n0:     "model_info": null, 
x3006c0s19b1n0:     "results_dir": "autotuning_results", 
x3006c0s19b1n0:     "exps_dir": "autotuning_exps", 
x3006c0s19b1n0:     "overwrite": true, 
x3006c0s19b1n0:     "fast": true, 
x3006c0s19b1n0:     "start_profile_step": 3, 
x3006c0s19b1n0:     "end_profile_step": 5, 
x3006c0s19b1n0:     "tuner_type": "gridsearch", 
x3006c0s19b1n0:     "tuner_early_stopping": 5, 
x3006c0s19b1n0:     "tuner_num_trials": 50, 
x3006c0s19b1n0:     "model_info_path": null, 
x3006c0s19b1n0:     "mp_size": 1, 
x3006c0s19b1n0:     "max_train_batch_size": null, 
x3006c0s19b1n0:     "min_train_batch_size": 1, 
x3006c0s19b1n0:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
x3006c0s19b1n0:     "min_train_micro_batch_size_per_gpu": 1, 
x3006c0s19b1n0:     "num_tuning_micro_batch_sizes": 3
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-29 16:44:10,050] [INFO] [config.py:1002:print]   bfloat16_enabled ............. True
x3006c0s19b1n0: [2024-03-29 16:44:10,050] [INFO] [config.py:1002:print]   bfloat16_immediate_grad_update  False
x3006c0s19b1n0: [2024-03-29 16:44:10,050] [INFO] [config.py:1002:print]   checkpoint_parallel_write_pipeline  False
x3006c0s19b1n0: [2024-03-29 16:44:10,050] [INFO] [config.py:1002:print]   checkpoint_tag_validation_enabled  True
x3006c0s19b1n0: [2024-03-29 16:44:10,050] [INFO] [config.py:1002:print]   checkpoint_tag_validation_fail  False
x3006c0s19b1n0: [2024-03-29 16:44:10,050] [INFO] [config.py:1002:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8127ef4460>
x3006c0s19b1n0: [2024-03-29 16:44:10,050] [INFO] [config.py:1002:print]   communication_data_type ...... None
x3006c0s19b1n0: [2024-03-29 16:44:10,050] [INFO] [config.py:1002:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
x3006c0s19b1n0: [2024-03-29 16:44:10,050] [INFO] [config.py:1002:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
x3006c0s19b1n0: [2024-03-29 16:44:10,050] [INFO] [config.py:1002:print]   curriculum_enabled_legacy .... False
x3006c0s19b1n0: [2024-03-29 16:44:10,050] [INFO] [config.py:1002:print]   curriculum_params_legacy ..... False
x3006c0s19b1n0: [2024-03-29 16:44:10,050] [INFO] [config.py:1002:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
x3006c0s19b1n0: [2024-03-29 16:44:10,050] [INFO] [config.py:1002:print]   data_efficiency_enabled ...... False
x3006c0s19b1n0: [2024-03-29 16:44:10,050] [INFO] [config.py:1002:print]   dataloader_drop_last ......... False
x3006c0s19b1n0: [2024-03-29 16:44:10,050] [INFO] [config.py:1002:print]   datastates_config ............ {
x3006c0s19b1n0:     "enabled": null, 
x3006c0s19b1n0:     "config": {
x3006c0s19b1n0:     }
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-29 16:44:10,050] [INFO] [config.py:1002:print]   disable_allgather ............ False
x3006c0s19b1n0: [2024-03-29 16:44:10,050] [INFO] [config.py:1002:print]   dump_state ................... False
x3006c0s19b1n0: [2024-03-29 16:44:10,050] [INFO] [config.py:1002:print]   dynamic_loss_scale_args ...... None
x3006c0s19b1n0: [2024-03-29 16:44:10,050] [INFO] [config.py:1002:print]   eigenvalue_enabled ........... False
x3006c0s19b1n0: [2024-03-29 16:44:10,050] [INFO] [config.py:1002:print]   eigenvalue_gas_boundary_resolution  1
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   eigenvalue_layer_name ........ bert.encoder.layer
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   eigenvalue_layer_num ......... 0
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   eigenvalue_max_iter .......... 100
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   eigenvalue_stability ......... 1e-06
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   eigenvalue_tol ............... 0.01
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   eigenvalue_verbose ........... False
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   elasticity_enabled ........... False
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   flops_profiler_config ........ {
x3006c0s19b1n0:     "enabled": false, 
x3006c0s19b1n0:     "recompute_fwd_factor": 0.0, 
x3006c0s19b1n0:     "profile_step": 1, 
x3006c0s19b1n0:     "module_depth": -1, 
x3006c0s19b1n0:     "top_modules": 1, 
x3006c0s19b1n0:     "detailed": true, 
x3006c0s19b1n0:     "output_file": null
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   fp16_auto_cast ............... None
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   fp16_enabled ................. False
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   fp16_master_weights_and_gradients  False
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   global_rank .................. 0
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   grad_accum_dtype ............. bf16
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   gradient_accumulation_steps .. 1
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   gradient_clipping ............ 0.0
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   gradient_predivide_factor .... 1.0
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   graph_harvesting ............. False
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   initial_dynamic_scale ........ 1
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   load_universal_checkpoint .... False
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   loss_scale ................... 1.0
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   memory_breakdown ............. True
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   mics_hierarchial_params_gather  False
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   mics_shard_size .............. -1
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   nebula_config ................ {
x3006c0s19b1n0:     "enabled": false, 
x3006c0s19b1n0:     "persistent_storage_path": null, 
x3006c0s19b1n0:     "persistent_time_interval": 100, 
x3006c0s19b1n0:     "num_of_version_in_retention": 2, 
x3006c0s19b1n0:     "enable_nebula_load": true, 
x3006c0s19b1n0:     "load_path": null
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   optimizer_legacy_fusion ...... False
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   optimizer_name ............... None
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   optimizer_params ............. None
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   pld_enabled .................. False
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   pld_params ................... False
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   prescale_gradients ........... False
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   scheduler_name ............... None
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   scheduler_params ............. None
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   seq_parallel_communication_data_type  torch.float32
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   sparse_attention ............. None
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   sparse_gradients_enabled ..... False
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   steps_per_print .............. 1
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   train_batch_size ............. 32
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   train_micro_batch_size_per_gpu  4
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   use_data_before_expert_parallel_  False
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   use_node_local_storage ....... False
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   wall_clock_breakdown ......... True
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   weight_quantization_config ... None
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   world_size ................... 8
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   zero_allow_untested_optimizer  False
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0, prefetch_optimizer=False, part_grads_async=False, prefetch_optimizer_gap=0) sub_group_size=100000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   zero_enabled ................. True
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   zero_force_ds_cpu_optimizer .. True
x3006c0s19b1n0: [2024-03-29 16:44:10,051] [INFO] [config.py:1002:print]   zero_optimization_stage ...... 3
x3006c0s19b1n0: [2024-03-29 16:44:10,052] [INFO] [config.py:988:print_user_config]   json = {
x3006c0s19b1n0:     "train_batch_size": 32, 
x3006c0s19b1n0:     "train_micro_batch_size_per_gpu": 4, 
x3006c0s19b1n0:     "steps_per_print": 1, 
x3006c0s19b1n0:     "zero_optimization": {
x3006c0s19b1n0:         "stage": 3, 
x3006c0s19b1n0:         "offload_optimizer": {
x3006c0s19b1n0:             "device": "cpu", 
x3006c0s19b1n0:             "ratio": 1, 
x3006c0s19b1n0:             "pin_memory": true, 
x3006c0s19b1n0:             "prefetch_optimizer": 0, 
x3006c0s19b1n0:             "part_grads_async": 0, 
x3006c0s19b1n0:             "prefetch_optimizer_gap": 0
x3006c0s19b1n0:         }, 
x3006c0s19b1n0:         "sub_group_size": 1.000000e+08
x3006c0s19b1n0:     }, 
x3006c0s19b1n0:     "bf16": {
x3006c0s19b1n0:         "enabled": true
x3006c0s19b1n0:     }, 
x3006c0s19b1n0:     "data_types": {
x3006c0s19b1n0:         "grad_accum_dtype": "bf16"
x3006c0s19b1n0:     }, 
x3006c0s19b1n0:     "wall_clock_breakdown": true, 
x3006c0s19b1n0:     "memory_breakdown": true, 
x3006c0s19b1n0:     "flops_profiler": {
x3006c0s19b1n0:         "enabled": false
x3006c0s19b1n0:     }
x3006c0s19b1n0: }
x3006c0s1b1n0: <TIMER:model-and-optimizer-setup,35.70154356956482>
x3006c0s1b1n0: <TIMER:model-and-optimizer-setup,35.70164966583252>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,35.70223784446716>
x3006c0s1b1n0: <TIMER:model-and-optimizer-setup,35.70328068733215>
x3006c0s1b1n0: <TIMER:model-and-optimizer-setup,35.70335268974304>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,35.7033326625824>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,35.70370841026306>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,35.704325914382935>
x3006c0s19b1n0: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-29 16:44:11 
x3006c0s19b1n0: > building train, validation, and test datasets ...
x3006c0s19b1n0:  > datasets target sizes (minimum size):
x3006c0s19b1n0:     train:      320
x3006c0s19b1n0:     validation: 0
x3006c0s19b1n0:     test:       0
x3006c0s19b1n0: > building train, validation, and test datasets for GPT ...
x3006c0s19b1n0: Single data path provided for train, valid & test
x3006c0s19b1n0:  > building dataset index ...
x3006c0s19b1n0:     reading sizes...
x3006c0s19b1n0:     reading pointers...
x3006c0s19b1n0:     reading document index...
x3006c0s19b1n0:     creating numpy buffer of mmap...
x3006c0s19b1n0:     creating memory view of numpy buffer...
x3006c0s19b1n0:  > finished creating indexed dataset in 0.001525 seconds
x3006c0s19b1n0:     number of documents: 79000
x3006c0s19b1n0:  > dataset split:
x3006c0s19b1n0:     train:
x3006c0s19b1n0:      document indices in [0, 74971) total of 74971 documents
x3006c0s19b1n0:     validation:
x3006c0s19b1n0:      document indices in [74971, 78921) total of 3950 documents
x3006c0s19b1n0:     test:
x3006c0s19b1n0:      document indices in [78921, 79000) total of 79 documents
x3006c0s19b1n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_doc_idx.npy
x3006c0s19b1n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_sample_idx.npy
x3006c0s19b1n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_shuffle_idx.npy
x3006c0s19b1n0:     loaded indexed file in 0.002 seconds
x3006c0s19b1n0:     total number of samples: 108448
x3006c0s19b1n0:     total number of epochs: 1
x3006c0s19b1n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_doc_idx.npy
x3006c0s19b1n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_sample_idx.npy
x3006c0s19b1n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_shuffle_idx.npy
x3006c0s19b1n0:     loaded indexed file in 0.004 seconds
x3006c0s19b1n0:     total number of samples: 5792
x3006c0s19b1n0:     total number of epochs: 1
x3006c0s19b1n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_doc_idx.npy
x3006c0s19b1n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_sample_idx.npy
x3006c0s19b1n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_shuffle_idx.npy
x3006c0s19b1n0:     loaded indexed file in 0.003 seconds
x3006c0s19b1n0:     total number of samples: 185
x3006c0s19b1n0:     total number of epochs: 1
x3006c0s19b1n0: > finished creating GPT datasets ...
x3006c0s1b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5290303230285645>
x3006c0s1b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5345802307128906>
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5392899513244629>
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5444750785827637><TIMER:train/valid/test-data-iterators-setup,0.5447843074798584>
x3006c0s19b1n0: 
x3006c0s1b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5977942943572998>
x3006c0s1b1n0: <TIMER:train/valid/test-data-iterators-setup,0.6015255451202393>
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.6529331207275391>
x3006c0s19b1n0: [after dataloaders are built] datetime: 2024-03-29 16:44:12 
x3006c0s19b1n0: done with setup ...
x3006c0s19b1n0: training ...
x3006c0s1b1n0: (min, max) time across ranks (ms):
x3006c0s1b1n0:     model-and-optimizer-setup ......................: (35701.54, 35704.33)
x3006c0s1b1n0:     train/valid/test-data-iterators-setup ..........: (529.03, 652.93)
x3006c0s19b1n0: [before training begins] datetime: 2024-03-29 16:44:12 
x3006c0s19b1n0: [before the start of training step] datetime: 2024-03-29 16:44:12 
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:44:12,390] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:44:12,391] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 7.45 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-29 16:44:12,391] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 181.51 GB, percent = 36.1%
x3006c0s19b1n0: [2024-03-29 16:44:12,514] [INFO] [checkpointing.py:539:forward] Activation Checkpointing Information
x3006c0s19b1n0: [2024-03-29 16:44:12,514] [INFO] [checkpointing.py:540:forward] ----Partition Activations False, CPU CHECKPOINTING False
x3006c0s19b1n0: [2024-03-29 16:44:12,514] [INFO] [checkpointing.py:541:forward] ----contiguous Memory Checkpointing False with 60 total layers
x3006c0s19b1n0: [2024-03-29 16:44:12,514] [INFO] [checkpointing.py:543:forward] ----Synchronization False
x3006c0s19b1n0: [2024-03-29 16:44:12,514] [INFO] [checkpointing.py:544:forward] ----Profiling time in checkpointing False
x3006c0s19b1n0: [2024-03-29 16:44:21,328] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:44:21,329] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 19.76 GB         CA 22.95 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-29 16:44:21,329] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 181.67 GB, percent = 36.1%
x3006c0s19b1n0: [2024-03-29 16:44:21,518] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:44:21,518] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 16.84 GB         CA 16.91 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-29 16:44:21,519] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 181.67 GB, percent = 36.1%
x3006c0s19b1n0: [2024-03-29 16:44:50,355] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:44:50,355] [INFO] [utils.py:801:see_memory_usage] MA 10.25 GB         Max_MA 20.43 GB         CA 10.33 GB         Max_CA 25 GB 
x3006c0s19b1n0: [2024-03-29 16:44:50,355] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 184.68 GB, percent = 36.7%
x3006c0s19b1n0: [2024-03-29 16:44:50,427] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:44:50,427] [INFO] [utils.py:801:see_memory_usage] MA 10.25 GB         Max_MA 10.25 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:44:50,427] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 184.72 GB, percent = 36.7%
x3006c0s19b1n0: [2024-03-29 16:44:59,668] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.105854511260986
x3006c0s19b1n0: [2024-03-29 16:44:59,668] [INFO] [stage3.py:2251:step] Full outer step loop took 9.106210470199585
x3006c0s1b1n0: [2024-03-29 16:44:59,957] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.394904375076294
x3006c0s1b1n0: [2024-03-29 16:44:59,957] [INFO] [stage3.py:2251:step] Full outer step loop took 9.395112752914429
x3006c0s19b1n0: [2024-03-29 16:44:59,986] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.424660921096802
x3006c0s19b1n0: [2024-03-29 16:44:59,999] [INFO] [stage3.py:2251:step] Full outer step loop took 9.436039447784424
x3006c0s1b1n0: [2024-03-29 16:45:00,089] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.509669303894043
x3006c0s1b1n0: [2024-03-29 16:45:00,089] [INFO] [stage3.py:2251:step] Full outer step loop took 9.509960889816284
x3006c0s1b1n0: [2024-03-29 16:45:00,093] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.53073000907898
x3006c0s1b1n0: [2024-03-29 16:45:00,093] [INFO] [stage3.py:2251:step] Full outer step loop took 9.530939817428589
x3006c0s1b1n0: [2024-03-29 16:45:00,095] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.533370971679688
x3006c0s1b1n0: [2024-03-29 16:45:00,095] [INFO] [stage3.py:2251:step] Full outer step loop took 9.533585548400879
x3006c0s19b1n0: [2024-03-29 16:45:00,123] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.560737133026123
x3006c0s19b1n0: [2024-03-29 16:45:00,123] [INFO] [stage3.py:2251:step] Full outer step loop took 9.561350345611572
x3006c0s19b1n0: [2024-03-29 16:45:00,177] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.61500883102417
x3006c0s19b1n0: [2024-03-29 16:45:00,177] [INFO] [stage3.py:2251:step] Full outer step loop took 9.615219831466675
x3006c0s1b1n0: [2024-03-29 16:45:00,188] [INFO] [stage3.py:2277:step] End to end step took 9.625808238983154
x3006c0s19b1n0: [2024-03-29 16:45:00,188] [INFO] [stage3.py:2277:step] End to end step took 9.625803470611572
x3006c0s1b1n0: [2024-03-29 16:45:00,188] [INFO] [stage3.py:2277:step] End to end step took 9.625896215438843
x3006c0s19b1n0: [2024-03-29 16:45:00,188] [INFO] [stage3.py:2277:step] End to end step took 9.625940322875977
x3006c0s19b1n0: [2024-03-29 16:45:00,188] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 9106.46
x3006c0s1b1n0: [2024-03-29 16:45:00,188] [INFO] [stage3.py:2277:step] End to end step took 9.626190662384033
x3006c0s19b1n0: [2024-03-29 16:45:00,188] [INFO] [stage3.py:2277:step] End to end step took 9.626265048980713
x3006c0s19b1n0: [2024-03-29 16:45:00,188] [WARNING] [stage3.py:2267:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3006c0s19b1n0: [2024-03-29 16:45:00,188] [INFO] [stage3.py:2277:step] End to end step took 9.62647795677185
x3006c0s1b1n0: [2024-03-29 16:45:00,188] [INFO] [stage3.py:2277:step] End to end step took 9.609131574630737
x3006c0s19b1n0: [2024-03-29 16:45:00,189] [INFO] [logging.py:96:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-29 16:45:00,189] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 9028.32 | bwd_microstep: 28622.58 | bwd_inner_microstep: 28497.05 | bwd_allreduce_microstep: 125.42 | step_microstep: 9761.24
x3006c0s19b1n0: [2024-03-29 16:45:00,189] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 9028.32 | bwd: 28622.57 | bwd_inner: 28497.05 | bwd_allreduce: 125.42 | step: 9761.23
x3006c0s19b1n0: [2024-03-29 16:45:00,285] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:45:00,286] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.25 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:45:00,286] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.3 GB, percent = 67.4%
x3006c0s19b1n0: <TIMER:interval-time,48.09442901611328><TIMER:interval-time,48.09443402290344><TIMER:interval-time,48.09442639350891>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,48.09442901611328>
x3006c0s1b1n0: <TIMER:interval-time,48.09447264671326><TIMER:interval-time,48.09447360038757><TIMER:interval-time,48.094473123550415>
x3006c0s1b1n0: 
x3006c0s1b1n0: 
x3006c0s1b1n0: <TIMER:interval-time,48.09448027610779>
x3006c0s19b1n0: [Rank 0] (after 1 iterations) memory (MB) | allocated: 9224.5439453125 | max allocated: 9224.54443359375 | reserved: 9296.0 | max reserved: 9296.0
x3006c0s1b1n0:  elapsed_time 48.094480 | consumed samples:           32 | consumed tokens:        65536 | elapsed time per iteration (ms): 48094.5 | learning rate: 3.000E-04 | global batch size:    32 | lm loss: 1.215015E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.665 | TFLOPs: 46.04 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:45:00,419] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:45:00,419] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-29 16:45:00,419] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.37 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:45:06,828] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:45:06,829] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:45:06,829] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.36 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:45:06,939] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:45:06,940] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:45:06,940] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.36 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:45:25,521] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:45:25,522] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-29 16:45:25,522] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.02 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:45:25,592] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:45:25,593] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:45:25,593] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.02 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:45:32,375] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.75551962852478
x3006c0s19b1n0: [2024-03-29 16:45:32,376] [INFO] [stage3.py:2251:step] Full outer step loop took 6.755912780761719
x3006c0s1b1n0: [2024-03-29 16:45:32,552] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.932457208633423
x3006c0s1b1n0: [2024-03-29 16:45:32,553] [INFO] [stage3.py:2251:step] Full outer step loop took 6.93281626701355
x3006c0s1b1n0: [2024-03-29 16:45:32,559] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.939322233200073
x3006c0s1b1n0: [2024-03-29 16:45:32,559] [INFO] [stage3.py:2251:step] Full outer step loop took 6.939507722854614
x3006c0s1b1n0: [2024-03-29 16:45:32,637] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.017608404159546
x3006c0s1b1n0: [2024-03-29 16:45:32,638] [INFO] [stage3.py:2251:step] Full outer step loop took 7.017772674560547
x3006c0s1b1n0: [2024-03-29 16:45:32,713] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.093619108200073
x3006c0s1b1n0: [2024-03-29 16:45:32,714] [INFO] [stage3.py:2251:step] Full outer step loop took 7.093775033950806
x3006c0s19b1n0: [2024-03-29 16:45:32,780] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.160303831100464
x3006c0s19b1n0: [2024-03-29 16:45:32,780] [INFO] [stage3.py:2251:step] Full outer step loop took 7.16046667098999
x3006c0s19b1n0: [2024-03-29 16:45:32,875] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.240082263946533
x3006c0s19b1n0: [2024-03-29 16:45:32,875] [INFO] [stage3.py:2251:step] Full outer step loop took 7.240257740020752
x3006c0s19b1n0: [2024-03-29 16:45:32,888] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.268535614013672
x3006c0s19b1n0: [2024-03-29 16:45:32,888] [INFO] [stage3.py:2251:step] Full outer step loop took 7.268689393997192
x3006c0s1b1n0: [2024-03-29 16:45:32,899] [INFO] [stage3.py:2277:step] End to end step took 7.2793354988098145
x3006c0s19b1n0: [2024-03-29 16:45:32,899] [INFO] [stage3.py:2277:step] End to end step took 7.26407790184021
x3006c0s19b1n0: [2024-03-29 16:45:32,899] [INFO] [stage3.py:2277:step] End to end step took 7.279485464096069
x3006c0s1b1n0: [2024-03-29 16:45:32,900] [INFO] [stage3.py:2277:step] End to end step took 7.279781818389893
x3006c0s19b1n0: [2024-03-29 16:45:32,899] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6756.20
x3006c0s19b1n0: [2024-03-29 16:45:32,900] [INFO] [stage3.py:2277:step] End to end step took 7.2798192501068115
x3006c0s1b1n0: [2024-03-29 16:45:32,900] [INFO] [stage3.py:2277:step] End to end step took 7.279909610748291
x3006c0s1b1n0: [2024-03-29 16:45:32,900] [INFO] [stage3.py:2277:step] End to end step took 7.279924392700195
x3006c0s19b1n0: [2024-03-29 16:45:32,900] [INFO] [stage3.py:2277:step] End to end step took 7.280034780502319
x3006c0s19b1n0: [2024-03-29 16:45:32,900] [INFO] [logging.py:96:log_dist] [Rank 0] step=2, skipped=0, lr=[0.0002918585038060976], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-29 16:45:32,900] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6369.87 | bwd_microstep: 18445.49 | bwd_inner_microstep: 18332.26 | bwd_allreduce_microstep: 113.07 | step_microstep: 7307.23
x3006c0s19b1n0: [2024-03-29 16:45:32,900] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6369.85 | bwd: 18445.49 | bwd_inner: 18332.29 | bwd_allreduce: 113.09 | step: 7307.24
x3006c0s19b1n0: [2024-03-29 16:45:33,014] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:45:33,015] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:45:33,015] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.06 GB, percent = 67.4%
x3006c0s19b1n0: <TIMER:interval-time,32.72887992858887><TIMER:interval-time,32.72887444496155><TIMER:interval-time,32.728880405426025>
x3006c0s19b1n0: <TIMER:interval-time,32.72888517379761>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b1n0: <TIMER:interval-time,32.72894358634949>
x3006c0s1b1n0: <TIMER:interval-time,32.72895050048828><TIMER:interval-time,32.72894740104675><TIMER:interval-time,32.728949546813965>
x3006c0s1b1n0: 
x3006c0s1b1n0: 
x3006c0s1b1n0:  elapsed_time 32.728947 | consumed samples:           64 | consumed tokens:       131072 | elapsed time per iteration (ms): 32728.9 | learning rate: 2.919E-04 | global batch size:    32 | lm loss: 1.216527E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.978 | TFLOPs: 67.65 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:45:33,117] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:45:33,117] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-29 16:45:33,117] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.1 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:45:39,393] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:45:39,394] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:45:39,394] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.07 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:45:39,470] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:45:39,470] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:45:39,471] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.01 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:45:58,063] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:45:58,064] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-29 16:45:58,064] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.24 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:45:58,142] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:45:58,142] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:45:58,142] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.24 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:46:05,077] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.909924507141113
x3006c0s19b1n0: [2024-03-29 16:46:05,077] [INFO] [stage3.py:2251:step] Full outer step loop took 6.910233974456787
x3006c0s1b1n0: [2024-03-29 16:46:05,082] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.91475248336792
x3006c0s1b1n0: [2024-03-29 16:46:05,082] [INFO] [stage3.py:2251:step] Full outer step loop took 6.915169715881348
x3006c0s19b1n0: [2024-03-29 16:46:05,259] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.091795921325684
x3006c0s19b1n0: [2024-03-29 16:46:05,259] [INFO] [stage3.py:2251:step] Full outer step loop took 7.092058420181274
x3006c0s1b1n0: [2024-03-29 16:46:05,276] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.109201908111572
x3006c0s1b1n0: [2024-03-29 16:46:05,277] [INFO] [stage3.py:2251:step] Full outer step loop took 7.109381675720215
x3006c0s1b1n0: [2024-03-29 16:46:05,284] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.116943836212158
x3006c0s1b1n0: [2024-03-29 16:46:05,284] [INFO] [stage3.py:2251:step] Full outer step loop took 7.117254972457886
x3006c0s19b1n0: [2024-03-29 16:46:05,360] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.193288564682007
x3006c0s19b1n0: [2024-03-29 16:46:05,361] [INFO] [stage3.py:2251:step] Full outer step loop took 7.193586587905884
x3006c0s1b1n0: [2024-03-29 16:46:05,369] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.201333999633789
x3006c0s1b1n0: [2024-03-29 16:46:05,369] [INFO] [stage3.py:2251:step] Full outer step loop took 7.201483249664307
x3006c0s19b1n0: [2024-03-29 16:46:05,497] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.327925682067871
x3006c0s19b1n0: [2024-03-29 16:46:05,497] [INFO] [stage3.py:2251:step] Full outer step loop took 7.32808256149292
x3006c0s19b1n0: [2024-03-29 16:46:05,508] [INFO] [stage3.py:2277:step] End to end step took 7.3388471603393555
x3006c0s1b1n0: [2024-03-29 16:46:05,508] [INFO] [stage3.py:2277:step] End to end step took 7.340424537658691
x3006c0s1b1n0: [2024-03-29 16:46:05,508] [INFO] [stage3.py:2277:step] End to end step took 7.340635061264038
x3006c0s19b1n0: [2024-03-29 16:46:05,508] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6910.48
x3006c0s1b1n0: [2024-03-29 16:46:05,508] [INFO] [stage3.py:2277:step] End to end step took 7.340803146362305
x3006c0s19b1n0: [2024-03-29 16:46:05,508] [INFO] [stage3.py:2277:step] End to end step took 7.340996503829956
x3006c0s1b1n0: [2024-03-29 16:46:05,508] [INFO] [stage3.py:2277:step] End to end step took 7.340969800949097
x3006c0s19b1n0: [2024-03-29 16:46:05,508] [INFO] [stage3.py:2277:step] End to end step took 7.340979099273682
x3006c0s19b1n0: [2024-03-29 16:46:05,508] [INFO] [stage3.py:2277:step] End to end step took 7.341013669967651
x3006c0s19b1n0: [2024-03-29 16:46:05,508] [INFO] [logging.py:96:log_dist] [Rank 0] step=3, skipped=0, lr=[0.00026841599982106197], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-29 16:46:05,509] [INFO] [timer.py:260:stop] epoch=0/micro_step=3/global_step=3, RunningAvgSamplesPerSec=0.9879600943340464, CurrSamplesPerSec=0.9879600943340464, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-29 16:46:05,509] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6242.19 | bwd_microstep: 18461.81 | bwd_inner_microstep: 18355.50 | bwd_allreduce_microstep: 106.20 | step_microstep: 7366.39
x3006c0s19b1n0: [2024-03-29 16:46:05,509] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6242.18 | bwd: 18461.81 | bwd_inner: 18355.52 | bwd_allreduce: 106.21 | step: 7366.39
x3006c0s19b1n0: [2024-03-29 16:46:05,619] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:46:05,620] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:46:05,620] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.27 GB, percent = 67.4%
x3006c0s19b1n0: <TIMER:interval-time,32.60459613800049><TIMER:interval-time,32.604599952697754><TIMER:interval-time,32.60460090637207>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,32.60461616516113>
x3006c0s1b1n0: <TIMER:interval-time,32.604573488235474><TIMER:interval-time,32.60457801818848><TIMER:interval-time,32.60457897186279>
x3006c0s1b1n0: 
x3006c0s1b1n0: 
x3006c0s1b1n0: <TIMER:interval-time,32.6045880317688>
x3006c0s1b1n0:  elapsed_time 32.604578 | consumed samples:           96 | consumed tokens:       196608 | elapsed time per iteration (ms): 32604.6 | learning rate: 2.684E-04 | global batch size:    32 | lm loss: 3.444557E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.981 | TFLOPs: 67.91 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:46:05,763] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:46:05,763] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-29 16:46:05,764] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.28 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:46:11,973] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:46:11,973] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:46:11,973] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.26 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:46:12,054] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:46:12,055] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:46:12,055] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.21 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:46:30,765] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:46:30,766] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-29 16:46:30,766] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.56 GB, percent = 67.5%
x3006c0s19b1n0: [2024-03-29 16:46:30,839] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:46:30,839] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:46:30,840] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.56 GB, percent = 67.5%
x3006c0s1b1n0: [2024-03-29 16:46:37,559] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.694845199584961
x3006c0s1b1n0: [2024-03-29 16:46:37,559] [INFO] [stage3.py:2251:step] Full outer step loop took 6.69514274597168
x3006c0s1b1n0: [2024-03-29 16:46:37,904] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.040369510650635
x3006c0s1b1n0: [2024-03-29 16:46:37,905] [INFO] [stage3.py:2251:step] Full outer step loop took 7.040665864944458
x3006c0s19b1n0: [2024-03-29 16:46:37,915] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.050915718078613
x3006c0s19b1n0: [2024-03-29 16:46:37,915] [INFO] [stage3.py:2251:step] Full outer step loop took 7.051177740097046
x3006c0s19b1n0: [2024-03-29 16:46:37,953] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.08903694152832
x3006c0s19b1n0: [2024-03-29 16:46:37,953] [INFO] [stage3.py:2251:step] Full outer step loop took 7.0892181396484375
x3006c0s1b1n0: [2024-03-29 16:46:37,994] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.129756689071655
x3006c0s1b1n0: [2024-03-29 16:46:37,994] [INFO] [stage3.py:2251:step] Full outer step loop took 7.1299662590026855
x3006c0s1b1n0: [2024-03-29 16:46:38,061] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.197030782699585
x3006c0s1b1n0: [2024-03-29 16:46:38,061] [INFO] [stage3.py:2251:step] Full outer step loop took 7.197200298309326
x3006c0s19b1n0: [2024-03-29 16:46:38,110] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.23040509223938
x3006c0s19b1n0: [2024-03-29 16:46:38,110] [INFO] [stage3.py:2251:step] Full outer step loop took 7.230655670166016
x3006c0s19b1n0: [2024-03-29 16:46:38,207] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.343512296676636
x3006c0s19b1n0: [2024-03-29 16:46:38,208] [INFO] [stage3.py:2251:step] Full outer step loop took 7.343663692474365
x3006c0s19b1n0: [2024-03-29 16:46:38,218] [INFO] [stage3.py:2277:step] End to end step took 7.354406356811523
x3006c0s1b1n0: [2024-03-29 16:46:38,218] [INFO] [stage3.py:2277:step] End to end step took 7.354384899139404
x3006c0s19b1n0: [2024-03-29 16:46:38,219] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 7051.41
x3006c0s1b1n0: [2024-03-29 16:46:38,219] [INFO] [stage3.py:2277:step] End to end step took 7.354825258255005
x3006c0s19b1n0: [2024-03-29 16:46:38,219] [INFO] [stage3.py:2277:step] End to end step took 7.339192867279053
x3006c0s1b1n0: [2024-03-29 16:46:38,219] [INFO] [stage3.py:2277:step] End to end step took 7.354877948760986
x3006c0s1b1n0: [2024-03-29 16:46:38,219] [INFO] [stage3.py:2277:step] End to end step took 7.354917287826538
x3006c0s19b1n0: [2024-03-29 16:46:38,219] [INFO] [stage3.py:2277:step] End to end step took 7.355059623718262
x3006c0s19b1n0: [2024-03-29 16:46:38,219] [INFO] [stage3.py:2277:step] End to end step took 7.355001926422119
x3006c0s19b1n0: [2024-03-29 16:46:38,219] [INFO] [logging.py:96:log_dist] [Rank 0] step=4, skipped=0, lr=[0.00023249999999999996], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-29 16:46:38,220] [INFO] [timer.py:260:stop] epoch=0/micro_step=4/global_step=4, RunningAvgSamplesPerSec=0.9869753086247263, CurrSamplesPerSec=0.9859924842034674, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-29 16:46:38,220] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6170.78 | bwd_microstep: 18557.66 | bwd_inner_microstep: 18433.57 | bwd_allreduce_microstep: 123.97 | step_microstep: 7380.10
x3006c0s19b1n0: [2024-03-29 16:46:38,220] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6170.76 | bwd: 18557.65 | bwd_inner: 18433.59 | bwd_allreduce: 123.98 | step: 7380.11
x3006c0s19b1n0: [2024-03-29 16:46:38,336] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:46:38,337] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:46:38,337] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.55 GB, percent = 67.5%
x3006c0s19b1n0: <TIMER:interval-time,32.71722364425659><TIMER:interval-time,32.717225551605225><TIMER:interval-time,32.717227935791016>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,32.71723675727844>
x3006c0s1b1n0: <TIMER:interval-time,32.717209577560425><TIMER:interval-time,32.71721005439758>
x3006c0s1b1n0: <TIMER:interval-time,32.7172155380249>
x3006c0s1b1n0: 
x3006c0s1b1n0: <TIMER:interval-time,32.717222929000854>
x3006c0s1b1n0:  elapsed_time 32.717223 | consumed samples:          128 | consumed tokens:       262144 | elapsed time per iteration (ms): 32717.2 | learning rate: 2.325E-04 | global batch size:    32 | lm loss: 2.249373E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.978 | TFLOPs: 67.67 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:46:38,457] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:46:38,457] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-29 16:46:38,458] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.28 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:46:44,652] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:46:44,652] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:46:44,652] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.26 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:46:44,732] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:46:44,732] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:46:44,732] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.26 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:47:03,504] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:47:03,505] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-29 16:47:03,505] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.43 GB, percent = 67.5%
x3006c0s19b1n0: [2024-03-29 16:47:03,576] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:47:03,576] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:47:03,576] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.43 GB, percent = 67.5%
x3006c0s1b1n0: [2024-03-29 16:47:10,588] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.986884832382202
x3006c0s1b1n0: [2024-03-29 16:47:10,588] [INFO] [stage3.py:2251:step] Full outer step loop took 6.987356185913086
x3006c0s1b1n0: [2024-03-29 16:47:10,598] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.997220754623413
x3006c0s1b1n0: [2024-03-29 16:47:10,598] [INFO] [stage3.py:2251:step] Full outer step loop took 6.997525215148926
x3006c0s1b1n0: [2024-03-29 16:47:10,714] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.112800598144531
x3006c0s1b1n0: [2024-03-29 16:47:10,714] [INFO] [stage3.py:2251:step] Full outer step loop took 7.11298131942749
x3006c0s1b1n0: [2024-03-29 16:47:10,724] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.110239744186401
x3006c0s1b1n0: [2024-03-29 16:47:10,725] [INFO] [stage3.py:2251:step] Full outer step loop took 7.110391616821289
x3006c0s19b1n0: [2024-03-29 16:47:10,793] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.191746950149536
x3006c0s19b1n0: [2024-03-29 16:47:10,794] [INFO] [stage3.py:2251:step] Full outer step loop took 7.192853689193726
x3006c0s19b1n0: [2024-03-29 16:47:10,814] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.212950944900513
x3006c0s19b1n0: [2024-03-29 16:47:10,814] [INFO] [stage3.py:2251:step] Full outer step loop took 7.213120937347412
x3006c0s19b1n0: [2024-03-29 16:47:10,822] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.220824241638184
x3006c0s19b1n0: [2024-03-29 16:47:10,822] [INFO] [stage3.py:2251:step] Full outer step loop took 7.220985651016235
x3006c0s19b1n0: [2024-03-29 16:47:10,829] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.2278358936309814
x3006c0s19b1n0: [2024-03-29 16:47:10,829] [INFO] [stage3.py:2251:step] Full outer step loop took 7.227991819381714
x3006c0s1b1n0: [2024-03-29 16:47:10,840] [INFO] [stage3.py:2277:step] End to end step took 7.225458145141602
x3006c0s19b1n0: [2024-03-29 16:47:10,839] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 7193.72
x3006c0s19b1n0: [2024-03-29 16:47:10,840] [INFO] [stage3.py:2277:step] End to end step took 7.238839149475098
x3006c0s19b1n0: [2024-03-29 16:47:10,840] [INFO] [stage3.py:2277:step] End to end step took 7.23890233039856
x3006c0s19b1n0: [2024-03-29 16:47:10,840] [INFO] [stage3.py:2277:step] End to end step took 7.2388951778411865
x3006c0s19b1n0: [2024-03-29 16:47:10,840] [INFO] [stage3.py:2277:step] End to end step took 7.239033460617065
x3006c0s19b1n0: [2024-03-29 16:47:10,840] [INFO] [logging.py:96:log_dist] [Rank 0] step=5, skipped=0, lr=[0.0001884425039850356], mom=[(0.9, 0.95)]
x3006c0s1b1n0: [2024-03-29 16:47:10,840] [INFO] [stage3.py:2277:step] End to end step took 7.239257097244263
x3006c0s1b1n0: [2024-03-29 16:47:10,840] [INFO] [stage3.py:2277:step] End to end step took 7.239430665969849
x3006c0s1b1n0: [2024-03-29 16:47:10,840] [INFO] [stage3.py:2277:step] End to end step took 7.239485025405884
x3006c0s19b1n0: [2024-03-29 16:47:10,840] [INFO] [timer.py:260:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=0.9873899420505878, CurrSamplesPerSec=0.9882202549190504, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-29 16:47:10,841] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6153.07 | bwd_microstep: 18616.91 | bwd_inner_microstep: 18488.47 | bwd_allreduce_microstep: 128.31 | step_microstep: 7264.04
x3006c0s19b1n0: [2024-03-29 16:47:10,841] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6153.06 | bwd: 18616.91 | bwd_inner: 18488.50 | bwd_allreduce: 128.31 | step: 7264.04
x3006c0s19b1n0: [2024-03-29 16:47:10,958] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:47:10,958] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:47:10,958] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.2 GB, percent = 67.4%
x3006c0s19b1n0: <TIMER:interval-time,32.62109398841858><TIMER:interval-time,32.621092081069946><TIMER:interval-time,32.62109684944153>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,32.621105909347534>
x3006c0s1b1n0: <TIMER:interval-time,32.62060618400574><TIMER:interval-time,32.620607137680054><TIMER:interval-time,32.62060785293579><TIMER:interval-time,32.62061095237732>
x3006c0s1b1n0: 
x3006c0s1b1n0: 
x3006c0s1b1n0: 
x3006c0s1b1n0:  elapsed_time 32.620608 | consumed samples:          160 | consumed tokens:       327680 | elapsed time per iteration (ms): 32620.6 | learning rate: 1.884E-04 | global batch size:    32 | lm loss: 1.748318E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.981 | TFLOPs: 67.87 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:47:11,095] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:47:11,096] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-29 16:47:11,096] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.2 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:47:17,757] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:47:17,758] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:47:17,758] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.18 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:47:17,839] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:47:17,840] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:47:17,840] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.18 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:47:37,481] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:47:37,481] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-29 16:47:37,481] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.16 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:47:37,550] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:47:37,551] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:47:37,551] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.16 GB, percent = 67.4%
x3006c0s1b1n0: [2024-03-29 16:47:44,420] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.8443310260772705
x3006c0s1b1n0: [2024-03-29 16:47:44,420] [INFO] [stage3.py:2251:step] Full outer step loop took 6.844605922698975
x3006c0s1b1n0: [2024-03-29 16:47:44,425] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.849474668502808
x3006c0s1b1n0: [2024-03-29 16:47:44,425] [INFO] [stage3.py:2251:step] Full outer step loop took 6.849750518798828
x3006c0s1b1n0: [2024-03-29 16:47:44,563] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.988056898117065
x3006c0s1b1n0: [2024-03-29 16:47:44,563] [INFO] [stage3.py:2251:step] Full outer step loop took 6.988227605819702
x3006c0s1b1n0: [2024-03-29 16:47:44,572] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.996900796890259
x3006c0s1b1n0: [2024-03-29 16:47:44,572] [INFO] [stage3.py:2251:step] Full outer step loop took 6.9970598220825195
x3006c0s19b1n0: [2024-03-29 16:47:44,806] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.213445663452148
x3006c0s19b1n0: [2024-03-29 16:47:44,807] [INFO] [stage3.py:2251:step] Full outer step loop took 7.214839696884155
x3006c0s19b1n0: [2024-03-29 16:47:44,833] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.257300138473511
x3006c0s19b1n0: [2024-03-29 16:47:44,833] [INFO] [stage3.py:2251:step] Full outer step loop took 7.257810115814209
x3006c0s19b1n0: [2024-03-29 16:47:44,847] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.27224326133728
x3006c0s19b1n0: [2024-03-29 16:47:44,848] [INFO] [stage3.py:2251:step] Full outer step loop took 7.27241325378418
x3006c0s19b1n0: [2024-03-29 16:47:44,923] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.348086833953857
x3006c0s19b1n0: [2024-03-29 16:47:44,923] [INFO] [stage3.py:2251:step] Full outer step loop took 7.348242521286011
x3006c0s19b1n0: [2024-03-29 16:47:44,934] [INFO] [stage3.py:2277:step] End to end step took 7.359034061431885
x3006c0s19b1n0: [2024-03-29 16:47:44,934] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 7272.59
x3006c0s1b1n0: [2024-03-29 16:47:44,934] [INFO] [stage3.py:2277:step] End to end step took 7.359058618545532
x3006c0s1b1n0: [2024-03-29 16:47:44,934] [INFO] [stage3.py:2277:step] End to end step took 7.359127521514893
x3006c0s19b1n0: [2024-03-29 16:47:44,934] [INFO] [stage3.py:2277:step] End to end step took 7.359207630157471
x3006c0s19b1n0: [2024-03-29 16:47:44,934] [INFO] [stage3.py:2277:step] End to end step took 7.359329700469971
x3006c0s19b1n0: [2024-03-29 16:47:44,935] [INFO] [logging.py:96:log_dist] [Rank 0] step=6, skipped=0, lr=[0.0001415574960149644], mom=[(0.9, 0.95)]
x3006c0s1b1n0: [2024-03-29 16:47:44,935] [INFO] [stage3.py:2277:step] End to end step took 7.359616041183472
x3006c0s1b1n0: [2024-03-29 16:47:44,935] [INFO] [stage3.py:2277:step] End to end step took 7.359645128250122
x3006c0s19b1n0: [2024-03-29 16:47:44,935] [INFO] [stage3.py:2277:step] End to end step took 7.34235954284668
x3006c0s19b1n0: [2024-03-29 16:47:44,935] [INFO] [timer.py:260:stop] epoch=0/micro_step=6/global_step=6, RunningAvgSamplesPerSec=0.9766258239228364, CurrSamplesPerSec=0.9456970264765617, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-29 16:47:44,935] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6621.65 | bwd_microstep: 19474.76 | bwd_inner_microstep: 19347.28 | bwd_allreduce_microstep: 127.37 | step_microstep: 7383.91
x3006c0s19b1n0: [2024-03-29 16:47:44,935] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6621.63 | bwd: 19474.76 | bwd_inner: 19347.29 | bwd_allreduce: 127.38 | step: 7383.90
x3006c0s19b1n0: [2024-03-29 16:47:45,049] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:47:45,049] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:47:45,049] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.2 GB, percent = 67.4%
x3006c0s19b1n0: <TIMER:interval-time,34.0908145904541><TIMER:interval-time,34.09081792831421><TIMER:interval-time,34.090819358825684>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,34.090829610824585>
x3006c0s1b1n0: <TIMER:interval-time,34.09080958366394><TIMER:interval-time,34.090811252593994>
x3006c0s1b1n0: 
x3006c0s1b1n0: <TIMER:interval-time,34.09092426300049><TIMER:interval-time,34.0909264087677>
x3006c0s1b1n0: 
x3006c0s1b1n0:  elapsed_time 34.090811 | consumed samples:          192 | consumed tokens:       393216 | elapsed time per iteration (ms): 34090.8 | learning rate: 1.416E-04 | global batch size:    32 | lm loss: 1.497432E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.939 | TFLOPs: 64.95 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:47:45,174] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:47:45,175] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-29 16:47:45,175] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.2 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:47:51,873] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:47:51,873] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:47:51,873] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.19 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:47:51,949] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:47:51,950] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:47:51,950] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.19 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:48:11,689] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:48:11,689] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-29 16:48:11,689] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.28 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:48:11,760] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:48:11,761] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:48:11,761] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.28 GB, percent = 67.4%
x3006c0s1b1n0: [2024-03-29 16:48:18,778] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.9921839237213135
x3006c0s1b1n0: [2024-03-29 16:48:18,779] [INFO] [stage3.py:2251:step] Full outer step loop took 6.993190765380859
x3006c0s1b1n0: [2024-03-29 16:48:18,819] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.0110392570495605
x3006c0s1b1n0: [2024-03-29 16:48:18,819] [INFO] [stage3.py:2251:step] Full outer step loop took 7.011237144470215
x3006c0s1b1n0: [2024-03-29 16:48:18,859] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.073354005813599
x3006c0s1b1n0: [2024-03-29 16:48:18,859] [INFO] [stage3.py:2251:step] Full outer step loop took 7.07370924949646
x3006c0s1b1n0: [2024-03-29 16:48:18,891] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.105846881866455
x3006c0s1b1n0: [2024-03-29 16:48:18,891] [INFO] [stage3.py:2251:step] Full outer step loop took 7.10600209236145
x3006c0s19b1n0: [2024-03-29 16:48:18,903] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.117910385131836
x3006c0s19b1n0: [2024-03-29 16:48:18,905] [INFO] [stage3.py:2251:step] Full outer step loop took 7.119434595108032
x3006c0s19b1n0: [2024-03-29 16:48:19,033] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.2473578453063965
x3006c0s19b1n0: [2024-03-29 16:48:19,035] [INFO] [stage3.py:2251:step] Full outer step loop took 7.249275207519531
x3006c0s19b1n0: [2024-03-29 16:48:19,087] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.301199197769165
x3006c0s19b1n0: [2024-03-29 16:48:19,087] [INFO] [stage3.py:2251:step] Full outer step loop took 7.301386594772339
x3006c0s19b1n0: [2024-03-29 16:48:19,098] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.312689781188965
x3006c0s19b1n0: [2024-03-29 16:48:19,098] [INFO] [stage3.py:2251:step] Full outer step loop took 7.312849998474121
x3006c0s19b1n0: [2024-03-29 16:48:19,109] [INFO] [stage3.py:2277:step] End to end step took 7.323709964752197
x3006c0s1b1n0: [2024-03-29 16:48:19,109] [INFO] [stage3.py:2277:step] End to end step took 7.323658227920532
x3006c0s1b1n0: [2024-03-29 16:48:19,109] [INFO] [stage3.py:2277:step] End to end step took 7.3012213706970215
x3006c0s19b1n0: [2024-03-29 16:48:19,109] [INFO] [stage3.py:2277:step] End to end step took 7.323872327804565
x3006c0s1b1n0: [2024-03-29 16:48:19,109] [INFO] [stage3.py:2277:step] End to end step took 7.323981523513794
x3006c0s1b1n0: [2024-03-29 16:48:19,109] [INFO] [stage3.py:2277:step] End to end step took 7.323975324630737
x3006c0s19b1n0: [2024-03-29 16:48:19,109] [INFO] [stage3.py:2277:step] End to end step took 7.324096441268921
x3006c0s19b1n0: [2024-03-29 16:48:19,109] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 7120.45
x3006c0s19b1n0: [2024-03-29 16:48:19,110] [INFO] [stage3.py:2277:step] End to end step took 7.324422121047974
x3006c0s19b1n0: [2024-03-29 16:48:19,110] [INFO] [logging.py:96:log_dist] [Rank 0] step=7, skipped=0, lr=[9.750000000000001e-05], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-29 16:48:19,110] [INFO] [timer.py:260:stop] epoch=0/micro_step=7/global_step=7, RunningAvgSamplesPerSec=0.9697126154625982, CurrSamplesPerSec=0.943011558319184, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-29 16:48:19,111] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6663.04 | bwd_microstep: 19582.44 | bwd_inner_microstep: 19453.25 | bwd_allreduce_microstep: 129.08 | step_microstep: 7349.42
x3006c0s19b1n0: [2024-03-29 16:48:19,111] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6663.03 | bwd: 19582.44 | bwd_inner: 19453.26 | bwd_allreduce: 129.09 | step: 7349.43
x3006c0s19b1n0: [2024-03-29 16:48:19,224] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:48:19,224] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:48:19,225] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.21 GB, percent = 67.4%
x3006c0s19b1n0: <TIMER:interval-time,34.17476439476013><TIMER:interval-time,34.17476296424866><TIMER:interval-time,34.174768924713135>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b1n0: <TIMER:interval-time,34.1747944355011><TIMER:interval-time,34.17479181289673>
x3006c0s1b1n0: 
x3006c0s1b1n0: <TIMER:interval-time,34.174800395965576>
x3006c0s19b1n0: <TIMER:interval-time,34.17488074302673>
x3006c0s1b1n0: <TIMER:interval-time,34.17491102218628>
x3006c0s1b1n0:  elapsed_time 34.174800 | consumed samples:          224 | consumed tokens:       458752 | elapsed time per iteration (ms): 34174.8 | learning rate: 9.750E-05 | global batch size:    32 | lm loss: 1.319906E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.936 | TFLOPs: 64.79 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:48:19,344] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:48:19,345] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-29 16:48:19,345] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.21 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:48:25,555] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:48:25,556] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:48:25,556] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.19 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:48:25,632] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:48:25,632] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:48:25,632] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.19 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:48:45,305] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:48:45,305] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-29 16:48:45,305] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.16 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:48:45,374] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:48:45,375] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:48:45,375] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.16 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:48:52,272] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.873823404312134
x3006c0s19b1n0: [2024-03-29 16:48:52,273] [INFO] [stage3.py:2251:step] Full outer step loop took 6.874523639678955
x3006c0s1b1n0: [2024-03-29 16:48:52,464] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.065754413604736
x3006c0s1b1n0: [2024-03-29 16:48:52,465] [INFO] [stage3.py:2251:step] Full outer step loop took 7.066096544265747
x3006c0s1b1n0: [2024-03-29 16:48:52,470] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.0709474086761475
x3006c0s1b1n0: [2024-03-29 16:48:52,470] [INFO] [stage3.py:2251:step] Full outer step loop took 7.071461200714111
x3006c0s1b1n0: [2024-03-29 16:48:52,484] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.085779666900635
x3006c0s1b1n0: [2024-03-29 16:48:52,485] [INFO] [stage3.py:2251:step] Full outer step loop took 7.085954904556274
x3006c0s1b1n0: [2024-03-29 16:48:52,557] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.158660411834717
x3006c0s1b1n0: [2024-03-29 16:48:52,557] [INFO] [stage3.py:2251:step] Full outer step loop took 7.1588122844696045
x3006c0s19b1n0: [2024-03-29 16:48:52,620] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.2213263511657715
x3006c0s19b1n0: [2024-03-29 16:48:52,621] [INFO] [stage3.py:2251:step] Full outer step loop took 7.2218732833862305
x3006c0s19b1n0: [2024-03-29 16:48:52,693] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.294809341430664
x3006c0s19b1n0: [2024-03-29 16:48:52,694] [INFO] [stage3.py:2251:step] Full outer step loop took 7.294997215270996
x3006c0s19b1n0: [2024-03-29 16:48:52,772] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.3735737800598145
x3006c0s19b1n0: [2024-03-29 16:48:52,772] [INFO] [stage3.py:2251:step] Full outer step loop took 7.373732328414917
x3006c0s19b1n0: [2024-03-29 16:48:52,783] [INFO] [stage3.py:2277:step] End to end step took 7.3843910694122314
x3006c0s19b1n0: [2024-03-29 16:48:52,783] [INFO] [stage3.py:2277:step] End to end step took 7.384714841842651
x3006c0s1b1n0: [2024-03-29 16:48:52,783] [INFO] [stage3.py:2277:step] End to end step took 7.384685516357422
x3006c0s1b1n0: [2024-03-29 16:48:52,784] [INFO] [stage3.py:2277:step] End to end step took 7.384874105453491
x3006c0s1b1n0: [2024-03-29 16:48:52,783] [INFO] [stage3.py:2277:step] End to end step took 7.384814739227295
x3006c0s19b1n0: [2024-03-29 16:48:52,783] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6874.98
x3006c0s19b1n0: [2024-03-29 16:48:52,784] [INFO] [stage3.py:2277:step] End to end step took 7.384845972061157
x3006c0s19b1n0: [2024-03-29 16:48:52,784] [INFO] [stage3.py:2277:step] End to end step took 7.385160684585571
x3006c0s19b1n0: [2024-03-29 16:48:52,784] [INFO] [logging.py:96:log_dist] [Rank 0] step=8, skipped=0, lr=[6.158400017893797e-05], mom=[(0.9, 0.95)]
x3006c0s1b1n0: [2024-03-29 16:48:52,784] [INFO] [stage3.py:2277:step] End to end step took 7.385416030883789
x3006c0s19b1n0: [2024-03-29 16:48:52,784] [INFO] [timer.py:260:stop] epoch=0/micro_step=8/global_step=8, RunningAvgSamplesPerSec=0.9675678239526175, CurrSamplesPerSec=0.9569846240227755, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-29 16:48:52,785] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6175.98 | bwd_microstep: 19507.23 | bwd_inner_microstep: 19384.57 | bwd_allreduce_microstep: 120.65 | step_microstep: 7409.69
x3006c0s19b1n0: [2024-03-29 16:48:52,785] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6175.97 | bwd: 19507.92 | bwd_inner: 19384.59 | bwd_allreduce: 121.65 | step: 7409.69
x3006c0s19b1n0: [2024-03-29 16:48:52,898] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:48:52,899] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:48:52,899] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.2 GB, percent = 67.4%
x3006c0s19b1n0: <TIMER:interval-time,33.674092531204224><TIMER:interval-time,33.6740939617157><TIMER:interval-time,33.6740939617157>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,33.674105644226074>
x3006c0s1b1n0: <TIMER:interval-time,33.6741201877594><TIMER:interval-time,33.67412257194519><TIMER:interval-time,33.674124002456665>
x3006c0s1b1n0: 
x3006c0s1b1n0: 
x3006c0s1b1n0: <TIMER:interval-time,33.674227476119995>
x3006c0s1b1n0:  elapsed_time 33.674123 | consumed samples:          256 | consumed tokens:       524288 | elapsed time per iteration (ms): 33674.1 | learning rate: 6.158E-05 | global batch size:    32 | lm loss: 1.228736E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.950 | TFLOPs: 65.75 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:48:53,031] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:48:53,032] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-29 16:48:53,032] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.21 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:48:59,420] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:48:59,421] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:48:59,421] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.19 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:48:59,507] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:48:59,507] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:48:59,508] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.19 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:49:19,054] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:49:19,055] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-29 16:49:19,055] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.14 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:49:19,137] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:49:19,138] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:49:19,138] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.14 GB, percent = 67.4%
x3006c0s1b1n0: [2024-03-29 16:49:26,168] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.005114555358887
x3006c0s1b1n0: [2024-03-29 16:49:26,169] [INFO] [stage3.py:2251:step] Full outer step loop took 7.006621599197388
x3006c0s1b1n0: [2024-03-29 16:49:26,197] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.034257173538208
x3006c0s1b1n0: [2024-03-29 16:49:26,197] [INFO] [stage3.py:2251:step] Full outer step loop took 7.034447193145752
x3006c0s1b1n0: [2024-03-29 16:49:26,212] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.049989461898804
x3006c0s1b1n0: [2024-03-29 16:49:26,212] [INFO] [stage3.py:2251:step] Full outer step loop took 7.050174951553345
x3006c0s1b1n0: [2024-03-29 16:49:26,223] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.060366153717041
x3006c0s1b1n0: [2024-03-29 16:49:26,223] [INFO] [stage3.py:2251:step] Full outer step loop took 7.060522794723511
x3006c0s19b1n0: [2024-03-29 16:49:26,260] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.098008394241333
x3006c0s19b1n0: [2024-03-29 16:49:26,261] [INFO] [stage3.py:2251:step] Full outer step loop took 7.09911036491394
x3006c0s19b1n0: [2024-03-29 16:49:26,471] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.307857036590576
x3006c0s19b1n0: [2024-03-29 16:49:26,471] [INFO] [stage3.py:2251:step] Full outer step loop took 7.30809211730957
x3006c0s19b1n0: [2024-03-29 16:49:26,560] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.397700309753418
x3006c0s19b1n0: [2024-03-29 16:49:26,560] [INFO] [stage3.py:2251:step] Full outer step loop took 7.397881746292114
x3006c0s19b1n0: [2024-03-29 16:49:26,591] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.428187370300293
x3006c0s19b1n0: [2024-03-29 16:49:26,591] [INFO] [stage3.py:2251:step] Full outer step loop took 7.428348064422607
x3006c0s19b1n0: [2024-03-29 16:49:26,601] [INFO] [stage3.py:2277:step] End to end step took 7.438891410827637
x3006c0s19b1n0: [2024-03-29 16:49:26,601] [INFO] [stage3.py:2277:step] End to end step took 7.438993453979492
x3006c0s1b1n0: [2024-03-29 16:49:26,602] [INFO] [stage3.py:2277:step] End to end step took 7.4392077922821045
x3006c0s19b1n0: [2024-03-29 16:49:26,601] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 7099.76
x3006c0s1b1n0: [2024-03-29 16:49:26,602] [INFO] [stage3.py:2277:step] End to end step took 7.439400672912598
x3006c0s19b1n0: [2024-03-29 16:49:26,602] [INFO] [stage3.py:2277:step] End to end step took 7.4386584758758545
x3006c0s19b1n0: [2024-03-29 16:49:26,602] [INFO] [stage3.py:2277:step] End to end step took 7.439599514007568
x3006c0s1b1n0: [2024-03-29 16:49:26,602] [INFO] [stage3.py:2277:step] End to end step took 7.439527988433838
x3006c0s1b1n0: [2024-03-29 16:49:26,602] [INFO] [stage3.py:2277:step] End to end step took 7.4395670890808105
x3006c0s19b1n0: [2024-03-29 16:49:26,602] [INFO] [logging.py:96:log_dist] [Rank 0] step=9, skipped=0, lr=[3.814149619390238e-05], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-29 16:49:26,602] [INFO] [timer.py:260:stop] epoch=0/micro_step=9/global_step=9, RunningAvgSamplesPerSec=0.9654964853761756, CurrSamplesPerSec=0.9532523308450958, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-29 16:49:26,603] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6347.19 | bwd_microstep: 19404.34 | bwd_inner_microstep: 19287.81 | bwd_allreduce_microstep: 116.41 | step_microstep: 7464.68
x3006c0s19b1n0: [2024-03-29 16:49:26,603] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6347.18 | bwd: 19404.34 | bwd_inner: 19287.83 | bwd_allreduce: 116.42 | step: 7464.68
x3006c0s19b1n0: [2024-03-29 16:49:26,720] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:49:26,720] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:49:26,720] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.14 GB, percent = 67.4%
x3006c0s19b1n0: <TIMER:interval-time,33.82067894935608><TIMER:interval-time,33.820679903030396><TIMER:interval-time,33.820679664611816>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,33.820687770843506>
x3006c0s1b1n0: <TIMER:interval-time,33.82067942619324><TIMER:interval-time,33.82068085670471><TIMER:interval-time,33.82067942619324>
x3006c0s1b1n0: 
x3006c0s1b1n0: 
x3006c0s1b1n0: <TIMER:interval-time,33.820791721343994>
x3006c0s1b1n0:  elapsed_time 33.820681 | consumed samples:          288 | consumed tokens:       589824 | elapsed time per iteration (ms): 33820.7 | learning rate: 3.814E-05 | global batch size:    32 | lm loss: 1.242802E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.946 | TFLOPs: 65.47 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:49:26,857] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:49:26,858] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-29 16:49:26,858] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.14 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:49:33,540] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:49:33,541] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:49:33,541] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.11 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:49:33,636] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:49:33,637] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:49:33,637] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.03 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:49:53,464] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:49:53,464] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-29 16:49:53,464] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.03 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:49:53,539] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:49:53,539] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:49:53,540] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.03 GB, percent = 67.4%
x3006c0s19b1n0: [2024-03-29 16:50:00,487] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.9218430519104
x3006c0s19b1n0: [2024-03-29 16:50:00,487] [INFO] [stage3.py:2251:step] Full outer step loop took 6.922177791595459
x3006c0s1b1n0: [2024-03-29 16:50:00,614] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.0494065284729
x3006c0s1b1n0: [2024-03-29 16:50:00,615] [INFO] [stage3.py:2251:step] Full outer step loop took 7.049711465835571
x3006c0s1b1n0: [2024-03-29 16:50:00,618] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.053071737289429
x3006c0s1b1n0: [2024-03-29 16:50:00,618] [INFO] [stage3.py:2251:step] Full outer step loop took 7.053273677825928
x3006c0s1b1n0: [2024-03-29 16:50:00,670] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.105245590209961
x3006c0s1b1n0: [2024-03-29 16:50:00,671] [INFO] [stage3.py:2251:step] Full outer step loop took 7.105936765670776
x3006c0s19b1n0: [2024-03-29 16:50:00,679] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.11410665512085
x3006c0s19b1n0: [2024-03-29 16:50:00,679] [INFO] [stage3.py:2251:step] Full outer step loop took 7.114324331283569
x3006c0s1b1n0: [2024-03-29 16:50:00,688] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.120839357376099
x3006c0s1b1n0: [2024-03-29 16:50:00,689] [INFO] [stage3.py:2251:step] Full outer step loop took 7.120992422103882
x3006c0s19b1n0: [2024-03-29 16:50:00,869] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.303756237030029
x3006c0s19b1n0: [2024-03-29 16:50:00,869] [INFO] [stage3.py:2251:step] Full outer step loop took 7.3039305210113525
x3006c0s19b1n0: [2024-03-29 16:50:00,948] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.383397102355957
x3006c0s19b1n0: [2024-03-29 16:50:00,948] [INFO] [stage3.py:2251:step] Full outer step loop took 7.383551597595215
x3006c0s19b1n0: [2024-03-29 16:50:00,958] [INFO] [stage3.py:2277:step] End to end step took 7.393515110015869
x3006c0s1b1n0: [2024-03-29 16:50:00,959] [INFO] [stage3.py:2277:step] End to end step took 7.3935816287994385
x3006c0s19b1n0: [2024-03-29 16:50:00,959] [INFO] [stage3.py:2277:step] End to end step took 7.39368748664856
x3006c0s1b1n0: [2024-03-29 16:50:00,959] [INFO] [stage3.py:2277:step] End to end step took 7.39382791519165
x3006c0s19b1n0: [2024-03-29 16:50:00,959] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6922.48
x3006c0s1b1n0: [2024-03-29 16:50:00,959] [INFO] [stage3.py:2277:step] End to end step took 7.393901586532593
x3006c0s19b1n0: [2024-03-29 16:50:00,959] [INFO] [stage3.py:2277:step] End to end step took 7.394073486328125
x3006c0s1b1n0: [2024-03-29 16:50:00,959] [INFO] [stage3.py:2277:step] End to end step took 7.391367673873901
x3006c0s19b1n0: [2024-03-29 16:50:00,959] [INFO] [stage3.py:2277:step] End to end step took 7.394122123718262
x3006c0s19b1n0: [2024-03-29 16:50:00,959] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[3e-05], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-29 16:50:00,960] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=0.9620256195837557, CurrSamplesPerSec=0.9384111431993989, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-29 16:50:00,960] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6645.05 | bwd_microstep: 19662.65 | bwd_inner_microstep: 19531.15 | bwd_allreduce_microstep: 131.36 | step_microstep: 7420.16
x3006c0s19b1n0: [2024-03-29 16:50:00,960] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6645.04 | bwd: 19662.65 | bwd_inner: 19531.18 | bwd_allreduce: 131.36 | step: 7420.16
x3006c0s19b1n0: [2024-03-29 16:50:01,078] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:50:01,078] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:50:01,079] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.06 GB, percent = 67.4%
x3006c0s19b1n0: <TIMER:interval-time,34.357938051223755><TIMER:interval-time,34.357938051223755><TIMER:interval-time,34.35793972015381>
x3006c0s19b1n0: <TIMER:interval-time,34.3579421043396>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b1n0: <TIMER:interval-time,34.357901096343994><TIMER:interval-time,34.35790228843689><TIMER:interval-time,34.35790395736694>
x3006c0s1b1n0: 
x3006c0s1b1n0: 
x3006c0s1b1n0: <TIMER:interval-time,34.35791230201721>
x3006c0s1b1n0:  elapsed_time 34.357901 | consumed samples:          320 | consumed tokens:       655360 | elapsed time per iteration (ms): 34357.9 | learning rate: 3.000E-05 | global batch size:    32 | lm loss: 1.122073E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.931 | TFLOPs: 64.44 |
x3006c0s1b1n0: <<<only_train:348.8958110809326>>>
x3006c0s19b1n0: <<<only_train:348.89561438560486>>>
x3006c0s1b1n0: <<<only_train:348.8956570625305>>>
x3006c0s19b1n0: <<<only_train:348.8958158493042>>><<<only_train:348.89581418037415>>>
x3006c0s19b1n0: 
x3006c0s19b1n0: <<<only_train:348.8956935405731>>>
x3006c0s1b1n0: <<<only_train:348.89588141441345>>><<<only_train:348.89571142196655>>>
x3006c0s1b1n0: 
x3006c0s19b1n0: [after training ends] datetime: 2024-03-29 16:50:01 
x3006c0s19b1n0: <<<full_time:348.8958959579468>>>
x3006c0s19b1n0: <<<full_time:348.89605379104614>>><<<full_time:348.89605617523193>>>
x3006c0s19b1n0: 
x3006c0s19b1n0: <<<full_time:348.8959045410156>>>
x3006c0s1b1n0: <<<full_time:348.8959412574768>>><<<full_time:348.8961172103882>>><<<full_time:348.895947933197>>>
x3006c0s1b1n0: 
x3006c0s1b1n0: 
x3006c0s1b1n0: <<<full_time:348.8962037563324>>>
x3006c0s1b1n0: [2024-03-29 16:50:04,837] [INFO] [launch.py:348:main] Process 64411 exits successfully.
x3006c0s19b1n0: [2024-03-29 16:50:05,312] [INFO] [launch.py:348:main] Process 46293 exits successfully.
x3006c0s1b1n0: [2024-03-29 16:50:07,841] [INFO] [launch.py:348:main] Process 64412 exits successfully.
x3006c0s1b1n0: [2024-03-29 16:50:07,841] [INFO] [launch.py:348:main] Process 64410 exits successfully.
x3006c0s1b1n0: [2024-03-29 16:50:07,841] [INFO] [launch.py:348:main] Process 64413 exits successfully.
x3006c0s19b1n0: [2024-03-29 16:50:08,315] [INFO] [launch.py:348:main] Process 46292 exits successfully.
x3006c0s19b1n0: [2024-03-29 16:50:08,315] [INFO] [launch.py:348:main] Process 46291 exits successfully.
x3006c0s19b1n0: [2024-03-29 16:50:08,316] [INFO] [launch.py:348:main] Process 46294 exits successfully.

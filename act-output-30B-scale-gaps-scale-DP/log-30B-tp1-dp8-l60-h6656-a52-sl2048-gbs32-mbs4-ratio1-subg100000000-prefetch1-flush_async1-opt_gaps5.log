[2024-03-29 15:50:59,588] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 15:51:02,751] [INFO] [runner.py:463:main] Using IP address of 10.140.58.110 for node x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-29 15:51:02,753] [INFO] [runner.py:557:main] deepspeed_env file = ./.deepspeed_env
[2024-03-29 15:51:02,753] [INFO] [multinode_runner.py:80:get_cmd] Running on the following workers: x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov,x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-29 15:51:02,753] [INFO] [runner.py:568:main] cmd = pdsh -S -f 1024 -w x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov,x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov export PYTHONUSERBASE=/home/am6429/.local/polaris/conda/2023-10-04; export PYTHONPATH=/home/am6429/dl-io/Megatron-DeepSpeed; export PATH=/home/am6429/.conda/envs/dspeed_env/bin:/soft/datascience/conda/2023-10-04/mconda3/condabin:/soft/compilers/cudatoolkit/cuda-11.8.0/bin:/soft/buildtools/cmake/cmake-3.23.2/cmake-3.23.2-linux-x86_64/bin:/opt/cray/pe/gcc/11.2.0/bin:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/include:/opt/cray/pe/pals/1.2.11/bin:/opt/cray/pe/craype/2.7.20/bin:/opt/cray/libfabric/1.15.2.0/bin:/home/am6429/.conda/envs/dspeed_env/bin:/opt/cray/pe/perftools/23.03.0/bin:/opt/cray/pe/papi/7.0.0.1/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/am6429/.local/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/am6429/veloc-build/include:/home/am6429/veloc-build/bin:/soft/datascience/conda/2023-01-10/mconda3/include:/opt/cray/pe/bin:/soft/datascience/conda/2023-01-10/mconda3/include; export LD_LIBRARY_PATH=/usr/lib64/:/soft/compilers/cudatoolkit/cuda-11.8.0/lib64:/opt/cray/pe/gcc/11.2.0/snos/lib64:/soft/compilers/cudatoolkit/cuda-11.8.0/extras/CUPTI/lib64:/soft/libraries/trt/TensorRT-8.5.3.1.Linux.x86_64-gnu.cuda-11.8.cudnn8.6/lib:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/lib:/soft/libraries/cudnn/cudnn-11-linux-x64-v8.6.0.163/lib:/opt/cray/libfabric/1.15.2.0/lib64:/usr/lib64:/opt/cray/pe/papi/7.0.0.1/lib64:/home/am6429/veloc-build/lib:/home/am6429/veloc-build/lib64:/home/am6429/nvcomp/lib:/soft/datascience/conda/2023-01-10/mconda3/lib:/soft/datascience/conda/2023-01-10/mconda3/lib/; export http_proxy=http://proxy.alcf.anl.gov:3128; export https_proxy=http://proxy.alcf.anl.gov:3128; export CC=gcc; export CXX=g++; export IBV_FORK_SAFE=1; export CFLAGS=-I/soft/datascience/conda/2023-01-10/mconda3/include/; export LDFLAGS=-L/soft/datascience/conda/2023-01-10/mconda3/lib/; export CUDA_DEVICE_MAX_CONNECTIONS=1; export TORCHSNAPSHOT_PER_RANK_MEMORY_BUDGET_BYTES=34359738368; export _DEFAULT_MAX_PER_RANK_IO_CONCURRENCY=1; export _MAX_PER_RANK_IO_CONCURRENCY=1; export NSYS_REPORT_DIR=/home/am6429/dl-io/dl-io-outputs/act-output-30B//rep-30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch1-flush_async1-opt_gaps5-%n;  cd /home/am6429/dl-io/Megatron-DeepSpeed; /home/am6429/.conda/envs/dspeed_env/bin/python -u -m deepspeed.launcher.launch --world_info=eyJ4MzEwNGMwczFiMG4wLmhzbi5jbS5wb2xhcmlzLmFsY2YuYW5sLmdvdiI6IFswLCAxLCAyLCAzXSwgIngzMTA0YzBzMjViMG4wLmhzbi5jbS5wb2xhcmlzLmFsY2YuYW5sLmdvdiI6IFswLCAxLCAyLCAzXX0= --node_rank=%n --master_addr=10.140.58.110 --master_port=29700 /home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py --tensor-model-parallel-size 1 --num-layers 60 --hidden-size 6656 --num-attention-heads 52 --micro-batch-size 4 --global-batch-size 32 --seq-length 2048 --max-position-embeddings 2048 --train-iters 10 --save /local/scratch/llama2/zero3-tp1}_dp8 --data-path /home/am6429/dl-io/datasets/meg-gpt2_text_document --vocab-file /home/am6429/dl-io/datasets/gpt2-vocab.json --merge-file /home/am6429/dl-io/datasets/gpt2-merges.txt --data-impl mmap --tokenizer-type GPTSentencePieceTokenizer --tokenizer-model /home/am6429/dl-io/datasets/tokenizer.model --split 949,50,1 --distributed-backend nccl --lr 3e-4 --lr-decay-style cosine --min-lr 3e-5 --weight-decay 0.1 --clip-grad 1 --lr-warmup-iters 1 --optimizer adam --adam-beta1 0.9 --adam-beta2 0.95 --log-interval 1 --save-interval 1000 --eval-interval 1000 --eval-iters 0 --bf16 --no-query-key-layer-scaling --attention-dropout 0 --hidden-dropout 0 --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization rmsnorm --disable-bias-linear --num-key-value-heads 4 --deepspeed --exit-interval 20 --deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json --zero-stage=3 --no-pipeline-parallel --cpu-optimizer --checkpoint-activations --deepspeed-activation-checkpointing
x3104c0s25b0n0: [2024-03-29 15:51:04,537] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 15:51:04,767] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s25b0n0: [2024-03-29 15:51:06,382] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3104c0s25b0n0: [2024-03-29 15:51:06,382] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=1
x3104c0s25b0n0: [2024-03-29 15:51:06,382] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3104c0s25b0n0: [2024-03-29 15:51:06,382] [INFO] [launch.py:163:main] dist_world_size=8
x3104c0s25b0n0: [2024-03-29 15:51:06,382] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3104c0s25b0n0: [2024-03-29 15:51:06,383] [INFO] [launch.py:253:main] process 19243 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s25b0n0: [2024-03-29 15:51:06,384] [INFO] [launch.py:253:main] process 19244 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s25b0n0: [2024-03-29 15:51:06,384] [INFO] [launch.py:253:main] process 19245 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s25b0n0: [2024-03-29 15:51:06,385] [INFO] [launch.py:253:main] process 19246 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s1b0n0: [2024-03-29 15:51:06,622] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3104c0s1b0n0: [2024-03-29 15:51:06,622] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=0
x3104c0s1b0n0: [2024-03-29 15:51:06,622] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3104c0s1b0n0: [2024-03-29 15:51:06,622] [INFO] [launch.py:163:main] dist_world_size=8
x3104c0s1b0n0: [2024-03-29 15:51:06,622] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3104c0s1b0n0: [2024-03-29 15:51:06,623] [INFO] [launch.py:253:main] process 56902 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s1b0n0: [2024-03-29 15:51:06,624] [INFO] [launch.py:253:main] process 56903 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s1b0n0: [2024-03-29 15:51:06,624] [INFO] [launch.py:253:main] process 56904 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s1b0n0: [2024-03-29 15:51:06,625] [INFO] [launch.py:253:main] process 56905 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s25b0n0: [2024-03-29 15:51:07,850] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s25b0n0: [2024-03-29 15:51:07,853] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s25b0n0: [2024-03-29 15:51:07,862] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s25b0n0: [2024-03-29 15:51:07,886] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 15:51:08,432] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 15:51:08,445] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 15:51:08,452] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 15:51:08,455] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s25b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s25b0n0:       meet the required dependencies to JIT install the op.
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: JIT compiled ops requires ninja
x3104c0s25b0n0: ninja .................. [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: op name ................ installed .. compatible
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s25b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s25b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s25b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s25b0n0:       meet the required dependencies to JIT install the op.
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: JIT compiled ops requires ninja
x3104c0s25b0n0: ninja .................. [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: op name ................ installed .. compatible
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s25b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s25b0n0:       meet the required dependencies to JIT install the op.
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: JIT compiled ops requires ninja
x3104c0s25b0n0: ninja .................. [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: op name ................ installed .. compatible
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s25b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s25b0n0:       meet the required dependencies to JIT install the op.
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: JIT compiled ops requires ninja
x3104c0s25b0n0: ninja .................. [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: op name ................ installed .. compatible
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s25b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s25b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s25b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed general environment info:
x3104c0s25b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s25b0n0: torch version .................... 2.0.1+cu118
x3104c0s25b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s25b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s25b0n0: torch cuda version ............... 11.8
x3104c0s25b0n0: torch hip version ................ None
x3104c0s25b0n0: nvcc version ..................... 11.8
x3104c0s25b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s25b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s25b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s25b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s25b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s25b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s25b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [2024-03-29 15:51:09,982] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s25b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s25b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s25b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed general environment info:
x3104c0s25b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s25b0n0: torch version .................... 2.0.1+cu118
x3104c0s25b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s25b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s25b0n0: torch cuda version ............... 11.8
x3104c0s25b0n0: torch hip version ................ None
x3104c0s25b0n0: nvcc version ..................... 11.8
x3104c0s25b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s25b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s25b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s25b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s25b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s25b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s25b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s25b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: DeepSpeed general environment info:
x3104c0s25b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s25b0n0: torch version .................... 2.0.1+cu118
x3104c0s25b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s25b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s25b0n0: torch cuda version ............... 11.8
x3104c0s25b0n0: torch hip version ................ None
x3104c0s25b0n0: nvcc version ..................... 11.8
x3104c0s25b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s25b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s25b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed general environment info:
x3104c0s25b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s25b0n0: torch version .................... 2.0.1+cu118
x3104c0s25b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s25b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s25b0n0: torch cuda version ............... 11.8
x3104c0s25b0n0: torch hip version ................ None
x3104c0s25b0n0: nvcc version ..................... 11.8
x3104c0s25b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s25b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s25b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s25b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s25b0n0: [2024-03-29 15:51:10,050] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s25b0n0: [2024-03-29 15:51:10,067] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s25b0n0: [2024-03-29 15:51:10,071] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s1b0n0:       meet the required dependencies to JIT install the op.
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: JIT compiled ops requires ninja
x3104c0s1b0n0: ninja .................. [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: op name ................ installed .. compatible
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s1b0n0:       meet the required dependencies to JIT install the op.
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: JIT compiled ops requires ninja
x3104c0s1b0n0: ninja .................. [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: op name ................ installed .. compatible
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s1b0n0:       meet the required dependencies to JIT install the op.
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: JIT compiled ops requires ninja
x3104c0s1b0n0: ninja .................. [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: op name ................ installed .. compatible
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s1b0n0:       meet the required dependencies to JIT install the op.
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: JIT compiled ops requires ninja
x3104c0s1b0n0: ninja .................. [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: op name ................ installed .. compatible
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: DeepSpeed general environment info:
x3104c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s1b0n0: torch version .................... 2.0.1+cu118
x3104c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s1b0n0: torch cuda version ............... 11.8
x3104c0s1b0n0: torch hip version ................ None
x3104c0s1b0n0: nvcc version ..................... 11.8
x3104c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed general environment info:
x3104c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s1b0n0: torch version .................... 2.0.1+cu118
x3104c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s1b0n0: torch cuda version ............... 11.8
x3104c0s1b0n0: torch hip version ................ None
x3104c0s1b0n0: nvcc version ..................... 11.8
x3104c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: DeepSpeed general environment info:
x3104c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s1b0n0: torch version .................... 2.0.1+cu118
x3104c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s1b0n0: torch cuda version ............... 11.8
x3104c0s1b0n0: torch hip version ................ None
x3104c0s1b0n0: nvcc version ..................... 11.8
x3104c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s1b0n0: DeepSpeed general environment info:
x3104c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s1b0n0: torch version .................... 2.0.1+cu118
x3104c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s1b0n0: torch cuda version ............... 11.8
x3104c0s1b0n0: torch hip version ................ None
x3104c0s1b0n0: nvcc version ..................... 11.8
x3104c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s1b0n0: INFO: overriding default arguments for tokenizer_type:GPTSentencePieceTokenizer                    with tokenizer_type:GPT2BPETokenizer
x3104c0s1b0n0: using world size: 8, data-parallel-size: 8, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
x3104c0s1b0n0: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
x3104c0s1b0n0: using torch.bfloat16 for parameters ...
x3104c0s1b0n0: ------------------------ arguments ------------------------
x3104c0s1b0n0:   accumulate_allreduce_grads_in_fp32 .............. True
x3104c0s1b0n0:   adam_beta1 ...................................... 0.9
x3104c0s1b0n0:   adam_beta2 ...................................... 0.95
x3104c0s1b0n0:   adam_eps ........................................ 1e-08
x3104c0s1b0n0:   add_bias_linear ................................. False
x3104c0s1b0n0:   add_position_embedding .......................... False
x3104c0s1b0n0:   adlr_autoresume ................................. False
x3104c0s1b0n0:   adlr_autoresume_interval ........................ 1000
x3104c0s1b0n0:   aml_data_download_path .......................... None
x3104c0s1b0n0:   apply_layernorm_1p .............................. False
x3104c0s1b0n0:   apply_query_key_layer_scaling ................... False
x3104c0s1b0n0:   apply_residual_connection_post_layernorm ........ False
x3104c0s1b0n0:   async_tensor_model_parallel_allreduce ........... False
x3104c0s1b0n0:   attention_dropout ............................... 0.0
x3104c0s1b0n0:   attention_softmax_in_fp32 ....................... False
x3104c0s1b0n0:   barrier_with_L1_time ............................ True
x3104c0s1b0n0:   bert_binary_head ................................ True
x3104c0s1b0n0:   bert_embedder_type .............................. megatron
x3104c0s1b0n0:   bert_load ....................................... None
x3104c0s1b0n0:   bf16 ............................................ True
x3104c0s1b0n0:   bias_dropout_fusion ............................. True
x3104c0s1b0n0:   bias_gelu_fusion ................................ False
x3104c0s1b0n0:   biencoder_projection_dim ........................ 0
x3104c0s1b0n0:   biencoder_shared_query_context_model ............ False
x3104c0s1b0n0:   block_data_path ................................. None
x3104c0s1b0n0:   checkpoint_activations .......................... True
x3104c0s1b0n0:   checkpoint_in_cpu ............................... False
x3104c0s1b0n0:   checkpoint_num_layers ........................... 1
x3104c0s1b0n0:   classes_fraction ................................ 1.0
x3104c0s1b0n0:   clip_grad ....................................... 1.0
x3104c0s1b0n0:   compression_training ............................ False
x3104c0s1b0n0:   consumed_train_samples .......................... 0
x3104c0s1b0n0:   consumed_train_tokens ........................... 0
x3104c0s1b0n0:   consumed_valid_samples .......................... 0
x3104c0s1b0n0:   contigious_checkpointing ........................ False
x3104c0s1b0n0:   cpu_optimizer ................................... True
x3104c0s1b0n0:   cpu_torch_adam .................................. False
x3104c0s1b0n0:   create_moe_param_group .......................... False
x3104c0s1b0n0:   curriculum_learning_legacy ...................... False
x3104c0s1b0n0:   data_cache_path ................................. None
x3104c0s1b0n0:   data_efficiency_curriculum_learning ............. False
x3104c0s1b0n0:   data_impl ....................................... mmap
x3104c0s1b0n0:   data_parallel_random_init ....................... False
x3104c0s1b0n0:   data_parallel_size .............................. 8
x3104c0s1b0n0:   data_path ....................................... ['/home/am6429/dl-io/datasets/meg-gpt2_text_document']
x3104c0s1b0n0:   data_per_class_fraction ......................... 1.0
x3104c0s1b0n0:   data_sharding ................................... True
x3104c0s1b0n0:   dataloader_type ................................. single
x3104c0s1b0n0:   DDP_impl ........................................ local
x3104c0s1b0n0:   decoder_num_layers .............................. None
x3104c0s1b0n0:   decoder_seq_length .............................. None
x3104c0s1b0n0:   deepscale ....................................... False
x3104c0s1b0n0:   deepscale_config ................................ None
x3104c0s1b0n0:   deepspeed ....................................... True
x3104c0s1b0n0:   deepspeed_activation_checkpointing .............. True
x3104c0s1b0n0:   deepspeed_config ................................ /home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json
x3104c0s1b0n0:   dino_bottleneck_size ............................ 256
x3104c0s1b0n0:   dino_freeze_last_layer .......................... 1
x3104c0s1b0n0:   dino_head_hidden_size ........................... 2048
x3104c0s1b0n0:   dino_local_crops_number ......................... 10
x3104c0s1b0n0:   dino_local_img_size ............................. 96
x3104c0s1b0n0:   dino_norm_last_layer ............................ False
x3104c0s1b0n0:   dino_teacher_temp ............................... 0.07
x3104c0s1b0n0:   dino_warmup_teacher_temp ........................ 0.04
x3104c0s1b0n0:   dino_warmup_teacher_temp_epochs ................. 30
x3104c0s1b0n0:   distribute_checkpointed_activations ............. False
x3104c0s1b0n0:   distribute_saved_activations .................... False
x3104c0s1b0n0:   distributed_backend ............................. nccl
x3104c0s1b0n0:   distributed_timeout_minutes ..................... 10
x3104c0s1b0n0:   ds_inference .................................... False
x3104c0s1b0n0:   ds_pipeline_enabled ............................. False
x3104c0s1b0n0:   ds_sequence_parallel_size ....................... 1
x3104c0s1b0n0:   embedding_path .................................. None
x3104c0s1b0n0:   embedding_weights_in_fp32 ....................... False
x3104c0s1b0n0:   empty_unused_memory_level ....................... 0
x3104c0s1b0n0:   enable_expert_tensor_parallelism ................ False
x3104c0s1b0n0:   encoder_num_layers .............................. 60
x3104c0s1b0n0:   encoder_seq_length .............................. 2048
x3104c0s1b0n0:   end_weight_decay ................................ 0.1
x3104c0s1b0n0:   eod_mask_loss ................................... False
x3104c0s1b0n0:   eval_interval ................................... 1000
x3104c0s1b0n0:   eval_iters ...................................... 0
x3104c0s1b0n0:   evidence_data_path .............................. None
x3104c0s1b0n0:   exit_duration_in_mins ........................... None
x3104c0s1b0n0:   exit_interval ................................... 20
x3104c0s1b0n0:   exit_on_missing_checkpoint ...................... False
x3104c0s1b0n0:   exit_signal_handler ............................. False
x3104c0s1b0n0:   expert_interval ................................. 2
x3104c0s1b0n0:   ffn_hidden_size ................................. 17728
x3104c0s1b0n0:   finetune ........................................ False
x3104c0s1b0n0:   force_ds_sequence_parallel ...................... False
x3104c0s1b0n0:   fp16 ............................................ False
x3104c0s1b0n0:   fp16_lm_cross_entropy ........................... False
x3104c0s1b0n0:   fp32_residual_connection ........................ False
x3104c0s1b0n0:   fp8_amax_compute_algo ........................... most_recent
x3104c0s1b0n0:   fp8_amax_history_len ............................ 1
x3104c0s1b0n0:   fp8_e4m3 ........................................ False
x3104c0s1b0n0:   fp8_hybrid ...................................... False
x3104c0s1b0n0:   fp8_interval .................................... 1
x3104c0s1b0n0:   fp8_margin ...................................... 0
x3104c0s1b0n0:   fp8_wgrad ....................................... True
x3104c0s1b0n0:   global_batch_size ............................... 32
x3104c0s1b0n0:   gradient_accumulation_fusion .................... True
x3104c0s1b0n0:   head_lr_mult .................................... 1.0
x3104c0s1b0n0:   hidden_dropout .................................. 0.0
x3104c0s1b0n0:   hidden_size ..................................... 6656
x3104c0s1b0n0:   hidden_size_teacher ............................. None
x3104c0s1b0n0:   hysteresis ...................................... 2
x3104c0s1b0n0:   ict_head_size ................................... None
x3104c0s1b0n0:   ict_load ........................................ None
x3104c0s1b0n0:   img_h ........................................... 224
x3104c0s1b0n0:   img_w ........................................... 224
x3104c0s1b0n0:   indexer_batch_size .............................. 128
x3104c0s1b0n0:   indexer_log_interval ............................ 1000
x3104c0s1b0n0:   inference ....................................... False
x3104c0s1b0n0:   inference_batch_times_seqlen_threshold .......... 512
x3104c0s1b0n0:   init_method_std ................................. 0.02
x3104c0s1b0n0:   init_method_xavier_uniform ...................... False
x3104c0s1b0n0:   initial_loss_scale .............................. 4294967296
x3104c0s1b0n0:   iter_per_epoch .................................. 1250
x3104c0s1b0n0:   kd .............................................. False
x3104c0s1b0n0:   kd_alpha_ce ..................................... 1
x3104c0s1b0n0:   kd_beta_ce ...................................... 1
x3104c0s1b0n0:   kd_temp ......................................... 1.0
x3104c0s1b0n0:   kv_channels ..................................... 128
x3104c0s1b0n0:   layernorm_epsilon ............................... 1e-05
x3104c0s1b0n0:   lazy_mpu_init ................................... None
x3104c0s1b0n0:   load ............................................ None
x3104c0s1b0n0:   load_teacher .................................... None
x3104c0s1b0n0:   local_rank ...................................... 0
x3104c0s1b0n0:   log_batch_size_to_tensorboard ................... False
x3104c0s1b0n0:   log_interval .................................... 1
x3104c0s1b0n0:   log_learning_rate_to_tensorboard ................ True
x3104c0s1b0n0:   log_loss_scale_to_tensorboard ................... True
x3104c0s1b0n0:   log_memory_to_tensorboard ....................... False
x3104c0s1b0n0:   log_num_zeros_in_grad ........................... False
x3104c0s1b0n0:   log_optimizer_states_to_tensorboard ............. False
x3104c0s1b0n0:   log_params_norm ................................. False
x3104c0s1b0n0:   log_timers_to_tensorboard ....................... False
x3104c0s1b0n0:   log_validation_ppl_to_tensorboard ............... False
x3104c0s1b0n0:   log_world_size_to_tensorboard ................... False
x3104c0s1b0n0:   loss_scale ...................................... None
x3104c0s1b0n0:   loss_scale_window ............................... 1000
x3104c0s1b0n0:   lr .............................................. 0.0003
x3104c0s1b0n0:   lr_decay_iters .................................. None
x3104c0s1b0n0:   lr_decay_samples ................................ None
x3104c0s1b0n0:   lr_decay_style .................................. cosine
x3104c0s1b0n0:   lr_decay_tokens ................................. None
x3104c0s1b0n0:   lr_warmup_fraction .............................. None
x3104c0s1b0n0:   lr_warmup_iters ................................. 1
x3104c0s1b0n0:   lr_warmup_samples ............................... 0
x3104c0s1b0n0:   lr_warmup_tokens ................................ None
x3104c0s1b0n0:   make_vocab_size_divisible_by .................... 128
x3104c0s1b0n0:   mask_factor ..................................... 1.0
x3104c0s1b0n0:   mask_prob ....................................... 0.15
x3104c0s1b0n0:   mask_type ....................................... random
x3104c0s1b0n0:   masked_softmax_fusion ........................... True
x3104c0s1b0n0:   max_position_embeddings ......................... 2048
x3104c0s1b0n0:   max_tokens_to_oom ............................... 12000
x3104c0s1b0n0:   memory_centric_tiled_linear ..................... False
x3104c0s1b0n0:   merge_file ...................................... /home/am6429/dl-io/datasets/gpt2-merges.txt
x3104c0s1b0n0:   micro_batch_size ................................ 4
x3104c0s1b0n0:   min_loss_scale .................................. 1.0
x3104c0s1b0n0:   min_lr .......................................... 3e-05
x3104c0s1b0n0:   mlp_type ........................................ standard
x3104c0s1b0n0:   mmap_warmup ..................................... False
x3104c0s1b0n0:   moe_eval_capacity_factor ........................ 1.0
x3104c0s1b0n0:   moe_expert_parallel_size ........................ 1
x3104c0s1b0n0:   moe_loss_coeff .................................. 0.1
x3104c0s1b0n0:   moe_min_capacity ................................ 4
x3104c0s1b0n0:   moe_token_dropping .............................. True
x3104c0s1b0n0:   moe_train_capacity_factor ....................... 1.0
x3104c0s1b0n0:   mos ............................................. False
x3104c0s1b0n0:   no_load_lr_state ................................ False
x3104c0s1b0n0:   no_load_optim ................................... None
x3104c0s1b0n0:   no_load_rng ..................................... None
x3104c0s1b0n0:   no_persist_layer_norm ........................... False
x3104c0s1b0n0:   no_pipeline_parallel ............................ True
x3104c0s1b0n0:   no_save_optim ................................... None
x3104c0s1b0n0:   no_save_rng ..................................... None
x3104c0s1b0n0:   normalization ................................... rmsnorm
x3104c0s1b0n0:   num_attention_heads ............................. 52
x3104c0s1b0n0:   num_attention_heads_teacher ..................... None
x3104c0s1b0n0:   num_channels .................................... 3
x3104c0s1b0n0:   num_classes ..................................... 1000
x3104c0s1b0n0:   num_experts ..................................... [1]
x3104c0s1b0n0:   num_experts_switch .............................. None
x3104c0s1b0n0:   num_experts_teacher ............................. [1]
x3104c0s1b0n0:   num_key_value_heads ............................. 4
x3104c0s1b0n0:   num_layers ...................................... 60
x3104c0s1b0n0:   num_layers_per_virtual_pipeline_stage ........... None
x3104c0s1b0n0:   num_layers_teacher .............................. None
x3104c0s1b0n0:   num_workers ..................................... 2
x3104c0s1b0n0:   onnx_safe ....................................... None
x3104c0s1b0n0:   openai_gelu ..................................... False
x3104c0s1b0n0:   optimizer ....................................... adam
x3104c0s1b0n0:   output_bert_embeddings .......................... False
x3104c0s1b0n0:   overlap_p2p_comm ................................ False
x3104c0s1b0n0:   override_opt_param_scheduler .................... False
x3104c0s1b0n0:   params_dtype .................................... torch.bfloat16
x3104c0s1b0n0:   partition_activations ........................... False
x3104c0s1b0n0:   patch_dim ....................................... 16
x3104c0s1b0n0:   perform_initialization .......................... True
x3104c0s1b0n0:   pipeline_model_parallel_size .................... 1
x3104c0s1b0n0:   pipeline_model_parallel_split_rank .............. None
x3104c0s1b0n0:   profile_backward ................................ False
x3104c0s1b0n0:   query_in_block_prob ............................. 0.1
x3104c0s1b0n0:   rampup_batch_size ............................... None
x3104c0s1b0n0:   random_ltd ...................................... False
x3104c0s1b0n0:   rank ............................................ 0
x3104c0s1b0n0:   recompute_granularity ........................... None
x3104c0s1b0n0:   recompute_method ................................ None
x3104c0s1b0n0:   recompute_num_layers ............................ 1
x3104c0s1b0n0:   remote_device ................................... none
x3104c0s1b0n0:   reset_attention_mask ............................ False
x3104c0s1b0n0:   reset_iteration ................................. False
x3104c0s1b0n0:   reset_position_ids .............................. False
x3104c0s1b0n0:   retriever_report_topk_accuracies ................ []
x3104c0s1b0n0:   retriever_score_scaling ......................... False
x3104c0s1b0n0:   retriever_seq_length ............................ 256
x3104c0s1b0n0:   retro_add_retriever ............................. False
x3104c0s1b0n0:   retro_cyclic_train_iters ........................ None
x3104c0s1b0n0:   retro_encoder_attention_dropout ................. 0.1
x3104c0s1b0n0:   retro_encoder_hidden_dropout .................... 0.1
x3104c0s1b0n0:   retro_encoder_layers ............................ 2
x3104c0s1b0n0:   retro_num_neighbors ............................. 2
x3104c0s1b0n0:   retro_num_retrieved_chunks ...................... 2
x3104c0s1b0n0:   retro_return_doc_ids ............................ False
x3104c0s1b0n0:   retro_workdir ................................... None
x3104c0s1b0n0:   return_data_index ............................... False
x3104c0s1b0n0:   rotary_percent .................................. 1.0
x3104c0s1b0n0:   sample_rate ..................................... 1.0
x3104c0s1b0n0:   save ............................................ /local/scratch/llama2/zero3-tp1}_dp8
x3104c0s1b0n0:   save_interval ................................... 1000
x3104c0s1b0n0:   scatter_gather_tensors_in_pipeline .............. True
x3104c0s1b0n0:   scattered_embeddings ............................ False
x3104c0s1b0n0:   seed ............................................ 1234
x3104c0s1b0n0:   seq_length ...................................... 2048
x3104c0s1b0n0:   sequence_parallel ............................... False
x3104c0s1b0n0:   sgd_momentum .................................... 0.9
x3104c0s1b0n0:   short_seq_prob .................................. 0.1
x3104c0s1b0n0:   skip_train ...................................... False
x3104c0s1b0n0:   split ........................................... 949,50,1
x3104c0s1b0n0:   split_transformers .............................. False
x3104c0s1b0n0:   squared_relu .................................... False
x3104c0s1b0n0:   standalone_embedding_stage ...................... False
x3104c0s1b0n0:   start_weight_decay .............................. 0.1
x3104c0s1b0n0:   swiglu .......................................... True
x3104c0s1b0n0:   swin_backbone_type .............................. tiny
x3104c0s1b0n0:   synchronize_each_layer .......................... False
x3104c0s1b0n0:   tensor_model_parallel_size ...................... 1
x3104c0s1b0n0:   tensorboard_dir ................................. None
x3104c0s1b0n0:   tensorboard_log_interval ........................ 1
x3104c0s1b0n0:   tensorboard_queue_size .......................... 1000
x3104c0s1b0n0:   test_data_path .................................. None
x3104c0s1b0n0:   tile_factor ..................................... 1
x3104c0s1b0n0:   timing_log_level ................................ 0
x3104c0s1b0n0:   timing_log_option ............................... minmax
x3104c0s1b0n0:   titles_data_path ................................ None
x3104c0s1b0n0:   tokenizer_model ................................. /home/am6429/dl-io/datasets/tokenizer.model
x3104c0s1b0n0:   tokenizer_type .................................. GPT2BPETokenizer
x3104c0s1b0n0:   topk ............................................ 1
x3104c0s1b0n0:   train_data_exact_num_epochs ..................... None
x3104c0s1b0n0:   train_data_path ................................. None
x3104c0s1b0n0:   train_desc_path ................................. None
x3104c0s1b0n0:   train_doc_idx_path .............................. None
x3104c0s1b0n0:   train_idx_path .................................. None
x3104c0s1b0n0:   train_iters ..................................... 10
x3104c0s1b0n0:   train_sample_idx_path ........................... None
x3104c0s1b0n0:   train_samples ................................... None
x3104c0s1b0n0:   train_shuffle_idx_path .......................... None
x3104c0s1b0n0:   train_tokens .................................... None
x3104c0s1b0n0:   transformer_impl ................................ local
x3104c0s1b0n0:   transformer_pipeline_model_parallel_size ........ 1
x3104c0s1b0n0:   untie_embeddings_and_output_weights ............. True
x3104c0s1b0n0:   use_checkpoint_args ............................. False
x3104c0s1b0n0:   use_checkpoint_opt_param_scheduler .............. False
x3104c0s1b0n0:   use_contiguous_buffers_in_local_ddp ............. True
x3104c0s1b0n0:   use_cpu_initialization .......................... None
x3104c0s1b0n0:   use_dataset_only ................................ False
x3104c0s1b0n0:   use_distributed_optimizer ....................... False
x3104c0s1b0n0:   use_flash_attn .................................. False
x3104c0s1b0n0:   use_flash_attn_triton ........................... False
x3104c0s1b0n0:   use_flash_attn_v1 ............................... False
x3104c0s1b0n0:   use_flash_attn_v2 ............................... False
x3104c0s1b0n0:   use_one_sent_docs ............................... False
x3104c0s1b0n0:   use_pin_memory .................................. False
x3104c0s1b0n0:   use_ring_exchange_p2p ........................... False
x3104c0s1b0n0:   use_rotary_position_embeddings .................. True
x3104c0s1b0n0:   use_tutel ....................................... False
x3104c0s1b0n0:   valid_data_path ................................. None
x3104c0s1b0n0:   variable_seq_lengths ............................ False
x3104c0s1b0n0:   virtual_pipeline_model_parallel_size ............ None
x3104c0s1b0n0:   vision_backbone_type ............................ vit
x3104c0s1b0n0:   vision_pretraining .............................. False
x3104c0s1b0n0:   vision_pretraining_type ......................... classify
x3104c0s1b0n0:   vocab_extra_ids ................................. 0
x3104c0s1b0n0:   vocab_file ...................................... /home/am6429/dl-io/datasets/gpt2-vocab.json
x3104c0s1b0n0:   vocab_size ...................................... None
x3104c0s1b0n0:   weight_decay .................................... 0.1
x3104c0s1b0n0:   weight_decay_incr_style ......................... constant
x3104c0s1b0n0:   world_size ...................................... 8
x3104c0s1b0n0:   zero_allgather_bucket_size ...................... 0.0
x3104c0s1b0n0:   zero_contigious_gradients ....................... False
x3104c0s1b0n0:   zero_reduce_bucket_size ......................... 0.0
x3104c0s1b0n0:   zero_reduce_scatter ............................. False
x3104c0s1b0n0:   zero_stage ...................................... 3
x3104c0s1b0n0: -------------------- end of arguments ---------------------
x3104c0s1b0n0: setting number of micro-batches to constant 1
x3104c0s1b0n0: > building GPT2BPETokenizer tokenizer ...
x3104c0s1b0n0: [2024-03-29 15:51:11,438] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0: [2024-03-29 15:51:11,450] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0: [2024-03-29 15:51:11,453] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
x3104c0s1b0n0: > initializing torch distributed ...
x3104c0s1b0n0: [2024-03-29 15:51:11,474] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0: [2024-03-29 15:51:11,474] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
x3104c0s1b0n0: > initialized tensor model parallel with size 1
x3104c0s1b0n0: > initialized pipeline model parallel with size 1
x3104c0s1b0n0: > setting random seeds to 1234 ...
x3104c0s1b0n0: [2024-03-29 15:51:12,911] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
x3104c0s1b0n0: > compiling dataset index builder ...
x3104c0s1b0n0: make: Entering directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3104c0s1b0n0: make: Nothing to be done for 'default'.
x3104c0s1b0n0: make: Leaving directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3104c0s1b0n0: >>> done with dataset index builder. Compilation time: 0.083 seconds
x3104c0s1b0n0: > compiling and loading fused kernels ...
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: >>> done with compiling and loading fused kernels. Compilation time: 3.216 seconds
x3104c0s25b0n0: <<<<<<<<<<< 4
x3104c0s25b0n0: <<<<<<<<<<< 6
x3104c0s25b0n0: <<<<<<<<<<< 5
x3104c0s25b0n0: <<<<<<<<<<< 7
x3104c0s1b0n0: initialize_megatron took 5.575615644454956
x3104c0s1b0n0: <<<<<<<<<<< 0
x3104c0s1b0n0: <<<<<<<<<<< 3
x3104c0s1b0n0: <<<<<<<<<<< 1
x3104c0s1b0n0: <<<<<<<<<<< 2
x3104c0s1b0n0: time to initialize megatron (seconds): 7.258
x3104c0s1b0n0: [after megatron is initialized] datetime: 2024-03-29 15:51:17 
x3104c0s1b0n0: get_accelerator and all_reduce  took 0.0043942928314208984
x3104c0s1b0n0: building GPT model ...
x3104c0s1b0n0: [2024-03-29 15:51:17,076] [INFO] [utils.py:800:see_memory_usage] Before Building Model
x3104c0s1b0n0: [2024-03-29 15:51:17,077] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 4.62 GB         CA 0.0 GB         Max_CA 5 GB 
x3104c0s1b0n0: [2024-03-29 15:51:17,077] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 17.88 GB, percent = 3.6%
x3104c0s1b0n0: [2024-03-29 15:51:22,635] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 423, num_elems = 27.64B
x3104c0s1b0n0: [2024-03-29 15:51:22,701] [INFO] [utils.py:800:see_memory_usage] After Building Model
x3104c0s1b0n0: [2024-03-29 15:51:22,702] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 7.07 GB         CA 21.64 GB         Max_CA 38 GB 
x3104c0s1b0n0: [2024-03-29 15:51:22,702] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 18.3 GB, percent = 3.6%
x3104c0s1b0n0:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 27635239424
x3104c0s25b0n0: ninja: no work to do.
x3104c0s25b0n0: Time to load cpu_adam op: 2.2833125591278076 seconds
x3104c0s25b0n0: Time to load cpu_adam op: 2.3172900676727295 seconds
x3104c0s25b0n0: Time to load cpu_adam op: 2.339787006378174 seconds
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: Time to load cpu_adam op: 2.434758424758911 seconds
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: Time to load cpu_adam op: 2.4491336345672607 seconds
x3104c0s25b0n0: Time to load cpu_adam op: 2.308533191680908 seconds
x3104c0s1b0n0: Time to load cpu_adam op: 2.469255208969116 seconds
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: Time to load cpu_adam op: 2.491962432861328 seconds
x3104c0s25b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s25b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s25b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s25b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s25b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s25b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: > learning rate decay style: cosine
x3104c0s1b0n0: DeepSpeed is enabled.
x3104c0s1b0n0: [2024-03-29 15:51:27,105] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.3+8074cd62, git-hash=8074cd62, git-branch=hybrid_opt_offload
x3104c0s1b0n0: [2024-03-29 15:51:27,175] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After args sanity test
x3104c0s1b0n0: [2024-03-29 15:51:27,176] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.64 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:51:27,176] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 24.43 GB, percent = 4.9%
x3104c0s1b0n0: [2024-03-29 15:51:27,233] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure distributed model
x3104c0s1b0n0: [2024-03-29 15:51:27,234] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.64 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:51:27,234] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 24.71 GB, percent = 4.9%
x3104c0s1b0n0: [2024-03-29 15:51:27,295] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After configure distributed model
x3104c0s1b0n0: [2024-03-29 15:51:27,296] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.64 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:51:27,296] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.03 GB, percent = 5.0%
x3104c0s1b0n0: [2024-03-29 15:51:27,296] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
x3104c0s25b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s25b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: [2024-03-29 15:51:27,350] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After setting model parameters
x3104c0s1b0n0: [2024-03-29 15:51:27,350] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.64 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:51:27,350] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.28 GB, percent = 5.0%
x3104c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: [2024-03-29 15:51:27,406] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure optimizer
x3104c0s1b0n0: [2024-03-29 15:51:27,406] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.64 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:51:27,406] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.48 GB, percent = 5.1%
x3104c0s1b0n0: [2024-03-29 15:51:27,407] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
x3104c0s1b0n0: [2024-03-29 15:51:27,407] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
x3104c0s1b0n0: [2024-03-29 15:51:27,425] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
x3104c0s1b0n0: [2024-03-29 15:51:27,425] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
x3104c0s1b0n0: [2024-03-29 15:51:27,425] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
x3104c0s1b0n0: [2024-03-29 15:51:27,425] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
x3104c0s1b0n0: [2024-03-29 15:51:27,476] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
x3104c0s1b0n0: [2024-03-29 15:51:27,477] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.64 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:51:27,477] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.59 GB, percent = 5.1%
x3104c0s1b0n0: [2024-03-29 15:51:27,479] [INFO] [stage3.py:137:__init__] Reduce bucket size 500,000,000
x3104c0s1b0n0: [2024-03-29 15:51:27,479] [INFO] [stage3.py:138:__init__] Prefetch bucket size 50,000,000
x3104c0s1b0n0: [2024-03-29 15:51:27,531] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
x3104c0s1b0n0: [2024-03-29 15:51:27,531] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.64 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:51:27,531] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.68 GB, percent = 5.1%
x3104c0s1b0n0: Parameter Offload: Total persistent parameters: 805376 in 121 params
x3104c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: [2024-03-29 15:51:27,609] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
x3104c0s1b0n0: [2024-03-29 15:51:27,610] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.64 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:51:27,610] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.74 GB, percent = 5.1%
x3104c0s1b0n0: [2024-03-29 15:51:27,665] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
x3104c0s1b0n0: [2024-03-29 15:51:27,666] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.64 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:51:27,666] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.74 GB, percent = 5.1%
x3104c0s1b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s1b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s1b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s1b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s25b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s25b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s25b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s25b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s1b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s1b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s1b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s1b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s25b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s25b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s25b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s25b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s1b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,667] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,668] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,669] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,670] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,671] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s1b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s1b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s1b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s25b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s25b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s25b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s25b0n0: [2024-03-29 15:51:27,672] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s1b0n0: [2024-03-29 15:51:31,242] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 31
x3104c0s1b0n0: [2024-03-29 15:51:31,242] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.44 GB         CA 6.44 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:51:31,243] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 33.16 GB, percent = 6.6%
x3104c0s1b0n0: [2024-03-29 15:51:31,314] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
x3104c0s1b0n0: [2024-03-29 15:51:31,314] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3104c0s1b0n0: [2024-03-29 15:51:31,314] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 34.3 GB, percent = 6.8%
x3104c0s1b0n0: [2024-03-29 15:51:50,361] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
x3104c0s1b0n0: [2024-03-29 15:51:50,361] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3104c0s1b0n0: [2024-03-29 15:51:50,361] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 215.8 GB, percent = 42.9%
x3104c0s1b0n0: [2024-03-29 15:51:50,884] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
x3104c0s1b0n0: [2024-03-29 15:51:50,884] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3104c0s1b0n0: [2024-03-29 15:51:50,885] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 219.44 GB, percent = 43.6%
x3104c0s1b0n0: [2024-03-29 15:51:57,215] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | init_optimizer_state: 6312.21
x3104c0s1b0n0: [2024-03-29 15:51:57,357] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
x3104c0s1b0n0: [2024-03-29 15:51:57,358] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3104c0s1b0n0: [2024-03-29 15:51:57,358] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 279.78 GB, percent = 55.6%
x3104c0s1b0n0: [2024-03-29 15:51:57,657] [INFO] [stage3.py:520:_setup_for_real_optimizer] optimizer state initialized
x3104c0s1b0n0: [2024-03-29 15:52:05,511] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
x3104c0s1b0n0: [2024-03-29 15:52:05,512] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 8.61 GB         CA 22.75 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 15:52:05,512] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.91 GB, percent = 62.0%
x3104c0s1b0n0: [2024-03-29 15:52:05,512] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
x3104c0s1b0n0: [2024-03-29 15:52:05,579] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure LR scheduler
x3104c0s1b0n0: [2024-03-29 15:52:05,580] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 15:52:05,580] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.93 GB, percent = 62.0%
x3104c0s1b0n0: [2024-03-29 15:52:05,580] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
x3104c0s1b0n0: [2024-03-29 15:52:05,580] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7f5bd2e88250>
x3104c0s1b0n0: [2024-03-29 15:52:05,580] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 15:52:05,643] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before rewriting optimizer step
x3104c0s1b0n0: [2024-03-29 15:52:05,644] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 15:52:05,644] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.93 GB, percent = 62.0%
x3104c0s1b0n0: [2024-03-29 15:52:05,706] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure checkpointing
x3104c0s1b0n0: [2024-03-29 15:52:05,706] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 15:52:05,706] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.91 GB, percent = 62.0%
x3104c0s1b0n0: [2024-03-29 15:52:05,706] [INFO] [config.py:998:print] DeepSpeedEngine configuration:
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   activation_checkpointing_config  {
x3104c0s1b0n0:     "partition_activations": false, 
x3104c0s1b0n0:     "contiguous_memory_optimization": false, 
x3104c0s1b0n0:     "cpu_checkpointing": false, 
x3104c0s1b0n0:     "number_checkpoints": null, 
x3104c0s1b0n0:     "synchronize_checkpoint_boundary": false, 
x3104c0s1b0n0:     "profile": false
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   amp_enabled .................. False
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   amp_params ................... False
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   autotuning_config ............ {
x3104c0s1b0n0:     "enabled": false, 
x3104c0s1b0n0:     "start_step": null, 
x3104c0s1b0n0:     "end_step": null, 
x3104c0s1b0n0:     "metric_path": null, 
x3104c0s1b0n0:     "arg_mappings": null, 
x3104c0s1b0n0:     "metric": "throughput", 
x3104c0s1b0n0:     "model_info": null, 
x3104c0s1b0n0:     "results_dir": "autotuning_results", 
x3104c0s1b0n0:     "exps_dir": "autotuning_exps", 
x3104c0s1b0n0:     "overwrite": true, 
x3104c0s1b0n0:     "fast": true, 
x3104c0s1b0n0:     "start_profile_step": 3, 
x3104c0s1b0n0:     "end_profile_step": 5, 
x3104c0s1b0n0:     "tuner_type": "gridsearch", 
x3104c0s1b0n0:     "tuner_early_stopping": 5, 
x3104c0s1b0n0:     "tuner_num_trials": 50, 
x3104c0s1b0n0:     "model_info_path": null, 
x3104c0s1b0n0:     "mp_size": 1, 
x3104c0s1b0n0:     "max_train_batch_size": null, 
x3104c0s1b0n0:     "min_train_batch_size": 1, 
x3104c0s1b0n0:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
x3104c0s1b0n0:     "min_train_micro_batch_size_per_gpu": 1, 
x3104c0s1b0n0:     "num_tuning_micro_batch_sizes": 3
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   bfloat16_enabled ............. True
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   bfloat16_immediate_grad_update  False
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   checkpoint_parallel_write_pipeline  False
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   checkpoint_tag_validation_enabled  True
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   checkpoint_tag_validation_fail  False
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5bd2e88be0>
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   communication_data_type ...... None
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   curriculum_enabled_legacy .... False
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   curriculum_params_legacy ..... False
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   data_efficiency_enabled ...... False
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   dataloader_drop_last ......... False
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   datastates_config ............ {
x3104c0s1b0n0:     "enabled": null, 
x3104c0s1b0n0:     "config": {
x3104c0s1b0n0:     }
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   disable_allgather ............ False
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   dump_state ................... False
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   dynamic_loss_scale_args ...... None
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   eigenvalue_enabled ........... False
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   eigenvalue_gas_boundary_resolution  1
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   eigenvalue_layer_name ........ bert.encoder.layer
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   eigenvalue_layer_num ......... 0
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   eigenvalue_max_iter .......... 100
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   eigenvalue_stability ......... 1e-06
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   eigenvalue_tol ............... 0.01
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   eigenvalue_verbose ........... False
x3104c0s1b0n0: [2024-03-29 15:52:05,707] [INFO] [config.py:1002:print]   elasticity_enabled ........... False
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   flops_profiler_config ........ {
x3104c0s1b0n0:     "enabled": false, 
x3104c0s1b0n0:     "recompute_fwd_factor": 0.0, 
x3104c0s1b0n0:     "profile_step": 1, 
x3104c0s1b0n0:     "module_depth": -1, 
x3104c0s1b0n0:     "top_modules": 1, 
x3104c0s1b0n0:     "detailed": true, 
x3104c0s1b0n0:     "output_file": null
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   fp16_auto_cast ............... None
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   fp16_enabled ................. False
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   fp16_master_weights_and_gradients  False
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   global_rank .................. 0
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   grad_accum_dtype ............. bf16
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   gradient_accumulation_steps .. 1
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   gradient_clipping ............ 0.0
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   gradient_predivide_factor .... 1.0
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   graph_harvesting ............. False
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   initial_dynamic_scale ........ 1
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   load_universal_checkpoint .... False
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   loss_scale ................... 1.0
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   memory_breakdown ............. True
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   mics_hierarchial_params_gather  False
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   mics_shard_size .............. -1
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   nebula_config ................ {
x3104c0s1b0n0:     "enabled": false, 
x3104c0s1b0n0:     "persistent_storage_path": null, 
x3104c0s1b0n0:     "persistent_time_interval": 100, 
x3104c0s1b0n0:     "num_of_version_in_retention": 2, 
x3104c0s1b0n0:     "enable_nebula_load": true, 
x3104c0s1b0n0:     "load_path": null
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   optimizer_legacy_fusion ...... False
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   optimizer_name ............... None
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   optimizer_params ............. None
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   pld_enabled .................. False
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   pld_params ................... False
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   prescale_gradients ........... False
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   scheduler_name ............... None
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   scheduler_params ............. None
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   seq_parallel_communication_data_type  torch.float32
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   sparse_attention ............. None
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   sparse_gradients_enabled ..... False
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   steps_per_print .............. 1
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   train_batch_size ............. 32
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   train_micro_batch_size_per_gpu  4
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   use_data_before_expert_parallel_  False
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   use_node_local_storage ....... False
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   wall_clock_breakdown ......... True
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   weight_quantization_config ... None
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   world_size ................... 8
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   zero_allow_untested_optimizer  False
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0, prefetch_optimizer=True, part_grads_async=True, prefetch_optimizer_gap=5) sub_group_size=100000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   zero_enabled ................. True
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   zero_force_ds_cpu_optimizer .. True
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:1002:print]   zero_optimization_stage ...... 3
x3104c0s1b0n0: [2024-03-29 15:52:05,708] [INFO] [config.py:988:print_user_config]   json = {
x3104c0s1b0n0:     "train_batch_size": 32, 
x3104c0s1b0n0:     "train_micro_batch_size_per_gpu": 4, 
x3104c0s1b0n0:     "steps_per_print": 1, 
x3104c0s1b0n0:     "zero_optimization": {
x3104c0s1b0n0:         "stage": 3, 
x3104c0s1b0n0:         "offload_optimizer": {
x3104c0s1b0n0:             "device": "cpu", 
x3104c0s1b0n0:             "ratio": 1, 
x3104c0s1b0n0:             "pin_memory": true, 
x3104c0s1b0n0:             "prefetch_optimizer": 1, 
x3104c0s1b0n0:             "part_grads_async": 1, 
x3104c0s1b0n0:             "prefetch_optimizer_gap": 5
x3104c0s1b0n0:         }, 
x3104c0s1b0n0:         "sub_group_size": 1.000000e+08
x3104c0s1b0n0:     }, 
x3104c0s1b0n0:     "bf16": {
x3104c0s1b0n0:         "enabled": true
x3104c0s1b0n0:     }, 
x3104c0s1b0n0:     "data_types": {
x3104c0s1b0n0:         "grad_accum_dtype": "bf16"
x3104c0s1b0n0:     }, 
x3104c0s1b0n0:     "wall_clock_breakdown": true, 
x3104c0s1b0n0:     "memory_breakdown": true, 
x3104c0s1b0n0:     "flops_profiler": {
x3104c0s1b0n0:         "enabled": false
x3104c0s1b0n0:     }
x3104c0s1b0n0: }
x3104c0s1b0n0: <TIMER:model-and-optimizer-setup,49.821534872055054>
x3104c0s25b0n0: <TIMER:model-and-optimizer-setup,49.82210421562195>
x3104c0s1b0n0: <TIMER:model-and-optimizer-setup,49.82255673408508>
x3104c0s25b0n0: <TIMER:model-and-optimizer-setup,49.82278037071228>
x3104c0s1b0n0: <TIMER:model-and-optimizer-setup,49.82304763793945>
x3104c0s25b0n0: <TIMER:model-and-optimizer-setup,49.82335662841797>
x3104c0s25b0n0: <TIMER:model-and-optimizer-setup,49.823423862457275>
x3104c0s1b0n0: <TIMER:model-and-optimizer-setup,49.823765993118286>
x3104c0s1b0n0: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-29 15:52:06 
x3104c0s1b0n0: > building train, validation, and test datasets ...
x3104c0s1b0n0:  > datasets target sizes (minimum size):
x3104c0s1b0n0:     train:      320
x3104c0s1b0n0:     validation: 0
x3104c0s1b0n0:     test:       0
x3104c0s1b0n0: > building train, validation, and test datasets for GPT ...
x3104c0s1b0n0: Single data path provided for train, valid & test
x3104c0s1b0n0:  > building dataset index ...
x3104c0s1b0n0:     reading sizes...
x3104c0s1b0n0:     reading pointers...
x3104c0s1b0n0:     reading document index...
x3104c0s1b0n0:     creating numpy buffer of mmap...
x3104c0s1b0n0:     creating memory view of numpy buffer...
x3104c0s1b0n0:  > finished creating indexed dataset in 0.002257 seconds
x3104c0s1b0n0:     number of documents: 79000
x3104c0s1b0n0:  > dataset split:
x3104c0s1b0n0:     train:
x3104c0s1b0n0:      document indices in [0, 74971) total of 74971 documents
x3104c0s1b0n0:     validation:
x3104c0s1b0n0:      document indices in [74971, 78921) total of 3950 documents
x3104c0s1b0n0:     test:
x3104c0s1b0n0:      document indices in [78921, 79000) total of 79 documents
x3104c0s1b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_doc_idx.npy
x3104c0s1b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_sample_idx.npy
x3104c0s1b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_shuffle_idx.npy
x3104c0s1b0n0:     loaded indexed file in 0.002 seconds
x3104c0s1b0n0:     total number of samples: 108448
x3104c0s1b0n0:     total number of epochs: 1
x3104c0s1b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_doc_idx.npy
x3104c0s1b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_sample_idx.npy
x3104c0s1b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_shuffle_idx.npy
x3104c0s1b0n0:     loaded indexed file in 0.003 seconds
x3104c0s1b0n0:     total number of samples: 5792
x3104c0s1b0n0:     total number of epochs: 1
x3104c0s1b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_doc_idx.npy
x3104c0s1b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_sample_idx.npy
x3104c0s1b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_shuffle_idx.npy
x3104c0s1b0n0:     loaded indexed file in 0.003 seconds
x3104c0s1b0n0:     total number of samples: 185
x3104c0s1b0n0:     total number of epochs: 1
x3104c0s1b0n0: > finished creating GPT datasets ...
x3104c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5058877468109131>
x3104c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5183680057525635>
x3104c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5254991054534912>
x3104c0s25b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5311517715454102>
x3104c0s25b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5320789813995361>
x3104c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5587766170501709>
x3104c0s25b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5645852088928223>
x3104c0s25b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5717310905456543>
x3104c0s1b0n0: [after dataloaders are built] datetime: 2024-03-29 15:52:07 
x3104c0s1b0n0: done with setup ...
x3104c0s25b0n0: (min, max) time across ranks (ms):
x3104c0s25b0n0:     model-and-optimizer-setup ......................: (49821.53, 49823.76)
x3104c0s25b0n0:     train/valid/test-data-iterators-setup ..........: (505.89, 571.73)
x3104c0s1b0n0: training ...
x3104c0s1b0n0: [before training begins] datetime: 2024-03-29 15:52:07 
x3104c0s1b0n0: [before the start of training step] datetime: 2024-03-29 15:52:07 
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:52:07,554] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:52:07,554] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 7.45 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 15:52:07,555] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.17 GB, percent = 62.4%
x3104c0s1b0n0: [2024-03-29 15:52:07,676] [INFO] [checkpointing.py:539:forward] Activation Checkpointing Information
x3104c0s1b0n0: [2024-03-29 15:52:07,676] [INFO] [checkpointing.py:540:forward] ----Partition Activations False, CPU CHECKPOINTING False
x3104c0s1b0n0: [2024-03-29 15:52:07,676] [INFO] [checkpointing.py:541:forward] ----contiguous Memory Checkpointing False with 60 total layers
x3104c0s1b0n0: [2024-03-29 15:52:07,676] [INFO] [checkpointing.py:543:forward] ----Synchronization False
x3104c0s1b0n0: [2024-03-29 15:52:07,676] [INFO] [checkpointing.py:544:forward] ----Profiling time in checkpointing False
x3104c0s1b0n0: [2024-03-29 15:52:15,534] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:52:15,535] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 19.76 GB         CA 22.95 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 15:52:15,535] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.41 GB, percent = 62.5%
x3104c0s1b0n0: [2024-03-29 15:52:15,710] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:52:15,711] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 16.84 GB         CA 16.91 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 15:52:15,711] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.4 GB, percent = 62.5%
x3104c0s1b0n0: [2024-03-29 15:52:39,406] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:52:39,407] [INFO] [utils.py:801:see_memory_usage] MA 11.51 GB         Max_MA 20.43 GB         CA 11.75 GB         Max_CA 28 GB 
x3104c0s1b0n0: [2024-03-29 15:52:39,407] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.4 GB, percent = 62.5%
x3104c0s1b0n0: [2024-03-29 15:52:39,486] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:52:39,486] [INFO] [utils.py:801:see_memory_usage] MA 11.51 GB         Max_MA 11.51 GB         CA 11.75 GB         Max_CA 12 GB 
x3104c0s1b0n0: [2024-03-29 15:52:39,486] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.53 GB, percent = 62.5%
x3104c0s1b0n0: [2024-03-29 15:52:46,040] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.489377498626709
x3104c0s1b0n0: [2024-03-29 15:52:46,086] [INFO] [stage3.py:2251:step] Full outer step loop took 6.535875082015991
x3104c0s1b0n0: [2024-03-29 15:52:46,121] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.569918394088745
x3104c0s1b0n0: [2024-03-29 15:52:46,134] [INFO] [stage3.py:2251:step] Full outer step loop took 6.583149433135986
x3104c0s1b0n0: [2024-03-29 15:52:46,191] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.64018177986145
x3104c0s1b0n0: [2024-03-29 15:52:46,194] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.643277883529663
x3104c0s1b0n0: [2024-03-29 15:52:46,200] [INFO] [stage3.py:2251:step] Full outer step loop took 6.649269104003906
x3104c0s1b0n0: [2024-03-29 15:52:46,203] [INFO] [stage3.py:2251:step] Full outer step loop took 6.652340412139893
x3104c0s25b0n0: [2024-03-29 15:52:46,784] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.232905626296997
x3104c0s25b0n0: [2024-03-29 15:52:46,816] [INFO] [stage3.py:2251:step] Full outer step loop took 7.265597581863403
x3104c0s25b0n0: [2024-03-29 15:52:47,246] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.690098524093628
x3104c0s25b0n0: [2024-03-29 15:52:47,246] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.6900999546051025
x3104c0s25b0n0: [2024-03-29 15:52:47,246] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.69007682800293
x3104c0s25b0n0: [2024-03-29 15:52:47,297] [INFO] [stage3.py:2251:step] Full outer step loop took 7.746630907058716
x3104c0s25b0n0: [2024-03-29 15:52:47,297] [INFO] [stage3.py:2251:step] Full outer step loop took 7.746633052825928
x3104c0s25b0n0: [2024-03-29 15:52:47,297] [INFO] [stage3.py:2251:step] Full outer step loop took 7.746660947799683
x3104c0s1b0n0: [2024-03-29 15:52:47,385] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6652.46
x3104c0s1b0n0: [2024-03-29 15:52:47,386] [INFO] [stage3.py:2277:step] End to end step took 7.83509635925293
x3104c0s1b0n0: [2024-03-29 15:52:47,386] [WARNING] [stage3.py:2267:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3104c0s1b0n0: [2024-03-29 15:52:47,386] [INFO] [stage3.py:2277:step] End to end step took 7.835285186767578
x3104c0s1b0n0: [2024-03-29 15:52:47,386] [INFO] [logging.py:96:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 15:52:47,386] [INFO] [stage3.py:2277:step] End to end step took 7.83568811416626
x3104c0s1b0n0: [2024-03-29 15:52:47,386] [INFO] [stage3.py:2277:step] End to end step took 7.835664749145508
x3104c0s1b0n0: [2024-03-29 15:52:47,386] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 8057.50 | bwd_microstep: 23543.37 | bwd_inner_microstep: 23465.87 | bwd_allreduce_microstep: 77.39 | step_microstep: 7899.47
x3104c0s1b0n0: [2024-03-29 15:52:47,386] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 8057.50 | bwd: 23543.37 | bwd_inner: 23465.88 | bwd_allreduce: 77.39 | step: 7899.47
x3104c0s25b0n0: [2024-03-29 15:52:47,396] [INFO] [stage3.py:2277:step] End to end step took 7.846116304397583
x3104c0s25b0n0: [2024-03-29 15:52:47,396] [INFO] [stage3.py:2277:step] End to end step took 7.846113920211792
x3104c0s25b0n0: [2024-03-29 15:52:47,396] [INFO] [stage3.py:2277:step] End to end step took 7.8461527824401855
x3104c0s25b0n0: [2024-03-29 15:52:47,396] [INFO] [stage3.py:2277:step] End to end step took 7.846017122268677
x3104c0s1b0n0: [2024-03-29 15:52:47,484] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:52:47,484] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.56 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:52:47,484] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.56 GB, percent = 79.0%
x3104c0s1b0n0: <TIMER:interval-time,40.06794810295105><TIMER:interval-time,40.06793832778931>
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,40.06795382499695><TIMER:interval-time,40.06795597076416>
x3104c0s1b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,40.067952394485474>
x3104c0s25b0n0: <TIMER:interval-time,40.06780958175659>
x3104c0s25b0n0: <TIMER:interval-time,40.06796598434448><TIMER:interval-time,40.06795835494995>
x3104c0s25b0n0: 
x3104c0s1b0n0: [Rank 0] (after 1 iterations) memory (MB) | allocated: 10510.36328125 | max allocated: 10510.36376953125 | reserved: 10586.0 | max reserved: 10586.0
x3104c0s25b0n0:  elapsed_time 40.067810 | consumed samples:           32 | consumed tokens:        65536 | elapsed time per iteration (ms): 40067.8 | learning rate: 3.000E-04 | global batch size:    32 | lm loss: 1.215015E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.799 | TFLOPs: 55.26 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:52:47,656] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:52:47,657] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 15:52:47,657] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.66 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:52:53,649] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:52:53,650] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:52:53,650] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.65 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:52:53,734] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:52:53,734] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:52:53,734] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.65 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:53:09,594] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:53:09,595] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 23.87 GB         CA 13.01 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 15:53:09,595] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.67 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:53:09,672] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:53:09,672] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 12.77 GB         CA 13.01 GB         Max_CA 13 GB 
x3104c0s1b0n0: [2024-03-29 15:53:09,672] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.67 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:53:14,479] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.782499551773071
x3104c0s1b0n0: [2024-03-29 15:53:14,481] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.784444093704224
x3104c0s1b0n0: [2024-03-29 15:53:14,486] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.78977632522583
x3104c0s1b0n0: [2024-03-29 15:53:14,488] [INFO] [stage3.py:2251:step] Full outer step loop took 4.792012453079224
x3104c0s1b0n0: [2024-03-29 15:53:14,490] [INFO] [stage3.py:2251:step] Full outer step loop took 4.794054746627808
x3104c0s1b0n0: [2024-03-29 15:53:14,495] [INFO] [stage3.py:2251:step] Full outer step loop took 4.798903703689575
x3104c0s1b0n0: [2024-03-29 15:53:14,547] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.850416660308838
x3104c0s1b0n0: [2024-03-29 15:53:14,556] [INFO] [stage3.py:2251:step] Full outer step loop took 4.859461545944214
x3104c0s25b0n0: [2024-03-29 15:53:14,563] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.86413049697876
x3104c0s25b0n0: [2024-03-29 15:53:14,579] [INFO] [stage3.py:2251:step] Full outer step loop took 4.880240440368652
x3104c0s25b0n0: [2024-03-29 15:53:14,583] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.8835272789001465
x3104c0s25b0n0: [2024-03-29 15:53:14,584] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.8846986293792725
x3104c0s25b0n0: [2024-03-29 15:53:14,593] [INFO] [stage3.py:2251:step] Full outer step loop took 4.893878221511841
x3104c0s25b0n0: [2024-03-29 15:53:14,594] [INFO] [stage3.py:2251:step] Full outer step loop took 4.894563674926758
x3104c0s25b0n0: [2024-03-29 15:53:14,651] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.951667785644531
x3104c0s25b0n0: [2024-03-29 15:53:14,660] [INFO] [stage3.py:2251:step] Full outer step loop took 4.960677862167358
x3104c0s1b0n0: [2024-03-29 15:53:14,669] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4859.65
x3104c0s1b0n0: [2024-03-29 15:53:14,669] [INFO] [stage3.py:2277:step] End to end step took 4.972769498825073
x3104c0s25b0n0: [2024-03-29 15:53:14,669] [INFO] [stage3.py:2277:step] End to end step took 4.96986722946167
x3104c0s1b0n0: [2024-03-29 15:53:14,669] [INFO] [stage3.py:2277:step] End to end step took 4.97293496131897
x3104c0s25b0n0: [2024-03-29 15:53:14,669] [INFO] [stage3.py:2277:step] End to end step took 4.970023155212402
x3104c0s25b0n0: [2024-03-29 15:53:14,669] [INFO] [stage3.py:2277:step] End to end step took 4.97003436088562
x3104c0s1b0n0: [2024-03-29 15:53:14,669] [INFO] [stage3.py:2277:step] End to end step took 4.972916841506958
x3104c0s25b0n0: [2024-03-29 15:53:14,669] [INFO] [stage3.py:2277:step] End to end step took 4.970160484313965
x3104c0s1b0n0: [2024-03-29 15:53:14,669] [INFO] [logging.py:96:log_dist] [Rank 0] step=2, skipped=0, lr=[0.0002918585038060976], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 15:53:14,669] [INFO] [stage3.py:2277:step] End to end step took 4.973113775253296
x3104c0s1b0n0: [2024-03-29 15:53:14,669] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 5953.05 | bwd_microstep: 15690.38 | bwd_inner_microstep: 15610.10 | bwd_allreduce_microstep: 80.22 | step_microstep: 4996.82
x3104c0s1b0n0: [2024-03-29 15:53:14,670] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 5953.04 | bwd: 15690.38 | bwd_inner: 15610.09 | bwd_allreduce: 80.23 | step: 4996.82
x3104c0s1b0n0: [2024-03-29 15:53:14,771] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:53:14,772] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.77 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:53:14,772] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.66 GB, percent = 79.0%
x3104c0s1b0n0: <TIMER:interval-time,27.28687834739685><TIMER:interval-time,27.286874771118164><TIMER:interval-time,27.28687834739685>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,27.286882400512695>
x3104c0s25b0n0: <TIMER:interval-time,27.286864519119263>
x3104c0s25b0n0: <TIMER:interval-time,27.286861181259155><TIMER:interval-time,27.286869525909424>
x3104c0s25b0n0: <TIMER:interval-time,27.286864280700684>
x3104c0s25b0n0: 
x3104c0s25b0n0:  elapsed_time 27.286870 | consumed samples:           64 | consumed tokens:       131072 | elapsed time per iteration (ms): 27286.9 | learning rate: 2.919E-04 | global batch size:    32 | lm loss: 1.082593E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.173 | TFLOPs: 81.14 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:53:14,920] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:53:14,921] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 15:53:14,921] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.69 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:53:20,728] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:53:20,729] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:53:20,729] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.68 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:53:20,811] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:53:20,812] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:53:20,812] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.68 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:53:36,574] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:53:36,575] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 23.87 GB         CA 13.01 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 15:53:36,575] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.68 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:53:36,650] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:53:36,651] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 12.77 GB         CA 13.01 GB         Max_CA 13 GB 
x3104c0s1b0n0: [2024-03-29 15:53:36,651] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.68 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:53:41,334] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.6588218212127686
x3104c0s1b0n0: [2024-03-29 15:53:41,343] [INFO] [stage3.py:2251:step] Full outer step loop took 4.667857646942139
x3104c0s1b0n0: [2024-03-29 15:53:41,421] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.744314908981323
x3104c0s1b0n0: [2024-03-29 15:53:41,463] [INFO] [stage3.py:2251:step] Full outer step loop took 4.787501096725464
x3104c0s1b0n0: [2024-03-29 15:53:41,484] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.808485984802246
x3104c0s1b0n0: [2024-03-29 15:53:41,498] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.822864770889282
x3104c0s1b0n0: [2024-03-29 15:53:41,501] [INFO] [stage3.py:2251:step] Full outer step loop took 4.825115203857422
x3104c0s1b0n0: [2024-03-29 15:53:41,507] [INFO] [stage3.py:2251:step] Full outer step loop took 4.831966161727905
x3104c0s25b0n0: [2024-03-29 15:53:41,616] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.940938234329224
x3104c0s25b0n0: [2024-03-29 15:53:41,626] [INFO] [stage3.py:2251:step] Full outer step loop took 4.950561046600342
x3104c0s25b0n0: [2024-03-29 15:53:41,627] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.951854944229126
x3104c0s25b0n0: [2024-03-29 15:53:41,636] [INFO] [stage3.py:2251:step] Full outer step loop took 4.9608612060546875
x3104c0s25b0n0: [2024-03-29 15:53:41,694] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.018555402755737
x3104c0s25b0n0: [2024-03-29 15:53:41,709] [INFO] [stage3.py:2251:step] Full outer step loop took 5.033140182495117
x3104c0s25b0n0: [2024-03-29 15:53:41,747] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.071133613586426
x3104c0s25b0n0: [2024-03-29 15:53:41,756] [INFO] [stage3.py:2251:step] Full outer step loop took 5.0801842212677
x3104c0s25b0n0: [2024-03-29 15:53:41,765] [INFO] [stage3.py:2277:step] End to end step took 5.089848041534424
x3104c0s25b0n0: [2024-03-29 15:53:41,765] [INFO] [stage3.py:2277:step] End to end step took 5.089993953704834
x3104c0s25b0n0: [2024-03-29 15:53:41,766] [INFO] [stage3.py:2277:step] End to end step took 5.090172052383423
x3104c0s25b0n0: [2024-03-29 15:53:41,766] [INFO] [stage3.py:2277:step] End to end step took 5.090302467346191
x3104c0s1b0n0: [2024-03-29 15:53:41,765] [INFO] [stage3.py:2277:step] End to end step took 5.089761972427368
x3104c0s1b0n0: [2024-03-29 15:53:41,765] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4789.69
x3104c0s1b0n0: [2024-03-29 15:53:41,765] [INFO] [stage3.py:2277:step] End to end step took 5.090006113052368
x3104c0s1b0n0: [2024-03-29 15:53:41,766] [INFO] [stage3.py:2277:step] End to end step took 5.0900232791900635
x3104c0s1b0n0: [2024-03-29 15:53:41,766] [INFO] [stage3.py:2277:step] End to end step took 5.0902557373046875
x3104c0s1b0n0: [2024-03-29 15:53:41,766] [INFO] [logging.py:96:log_dist] [Rank 0] step=3, skipped=0, lr=[0.00026841599982106197], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 15:53:41,766] [INFO] [timer.py:260:stop] epoch=0/micro_step=3/global_step=3, RunningAvgSamplesPerSec=1.1920846018875402, CurrSamplesPerSec=1.1920846018875402, MemAllocated=10.26GB, MaxMemAllocated=12.77GB
x3104c0s1b0n0: [2024-03-29 15:53:41,766] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 5766.93 | bwd_microstep: 15589.56 | bwd_inner_microstep: 15514.54 | bwd_allreduce_microstep: 74.96 | step_microstep: 5114.87
x3104c0s1b0n0: [2024-03-29 15:53:41,767] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 5766.92 | bwd: 15589.56 | bwd_inner: 15514.53 | bwd_allreduce: 74.97 | step: 5114.87
x3104c0s1b0n0: [2024-03-29 15:53:41,859] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:53:41,859] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.77 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:53:41,859] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.68 GB, percent = 79.0%
x3104c0s1b0n0: <TIMER:interval-time,27.086989641189575><TIMER:interval-time,27.08700132369995>
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,27.08700180053711>
x3104c0s25b0n0: <TIMER:interval-time,27.08699131011963><TIMER:interval-time,27.08699083328247><TIMER:interval-time,27.08699631690979>
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,27.087098836898804>
x3104c0s1b0n0: <TIMER:interval-time,27.087119579315186>
x3104c0s25b0n0:  elapsed_time 27.086996 | consumed samples:           96 | consumed tokens:       196608 | elapsed time per iteration (ms): 27087.0 | learning rate: 2.684E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.181 | TFLOPs: 81.74 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:53:41,973] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:53:41,973] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 15:53:41,974] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.71 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:53:48,337] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:53:48,338] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:53:48,338] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.7 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:53:48,419] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:53:48,420] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:53:48,420] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.7 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:54:04,587] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:54:04,588] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 23.87 GB         CA 13.01 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 15:54:04,588] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.71 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:54:04,659] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:54:04,659] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 12.77 GB         CA 13.01 GB         Max_CA 13 GB 
x3104c0s1b0n0: [2024-03-29 15:54:04,660] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.71 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:54:09,346] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.6626622676849365
x3104c0s1b0n0: [2024-03-29 15:54:09,402] [INFO] [stage3.py:2251:step] Full outer step loop took 4.718543767929077
x3104c0s1b0n0: [2024-03-29 15:54:09,490] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.8068835735321045
x3104c0s1b0n0: [2024-03-29 15:54:09,490] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.807018995285034
x3104c0s1b0n0: [2024-03-29 15:54:09,490] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.80733585357666
x3104c0s1b0n0: [2024-03-29 15:54:09,499] [INFO] [stage3.py:2251:step] Full outer step loop took 4.815999746322632
x3104c0s1b0n0: [2024-03-29 15:54:09,499] [INFO] [stage3.py:2251:step] Full outer step loop took 4.816201448440552
x3104c0s1b0n0: [2024-03-29 15:54:09,500] [INFO] [stage3.py:2251:step] Full outer step loop took 4.8167266845703125
x3104c0s25b0n0: [2024-03-29 15:54:09,551] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.868314981460571
x3104c0s25b0n0: [2024-03-29 15:54:09,561] [INFO] [stage3.py:2251:step] Full outer step loop took 4.877516269683838
x3104c0s25b0n0: [2024-03-29 15:54:09,712] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.029237270355225
x3104c0s25b0n0: [2024-03-29 15:54:09,730] [INFO] [stage3.py:2251:step] Full outer step loop took 5.046586275100708
x3104c0s25b0n0: [2024-03-29 15:54:09,731] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.04799485206604
x3104c0s25b0n0: [2024-03-29 15:54:09,732] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.04861307144165
x3104c0s25b0n0: [2024-03-29 15:54:09,740] [INFO] [stage3.py:2251:step] Full outer step loop took 5.05702018737793
x3104c0s25b0n0: [2024-03-29 15:54:09,741] [INFO] [stage3.py:2251:step] Full outer step loop took 5.057988166809082
x3104c0s25b0n0: [2024-03-29 15:54:09,751] [INFO] [stage3.py:2277:step] End to end step took 5.067599296569824
x3104c0s25b0n0: [2024-03-29 15:54:09,751] [INFO] [stage3.py:2277:step] End to end step took 5.067631483078003
x3104c0s1b0n0: [2024-03-29 15:54:09,751] [INFO] [stage3.py:2277:step] End to end step took 5.067619562149048
x3104c0s1b0n0: [2024-03-29 15:54:09,751] [INFO] [stage3.py:2277:step] End to end step took 5.067531108856201
x3104c0s25b0n0: [2024-03-29 15:54:09,751] [INFO] [stage3.py:2277:step] End to end step took 5.067793607711792
x3104c0s1b0n0: [2024-03-29 15:54:09,751] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4721.50
x3104c0s1b0n0: [2024-03-29 15:54:09,751] [INFO] [stage3.py:2277:step] End to end step took 5.068135023117065
x3104c0s25b0n0: [2024-03-29 15:54:09,751] [INFO] [stage3.py:2277:step] End to end step took 5.068093538284302
x3104c0s1b0n0: [2024-03-29 15:54:09,751] [INFO] [stage3.py:2277:step] End to end step took 5.068321943283081
x3104c0s1b0n0: [2024-03-29 15:54:09,752] [INFO] [logging.py:96:log_dist] [Rank 0] step=4, skipped=0, lr=[0.00023249999999999996], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 15:54:09,752] [INFO] [timer.py:260:stop] epoch=0/micro_step=4/global_step=4, RunningAvgSamplesPerSec=1.1717129118584513, CurrSamplesPerSec=1.1520257920813513, MemAllocated=10.26GB, MaxMemAllocated=12.77GB
x3104c0s1b0n0: [2024-03-29 15:54:09,752] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6321.60 | bwd_microstep: 15999.56 | bwd_inner_microstep: 15916.97 | bwd_allreduce_microstep: 82.51 | step_microstep: 5092.41
x3104c0s1b0n0: [2024-03-29 15:54:09,752] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6321.58 | bwd: 15999.56 | bwd_inner: 15916.96 | bwd_allreduce: 82.53 | step: 5092.41
x3104c0s1b0n0: [2024-03-29 15:54:09,846] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:54:09,847] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.77 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:54:09,847] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.7 GB, percent = 79.0%
x3104c0s1b0n0: <TIMER:interval-time,27.987297534942627><TIMER:interval-time,27.98729157447815><TIMER:interval-time,27.98730158805847><TIMER:interval-time,27.987285614013672>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,27.987274408340454><TIMER:interval-time,27.987287998199463><TIMER:interval-time,27.987290143966675>
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,27.987404584884644>
x3104c0s25b0n0:  elapsed_time 27.987405 | consumed samples:          128 | consumed tokens:       262144 | elapsed time per iteration (ms): 27987.4 | learning rate: 2.325E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.143 | TFLOPs: 79.11 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:54:09,943] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:54:09,943] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 15:54:09,943] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.7 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:54:16,126] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:54:16,127] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:54:16,127] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.69 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:54:16,201] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:54:16,202] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:54:16,202] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.69 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:54:32,832] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:54:32,833] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 23.87 GB         CA 13.01 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 15:54:32,833] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.7 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:54:32,904] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:54:32,904] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 12.77 GB         CA 13.01 GB         Max_CA 13 GB 
x3104c0s1b0n0: [2024-03-29 15:54:32,904] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.7 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:54:37,767] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.839290618896484
x3104c0s1b0n0: [2024-03-29 15:54:37,772] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.8442370891571045
x3104c0s1b0n0: [2024-03-29 15:54:37,778] [INFO] [stage3.py:2251:step] Full outer step loop took 4.850279092788696
x3104c0s25b0n0: [2024-03-29 15:54:37,784] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.856143951416016
x3104c0s25b0n0: [2024-03-29 15:54:37,794] [INFO] [stage3.py:2251:step] Full outer step loop took 4.865607023239136
x3104c0s1b0n0: [2024-03-29 15:54:37,804] [INFO] [stage3.py:2251:step] Full outer step loop took 4.876101016998291
x3104c0s1b0n0: [2024-03-29 15:54:37,807] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.879119157791138
x3104c0s1b0n0: [2024-03-29 15:54:37,823] [INFO] [stage3.py:2251:step] Full outer step loop took 4.8952202796936035
x3104c0s25b0n0: [2024-03-29 15:54:37,825] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.896883964538574
x3104c0s25b0n0: [2024-03-29 15:54:37,834] [INFO] [stage3.py:2251:step] Full outer step loop took 4.905892848968506
x3104c0s1b0n0: [2024-03-29 15:54:37,861] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.932347536087036
x3104c0s1b0n0: [2024-03-29 15:54:37,870] [INFO] [stage3.py:2251:step] Full outer step loop took 4.941362142562866
x3104c0s25b0n0: [2024-03-29 15:54:37,953] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.025444269180298
x3104c0s25b0n0: [2024-03-29 15:54:37,955] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.026855707168579
x3104c0s25b0n0: [2024-03-29 15:54:37,962] [INFO] [stage3.py:2251:step] Full outer step loop took 5.034497022628784
x3104c0s25b0n0: [2024-03-29 15:54:37,964] [INFO] [stage3.py:2251:step] Full outer step loop took 5.0358967781066895
x3104c0s1b0n0: [2024-03-29 15:54:37,973] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4941.50
x3104c0s1b0n0: [2024-03-29 15:54:37,973] [INFO] [stage3.py:2277:step] End to end step took 5.045040607452393
x3104c0s25b0n0: [2024-03-29 15:54:37,973] [INFO] [stage3.py:2277:step] End to end step took 5.045229196548462
x3104c0s25b0n0: [2024-03-29 15:54:37,973] [INFO] [stage3.py:2277:step] End to end step took 5.045233249664307
x3104c0s1b0n0: [2024-03-29 15:54:37,974] [INFO] [logging.py:96:log_dist] [Rank 0] step=5, skipped=0, lr=[0.0001884425039850356], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 15:54:37,974] [INFO] [stage3.py:2277:step] End to end step took 5.0453901290893555
x3104c0s1b0n0: [2024-03-29 15:54:37,974] [INFO] [stage3.py:2277:step] End to end step took 5.045454740524292
x3104c0s25b0n0: [2024-03-29 15:54:37,974] [INFO] [stage3.py:2277:step] End to end step took 5.045663356781006
x3104c0s1b0n0: [2024-03-29 15:54:37,974] [INFO] [stage3.py:2277:step] End to end step took 5.045722007751465
x3104c0s1b0n0: [2024-03-29 15:54:37,974] [INFO] [timer.py:260:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=1.1615237322749496, CurrSamplesPerSec=1.1416679138719381, MemAllocated=10.26GB, MaxMemAllocated=12.77GB
x3104c0s25b0n0: [2024-03-29 15:54:37,974] [INFO] [stage3.py:2277:step] End to end step took 5.0457682609558105
x3104c0s1b0n0: [2024-03-29 15:54:37,974] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6149.59 | bwd_microstep: 16464.15 | bwd_inner_microstep: 16383.33 | bwd_allreduce_microstep: 80.76 | step_microstep: 5069.54
x3104c0s1b0n0: [2024-03-29 15:54:37,974] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6149.58 | bwd: 16464.15 | bwd_inner: 16383.32 | bwd_allreduce: 80.77 | step: 5069.54
x3104c0s1b0n0: [2024-03-29 15:54:38,083] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:54:38,083] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.77 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:54:38,083] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.7 GB, percent = 79.0%
x3104c0s1b0n0: <TIMER:interval-time,28.235955476760864>
x3104c0s1b0n0: <TIMER:interval-time,28.235962390899658><TIMER:interval-time,28.235960006713867><TIMER:interval-time,28.235958576202393>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,28.235954523086548><TIMER:interval-time,28.23595643043518><TIMER:interval-time,28.235957622528076>
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,28.235946893692017>
x3104c0s25b0n0:  elapsed_time 28.235947 | consumed samples:          160 | consumed tokens:       327680 | elapsed time per iteration (ms): 28235.9 | learning rate: 1.884E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.133 | TFLOPs: 78.41 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:54:38,226] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:54:38,227] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 15:54:38,227] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.69 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:54:44,446] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:54:44,447] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:54:44,447] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.68 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:54:44,542] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:54:44,543] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:54:44,543] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.68 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:55:01,015] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:55:01,016] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 23.87 GB         CA 13.01 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 15:55:01,016] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.68 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:55:01,093] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:55:01,093] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 12.77 GB         CA 13.01 GB         Max_CA 13 GB 
x3104c0s1b0n0: [2024-03-29 15:55:01,093] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.68 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:55:05,787] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.669707775115967
x3104c0s1b0n0: [2024-03-29 15:55:05,796] [INFO] [stage3.py:2251:step] Full outer step loop took 4.678747653961182
x3104c0s1b0n0: [2024-03-29 15:55:05,932] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.815062761306763
x3104c0s1b0n0: [2024-03-29 15:55:05,942] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.824175834655762
x3104c0s1b0n0: [2024-03-29 15:55:05,942] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.824517726898193
x3104c0s1b0n0: [2024-03-29 15:55:05,944] [INFO] [stage3.py:2251:step] Full outer step loop took 4.826936721801758
x3104c0s1b0n0: [2024-03-29 15:55:05,951] [INFO] [stage3.py:2251:step] Full outer step loop took 4.833321571350098
x3104c0s1b0n0: [2024-03-29 15:55:05,951] [INFO] [stage3.py:2251:step] Full outer step loop took 4.833563327789307
x3104c0s25b0n0: [2024-03-29 15:55:06,030] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.913111448287964
x3104c0s25b0n0: [2024-03-29 15:55:06,041] [INFO] [stage3.py:2251:step] Full outer step loop took 4.923534631729126
x3104c0s25b0n0: [2024-03-29 15:55:06,200] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.082979917526245
x3104c0s25b0n0: [2024-03-29 15:55:06,200] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.082965612411499
x3104c0s25b0n0: [2024-03-29 15:55:06,206] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.088827848434448
x3104c0s25b0n0: [2024-03-29 15:55:06,209] [INFO] [stage3.py:2251:step] Full outer step loop took 5.092022657394409
x3104c0s25b0n0: [2024-03-29 15:55:06,209] [INFO] [stage3.py:2251:step] Full outer step loop took 5.092412233352661
x3104c0s25b0n0: [2024-03-29 15:55:06,215] [INFO] [stage3.py:2251:step] Full outer step loop took 5.097835302352905
x3104c0s1b0n0: [2024-03-29 15:55:06,224] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4833.72
x3104c0s1b0n0: [2024-03-29 15:55:06,224] [INFO] [stage3.py:2277:step] End to end step took 5.106796979904175
x3104c0s25b0n0: [2024-03-29 15:55:06,224] [INFO] [stage3.py:2277:step] End to end step took 5.107009410858154
x3104c0s25b0n0: [2024-03-29 15:55:06,224] [INFO] [stage3.py:2277:step] End to end step took 5.1070716381073
x3104c0s25b0n0: [2024-03-29 15:55:06,224] [INFO] [stage3.py:2277:step] End to end step took 5.107088804244995
x3104c0s1b0n0: [2024-03-29 15:55:06,224] [INFO] [stage3.py:2277:step] End to end step took 5.107032537460327
x3104c0s1b0n0: [2024-03-29 15:55:06,224] [INFO] [stage3.py:2277:step] End to end step took 5.106920003890991
x3104c0s1b0n0: [2024-03-29 15:55:06,225] [INFO] [logging.py:96:log_dist] [Rank 0] step=6, skipped=0, lr=[0.0001415574960149644], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 15:55:06,225] [INFO] [stage3.py:2277:step] End to end step took 5.107370376586914
x3104c0s25b0n0: [2024-03-29 15:55:06,224] [INFO] [stage3.py:2277:step] End to end step took 5.107475996017456
x3104c0s1b0n0: [2024-03-29 15:55:06,225] [INFO] [timer.py:260:stop] epoch=0/micro_step=6/global_step=6, RunningAvgSamplesPerSec=1.1568326826089619, CurrSamplesPerSec=1.1429841610898834, MemAllocated=10.26GB, MaxMemAllocated=12.77GB
x3104c0s1b0n0: [2024-03-29 15:55:06,225] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6179.02 | bwd_microstep: 16295.22 | bwd_inner_microstep: 16218.83 | bwd_allreduce_microstep: 76.33 | step_microstep: 5131.46
x3104c0s1b0n0: [2024-03-29 15:55:06,225] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6179.00 | bwd: 16295.22 | bwd_inner: 16218.82 | bwd_allreduce: 76.34 | step: 5131.46
x3104c0s1b0n0: [2024-03-29 15:55:06,326] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:55:06,326] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.77 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:55:06,326] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.68 GB, percent = 79.0%
x3104c0s1b0n0: <TIMER:interval-time,28.2427761554718><TIMER:interval-time,28.242777109146118><TIMER:interval-time,28.24277901649475>
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,28.242780208587646>
x3104c0s1b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,28.242769956588745><TIMER:interval-time,28.242772817611694><TIMER:interval-time,28.242775201797485>
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,28.242783069610596>
x3104c0s25b0n0:  elapsed_time 28.242775 | consumed samples:          192 | consumed tokens:       393216 | elapsed time per iteration (ms): 28242.8 | learning rate: 1.416E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.133 | TFLOPs: 78.40 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:55:06,448] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:55:06,449] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 15:55:06,449] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.69 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:55:12,239] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:55:12,240] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:55:12,240] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.68 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:55:12,322] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:55:12,322] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:55:12,323] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.68 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:55:28,101] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:55:28,102] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 23.87 GB         CA 13.01 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 15:55:28,102] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.69 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:55:28,175] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:55:28,176] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 12.77 GB         CA 13.01 GB         Max_CA 13 GB 
x3104c0s1b0n0: [2024-03-29 15:55:28,176] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.68 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:55:32,936] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.735808372497559
x3104c0s1b0n0: [2024-03-29 15:55:32,945] [INFO] [stage3.py:2251:step] Full outer step loop took 4.745003700256348
x3104c0s1b0n0: [2024-03-29 15:55:32,945] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.745563268661499
x3104c0s1b0n0: [2024-03-29 15:55:32,954] [INFO] [stage3.py:2251:step] Full outer step loop took 4.754636764526367
x3104c0s1b0n0: [2024-03-29 15:55:33,059] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.859278440475464
x3104c0s1b0n0: [2024-03-29 15:55:33,062] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.8626344203948975
x3104c0s1b0n0: [2024-03-29 15:55:33,069] [INFO] [stage3.py:2251:step] Full outer step loop took 4.86948823928833
x3104c0s1b0n0: [2024-03-29 15:55:33,071] [INFO] [stage3.py:2251:step] Full outer step loop took 4.871758699417114
x3104c0s25b0n0: [2024-03-29 15:55:33,244] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.043982028961182
x3104c0s25b0n0: [2024-03-29 15:55:33,260] [INFO] [stage3.py:2251:step] Full outer step loop took 5.060927152633667
x3104c0s25b0n0: [2024-03-29 15:55:33,262] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.062148094177246
x3104c0s25b0n0: [2024-03-29 15:55:33,263] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.063910484313965
x3104c0s25b0n0: [2024-03-29 15:55:33,265] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.0656468868255615
x3104c0s25b0n0: [2024-03-29 15:55:33,271] [INFO] [stage3.py:2251:step] Full outer step loop took 5.071566343307495
x3104c0s25b0n0: [2024-03-29 15:55:33,272] [INFO] [stage3.py:2251:step] Full outer step loop took 5.072942733764648
x3104c0s25b0n0: [2024-03-29 15:55:33,274] [INFO] [stage3.py:2251:step] Full outer step loop took 5.074783086776733
x3104c0s25b0n0: [2024-03-29 15:55:33,284] [INFO] [stage3.py:2277:step] End to end step took 5.084133148193359
x3104c0s25b0n0: [2024-03-29 15:55:33,284] [INFO] [stage3.py:2277:step] End to end step took 5.084243297576904
x3104c0s1b0n0: [2024-03-29 15:55:33,284] [INFO] [stage3.py:2277:step] End to end step took 5.084198236465454
x3104c0s25b0n0: [2024-03-29 15:55:33,284] [INFO] [stage3.py:2277:step] End to end step took 5.084313154220581
x3104c0s25b0n0: [2024-03-29 15:55:33,284] [INFO] [stage3.py:2277:step] End to end step took 5.084350824356079
x3104c0s1b0n0: [2024-03-29 15:55:33,284] [INFO] [stage3.py:2277:step] End to end step took 5.084347486495972
x3104c0s1b0n0: [2024-03-29 15:55:33,284] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4754.81
x3104c0s1b0n0: [2024-03-29 15:55:33,284] [INFO] [stage3.py:2277:step] End to end step took 5.08471941947937
x3104c0s1b0n0: [2024-03-29 15:55:33,284] [INFO] [stage3.py:2277:step] End to end step took 5.084705114364624
x3104c0s1b0n0: [2024-03-29 15:55:33,285] [INFO] [logging.py:96:log_dist] [Rank 0] step=7, skipped=0, lr=[9.750000000000001e-05], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 15:55:33,285] [INFO] [timer.py:260:stop] epoch=0/micro_step=7/global_step=7, RunningAvgSamplesPerSec=1.1637912763507352, CurrSamplesPerSec=1.192483438730533, MemAllocated=10.26GB, MaxMemAllocated=12.77GB
x3104c0s1b0n0: [2024-03-29 15:55:33,285] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 5750.33 | bwd_microstep: 15613.00 | bwd_inner_microstep: 15540.06 | bwd_allreduce_microstep: 72.87 | step_microstep: 5109.38
x3104c0s1b0n0: [2024-03-29 15:55:33,286] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 5750.32 | bwd: 15612.99 | bwd_inner: 15540.05 | bwd_allreduce: 72.88 | step: 5109.38
x3104c0s1b0n0: [2024-03-29 15:55:33,386] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:55:33,387] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.77 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:55:33,387] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.68 GB, percent = 79.0%
x3104c0s1b0n0: <TIMER:interval-time,27.060003995895386><TIMER:interval-time,27.060006141662598><TIMER:interval-time,27.060002088546753>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,27.060008764266968>
x3104c0s25b0n0: <TIMER:interval-time,27.05997633934021><TIMER:interval-time,27.059977769851685><TIMER:interval-time,27.059978008270264>
x3104c0s25b0n0: <TIMER:interval-time,27.059980630874634>
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0:  elapsed_time 27.059981 | consumed samples:          224 | consumed tokens:       458752 | elapsed time per iteration (ms): 27060.0 | learning rate: 9.750E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.183 | TFLOPs: 81.82 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:55:33,534] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:55:33,534] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 15:55:33,534] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.69 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:55:40,243] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:55:40,244] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:55:40,244] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.69 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:55:40,365] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:55:40,366] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:55:40,366] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.69 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:55:56,537] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:55:56,537] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 23.87 GB         CA 13.01 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 15:55:56,538] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.69 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:55:56,613] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:55:56,613] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 12.77 GB         CA 13.01 GB         Max_CA 13 GB 
x3104c0s1b0n0: [2024-03-29 15:55:56,613] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.69 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:56:01,314] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.677201509475708
x3104c0s1b0n0: [2024-03-29 15:56:01,354] [INFO] [stage3.py:2251:step] Full outer step loop took 4.7165610790252686
x3104c0s1b0n0: [2024-03-29 15:56:01,368] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.730750560760498
x3104c0s1b0n0: [2024-03-29 15:56:01,377] [INFO] [stage3.py:2251:step] Full outer step loop took 4.739843368530273
x3104c0s1b0n0: [2024-03-29 15:56:01,483] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.846158504486084
x3104c0s1b0n0: [2024-03-29 15:56:01,483] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.846304416656494
x3104c0s1b0n0: [2024-03-29 15:56:01,492] [INFO] [stage3.py:2251:step] Full outer step loop took 4.855291128158569
x3104c0s1b0n0: [2024-03-29 15:56:01,492] [INFO] [stage3.py:2251:step] Full outer step loop took 4.855434894561768
x3104c0s25b0n0: [2024-03-29 15:56:01,775] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.13342809677124
x3104c0s25b0n0: [2024-03-29 15:56:01,764] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.116061449050903
x3104c0s25b0n0: [2024-03-29 15:56:01,784] [INFO] [stage3.py:2251:step] Full outer step loop took 5.1422083377838135
x3104c0s25b0n0: [2024-03-29 15:56:01,785] [INFO] [stage3.py:2251:step] Full outer step loop took 5.1434807777404785
x3104c0s25b0n0: [2024-03-29 15:56:01,786] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.143844842910767
x3104c0s25b0n0: [2024-03-29 15:56:01,788] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.146647214889526
x3104c0s25b0n0: [2024-03-29 15:56:01,795] [INFO] [stage3.py:2251:step] Full outer step loop took 5.153380393981934
x3104c0s25b0n0: [2024-03-29 15:56:01,797] [INFO] [stage3.py:2251:step] Full outer step loop took 5.155819416046143
x3104c0s1b0n0: [2024-03-29 15:56:01,807] [INFO] [stage3.py:2277:step] End to end step took 5.1697022914886475
x3104c0s1b0n0: [2024-03-29 15:56:01,807] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4740.04
x3104c0s1b0n0: [2024-03-29 15:56:01,807] [INFO] [stage3.py:2277:step] End to end step took 5.170182228088379
x3104c0s1b0n0: [2024-03-29 15:56:01,807] [INFO] [stage3.py:2277:step] End to end step took 5.170231819152832
x3104c0s1b0n0: [2024-03-29 15:56:01,807] [INFO] [stage3.py:2277:step] End to end step took 5.170385122299194
x3104c0s1b0n0: [2024-03-29 15:56:01,808] [INFO] [logging.py:96:log_dist] [Rank 0] step=8, skipped=0, lr=[6.158400017893797e-05], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 15:56:01,808] [INFO] [timer.py:260:stop] epoch=0/micro_step=8/global_step=8, RunningAvgSamplesPerSec=1.158342643907987, CurrSamplesPerSec=1.131847258446998, MemAllocated=10.26GB, MaxMemAllocated=12.77GB
x3104c0s1b0n0: [2024-03-29 15:56:01,808] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6667.98 | bwd_microstep: 16001.34 | bwd_inner_microstep: 15917.33 | bwd_allreduce_microstep: 83.94 | step_microstep: 5194.63
x3104c0s1b0n0: [2024-03-29 15:56:01,808] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6667.97 | bwd: 16001.34 | bwd_inner: 15917.32 | bwd_allreduce: 83.95 | step: 5194.64
x3104c0s25b0n0: [2024-03-29 15:56:01,810] [INFO] [stage3.py:2277:step] End to end step took 5.168008327484131
x3104c0s25b0n0: [2024-03-29 15:56:01,810] [INFO] [stage3.py:2277:step] End to end step took 5.1680333614349365
x3104c0s25b0n0: [2024-03-29 15:56:01,810] [INFO] [stage3.py:2277:step] End to end step took 5.168020248413086
x3104c0s25b0n0: [2024-03-29 15:56:01,810] [INFO] [stage3.py:2277:step] End to end step took 5.168033123016357
x3104c0s1b0n0: [2024-03-29 15:56:01,909] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:56:01,909] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.77 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:56:01,909] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.69 GB, percent = 79.0%
x3104c0s1b0n0: <TIMER:interval-time,28.52239155769348><TIMER:interval-time,28.522398471832275>
x3104c0s1b0n0: <TIMER:interval-time,28.522393703460693>
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,28.522398233413696>
x3104c0s25b0n0: <TIMER:interval-time,28.522518157958984>
x3104c0s25b0n0: <TIMER:interval-time,28.5225191116333><TIMER:interval-time,28.522520065307617>
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,28.5225191116333>
x3104c0s25b0n0:  elapsed_time 28.522520 | consumed samples:          256 | consumed tokens:       524288 | elapsed time per iteration (ms): 28522.5 | learning rate: 6.158E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.122 | TFLOPs: 77.63 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:56:02,060] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:56:02,060] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 15:56:02,060] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.69 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:56:08,250] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:56:08,251] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:56:08,251] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.69 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:56:08,333] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:56:08,333] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:56:08,333] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.69 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:56:24,785] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:56:24,786] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 23.87 GB         CA 13.01 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 15:56:24,786] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.69 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:56:24,859] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:56:24,860] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 12.77 GB         CA 13.01 GB         Max_CA 13 GB 
x3104c0s1b0n0: [2024-03-29 15:56:24,860] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.69 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:56:29,449] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.565677881240845
x3104c0s1b0n0: [2024-03-29 15:56:29,459] [INFO] [stage3.py:2251:step] Full outer step loop took 4.5754876136779785
x3104c0s1b0n0: [2024-03-29 15:56:29,647] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.763485670089722
x3104c0s1b0n0: [2024-03-29 15:56:29,685] [INFO] [stage3.py:2251:step] Full outer step loop took 4.801067113876343
x3104c0s25b0n0: [2024-03-29 15:56:29,690] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.806444406509399
x3104c0s25b0n0: [2024-03-29 15:56:29,699] [INFO] [stage3.py:2251:step] Full outer step loop took 4.815703630447388
x3104c0s1b0n0: [2024-03-29 15:56:29,710] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.8269712924957275
x3104c0s1b0n0: [2024-03-29 15:56:29,717] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.83372163772583
x3104c0s1b0n0: [2024-03-29 15:56:29,720] [INFO] [stage3.py:2251:step] Full outer step loop took 4.836693286895752
x3104c0s1b0n0: [2024-03-29 15:56:29,726] [INFO] [stage3.py:2251:step] Full outer step loop took 4.842727899551392
x3104c0s25b0n0: [2024-03-29 15:56:29,862] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.97864842414856
x3104c0s25b0n0: [2024-03-29 15:56:29,864] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.9804487228393555
x3104c0s25b0n0: [2024-03-29 15:56:29,868] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.9850263595581055
x3104c0s25b0n0: [2024-03-29 15:56:29,871] [INFO] [stage3.py:2251:step] Full outer step loop took 4.98785400390625
x3104c0s25b0n0: [2024-03-29 15:56:29,873] [INFO] [stage3.py:2251:step] Full outer step loop took 4.989873170852661
x3104c0s25b0n0: [2024-03-29 15:56:29,877] [INFO] [stage3.py:2251:step] Full outer step loop took 4.9940619468688965
x3104c0s1b0n0: [2024-03-29 15:56:29,887] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4842.86
x3104c0s1b0n0: [2024-03-29 15:56:29,887] [INFO] [stage3.py:2277:step] End to end step took 5.003271579742432
x3104c0s25b0n0: [2024-03-29 15:56:29,887] [INFO] [stage3.py:2277:step] End to end step took 5.003440856933594
x3104c0s25b0n0: [2024-03-29 15:56:29,887] [INFO] [stage3.py:2277:step] End to end step took 5.003514051437378
x3104c0s25b0n0: [2024-03-29 15:56:29,887] [INFO] [stage3.py:2277:step] End to end step took 5.003530502319336
x3104c0s1b0n0: [2024-03-29 15:56:29,887] [INFO] [stage3.py:2277:step] End to end step took 5.0034239292144775
x3104c0s1b0n0: [2024-03-29 15:56:29,887] [INFO] [logging.py:96:log_dist] [Rank 0] step=9, skipped=0, lr=[3.814149619390238e-05], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 15:56:29,887] [INFO] [timer.py:260:stop] epoch=0/micro_step=9/global_step=9, RunningAvgSamplesPerSec=1.1571459211915973, CurrSamplesPerSec=1.1500171927139018, MemAllocated=10.26GB, MaxMemAllocated=12.77GB
x3104c0s25b0n0: [2024-03-29 15:56:29,887] [INFO] [stage3.py:2277:step] End to end step took 5.004026889801025
x3104c0s1b0n0: [2024-03-29 15:56:29,887] [INFO] [stage3.py:2277:step] End to end step took 5.004062652587891
x3104c0s1b0n0: [2024-03-29 15:56:29,887] [INFO] [stage3.py:2277:step] End to end step took 5.0038886070251465
x3104c0s1b0n0: [2024-03-29 15:56:29,888] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6150.41 | bwd_microstep: 16285.22 | bwd_inner_microstep: 16206.32 | bwd_allreduce_microstep: 78.82 | step_microstep: 5027.37
x3104c0s1b0n0: [2024-03-29 15:56:29,888] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6150.40 | bwd: 16285.21 | bwd_inner: 16206.32 | bwd_allreduce: 78.84 | step: 5027.37
x3104c0s1b0n0: [2024-03-29 15:56:29,982] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:56:29,982] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.77 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:56:29,983] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.69 GB, percent = 79.0%
x3104c0s1b0n0: <TIMER:interval-time,28.07273006439209><TIMER:interval-time,28.07272958755493>
x3104c0s1b0n0: <TIMER:interval-time,28.07272744178772>
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,28.072738647460938>
x3104c0s25b0n0: <TIMER:interval-time,28.072730541229248><TIMER:interval-time,28.07273268699646>
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,28.072736263275146>
x3104c0s25b0n0: <TIMER:interval-time,28.07285237312317>
x3104c0s25b0n0:  elapsed_time 28.072733 | consumed samples:          288 | consumed tokens:       589824 | elapsed time per iteration (ms): 28072.7 | learning rate: 3.814E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.140 | TFLOPs: 78.87 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:56:30,126] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:56:30,127] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 15:56:30,127] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.7 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:56:36,555] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:56:36,556] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:56:36,556] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.69 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:56:36,636] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:56:36,636] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:56:36,636] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.69 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:56:52,427] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:56:52,427] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 23.87 GB         CA 13.01 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 15:56:52,428] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.69 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:56:52,496] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:56:52,497] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 12.77 GB         CA 13.01 GB         Max_CA 13 GB 
x3104c0s1b0n0: [2024-03-29 15:56:52,497] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.69 GB, percent = 79.0%
x3104c0s1b0n0: [2024-03-29 15:56:57,222] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.70074462890625
x3104c0s1b0n0: [2024-03-29 15:56:57,231] [INFO] [stage3.py:2251:step] Full outer step loop took 4.710104465484619
x3104c0s1b0n0: [2024-03-29 15:56:57,367] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.8462114334106445
x3104c0s1b0n0: [2024-03-29 15:56:57,373] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.852463006973267
x3104c0s1b0n0: [2024-03-29 15:56:57,382] [INFO] [stage3.py:2251:step] Full outer step loop took 4.860740423202515
x3104c0s1b0n0: [2024-03-29 15:56:57,398] [INFO] [stage3.py:2251:step] Full outer step loop took 4.877699613571167
x3104c0s1b0n0: [2024-03-29 15:56:57,401] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.879893779754639
x3104c0s1b0n0: [2024-03-29 15:56:57,411] [INFO] [stage3.py:2251:step] Full outer step loop took 4.889636278152466
x3104c0s25b0n0: [2024-03-29 15:56:57,573] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.052732467651367
x3104c0s25b0n0: [2024-03-29 15:56:57,584] [INFO] [stage3.py:2251:step] Full outer step loop took 5.063429594039917
x3104c0s25b0n0: [2024-03-29 15:56:57,600] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.079504728317261
x3104c0s25b0n0: [2024-03-29 15:56:57,603] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.082637786865234
x3104c0s25b0n0: [2024-03-29 15:56:57,609] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.088470697402954
x3104c0s25b0n0: [2024-03-29 15:56:57,610] [INFO] [stage3.py:2251:step] Full outer step loop took 5.0894598960876465
x3104c0s25b0n0: [2024-03-29 15:56:57,612] [INFO] [stage3.py:2251:step] Full outer step loop took 5.09169864654541
x3104c0s25b0n0: [2024-03-29 15:56:57,618] [INFO] [stage3.py:2251:step] Full outer step loop took 5.097575426101685
x3104c0s25b0n0: [2024-03-29 15:56:57,627] [INFO] [stage3.py:2277:step] End to end step took 5.106526613235474
x3104c0s1b0n0: [2024-03-29 15:56:57,627] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4889.80
x3104c0s25b0n0: [2024-03-29 15:56:57,627] [INFO] [stage3.py:2277:step] End to end step took 5.106654167175293
x3104c0s25b0n0: [2024-03-29 15:56:57,627] [INFO] [stage3.py:2277:step] End to end step took 5.1066505908966064
x3104c0s25b0n0: [2024-03-29 15:56:57,627] [INFO] [stage3.py:2277:step] End to end step took 5.1066975593566895
x3104c0s1b0n0: [2024-03-29 15:56:57,628] [INFO] [stage3.py:2277:step] End to end step took 5.106545686721802
x3104c0s1b0n0: [2024-03-29 15:56:57,628] [INFO] [stage3.py:2277:step] End to end step took 5.106720447540283
x3104c0s1b0n0: [2024-03-29 15:56:57,628] [INFO] [stage3.py:2277:step] End to end step took 5.106870412826538
x3104c0s1b0n0: [2024-03-29 15:56:57,628] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[3e-05], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 15:56:57,628] [INFO] [stage3.py:2277:step] End to end step took 5.107072353363037
x3104c0s1b0n0: [2024-03-29 15:56:57,628] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=1.1579526972112042, CurrSamplesPerSec=1.1636317834955587, MemAllocated=10.26GB, MaxMemAllocated=12.77GB
x3104c0s1b0n0: [2024-03-29 15:56:57,628] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6394.65 | bwd_microstep: 15627.90 | bwd_inner_microstep: 15544.72 | bwd_allreduce_microstep: 83.11 | step_microstep: 5130.98
x3104c0s1b0n0: [2024-03-29 15:56:57,629] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6394.64 | bwd: 15627.89 | bwd_inner: 15544.71 | bwd_allreduce: 83.12 | step: 5130.98
x3104c0s1b0n0: [2024-03-29 15:56:57,730] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:56:57,731] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.77 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:56:57,731] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 397.7 GB, percent = 79.0%
x3104c0s1b0n0: <TIMER:interval-time,27.747876405715942><TIMER:interval-time,27.74787664413452><TIMER:interval-time,27.747883796691895>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,27.747909545898438>
x3104c0s25b0n0: <TIMER:interval-time,27.747924089431763><TIMER:interval-time,27.74792456626892><TIMER:interval-time,27.747908353805542><TIMER:interval-time,27.747926950454712>
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0:  elapsed_time 27.747925 | consumed samples:          320 | consumed tokens:       655360 | elapsed time per iteration (ms): 27747.9 | learning rate: 3.000E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.153 | TFLOPs: 79.79 |
x3104c0s25b0n0: <<<only_train:290.32193660736084>>>
x3104c0s25b0n0: <<<only_train:290.3220708370209>>>
x3104c0s25b0n0: <<<only_train:290.3219563961029>>>
x3104c0s25b0n0: <<<only_train:290.32191252708435>>>
x3104c0s1b0n0: <<<only_train:290.32219076156616>>>
x3104c0s1b0n0: <<<only_train:290.322238445282>>>
x3104c0s1b0n0: <<<only_train:290.32219886779785>>>
x3104c0s1b0n0: <<<only_train:290.3222441673279>>>
x3104c0s1b0n0: [after training ends] datetime: 2024-03-29 15:56:57 
x3104c0s1b0n0: <<<full_time:290.32242369651794>>><<<full_time:290.32241559028625>>>
x3104c0s1b0n0: 
x3104c0s1b0n0: <<<full_time:290.32248544692993>>>
x3104c0s1b0n0: <<<full_time:290.32246565818787>>>
x3104c0s25b0n0: <<<full_time:290.3751494884491>>><<<full_time:290.37503600120544>>>
x3104c0s25b0n0: 
x3104c0s25b0n0: <<<full_time:290.37506198883057>>>
x3104c0s25b0n0: <<<full_time:290.3749885559082>>>
x3104c0s1b0n0: [2024-03-29 15:57:06,073] [INFO] [launch.py:348:main] Process 56902 exits successfully.
x3104c0s25b0n0: [2024-03-29 15:57:08,848] [INFO] [launch.py:348:main] Process 19245 exits successfully.
x3104c0s1b0n0: [2024-03-29 15:57:14,082] [INFO] [launch.py:348:main] Process 56904 exits successfully.
x3104c0s1b0n0: [2024-03-29 15:57:14,082] [INFO] [launch.py:348:main] Process 56905 exits successfully.
x3104c0s1b0n0: [2024-03-29 15:57:14,082] [INFO] [launch.py:348:main] Process 56903 exits successfully.
x3104c0s25b0n0: [2024-03-29 15:57:16,855] [INFO] [launch.py:348:main] Process 19246 exits successfully.
x3104c0s25b0n0: [2024-03-29 15:57:16,855] [INFO] [launch.py:348:main] Process 19244 exits successfully.
x3104c0s25b0n0: [2024-03-29 15:57:16,855] [INFO] [launch.py:348:main] Process 19243 exits successfully.

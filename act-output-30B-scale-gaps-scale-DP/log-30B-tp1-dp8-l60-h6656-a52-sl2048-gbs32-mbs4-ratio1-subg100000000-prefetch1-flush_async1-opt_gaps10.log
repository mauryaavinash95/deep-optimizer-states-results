[2024-03-29 16:10:40,773] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 16:10:43,880] [INFO] [runner.py:463:main] Using IP address of 10.140.58.110 for node x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-29 16:10:43,883] [INFO] [runner.py:557:main] deepspeed_env file = ./.deepspeed_env
[2024-03-29 16:10:43,883] [INFO] [multinode_runner.py:80:get_cmd] Running on the following workers: x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov,x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-29 16:10:43,883] [INFO] [runner.py:568:main] cmd = pdsh -S -f 1024 -w x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov,x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov export PYTHONUSERBASE=/home/am6429/.local/polaris/conda/2023-10-04; export PYTHONPATH=/home/am6429/dl-io/Megatron-DeepSpeed; export PATH=/home/am6429/.conda/envs/dspeed_env/bin:/soft/datascience/conda/2023-10-04/mconda3/condabin:/soft/compilers/cudatoolkit/cuda-11.8.0/bin:/soft/buildtools/cmake/cmake-3.23.2/cmake-3.23.2-linux-x86_64/bin:/opt/cray/pe/gcc/11.2.0/bin:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/include:/opt/cray/pe/pals/1.2.11/bin:/opt/cray/pe/craype/2.7.20/bin:/opt/cray/libfabric/1.15.2.0/bin:/home/am6429/.conda/envs/dspeed_env/bin:/opt/cray/pe/perftools/23.03.0/bin:/opt/cray/pe/papi/7.0.0.1/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/am6429/.local/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/am6429/veloc-build/include:/home/am6429/veloc-build/bin:/soft/datascience/conda/2023-01-10/mconda3/include:/opt/cray/pe/bin:/soft/datascience/conda/2023-01-10/mconda3/include; export LD_LIBRARY_PATH=/usr/lib64/:/soft/compilers/cudatoolkit/cuda-11.8.0/lib64:/opt/cray/pe/gcc/11.2.0/snos/lib64:/soft/compilers/cudatoolkit/cuda-11.8.0/extras/CUPTI/lib64:/soft/libraries/trt/TensorRT-8.5.3.1.Linux.x86_64-gnu.cuda-11.8.cudnn8.6/lib:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/lib:/soft/libraries/cudnn/cudnn-11-linux-x64-v8.6.0.163/lib:/opt/cray/libfabric/1.15.2.0/lib64:/usr/lib64:/opt/cray/pe/papi/7.0.0.1/lib64:/home/am6429/veloc-build/lib:/home/am6429/veloc-build/lib64:/home/am6429/nvcomp/lib:/soft/datascience/conda/2023-01-10/mconda3/lib:/soft/datascience/conda/2023-01-10/mconda3/lib/; export http_proxy=http://proxy.alcf.anl.gov:3128; export https_proxy=http://proxy.alcf.anl.gov:3128; export CC=gcc; export CXX=g++; export IBV_FORK_SAFE=1; export CFLAGS=-I/soft/datascience/conda/2023-01-10/mconda3/include/; export LDFLAGS=-L/soft/datascience/conda/2023-01-10/mconda3/lib/; export CUDA_DEVICE_MAX_CONNECTIONS=1; export TORCHSNAPSHOT_PER_RANK_MEMORY_BUDGET_BYTES=34359738368; export _DEFAULT_MAX_PER_RANK_IO_CONCURRENCY=1; export _MAX_PER_RANK_IO_CONCURRENCY=1; export NSYS_REPORT_DIR=/home/am6429/dl-io/dl-io-outputs/act-output-30B//rep-30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch1-flush_async1-opt_gaps10-%n;  cd /home/am6429/dl-io/Megatron-DeepSpeed; /home/am6429/.conda/envs/dspeed_env/bin/python -u -m deepspeed.launcher.launch --world_info=eyJ4MzEwNGMwczFiMG4wLmhzbi5jbS5wb2xhcmlzLmFsY2YuYW5sLmdvdiI6IFswLCAxLCAyLCAzXSwgIngzMTA0YzBzMjViMG4wLmhzbi5jbS5wb2xhcmlzLmFsY2YuYW5sLmdvdiI6IFswLCAxLCAyLCAzXX0= --node_rank=%n --master_addr=10.140.58.110 --master_port=29700 /home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py --tensor-model-parallel-size 1 --num-layers 60 --hidden-size 6656 --num-attention-heads 52 --micro-batch-size 4 --global-batch-size 32 --seq-length 2048 --max-position-embeddings 2048 --train-iters 10 --save /local/scratch/llama2/zero3-tp1}_dp8 --data-path /home/am6429/dl-io/datasets/meg-gpt2_text_document --vocab-file /home/am6429/dl-io/datasets/gpt2-vocab.json --merge-file /home/am6429/dl-io/datasets/gpt2-merges.txt --data-impl mmap --tokenizer-type GPTSentencePieceTokenizer --tokenizer-model /home/am6429/dl-io/datasets/tokenizer.model --split 949,50,1 --distributed-backend nccl --lr 3e-4 --lr-decay-style cosine --min-lr 3e-5 --weight-decay 0.1 --clip-grad 1 --lr-warmup-iters 1 --optimizer adam --adam-beta1 0.9 --adam-beta2 0.95 --log-interval 1 --save-interval 1000 --eval-interval 1000 --eval-iters 0 --bf16 --no-query-key-layer-scaling --attention-dropout 0 --hidden-dropout 0 --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization rmsnorm --disable-bias-linear --num-key-value-heads 4 --deepspeed --exit-interval 20 --deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json --zero-stage=3 --no-pipeline-parallel --cpu-optimizer --checkpoint-activations --deepspeed-activation-checkpointing
x3104c0s25b0n0: [2024-03-29 16:10:45,686] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 16:10:45,905] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s25b0n0: [2024-03-29 16:10:47,545] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3104c0s25b0n0: [2024-03-29 16:10:47,545] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=1
x3104c0s25b0n0: [2024-03-29 16:10:47,545] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3104c0s25b0n0: [2024-03-29 16:10:47,545] [INFO] [launch.py:163:main] dist_world_size=8
x3104c0s25b0n0: [2024-03-29 16:10:47,545] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3104c0s25b0n0: [2024-03-29 16:10:47,546] [INFO] [launch.py:253:main] process 27867 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s25b0n0: [2024-03-29 16:10:47,546] [INFO] [launch.py:253:main] process 27868 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s25b0n0: [2024-03-29 16:10:47,547] [INFO] [launch.py:253:main] process 27869 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s25b0n0: [2024-03-29 16:10:47,547] [INFO] [launch.py:253:main] process 27870 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s1b0n0: [2024-03-29 16:10:47,782] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3104c0s1b0n0: [2024-03-29 16:10:47,783] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=0
x3104c0s1b0n0: [2024-03-29 16:10:47,783] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3104c0s1b0n0: [2024-03-29 16:10:47,783] [INFO] [launch.py:163:main] dist_world_size=8
x3104c0s1b0n0: [2024-03-29 16:10:47,783] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3104c0s1b0n0: [2024-03-29 16:10:47,783] [INFO] [launch.py:253:main] process 2083 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s1b0n0: [2024-03-29 16:10:47,784] [INFO] [launch.py:253:main] process 2084 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s1b0n0: [2024-03-29 16:10:47,785] [INFO] [launch.py:253:main] process 2085 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s1b0n0: [2024-03-29 16:10:47,785] [INFO] [launch.py:253:main] process 2086 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s25b0n0: [2024-03-29 16:10:49,038] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s25b0n0: [2024-03-29 16:10:49,073] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s25b0n0: [2024-03-29 16:10:49,076] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s25b0n0: [2024-03-29 16:10:49,086] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 16:10:49,602] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 16:10:49,615] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 16:10:49,618] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 16:10:49,629] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s25b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s25b0n0:       meet the required dependencies to JIT install the op.
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: JIT compiled ops requires ninja
x3104c0s25b0n0: ninja .................. [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: op name ................ installed .. compatible
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s25b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s25b0n0:       meet the required dependencies to JIT install the op.
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: JIT compiled ops requires ninja
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed C++/CUDA extension op report--------------------------------------------------
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: 
x3104c0s25b0n0: DeepSpeed C++/CUDA extension op reportNOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s25b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s25b0n0:       meet the required dependencies to JIT install the op.
x3104c0s25b0n0: 
x3104c0s25b0n0: ----------------------------------------------------------------------------------------------------
x3104c0s25b0n0: 
x3104c0s25b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s25b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s25b0n0:       meet the required dependencies to JIT install the op.JIT compiled ops requires ninja
x3104c0s25b0n0: 
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: JIT compiled ops requires ninja
x3104c0s25b0n0: ninja .................. [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: op name ................ installed .. compatible
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s25b0n0: ninja .................. [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: op name ................ installed .. compatible
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: ninja .................. async_copier[92m[OKAY][0m 
x3104c0s25b0n0: ........... [92m[YES][0m-------------------------------------------------- 
x3104c0s25b0n0: ......op name  [92m[OKAY][0m................
x3104c0s25b0n0:  installed .. compatible
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s25b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s25b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s25b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s25b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s25b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s25b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s25b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed general environment info:
x3104c0s25b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s25b0n0: torch version .................... 2.0.1+cu118
x3104c0s25b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s25b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s25b0n0: torch cuda version ............... 11.8
x3104c0s25b0n0: torch hip version ................ None
x3104c0s25b0n0: nvcc version ..................... 11.8
x3104c0s25b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s25b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s25b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s25b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s25b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s25b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s25b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s25b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: DeepSpeed general environment info:
x3104c0s25b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s25b0n0: torch version .................... 2.0.1+cu118
x3104c0s25b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s25b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s25b0n0: torch cuda version ............... 11.8
x3104c0s25b0n0: torch hip version ................ None
x3104c0s25b0n0: nvcc version ..................... 11.8
x3104c0s25b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s25b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s25b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s25b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s25b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed general environment info:
x3104c0s25b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s25b0n0: torch version .................... 2.0.1+cu118
x3104c0s25b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s25b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s25b0n0: torch cuda version ............... 11.8
x3104c0s25b0n0: torch hip version ................ None
x3104c0s25b0n0: nvcc version ..................... 11.8
x3104c0s25b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s25b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s25b0n0: DeepSpeed general environment info:
x3104c0s25b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s25b0n0: torch version .................... 2.0.1+cu118
x3104c0s25b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s25b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s25b0n0: torch cuda version ............... 11.8
x3104c0s25b0n0: torch hip version ................ None
x3104c0s25b0n0: nvcc version ..................... 11.8
x3104c0s25b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s25b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s25b0n0: [2024-03-29 16:10:51,296] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s25b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s25b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s25b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s25b0n0: [2024-03-29 16:10:51,345] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s25b0n0: [2024-03-29 16:10:51,347] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s25b0n0: [2024-03-29 16:10:51,349] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s1b0n0:       meet the required dependencies to JIT install the op.
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: JIT compiled ops requires ninja
x3104c0s1b0n0: ninja .................. [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: op name ................ installed .. compatible
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s1b0n0:       meet the required dependencies to JIT install the op.
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: JIT compiled ops requires ninja
x3104c0s1b0n0: ninja .................. [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: op name ................ installed .. compatible
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s1b0n0:       meet the required dependencies to JIT install the op.
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: JIT compiled ops requires ninja
x3104c0s1b0n0: ninja .................. [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: op name ................ installed .. compatible
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s1b0n0:       meet the required dependencies to JIT install the op.
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: JIT compiled ops requires ninja
x3104c0s1b0n0: ninja .................. [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: op name ................ installed .. compatible
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed general environment info:
x3104c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s1b0n0: torch version .................... 2.0.1+cu118
x3104c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s1b0n0: torch cuda version ............... 11.8
x3104c0s1b0n0: torch hip version ................ None
x3104c0s1b0n0: nvcc version ..................... 11.8
x3104c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: DeepSpeed general environment info:
x3104c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s1b0n0: torch version .................... 2.0.1+cu118
x3104c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s1b0n0: torch cuda version ............... 11.8
x3104c0s1b0n0: torch hip version ................ None
x3104c0s1b0n0: nvcc version ..................... 11.8
x3104c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: DeepSpeed general environment info:
x3104c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s1b0n0: torch version .................... 2.0.1+cu118
x3104c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s1b0n0: torch cuda version ............... 11.8
x3104c0s1b0n0: torch hip version ................ None
x3104c0s1b0n0: nvcc version ..................... 11.8
x3104c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed general environment info:
x3104c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s1b0n0: torch version .................... 2.0.1+cu118
x3104c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s1b0n0: torch cuda version ............... 11.8
x3104c0s1b0n0: torch hip version ................ None
x3104c0s1b0n0: nvcc version ..................... 11.8
x3104c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s1b0n0: [2024-03-29 16:10:52,622] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s1b0n0: INFO: overriding default arguments for tokenizer_type:GPTSentencePieceTokenizer                    with tokenizer_type:GPT2BPETokenizer
x3104c0s1b0n0: using world size: 8, data-parallel-size: 8, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
x3104c0s1b0n0: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
x3104c0s1b0n0: using torch.bfloat16 for parameters ...
x3104c0s1b0n0: ------------------------ arguments ------------------------
x3104c0s1b0n0:   accumulate_allreduce_grads_in_fp32 .............. True
x3104c0s1b0n0:   adam_beta1 ...................................... 0.9
x3104c0s1b0n0:   adam_beta2 ...................................... 0.95
x3104c0s1b0n0:   adam_eps ........................................ 1e-08
x3104c0s1b0n0:   add_bias_linear ................................. False
x3104c0s1b0n0:   add_position_embedding .......................... False
x3104c0s1b0n0:   adlr_autoresume ................................. False
x3104c0s1b0n0:   adlr_autoresume_interval ........................ 1000
x3104c0s1b0n0:   aml_data_download_path .......................... None
x3104c0s1b0n0:   apply_layernorm_1p .............................. False
x3104c0s1b0n0:   apply_query_key_layer_scaling ................... False
x3104c0s1b0n0:   apply_residual_connection_post_layernorm ........ False
x3104c0s1b0n0:   async_tensor_model_parallel_allreduce ........... False
x3104c0s1b0n0:   attention_dropout ............................... 0.0
x3104c0s1b0n0:   attention_softmax_in_fp32 ....................... False
x3104c0s1b0n0:   barrier_with_L1_time ............................ True
x3104c0s1b0n0:   bert_binary_head ................................ True
x3104c0s1b0n0:   bert_embedder_type .............................. megatron
x3104c0s1b0n0:   bert_load ....................................... None
x3104c0s1b0n0:   bf16 ............................................ True
x3104c0s1b0n0:   bias_dropout_fusion ............................. True
x3104c0s1b0n0:   bias_gelu_fusion ................................ False
x3104c0s1b0n0:   biencoder_projection_dim ........................ 0
x3104c0s1b0n0:   biencoder_shared_query_context_model ............ False
x3104c0s1b0n0:   block_data_path ................................. None
x3104c0s1b0n0:   checkpoint_activations .......................... True
x3104c0s1b0n0:   checkpoint_in_cpu ............................... False
x3104c0s1b0n0:   checkpoint_num_layers ........................... 1
x3104c0s1b0n0:   classes_fraction ................................ 1.0
x3104c0s1b0n0:   clip_grad ....................................... 1.0
x3104c0s1b0n0:   compression_training ............................ False
x3104c0s1b0n0:   consumed_train_samples .......................... 0
x3104c0s1b0n0:   consumed_train_tokens ........................... 0
x3104c0s1b0n0:   consumed_valid_samples .......................... 0
x3104c0s1b0n0:   contigious_checkpointing ........................ False
x3104c0s1b0n0:   cpu_optimizer ................................... True
x3104c0s1b0n0:   cpu_torch_adam .................................. False
x3104c0s1b0n0:   create_moe_param_group .......................... False
x3104c0s1b0n0:   curriculum_learning_legacy ...................... False
x3104c0s1b0n0:   data_cache_path ................................. None
x3104c0s1b0n0:   data_efficiency_curriculum_learning ............. False
x3104c0s1b0n0:   data_impl ....................................... mmap
x3104c0s1b0n0:   data_parallel_random_init ....................... False
x3104c0s1b0n0:   data_parallel_size .............................. 8
x3104c0s1b0n0:   data_path ....................................... ['/home/am6429/dl-io/datasets/meg-gpt2_text_document']
x3104c0s1b0n0:   data_per_class_fraction ......................... 1.0
x3104c0s1b0n0:   data_sharding ................................... True
x3104c0s1b0n0:   dataloader_type ................................. single
x3104c0s1b0n0:   DDP_impl ........................................ local
x3104c0s1b0n0:   decoder_num_layers .............................. None
x3104c0s1b0n0:   decoder_seq_length .............................. None
x3104c0s1b0n0:   deepscale ....................................... False
x3104c0s1b0n0:   deepscale_config ................................ None
x3104c0s1b0n0:   deepspeed ....................................... True
x3104c0s1b0n0:   deepspeed_activation_checkpointing .............. True
x3104c0s1b0n0:   deepspeed_config ................................ /home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json
x3104c0s1b0n0:   dino_bottleneck_size ............................ 256
x3104c0s1b0n0:   dino_freeze_last_layer .......................... 1
x3104c0s1b0n0:   dino_head_hidden_size ........................... 2048
x3104c0s1b0n0:   dino_local_crops_number ......................... 10
x3104c0s1b0n0:   dino_local_img_size ............................. 96
x3104c0s1b0n0:   dino_norm_last_layer ............................ False
x3104c0s1b0n0:   dino_teacher_temp ............................... 0.07
x3104c0s1b0n0:   dino_warmup_teacher_temp ........................ 0.04
x3104c0s1b0n0:   dino_warmup_teacher_temp_epochs ................. 30
x3104c0s1b0n0:   distribute_checkpointed_activations ............. False
x3104c0s1b0n0:   distribute_saved_activations .................... False
x3104c0s1b0n0:   distributed_backend ............................. nccl
x3104c0s1b0n0:   distributed_timeout_minutes ..................... 10
x3104c0s1b0n0:   ds_inference .................................... False
x3104c0s1b0n0:   ds_pipeline_enabled ............................. False
x3104c0s1b0n0:   ds_sequence_parallel_size ....................... 1
x3104c0s1b0n0:   embedding_path .................................. None
x3104c0s1b0n0:   embedding_weights_in_fp32 ....................... False
x3104c0s1b0n0:   empty_unused_memory_level ....................... 0
x3104c0s1b0n0:   enable_expert_tensor_parallelism ................ False
x3104c0s1b0n0:   encoder_num_layers .............................. 60
x3104c0s1b0n0:   encoder_seq_length .............................. 2048
x3104c0s1b0n0:   end_weight_decay ................................ 0.1
x3104c0s1b0n0:   eod_mask_loss ................................... False
x3104c0s1b0n0:   eval_interval ................................... 1000
x3104c0s1b0n0:   eval_iters ...................................... 0
x3104c0s1b0n0:   evidence_data_path .............................. None
x3104c0s1b0n0:   exit_duration_in_mins ........................... None
x3104c0s1b0n0:   exit_interval ................................... 20
x3104c0s1b0n0:   exit_on_missing_checkpoint ...................... False
x3104c0s1b0n0:   exit_signal_handler ............................. False
x3104c0s1b0n0:   expert_interval ................................. 2
x3104c0s1b0n0:   ffn_hidden_size ................................. 17728
x3104c0s1b0n0:   finetune ........................................ False
x3104c0s1b0n0:   force_ds_sequence_parallel ...................... False
x3104c0s1b0n0:   fp16 ............................................ False
x3104c0s1b0n0:   fp16_lm_cross_entropy ........................... False
x3104c0s1b0n0:   fp32_residual_connection ........................ False
x3104c0s1b0n0:   fp8_amax_compute_algo ........................... most_recent
x3104c0s1b0n0:   fp8_amax_history_len ............................ 1
x3104c0s1b0n0:   fp8_e4m3 ........................................ False
x3104c0s1b0n0:   fp8_hybrid ...................................... False
x3104c0s1b0n0:   fp8_interval .................................... 1
x3104c0s1b0n0:   fp8_margin ...................................... 0
x3104c0s1b0n0:   fp8_wgrad ....................................... True
x3104c0s1b0n0:   global_batch_size ............................... 32
x3104c0s1b0n0:   gradient_accumulation_fusion .................... True
x3104c0s1b0n0:   head_lr_mult .................................... 1.0
x3104c0s1b0n0:   hidden_dropout .................................. 0.0
x3104c0s1b0n0:   hidden_size ..................................... 6656
x3104c0s1b0n0:   hidden_size_teacher ............................. None
x3104c0s1b0n0:   hysteresis ...................................... 2
x3104c0s1b0n0:   ict_head_size ................................... None
x3104c0s1b0n0:   ict_load ........................................ None
x3104c0s1b0n0:   img_h ........................................... 224
x3104c0s1b0n0:   img_w ........................................... 224
x3104c0s1b0n0:   indexer_batch_size .............................. 128
x3104c0s1b0n0:   indexer_log_interval ............................ 1000
x3104c0s1b0n0:   inference ....................................... False
x3104c0s1b0n0:   inference_batch_times_seqlen_threshold .......... 512
x3104c0s1b0n0:   init_method_std ................................. 0.02
x3104c0s1b0n0:   init_method_xavier_uniform ...................... False
x3104c0s1b0n0:   initial_loss_scale .............................. 4294967296
x3104c0s1b0n0:   iter_per_epoch .................................. 1250
x3104c0s1b0n0:   kd .............................................. False
x3104c0s1b0n0:   kd_alpha_ce ..................................... 1
x3104c0s1b0n0:   kd_beta_ce ...................................... 1
x3104c0s1b0n0:   kd_temp ......................................... 1.0
x3104c0s1b0n0:   kv_channels ..................................... 128
x3104c0s1b0n0:   layernorm_epsilon ............................... 1e-05
x3104c0s1b0n0:   lazy_mpu_init ................................... None
x3104c0s1b0n0:   load ............................................ None
x3104c0s1b0n0:   load_teacher .................................... None
x3104c0s1b0n0:   local_rank ...................................... 0
x3104c0s1b0n0:   log_batch_size_to_tensorboard ................... False
x3104c0s1b0n0:   log_interval .................................... 1
x3104c0s1b0n0:   log_learning_rate_to_tensorboard ................ True
x3104c0s1b0n0:   log_loss_scale_to_tensorboard ................... True
x3104c0s1b0n0:   log_memory_to_tensorboard ....................... False
x3104c0s1b0n0:   log_num_zeros_in_grad ........................... False
x3104c0s1b0n0:   log_optimizer_states_to_tensorboard ............. False
x3104c0s1b0n0:   log_params_norm ................................. False
x3104c0s1b0n0:   log_timers_to_tensorboard ....................... False
x3104c0s1b0n0:   log_validation_ppl_to_tensorboard ............... False
x3104c0s1b0n0:   log_world_size_to_tensorboard ................... False
x3104c0s1b0n0:   loss_scale ...................................... None
x3104c0s1b0n0:   loss_scale_window ............................... 1000
x3104c0s1b0n0:   lr .............................................. 0.0003
x3104c0s1b0n0:   lr_decay_iters .................................. None
x3104c0s1b0n0:   lr_decay_samples ................................ None
x3104c0s1b0n0:   lr_decay_style .................................. cosine
x3104c0s1b0n0:   lr_decay_tokens ................................. None
x3104c0s1b0n0:   lr_warmup_fraction .............................. None
x3104c0s1b0n0:   lr_warmup_iters ................................. 1
x3104c0s1b0n0:   lr_warmup_samples ............................... 0
x3104c0s1b0n0:   lr_warmup_tokens ................................ None
x3104c0s1b0n0:   make_vocab_size_divisible_by .................... 128
x3104c0s1b0n0:   mask_factor ..................................... 1.0
x3104c0s1b0n0:   mask_prob ....................................... 0.15
x3104c0s1b0n0:   mask_type ....................................... random
x3104c0s1b0n0:   masked_softmax_fusion ........................... True
x3104c0s1b0n0:   max_position_embeddings ......................... 2048
x3104c0s1b0n0:   max_tokens_to_oom ............................... 12000
x3104c0s1b0n0:   memory_centric_tiled_linear ..................... False
x3104c0s1b0n0:   merge_file ...................................... /home/am6429/dl-io/datasets/gpt2-merges.txt
x3104c0s1b0n0:   micro_batch_size ................................ 4
x3104c0s1b0n0:   min_loss_scale .................................. 1.0
x3104c0s1b0n0:   min_lr .......................................... 3e-05
x3104c0s1b0n0:   mlp_type ........................................ standard
x3104c0s1b0n0:   mmap_warmup ..................................... False
x3104c0s1b0n0:   moe_eval_capacity_factor ........................ 1.0
x3104c0s1b0n0:   moe_expert_parallel_size ........................ 1
x3104c0s1b0n0:   moe_loss_coeff .................................. 0.1
x3104c0s1b0n0:   moe_min_capacity ................................ 4
x3104c0s1b0n0:   moe_token_dropping .............................. True
x3104c0s1b0n0:   moe_train_capacity_factor ....................... 1.0
x3104c0s1b0n0:   mos ............................................. False
x3104c0s1b0n0:   no_load_lr_state ................................ False
x3104c0s1b0n0:   no_load_optim ................................... None
x3104c0s1b0n0:   no_load_rng ..................................... None
x3104c0s1b0n0:   no_persist_layer_norm ........................... False
x3104c0s1b0n0:   no_pipeline_parallel ............................ True
x3104c0s1b0n0:   no_save_optim ................................... None
x3104c0s1b0n0:   no_save_rng ..................................... None
x3104c0s1b0n0:   normalization ................................... rmsnorm
x3104c0s1b0n0:   num_attention_heads ............................. 52
x3104c0s1b0n0:   num_attention_heads_teacher ..................... None
x3104c0s1b0n0:   num_channels .................................... 3
x3104c0s1b0n0:   num_classes ..................................... 1000
x3104c0s1b0n0:   num_experts ..................................... [1]
x3104c0s1b0n0:   num_experts_switch .............................. None
x3104c0s1b0n0:   num_experts_teacher ............................. [1]
x3104c0s1b0n0:   num_key_value_heads ............................. 4
x3104c0s1b0n0:   num_layers ...................................... 60
x3104c0s1b0n0:   num_layers_per_virtual_pipeline_stage ........... None
x3104c0s1b0n0:   num_layers_teacher .............................. None
x3104c0s1b0n0:   num_workers ..................................... 2
x3104c0s1b0n0:   onnx_safe ....................................... None
x3104c0s1b0n0:   openai_gelu ..................................... False
x3104c0s1b0n0:   optimizer ....................................... adam
x3104c0s1b0n0:   output_bert_embeddings .......................... False
x3104c0s1b0n0:   overlap_p2p_comm ................................ False
x3104c0s1b0n0:   override_opt_param_scheduler .................... False
x3104c0s1b0n0:   params_dtype .................................... torch.bfloat16
x3104c0s1b0n0:   partition_activations ........................... False
x3104c0s1b0n0:   patch_dim ....................................... 16
x3104c0s1b0n0:   perform_initialization .......................... True
x3104c0s1b0n0:   pipeline_model_parallel_size .................... 1
x3104c0s1b0n0:   pipeline_model_parallel_split_rank .............. None
x3104c0s1b0n0:   profile_backward ................................ False
x3104c0s1b0n0:   query_in_block_prob ............................. 0.1
x3104c0s1b0n0:   rampup_batch_size ............................... None
x3104c0s1b0n0:   random_ltd ...................................... False
x3104c0s1b0n0:   rank ............................................ 0
x3104c0s1b0n0:   recompute_granularity ........................... None
x3104c0s1b0n0:   recompute_method ................................ None
x3104c0s1b0n0:   recompute_num_layers ............................ 1
x3104c0s1b0n0:   remote_device ................................... none
x3104c0s1b0n0:   reset_attention_mask ............................ False
x3104c0s1b0n0:   reset_iteration ................................. False
x3104c0s1b0n0:   reset_position_ids .............................. False
x3104c0s1b0n0:   retriever_report_topk_accuracies ................ []
x3104c0s1b0n0:   retriever_score_scaling ......................... False
x3104c0s1b0n0:   retriever_seq_length ............................ 256
x3104c0s1b0n0:   retro_add_retriever ............................. False
x3104c0s1b0n0:   retro_cyclic_train_iters ........................ None
x3104c0s1b0n0:   retro_encoder_attention_dropout ................. 0.1
x3104c0s1b0n0:   retro_encoder_hidden_dropout .................... 0.1
x3104c0s1b0n0:   retro_encoder_layers ............................ 2
x3104c0s1b0n0:   retro_num_neighbors ............................. 2
x3104c0s1b0n0:   retro_num_retrieved_chunks ...................... 2
x3104c0s1b0n0:   retro_return_doc_ids ............................ False
x3104c0s1b0n0:   retro_workdir ................................... None
x3104c0s1b0n0:   return_data_index ............................... False
x3104c0s1b0n0:   rotary_percent .................................. 1.0
x3104c0s1b0n0:   sample_rate ..................................... 1.0
x3104c0s1b0n0:   save ............................................ /local/scratch/llama2/zero3-tp1}_dp8
x3104c0s1b0n0:   save_interval ................................... 1000
x3104c0s1b0n0:   scatter_gather_tensors_in_pipeline .............. True
x3104c0s1b0n0:   scattered_embeddings ............................ False
x3104c0s1b0n0:   seed ............................................ 1234
x3104c0s1b0n0:   seq_length ...................................... 2048
x3104c0s1b0n0:   sequence_parallel ............................... False
x3104c0s1b0n0:   sgd_momentum .................................... 0.9
x3104c0s1b0n0:   short_seq_prob .................................. 0.1
x3104c0s1b0n0:   skip_train ...................................... False
x3104c0s1b0n0:   split ........................................... 949,50,1
x3104c0s1b0n0:   split_transformers .............................. False
x3104c0s1b0n0:   squared_relu .................................... False
x3104c0s1b0n0:   standalone_embedding_stage ...................... False
x3104c0s1b0n0:   start_weight_decay .............................. 0.1
x3104c0s1b0n0:   swiglu .......................................... True
x3104c0s1b0n0:   swin_backbone_type .............................. tiny
x3104c0s1b0n0:   synchronize_each_layer .......................... False
x3104c0s1b0n0:   tensor_model_parallel_size ...................... 1
x3104c0s1b0n0:   tensorboard_dir ................................. None
x3104c0s1b0n0:   tensorboard_log_interval ........................ 1
x3104c0s1b0n0:   tensorboard_queue_size .......................... 1000
x3104c0s1b0n0:   test_data_path .................................. None
x3104c0s1b0n0:   tile_factor ..................................... 1
x3104c0s1b0n0:   timing_log_level ................................ 0
x3104c0s1b0n0:   timing_log_option ............................... minmax
x3104c0s1b0n0:   titles_data_path ................................ None
x3104c0s1b0n0:   tokenizer_model ................................. /home/am6429/dl-io/datasets/tokenizer.model
x3104c0s1b0n0:   tokenizer_type .................................. GPT2BPETokenizer
x3104c0s1b0n0:   topk ............................................ 1
x3104c0s1b0n0:   train_data_exact_num_epochs ..................... None
x3104c0s1b0n0:   train_data_path ................................. None
x3104c0s1b0n0:   train_desc_path ................................. None
x3104c0s1b0n0:   train_doc_idx_path .............................. None
x3104c0s1b0n0:   train_idx_path .................................. None
x3104c0s1b0n0:   train_iters ..................................... 10
x3104c0s1b0n0:   train_sample_idx_path ........................... None
x3104c0s1b0n0:   train_samples ................................... None
x3104c0s1b0n0:   train_shuffle_idx_path .......................... None
x3104c0s1b0n0:   train_tokens .................................... None
x3104c0s1b0n0:   transformer_impl ................................ local
x3104c0s1b0n0:   transformer_pipeline_model_parallel_size ........ 1
x3104c0s1b0n0:   untie_embeddings_and_output_weights ............. True
x3104c0s1b0n0:   use_checkpoint_args ............................. False
x3104c0s1b0n0:   use_checkpoint_opt_param_scheduler .............. False
x3104c0s1b0n0:   use_contiguous_buffers_in_local_ddp ............. True
x3104c0s1b0n0:   use_cpu_initialization .......................... None
x3104c0s1b0n0:   use_dataset_only ................................ False
x3104c0s1b0n0:   use_distributed_optimizer ....................... False
x3104c0s1b0n0:   use_flash_attn .................................. False
x3104c0s1b0n0:   use_flash_attn_triton ........................... False
x3104c0s1b0n0:   use_flash_attn_v1 ............................... False
x3104c0s1b0n0:   use_flash_attn_v2 ............................... False
x3104c0s1b0n0:   use_one_sent_docs ............................... False
x3104c0s1b0n0:   use_pin_memory .................................. False
x3104c0s1b0n0:   use_ring_exchange_p2p ........................... False
x3104c0s1b0n0:   use_rotary_position_embeddings .................. True
x3104c0s1b0n0:   use_tutel ....................................... False
x3104c0s1b0n0:   valid_data_path ................................. None
x3104c0s1b0n0:   variable_seq_lengths ............................ False
x3104c0s1b0n0:   virtual_pipeline_model_parallel_size ............ None
x3104c0s1b0n0:   vision_backbone_type ............................ vit
x3104c0s1b0n0:   vision_pretraining .............................. False
x3104c0s1b0n0:   vision_pretraining_type ......................... classify
x3104c0s1b0n0:   vocab_extra_ids ................................. 0
x3104c0s1b0n0:   vocab_file ...................................... /home/am6429/dl-io/datasets/gpt2-vocab.json
x3104c0s1b0n0:   vocab_size ...................................... None
x3104c0s1b0n0:   weight_decay .................................... 0.1
x3104c0s1b0n0:   weight_decay_incr_style ......................... constant
x3104c0s1b0n0:   world_size ...................................... 8
x3104c0s1b0n0:   zero_allgather_bucket_size ...................... 0.0
x3104c0s1b0n0:   zero_contigious_gradients ....................... False
x3104c0s1b0n0:   zero_reduce_bucket_size ......................... 0.0
x3104c0s1b0n0:   zero_reduce_scatter ............................. False
x3104c0s1b0n0:   zero_stage ...................................... 3
x3104c0s1b0n0: -------------------- end of arguments ---------------------
x3104c0s1b0n0: setting number of micro-batches to constant 1
x3104c0s1b0n0: > building GPT2BPETokenizer tokenizer ...
x3104c0s1b0n0: [2024-03-29 16:10:52,651] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0: [2024-03-29 16:10:52,667] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
x3104c0s1b0n0: > initializing torch distributed ...
x3104c0s1b0n0: [2024-03-29 16:10:52,682] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0: [2024-03-29 16:10:52,682] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
x3104c0s1b0n0: > initialized tensor model parallel with size 1
x3104c0s1b0n0: > initialized pipeline model parallel with size 1
x3104c0s1b0n0: > setting random seeds to 1234 ...
x3104c0s1b0n0: [2024-03-29 16:10:54,126] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
x3104c0s1b0n0: > compiling dataset index builder ...
x3104c0s1b0n0: make: Entering directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3104c0s1b0n0: make: Nothing to be done for 'default'.
x3104c0s1b0n0: make: Leaving directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3104c0s1b0n0: >>> done with dataset index builder. Compilation time: 0.084 seconds
x3104c0s1b0n0: > compiling and loading fused kernels ...
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: >>> done with compiling and loading fused kernels. Compilation time: 2.950 seconds
x3104c0s1b0n0: initialize_megatron took 5.298670768737793
x3104c0s1b0n0: <<<<<<<<<<< 0
x3104c0s25b0n0: <<<<<<<<<<< 6
x3104c0s25b0n0: <<<<<<<<<<< 7
x3104c0s25b0n0: <<<<<<<<<<< 5
x3104c0s25b0n0: <<<<<<<<<<< 4
x3104c0s1b0n0: <<<<<<<<<<< 3
x3104c0s1b0n0: <<<<<<<<<<< 2
x3104c0s1b0n0: <<<<<<<<<<< 1
x3104c0s1b0n0: time to initialize megatron (seconds): 6.935
x3104c0s1b0n0: [after megatron is initialized] datetime: 2024-03-29 16:10:57 
x3104c0s1b0n0: get_accelerator and all_reduce  took 0.019902467727661133
x3104c0s1b0n0: building GPT model ...
x3104c0s1b0n0: [2024-03-29 16:10:58,028] [INFO] [utils.py:800:see_memory_usage] Before Building Model
x3104c0s1b0n0: [2024-03-29 16:10:58,028] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 4.62 GB         CA 0.0 GB         Max_CA 5 GB 
x3104c0s1b0n0: [2024-03-29 16:10:58,028] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 17.86 GB, percent = 3.5%
x3104c0s1b0n0: [2024-03-29 16:11:03,767] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 423, num_elems = 27.64B
x3104c0s1b0n0: [2024-03-29 16:11:03,835] [INFO] [utils.py:800:see_memory_usage] After Building Model
x3104c0s1b0n0: [2024-03-29 16:11:03,836] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 7.07 GB         CA 21.65 GB         Max_CA 38 GB 
x3104c0s1b0n0: [2024-03-29 16:11:03,836] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 18.29 GB, percent = 3.6%
x3104c0s1b0n0:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 27635239424
x3104c0s25b0n0: ninja: no work to do.
x3104c0s25b0n0: Time to load cpu_adam op: 2.290257692337036 seconds
x3104c0s25b0n0: Time to load cpu_adam op: 2.314870595932007 seconds
x3104c0s25b0n0: Time to load cpu_adam op: 2.317213773727417 seconds
x3104c0s1b0n0: Time to load cpu_adam op: 2.4654901027679443 seconds
x3104c0s25b0n0: ninja: no work to do.
x3104c0s25b0n0: Time to load cpu_adam op: 2.285916566848755 seconds
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: Time to load cpu_adam op: 2.4400742053985596 seconds
x3104c0s1b0n0: Time to load cpu_adam op: 2.467944860458374 seconds
x3104c0s1b0n0: Time to load cpu_adam op: 2.4375147819519043 seconds
x3104c0s25b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s25b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s25b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s25b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s25b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s25b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: > learning rate decay style: cosine
x3104c0s1b0n0: DeepSpeed is enabled.
x3104c0s1b0n0: [2024-03-29 16:11:08,304] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.3+8074cd62, git-hash=8074cd62, git-branch=hybrid_opt_offload
x3104c0s25b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s25b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: [2024-03-29 16:11:08,368] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After args sanity test
x3104c0s1b0n0: [2024-03-29 16:11:08,369] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.65 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 16:11:08,369] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 24.71 GB, percent = 4.9%
x3104c0s1b0n0: [2024-03-29 16:11:08,421] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure distributed model
x3104c0s1b0n0: [2024-03-29 16:11:08,421] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.65 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 16:11:08,422] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 24.96 GB, percent = 5.0%
x3104c0s1b0n0: [2024-03-29 16:11:08,479] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After configure distributed model
x3104c0s1b0n0: [2024-03-29 16:11:08,479] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.65 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 16:11:08,479] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.25 GB, percent = 5.0%
x3104c0s1b0n0: [2024-03-29 16:11:08,479] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
x3104c0s1b0n0: [2024-03-29 16:11:08,528] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After setting model parameters
x3104c0s1b0n0: [2024-03-29 16:11:08,529] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.65 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 16:11:08,529] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.51 GB, percent = 5.1%
x3104c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: [2024-03-29 16:11:08,579] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure optimizer
x3104c0s1b0n0: [2024-03-29 16:11:08,579] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.65 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 16:11:08,579] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.67 GB, percent = 5.1%
x3104c0s1b0n0: [2024-03-29 16:11:08,580] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
x3104c0s1b0n0: [2024-03-29 16:11:08,580] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
x3104c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: [2024-03-29 16:11:08,598] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
x3104c0s1b0n0: [2024-03-29 16:11:08,598] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
x3104c0s1b0n0: [2024-03-29 16:11:08,598] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
x3104c0s1b0n0: [2024-03-29 16:11:08,598] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
x3104c0s1b0n0: [2024-03-29 16:11:08,646] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
x3104c0s1b0n0: [2024-03-29 16:11:08,647] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.65 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 16:11:08,647] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.72 GB, percent = 5.1%
x3104c0s1b0n0: [2024-03-29 16:11:08,649] [INFO] [stage3.py:137:__init__] Reduce bucket size 500,000,000
x3104c0s1b0n0: [2024-03-29 16:11:08,649] [INFO] [stage3.py:138:__init__] Prefetch bucket size 50,000,000
x3104c0s1b0n0: [2024-03-29 16:11:08,699] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
x3104c0s1b0n0: [2024-03-29 16:11:08,699] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.65 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 16:11:08,699] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.72 GB, percent = 5.1%
x3104c0s1b0n0: Parameter Offload: Total persistent parameters: 805376 in 121 params
x3104c0s1b0n0: [2024-03-29 16:11:08,773] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
x3104c0s1b0n0: [2024-03-29 16:11:08,773] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.65 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 16:11:08,773] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.72 GB, percent = 5.1%
x3104c0s1b0n0: [2024-03-29 16:11:08,825] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
x3104c0s1b0n0: [2024-03-29 16:11:08,825] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.65 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 16:11:08,826] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.72 GB, percent = 5.1%
x3104c0s1b0n0: [2024-03-29 16:11:08,826] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s1b0n0: [2024-03-29 16:11:08,826] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s1b0n0: [2024-03-29 16:11:08,826] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s1b0n0: [2024-03-29 16:11:08,826] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s25b0n0: [2024-03-29 16:11:08,826] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s25b0n0: [2024-03-29 16:11:08,826] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s25b0n0: [2024-03-29 16:11:08,826] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s25b0n0: [2024-03-29 16:11:08,826] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s1b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s1b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s1b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s1b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s25b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s25b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s25b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s25b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s1b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,827] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,828] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,829] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,830] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s25b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s1b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s1b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s1b0n0: [2024-03-29 16:11:08,831] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s1b0n0: [2024-03-29 16:11:08,832] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s25b0n0: [2024-03-29 16:11:08,832] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s25b0n0: [2024-03-29 16:11:08,832] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s25b0n0: [2024-03-29 16:11:08,832] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s25b0n0: [2024-03-29 16:11:08,832] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s1b0n0: [2024-03-29 16:11:11,051] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 31
x3104c0s1b0n0: [2024-03-29 16:11:11,052] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.44 GB         CA 6.44 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 16:11:11,052] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 45.07 GB, percent = 9.0%
x3104c0s1b0n0: [2024-03-29 16:11:11,114] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
x3104c0s1b0n0: [2024-03-29 16:11:11,114] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3104c0s1b0n0: [2024-03-29 16:11:11,114] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 45.07 GB, percent = 9.0%
x3104c0s1b0n0: [2024-03-29 16:11:30,440] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
x3104c0s1b0n0: [2024-03-29 16:11:30,441] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3104c0s1b0n0: [2024-03-29 16:11:30,441] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 214.12 GB, percent = 42.6%
x3104c0s1b0n0: [2024-03-29 16:11:31,381] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
x3104c0s1b0n0: [2024-03-29 16:11:31,381] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3104c0s1b0n0: [2024-03-29 16:11:31,381] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 219.12 GB, percent = 43.5%
x3104c0s1b0n0: [2024-03-29 16:11:37,952] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | init_optimizer_state: 6558.99
x3104c0s1b0n0: [2024-03-29 16:11:38,042] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
x3104c0s1b0n0: [2024-03-29 16:11:38,042] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3104c0s1b0n0: [2024-03-29 16:11:38,042] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 279.84 GB, percent = 55.6%
x3104c0s1b0n0: [2024-03-29 16:11:38,152] [INFO] [stage3.py:520:_setup_for_real_optimizer] optimizer state initialized
x3104c0s1b0n0: [2024-03-29 16:11:46,389] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
x3104c0s1b0n0: [2024-03-29 16:11:46,389] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 8.61 GB         CA 22.75 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 16:11:46,389] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.86 GB, percent = 62.0%
x3104c0s1b0n0: [2024-03-29 16:11:46,390] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
x3104c0s1b0n0: [2024-03-29 16:11:46,455] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure LR scheduler
x3104c0s1b0n0: [2024-03-29 16:11:46,456] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 16:11:46,456] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.86 GB, percent = 62.0%
x3104c0s1b0n0: [2024-03-29 16:11:46,456] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
x3104c0s1b0n0: [2024-03-29 16:11:46,456] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7f0cd09cc220>
x3104c0s1b0n0: [2024-03-29 16:11:46,456] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 16:11:46,519] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before rewriting optimizer step
x3104c0s1b0n0: [2024-03-29 16:11:46,520] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 16:11:46,520] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.87 GB, percent = 62.0%
x3104c0s1b0n0: [2024-03-29 16:11:46,584] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure checkpointing
x3104c0s1b0n0: [2024-03-29 16:11:46,585] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 16:11:46,585] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.86 GB, percent = 62.0%
x3104c0s1b0n0: [2024-03-29 16:11:46,585] [INFO] [config.py:998:print] DeepSpeedEngine configuration:
x3104c0s1b0n0: [2024-03-29 16:11:46,585] [INFO] [config.py:1002:print]   activation_checkpointing_config  {
x3104c0s1b0n0:     "partition_activations": false, 
x3104c0s1b0n0:     "contiguous_memory_optimization": false, 
x3104c0s1b0n0:     "cpu_checkpointing": false, 
x3104c0s1b0n0:     "number_checkpoints": null, 
x3104c0s1b0n0:     "synchronize_checkpoint_boundary": false, 
x3104c0s1b0n0:     "profile": false
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 16:11:46,585] [INFO] [config.py:1002:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
x3104c0s1b0n0: [2024-03-29 16:11:46,585] [INFO] [config.py:1002:print]   amp_enabled .................. False
x3104c0s1b0n0: [2024-03-29 16:11:46,585] [INFO] [config.py:1002:print]   amp_params ................... False
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   autotuning_config ............ {
x3104c0s1b0n0:     "enabled": false, 
x3104c0s1b0n0:     "start_step": null, 
x3104c0s1b0n0:     "end_step": null, 
x3104c0s1b0n0:     "metric_path": null, 
x3104c0s1b0n0:     "arg_mappings": null, 
x3104c0s1b0n0:     "metric": "throughput", 
x3104c0s1b0n0:     "model_info": null, 
x3104c0s1b0n0:     "results_dir": "autotuning_results", 
x3104c0s1b0n0:     "exps_dir": "autotuning_exps", 
x3104c0s1b0n0:     "overwrite": true, 
x3104c0s1b0n0:     "fast": true, 
x3104c0s1b0n0:     "start_profile_step": 3, 
x3104c0s1b0n0:     "end_profile_step": 5, 
x3104c0s1b0n0:     "tuner_type": "gridsearch", 
x3104c0s1b0n0:     "tuner_early_stopping": 5, 
x3104c0s1b0n0:     "tuner_num_trials": 50, 
x3104c0s1b0n0:     "model_info_path": null, 
x3104c0s1b0n0:     "mp_size": 1, 
x3104c0s1b0n0:     "max_train_batch_size": null, 
x3104c0s1b0n0:     "min_train_batch_size": 1, 
x3104c0s1b0n0:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
x3104c0s1b0n0:     "min_train_micro_batch_size_per_gpu": 1, 
x3104c0s1b0n0:     "num_tuning_micro_batch_sizes": 3
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   bfloat16_enabled ............. True
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   bfloat16_immediate_grad_update  False
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   checkpoint_parallel_write_pipeline  False
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   checkpoint_tag_validation_enabled  True
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   checkpoint_tag_validation_fail  False
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f0cd09cc4c0>
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   communication_data_type ...... None
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   curriculum_enabled_legacy .... False
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   curriculum_params_legacy ..... False
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   data_efficiency_enabled ...... False
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   dataloader_drop_last ......... False
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   datastates_config ............ {
x3104c0s1b0n0:     "enabled": null, 
x3104c0s1b0n0:     "config": {
x3104c0s1b0n0:     }
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   disable_allgather ............ False
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   dump_state ................... False
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   dynamic_loss_scale_args ...... None
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   eigenvalue_enabled ........... False
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   eigenvalue_gas_boundary_resolution  1
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   eigenvalue_layer_name ........ bert.encoder.layer
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   eigenvalue_layer_num ......... 0
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   eigenvalue_max_iter .......... 100
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   eigenvalue_stability ......... 1e-06
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   eigenvalue_tol ............... 0.01
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   eigenvalue_verbose ........... False
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   elasticity_enabled ........... False
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   flops_profiler_config ........ {
x3104c0s1b0n0:     "enabled": false, 
x3104c0s1b0n0:     "recompute_fwd_factor": 0.0, 
x3104c0s1b0n0:     "profile_step": 1, 
x3104c0s1b0n0:     "module_depth": -1, 
x3104c0s1b0n0:     "top_modules": 1, 
x3104c0s1b0n0:     "detailed": true, 
x3104c0s1b0n0:     "output_file": null
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   fp16_auto_cast ............... None
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   fp16_enabled ................. False
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   fp16_master_weights_and_gradients  False
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   global_rank .................. 0
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   grad_accum_dtype ............. bf16
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   gradient_accumulation_steps .. 1
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   gradient_clipping ............ 0.0
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   gradient_predivide_factor .... 1.0
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   graph_harvesting ............. False
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   initial_dynamic_scale ........ 1
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   load_universal_checkpoint .... False
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   loss_scale ................... 1.0
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   memory_breakdown ............. True
x3104c0s1b0n0: [2024-03-29 16:11:46,586] [INFO] [config.py:1002:print]   mics_hierarchial_params_gather  False
x3104c0s1b0n0: [2024-03-29 16:11:46,587] [INFO] [config.py:1002:print]   mics_shard_size .............. -1
x3104c0s1b0n0: [2024-03-29 16:11:46,587] [INFO] [config.py:1002:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
x3104c0s1b0n0: [2024-03-29 16:11:46,587] [INFO] [config.py:1002:print]   nebula_config ................ {
x3104c0s1b0n0:     "enabled": false, 
x3104c0s1b0n0:     "persistent_storage_path": null, 
x3104c0s1b0n0:     "persistent_time_interval": 100, 
x3104c0s1b0n0:     "num_of_version_in_retention": 2, 
x3104c0s1b0n0:     "enable_nebula_load": true, 
x3104c0s1b0n0:     "load_path": null
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 16:11:46,587] [INFO] [config.py:1002:print]   optimizer_legacy_fusion ...... False
x3104c0s1b0n0: [2024-03-29 16:11:46,587] [INFO] [config.py:1002:print]   optimizer_name ............... None
x3104c0s1b0n0: [2024-03-29 16:11:46,587] [INFO] [config.py:1002:print]   optimizer_params ............. None
x3104c0s1b0n0: [2024-03-29 16:11:46,587] [INFO] [config.py:1002:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
x3104c0s1b0n0: [2024-03-29 16:11:46,587] [INFO] [config.py:1002:print]   pld_enabled .................. False
x3104c0s1b0n0: [2024-03-29 16:11:46,587] [INFO] [config.py:1002:print]   pld_params ................... False
x3104c0s1b0n0: [2024-03-29 16:11:46,587] [INFO] [config.py:1002:print]   prescale_gradients ........... False
x3104c0s1b0n0: [2024-03-29 16:11:46,587] [INFO] [config.py:1002:print]   scheduler_name ............... None
x3104c0s1b0n0: [2024-03-29 16:11:46,587] [INFO] [config.py:1002:print]   scheduler_params ............. None
x3104c0s1b0n0: [2024-03-29 16:11:46,587] [INFO] [config.py:1002:print]   seq_parallel_communication_data_type  torch.float32
x3104c0s1b0n0: [2024-03-29 16:11:46,587] [INFO] [config.py:1002:print]   sparse_attention ............. None
x3104c0s1b0n0: [2024-03-29 16:11:46,587] [INFO] [config.py:1002:print]   sparse_gradients_enabled ..... False
x3104c0s1b0n0: [2024-03-29 16:11:46,587] [INFO] [config.py:1002:print]   steps_per_print .............. 1
x3104c0s1b0n0: [2024-03-29 16:11:46,587] [INFO] [config.py:1002:print]   train_batch_size ............. 32
x3104c0s1b0n0: [2024-03-29 16:11:46,587] [INFO] [config.py:1002:print]   train_micro_batch_size_per_gpu  4
x3104c0s1b0n0: [2024-03-29 16:11:46,587] [INFO] [config.py:1002:print]   use_data_before_expert_parallel_  False
x3104c0s1b0n0: [2024-03-29 16:11:46,587] [INFO] [config.py:1002:print]   use_node_local_storage ....... False
x3104c0s1b0n0: [2024-03-29 16:11:46,587] [INFO] [config.py:1002:print]   wall_clock_breakdown ......... True
x3104c0s1b0n0: [2024-03-29 16:11:46,587] [INFO] [config.py:1002:print]   weight_quantization_config ... None
x3104c0s1b0n0: [2024-03-29 16:11:46,587] [INFO] [config.py:1002:print]   world_size ................... 8
x3104c0s1b0n0: [2024-03-29 16:11:46,587] [INFO] [config.py:1002:print]   zero_allow_untested_optimizer  False
x3104c0s1b0n0: [2024-03-29 16:11:46,587] [INFO] [config.py:1002:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0, prefetch_optimizer=True, part_grads_async=True, prefetch_optimizer_gap=10) sub_group_size=100000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
x3104c0s1b0n0: [2024-03-29 16:11:46,587] [INFO] [config.py:1002:print]   zero_enabled ................. True
x3104c0s1b0n0: [2024-03-29 16:11:46,587] [INFO] [config.py:1002:print]   zero_force_ds_cpu_optimizer .. True
x3104c0s1b0n0: [2024-03-29 16:11:46,587] [INFO] [config.py:1002:print]   zero_optimization_stage ...... 3
x3104c0s1b0n0: [2024-03-29 16:11:46,587] [INFO] [config.py:988:print_user_config]   json = {
x3104c0s1b0n0:     "train_batch_size": 32, 
x3104c0s1b0n0:     "train_micro_batch_size_per_gpu": 4, 
x3104c0s1b0n0:     "steps_per_print": 1, 
x3104c0s1b0n0:     "zero_optimization": {
x3104c0s1b0n0:         "stage": 3, 
x3104c0s1b0n0:         "offload_optimizer": {
x3104c0s1b0n0:             "device": "cpu", 
x3104c0s1b0n0:             "ratio": 1, 
x3104c0s1b0n0:             "pin_memory": true, 
x3104c0s1b0n0:             "prefetch_optimizer": 1, 
x3104c0s1b0n0:             "part_grads_async": 1, 
x3104c0s1b0n0:             "prefetch_optimizer_gap": 10
x3104c0s1b0n0:         }, 
x3104c0s1b0n0:         "sub_group_size": 1.000000e+08
x3104c0s1b0n0:     }, 
x3104c0s1b0n0:     "bf16": {
x3104c0s1b0n0:         "enabled": true
x3104c0s1b0n0:     }, 
x3104c0s1b0n0:     "data_types": {
x3104c0s1b0n0:         "grad_accum_dtype": "bf16"
x3104c0s1b0n0:     }, 
x3104c0s1b0n0:     "wall_clock_breakdown": true, 
x3104c0s1b0n0:     "memory_breakdown": true, 
x3104c0s1b0n0:     "flops_profiler": {
x3104c0s1b0n0:         "enabled": false
x3104c0s1b0n0:     }
x3104c0s1b0n0: }
x3104c0s25b0n0: <TIMER:model-and-optimizer-setup,49.88332462310791>
x3104c0s25b0n0: <TIMER:model-and-optimizer-setup,49.88403677940369>
x3104c0s25b0n0: <TIMER:model-and-optimizer-setup,49.884690046310425>
x3104c0s1b0n0: <TIMER:model-and-optimizer-setup,49.8849823474884>
x3104c0s25b0n0: <TIMER:model-and-optimizer-setup,49.88533401489258>
x3104c0s1b0n0: <TIMER:model-and-optimizer-setup,49.8861186504364>
x3104c0s1b0n0: <TIMER:model-and-optimizer-setup,49.886712312698364>
x3104c0s1b0n0: <TIMER:model-and-optimizer-setup,49.88735342025757>
x3104c0s1b0n0: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-29 16:11:47 
x3104c0s1b0n0: > building train, validation, and test datasets ...
x3104c0s1b0n0:  > datasets target sizes (minimum size):
x3104c0s1b0n0:     train:      320
x3104c0s1b0n0:     validation: 0
x3104c0s1b0n0:     test:       0
x3104c0s1b0n0: > building train, validation, and test datasets for GPT ...
x3104c0s1b0n0: Single data path provided for train, valid & test
x3104c0s1b0n0:  > building dataset index ...
x3104c0s1b0n0:     reading sizes...
x3104c0s1b0n0:     reading pointers...
x3104c0s1b0n0:     reading document index...
x3104c0s1b0n0:     creating numpy buffer of mmap...
x3104c0s1b0n0:     creating memory view of numpy buffer...
x3104c0s1b0n0:  > finished creating indexed dataset in 0.002304 seconds
x3104c0s1b0n0:     number of documents: 79000
x3104c0s1b0n0:  > dataset split:
x3104c0s1b0n0:     train:
x3104c0s1b0n0:      document indices in [0, 74971) total of 74971 documents
x3104c0s1b0n0:     validation:
x3104c0s1b0n0:      document indices in [74971, 78921) total of 3950 documents
x3104c0s1b0n0:     test:
x3104c0s1b0n0:      document indices in [78921, 79000) total of 79 documents
x3104c0s1b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_doc_idx.npy
x3104c0s1b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_sample_idx.npy
x3104c0s1b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_shuffle_idx.npy
x3104c0s1b0n0:     loaded indexed file in 0.002 seconds
x3104c0s1b0n0:     total number of samples: 108448
x3104c0s1b0n0:     total number of epochs: 1
x3104c0s1b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_doc_idx.npy
x3104c0s1b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_sample_idx.npy
x3104c0s1b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_shuffle_idx.npy
x3104c0s1b0n0:     loaded indexed file in 0.003 seconds
x3104c0s1b0n0:     total number of samples: 5792
x3104c0s1b0n0:     total number of epochs: 1
x3104c0s1b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_doc_idx.npy
x3104c0s1b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_sample_idx.npy
x3104c0s1b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_shuffle_idx.npy
x3104c0s1b0n0:     loaded indexed file in 0.003 seconds
x3104c0s1b0n0:     total number of samples: 185
x3104c0s1b0n0:     total number of epochs: 1
x3104c0s1b0n0: > finished creating GPT datasets ...
x3104c0s25b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5097692012786865>
x3104c0s25b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5200765132904053>
x3104c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5204839706420898>
x3104c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5268228054046631>
x3104c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5303137302398682>
x3104c0s25b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5511555671691895>
x3104c0s25b0n0: <TIMER:train/valid/test-data-iterators-setup,0.558809757232666>
x3104c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5591363906860352>
x3104c0s1b0n0: [after dataloaders are built] datetime: 2024-03-29 16:11:48 
x3104c0s1b0n0: done with setup ...
x3104c0s25b0n0: (min, max) time across ranks (ms):
x3104c0s25b0n0:     model-and-optimizer-setup ......................: (49883.32, 49887.35)
x3104c0s25b0n0:     train/valid/test-data-iterators-setup ..........: (509.77, 559.14)
x3104c0s1b0n0: training ...
x3104c0s1b0n0: [before training begins] datetime: 2024-03-29 16:11:48 
x3104c0s1b0n0: [before the start of training step] datetime: 2024-03-29 16:11:48 
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 16:11:48,638] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 16:11:48,639] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 7.45 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 16:11:48,639] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.28 GB, percent = 62.5%
x3104c0s1b0n0: [2024-03-29 16:11:48,764] [INFO] [checkpointing.py:539:forward] Activation Checkpointing Information
x3104c0s1b0n0: [2024-03-29 16:11:48,764] [INFO] [checkpointing.py:540:forward] ----Partition Activations False, CPU CHECKPOINTING False
x3104c0s1b0n0: [2024-03-29 16:11:48,764] [INFO] [checkpointing.py:541:forward] ----contiguous Memory Checkpointing False with 60 total layers
x3104c0s1b0n0: [2024-03-29 16:11:48,764] [INFO] [checkpointing.py:543:forward] ----Synchronization False
x3104c0s1b0n0: [2024-03-29 16:11:48,764] [INFO] [checkpointing.py:544:forward] ----Profiling time in checkpointing False
x3104c0s1b0n0: [2024-03-29 16:11:57,071] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 16:11:57,072] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 19.76 GB         CA 22.95 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 16:11:57,072] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.43 GB, percent = 62.5%
x3104c0s1b0n0: [2024-03-29 16:11:57,261] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 16:11:57,261] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 16.84 GB         CA 16.91 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 16:11:57,262] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.44 GB, percent = 62.5%
x3104c0s1b0n0: [2024-03-29 16:12:22,708] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 16:12:22,709] [INFO] [utils.py:801:see_memory_usage] MA 10.88 GB         Max_MA 20.43 GB         CA 11.08 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:12:22,709] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.46 GB, percent = 62.5%
x3104c0s1b0n0: [2024-03-29 16:12:22,778] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 16:12:22,779] [INFO] [utils.py:801:see_memory_usage] MA 10.88 GB         Max_MA 10.88 GB         CA 11.08 GB         Max_CA 11 GB 
x3104c0s1b0n0: [2024-03-29 16:12:22,779] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.56 GB, percent = 62.5%
x3104c0s1b0n0: [2024-03-29 16:12:29,839] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.987974643707275
x3104c0s1b0n0: [2024-03-29 16:12:29,849] [INFO] [stage3.py:2251:step] Full outer step loop took 6.99792218208313
x3104c0s1b0n0: [2024-03-29 16:12:29,877] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.02614164352417
x3104c0s1b0n0: [2024-03-29 16:12:29,889] [INFO] [stage3.py:2251:step] Full outer step loop took 7.038034677505493
x3104c0s1b0n0: [2024-03-29 16:12:30,019] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.167853832244873
x3104c0s1b0n0: [2024-03-29 16:12:30,024] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.173480033874512
x3104c0s1b0n0: [2024-03-29 16:12:30,031] [INFO] [stage3.py:2251:step] Full outer step loop took 7.179999828338623
x3104c0s1b0n0: [2024-03-29 16:12:30,034] [INFO] [stage3.py:2251:step] Full outer step loop took 7.18280291557312
x3104c0s25b0n0: [2024-03-29 16:12:30,038] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.187016010284424
x3104c0s25b0n0: [2024-03-29 16:12:30,074] [INFO] [stage3.py:2251:step] Full outer step loop took 7.222917556762695
x3104c0s25b0n0: [2024-03-29 16:12:30,161] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.310179233551025
x3104c0s25b0n0: [2024-03-29 16:12:30,191] [INFO] [stage3.py:2251:step] Full outer step loop took 7.339689493179321
x3104c0s25b0n0: [2024-03-29 16:12:30,197] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.345890998840332
x3104c0s25b0n0: [2024-03-29 16:12:30,228] [INFO] [stage3.py:2251:step] Full outer step loop took 7.3775107860565186
x3104c0s25b0n0: [2024-03-29 16:12:30,231] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.3798933029174805
x3104c0s25b0n0: [2024-03-29 16:12:30,240] [INFO] [stage3.py:2251:step] Full outer step loop took 7.389066934585571
x3104c0s25b0n0: [2024-03-29 16:12:30,249] [INFO] [stage3.py:2277:step] End to end step took 7.398540019989014
x3104c0s25b0n0: [2024-03-29 16:12:30,249] [INFO] [stage3.py:2277:step] End to end step took 7.3986656665802
x3104c0s1b0n0: [2024-03-29 16:12:30,249] [INFO] [stage3.py:2277:step] End to end step took 7.398578405380249
x3104c0s1b0n0: [2024-03-29 16:12:30,249] [INFO] [stage3.py:2277:step] End to end step took 7.398672580718994
x3104c0s25b0n0: [2024-03-29 16:12:30,250] [INFO] [stage3.py:2277:step] End to end step took 7.398836135864258
x3104c0s1b0n0: [2024-03-29 16:12:30,250] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6998.16
x3104c0s1b0n0: [2024-03-29 16:12:30,250] [INFO] [stage3.py:2277:step] End to end step took 7.399106740951538
x3104c0s1b0n0: [2024-03-29 16:12:30,250] [WARNING] [stage3.py:2267:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3104c0s1b0n0: [2024-03-29 16:12:30,250] [INFO] [stage3.py:2277:step] End to end step took 7.3993239402771
x3104c0s25b0n0: [2024-03-29 16:12:30,250] [INFO] [stage3.py:2277:step] End to end step took 7.399312496185303
x3104c0s1b0n0: [2024-03-29 16:12:30,250] [INFO] [logging.py:96:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 16:12:30,251] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 8524.35 | bwd_microstep: 25297.87 | bwd_inner_microstep: 25210.72 | bwd_allreduce_microstep: 87.04 | step_microstep: 7471.31
x3104c0s1b0n0: [2024-03-29 16:12:30,251] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 8524.35 | bwd: 25297.87 | bwd_inner: 25210.73 | bwd_allreduce: 87.05 | step: 7471.31
x3104c0s1b0n0: [2024-03-29 16:12:30,347] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 16:12:30,348] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 11.94 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 16:12:30,348] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.67 GB, percent = 81.0%
x3104c0s1b0n0: <TIMER:interval-time,41.93076491355896><TIMER:interval-time,41.930784940719604><TIMER:interval-time,41.93077516555786><TIMER:interval-time,41.93077325820923>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,41.93077564239502><TIMER:interval-time,41.93078875541687><TIMER:interval-time,41.93078804016113>
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,41.9307963848114>
x3104c0s1b0n0: [Rank 0] (after 1 iterations) memory (MB) | allocated: 10510.36328125 | max allocated: 10510.36376953125 | reserved: 10586.0 | max reserved: 10586.0
x3104c0s25b0n0:  elapsed_time 41.930796 | consumed samples:           32 | consumed tokens:        65536 | elapsed time per iteration (ms): 41930.8 | learning rate: 3.000E-04 | global batch size:    32 | lm loss: 1.215015E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.763 | TFLOPs: 52.80 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 16:12:30,476] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 16:12:30,477] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 16:12:30,477] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.75 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:12:36,834] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 16:12:36,835] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:12:36,835] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.75 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:12:36,927] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 16:12:36,927] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:12:36,928] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.75 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:12:53,866] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 16:12:53,867] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 23.87 GB         CA 12.34 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 16:12:53,867] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.76 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:12:53,937] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 16:12:53,937] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 12.14 GB         CA 12.34 GB         Max_CA 12 GB 
x3104c0s1b0n0: [2024-03-29 16:12:53,938] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.76 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:12:58,790] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.828346252441406
x3104c0s1b0n0: [2024-03-29 16:12:58,791] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.829930782318115
x3104c0s1b0n0: [2024-03-29 16:12:58,799] [INFO] [stage3.py:2251:step] Full outer step loop took 4.837577819824219
x3104c0s1b0n0: [2024-03-29 16:12:58,803] [INFO] [stage3.py:2251:step] Full outer step loop took 4.841804504394531
x3104c0s1b0n0: [2024-03-29 16:12:58,907] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.945127725601196
x3104c0s1b0n0: [2024-03-29 16:12:58,908] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.946420907974243
x3104c0s1b0n0: [2024-03-29 16:12:58,916] [INFO] [stage3.py:2251:step] Full outer step loop took 4.95448112487793
x3104c0s1b0n0: [2024-03-29 16:12:58,917] [INFO] [stage3.py:2251:step] Full outer step loop took 4.955679416656494
x3104c0s25b0n0: [2024-03-29 16:12:58,980] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.018732070922852
x3104c0s25b0n0: [2024-03-29 16:12:59,005] [INFO] [stage3.py:2251:step] Full outer step loop took 5.04335355758667
x3104c0s25b0n0: [2024-03-29 16:12:59,110] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.148959636688232
x3104c0s25b0n0: [2024-03-29 16:12:59,118] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.156529664993286
x3104c0s25b0n0: [2024-03-29 16:12:59,124] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.162440299987793
x3104c0s25b0n0: [2024-03-29 16:12:59,125] [INFO] [stage3.py:2251:step] Full outer step loop took 5.163606643676758
x3104c0s25b0n0: [2024-03-29 16:12:59,128] [INFO] [stage3.py:2251:step] Full outer step loop took 5.166375637054443
x3104c0s25b0n0: [2024-03-29 16:12:59,134] [INFO] [stage3.py:2251:step] Full outer step loop took 5.172987699508667
x3104c0s1b0n0: [2024-03-29 16:12:59,144] [INFO] [stage3.py:2277:step] End to end step took 5.182813882827759
x3104c0s1b0n0: [2024-03-29 16:12:59,144] [INFO] [stage3.py:2277:step] End to end step took 5.1828453540802
x3104c0s25b0n0: [2024-03-29 16:12:59,144] [INFO] [stage3.py:2277:step] End to end step took 5.182819604873657
x3104c0s25b0n0: [2024-03-29 16:12:59,144] [INFO] [stage3.py:2277:step] End to end step took 5.1829681396484375
x3104c0s25b0n0: [2024-03-29 16:12:59,144] [INFO] [stage3.py:2277:step] End to end step took 5.18298077583313
x3104c0s1b0n0: [2024-03-29 16:12:59,144] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4837.76
x3104c0s25b0n0: [2024-03-29 16:12:59,145] [INFO] [stage3.py:2277:step] End to end step took 5.183356046676636
x3104c0s1b0n0: [2024-03-29 16:12:59,145] [INFO] [stage3.py:2277:step] End to end step took 5.183367013931274
x3104c0s1b0n0: [2024-03-29 16:12:59,145] [INFO] [stage3.py:2277:step] End to end step took 5.183316946029663
x3104c0s1b0n0: [2024-03-29 16:12:59,145] [INFO] [logging.py:96:log_dist] [Rank 0] step=2, skipped=0, lr=[0.0002918585038060976], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 16:12:59,146] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6320.04 | bwd_microstep: 16778.46 | bwd_inner_microstep: 16699.06 | bwd_allreduce_microstep: 79.32 | step_microstep: 5207.77
x3104c0s1b0n0: [2024-03-29 16:12:59,146] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6320.03 | bwd: 16778.46 | bwd_inner: 16699.06 | bwd_allreduce: 79.34 | step: 5207.78
x3104c0s1b0n0: [2024-03-29 16:12:59,238] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 16:12:59,239] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.14 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 16:12:59,239] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.76 GB, percent = 81.0%
x3104c0s1b0n0: <TIMER:interval-time,28.89073133468628><TIMER:interval-time,28.89073133468628>
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,28.890734672546387><TIMER:interval-time,28.890738248825073>
x3104c0s1b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,28.890753746032715><TIMER:interval-time,28.890754222869873>
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,28.890766143798828>
x3104c0s25b0n0: <TIMER:interval-time,28.890869140625>
x3104c0s25b0n0:  elapsed_time 28.890869 | consumed samples:           64 | consumed tokens:       131072 | elapsed time per iteration (ms): 28890.9 | learning rate: 2.919E-04 | global batch size:    32 | lm loss: 1.082593E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.108 | TFLOPs: 76.64 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 16:12:59,369] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 16:12:59,370] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 16:12:59,370] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.78 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:13:05,736] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 16:13:05,737] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:13:05,737] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.78 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:13:05,819] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 16:13:05,820] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:13:05,820] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.78 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:13:22,741] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 16:13:22,742] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 23.87 GB         CA 12.34 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 16:13:22,742] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.78 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:13:22,814] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 16:13:22,814] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 12.14 GB         CA 12.34 GB         Max_CA 12 GB 
x3104c0s1b0n0: [2024-03-29 16:13:22,814] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.78 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:13:27,661] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.822208881378174
x3104c0s1b0n0: [2024-03-29 16:13:27,672] [INFO] [stage3.py:2251:step] Full outer step loop took 4.83342981338501
x3104c0s1b0n0: [2024-03-29 16:13:27,691] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.853257656097412
x3104c0s1b0n0: [2024-03-29 16:13:27,703] [INFO] [stage3.py:2251:step] Full outer step loop took 4.864882469177246
x3104c0s1b0n0: [2024-03-29 16:13:27,767] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.929139614105225
x3104c0s1b0n0: [2024-03-29 16:13:27,797] [INFO] [stage3.py:2251:step] Full outer step loop took 4.958845853805542
x3104c0s25b0n0: [2024-03-29 16:13:27,799] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.960621118545532
x3104c0s1b0n0: [2024-03-29 16:13:27,803] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.96430778503418
x3104c0s1b0n0: [2024-03-29 16:13:27,812] [INFO] [stage3.py:2251:step] Full outer step loop took 4.973449468612671
x3104c0s25b0n0: [2024-03-29 16:13:27,812] [INFO] [stage3.py:2251:step] Full outer step loop took 4.973977327346802
x3104c0s25b0n0: [2024-03-29 16:13:27,818] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.979877471923828
x3104c0s25b0n0: [2024-03-29 16:13:27,827] [INFO] [stage3.py:2251:step] Full outer step loop took 4.988994121551514
x3104c0s25b0n0: [2024-03-29 16:13:27,917] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.078550338745117
x3104c0s25b0n0: [2024-03-29 16:13:27,917] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.078543186187744
x3104c0s25b0n0: [2024-03-29 16:13:27,926] [INFO] [stage3.py:2251:step] Full outer step loop took 5.087827444076538
x3104c0s25b0n0: [2024-03-29 16:13:27,926] [INFO] [stage3.py:2251:step] Full outer step loop took 5.087903261184692
x3104c0s25b0n0: [2024-03-29 16:13:27,936] [INFO] [stage3.py:2277:step] End to end step took 5.097318172454834
x3104c0s1b0n0: [2024-03-29 16:13:27,936] [INFO] [stage3.py:2277:step] End to end step took 5.09735369682312
x3104c0s1b0n0: [2024-03-29 16:13:27,936] [INFO] [stage3.py:2277:step] End to end step took 5.0973944664001465
x3104c0s25b0n0: [2024-03-29 16:13:27,936] [INFO] [stage3.py:2277:step] End to end step took 5.097469329833984
x3104c0s1b0n0: [2024-03-29 16:13:27,936] [INFO] [stage3.py:2277:step] End to end step took 5.097638845443726
x3104c0s25b0n0: [2024-03-29 16:13:27,936] [INFO] [stage3.py:2277:step] End to end step took 5.097702264785767
x3104c0s25b0n0: [2024-03-29 16:13:27,936] [INFO] [stage3.py:2277:step] End to end step took 5.097867965698242
x3104c0s1b0n0: [2024-03-29 16:13:27,936] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4833.66
x3104c0s1b0n0: [2024-03-29 16:13:27,936] [INFO] [stage3.py:2277:step] End to end step took 5.098084926605225
x3104c0s1b0n0: [2024-03-29 16:13:27,937] [INFO] [logging.py:96:log_dist] [Rank 0] step=3, skipped=0, lr=[0.00026841599982106197], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 16:13:27,937] [INFO] [timer.py:260:stop] epoch=0/micro_step=3/global_step=3, RunningAvgSamplesPerSec=1.120217525187606, CurrSamplesPerSec=1.120217525187606, MemAllocated=10.26GB, MaxMemAllocated=12.14GB
x3104c0s1b0n0: [2024-03-29 16:13:27,937] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6328.30 | bwd_microstep: 16761.43 | bwd_inner_microstep: 16681.27 | bwd_allreduce_microstep: 80.09 | step_microstep: 5122.57
x3104c0s1b0n0: [2024-03-29 16:13:27,937] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6328.29 | bwd: 16761.43 | bwd_inner: 16681.26 | bwd_allreduce: 80.11 | step: 5122.58
x3104c0s1b0n0: [2024-03-29 16:13:28,035] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 16:13:28,036] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.14 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 16:13:28,036] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.78 GB, percent = 81.0%
x3104c0s25b0n0: <TIMER:interval-time,28.79662013053894>
x3104c0s1b0n0: <TIMER:interval-time,28.796602725982666>
x3104c0s1b0n0: <TIMER:interval-time,28.79661536216736><TIMER:interval-time,28.79660940170288><TIMER:interval-time,28.796614408493042>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,28.796620845794678><TIMER:interval-time,28.796629905700684>
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,28.79673981666565>
x3104c0s25b0n0:  elapsed_time 28.796621 | consumed samples:           96 | consumed tokens:       196608 | elapsed time per iteration (ms): 28796.6 | learning rate: 2.684E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.111 | TFLOPs: 76.89 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 16:13:28,172] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 16:13:28,172] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 16:13:28,173] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.79 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:13:34,593] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 16:13:34,593] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:13:34,593] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.78 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:13:34,674] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 16:13:34,674] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:13:34,675] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.78 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:13:51,685] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 16:13:51,686] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 23.87 GB         CA 12.34 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 16:13:51,686] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.78 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:13:51,760] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 16:13:51,761] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 12.14 GB         CA 12.34 GB         Max_CA 12 GB 
x3104c0s1b0n0: [2024-03-29 16:13:51,761] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.78 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:13:56,584] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.7994983196258545
x3104c0s1b0n0: [2024-03-29 16:13:56,589] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.804098606109619
x3104c0s1b0n0: [2024-03-29 16:13:56,596] [INFO] [stage3.py:2251:step] Full outer step loop took 4.8114166259765625
x3104c0s1b0n0: [2024-03-29 16:13:56,598] [INFO] [stage3.py:2251:step] Full outer step loop took 4.813269376754761
x3104c0s1b0n0: [2024-03-29 16:13:56,734] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.950139760971069
x3104c0s1b0n0: [2024-03-29 16:13:56,734] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.950057506561279
x3104c0s1b0n0: [2024-03-29 16:13:56,744] [INFO] [stage3.py:2251:step] Full outer step loop took 4.959340810775757
x3104c0s1b0n0: [2024-03-29 16:13:56,744] [INFO] [stage3.py:2251:step] Full outer step loop took 4.959438323974609
x3104c0s25b0n0: [2024-03-29 16:13:56,848] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.06371283531189
x3104c0s25b0n0: [2024-03-29 16:13:56,863] [INFO] [stage3.py:2251:step] Full outer step loop took 5.079110145568848
x3104c0s25b0n0: [2024-03-29 16:13:57,001] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.216877460479736
x3104c0s25b0n0: [2024-03-29 16:13:57,007] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.222834587097168
x3104c0s25b0n0: [2024-03-29 16:13:57,008] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.2237279415130615
x3104c0s25b0n0: [2024-03-29 16:13:57,013] [INFO] [stage3.py:2251:step] Full outer step loop took 5.229129314422607
x3104c0s25b0n0: [2024-03-29 16:13:57,016] [INFO] [stage3.py:2251:step] Full outer step loop took 5.232065677642822
x3104c0s25b0n0: [2024-03-29 16:13:57,017] [INFO] [stage3.py:2251:step] Full outer step loop took 5.232960224151611
x3104c0s25b0n0: [2024-03-29 16:13:57,027] [INFO] [stage3.py:2277:step] End to end step took 5.2425501346588135
x3104c0s25b0n0: [2024-03-29 16:13:57,027] [INFO] [stage3.py:2277:step] End to end step took 5.242481470108032
x3104c0s25b0n0: [2024-03-29 16:13:57,027] [INFO] [stage3.py:2277:step] End to end step took 5.242583274841309
x3104c0s1b0n0: [2024-03-29 16:13:57,027] [INFO] [stage3.py:2277:step] End to end step took 5.24261999130249
x3104c0s1b0n0: [2024-03-29 16:13:57,027] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4813.43
x3104c0s1b0n0: [2024-03-29 16:13:57,027] [INFO] [stage3.py:2277:step] End to end step took 5.243037939071655
x3104c0s1b0n0: [2024-03-29 16:13:57,027] [INFO] [stage3.py:2277:step] End to end step took 5.2429587841033936
x3104c0s25b0n0: [2024-03-29 16:13:57,027] [INFO] [stage3.py:2277:step] End to end step took 5.243085145950317
x3104c0s1b0n0: [2024-03-29 16:13:57,028] [INFO] [stage3.py:2277:step] End to end step took 5.24320912361145
x3104c0s1b0n0: [2024-03-29 16:13:57,028] [INFO] [logging.py:96:log_dist] [Rank 0] step=4, skipped=0, lr=[0.00023249999999999996], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 16:13:57,028] [INFO] [timer.py:260:stop] epoch=0/micro_step=4/global_step=4, RunningAvgSamplesPerSec=1.1145930179098789, CurrSamplesPerSec=1.109024708715373, MemAllocated=10.26GB, MaxMemAllocated=12.14GB
x3104c0s1b0n0: [2024-03-29 16:13:57,028] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6381.71 | bwd_microstep: 16844.02 | bwd_inner_microstep: 16761.84 | bwd_allreduce_microstep: 82.12 | step_microstep: 5267.46
x3104c0s1b0n0: [2024-03-29 16:13:57,029] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6381.69 | bwd: 16844.02 | bwd_inner: 16761.83 | bwd_allreduce: 82.13 | step: 5267.46
x3104c0s1b0n0: [2024-03-29 16:13:57,119] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 16:13:57,119] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.14 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 16:13:57,119] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.78 GB, percent = 81.0%
x3104c0s1b0n0: <TIMER:interval-time,29.083227157592773><TIMER:interval-time,29.08323907852173>
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,29.083242893218994>
x3104c0s1b0n0: <TIMER:interval-time,29.08324408531189>
x3104c0s25b0n0: <TIMER:interval-time,29.08327317237854><TIMER:interval-time,29.083274841308594><TIMER:interval-time,29.083277463912964>
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,29.0833957195282>
x3104c0s25b0n0:  elapsed_time 29.083396 | consumed samples:          128 | consumed tokens:       262144 | elapsed time per iteration (ms): 29083.4 | learning rate: 2.325E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.100 | TFLOPs: 76.13 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 16:13:57,250] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 16:13:57,251] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 16:13:57,251] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.79 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:14:04,573] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 16:14:04,574] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:14:04,574] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.79 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:14:04,654] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 16:14:04,655] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:14:04,655] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.79 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:14:21,701] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 16:14:21,702] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 23.87 GB         CA 12.34 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 16:14:21,702] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.79 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:14:21,773] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 16:14:21,774] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 12.14 GB         CA 12.34 GB         Max_CA 12 GB 
x3104c0s1b0n0: [2024-03-29 16:14:21,774] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.79 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:14:26,592] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.79405665397644
x3104c0s1b0n0: [2024-03-29 16:14:26,602] [INFO] [stage3.py:2251:step] Full outer step loop took 4.804450988769531
x3104c0s1b0n0: [2024-03-29 16:14:26,675] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.877099275588989
x3104c0s1b0n0: [2024-03-29 16:14:26,684] [INFO] [stage3.py:2251:step] Full outer step loop took 4.886232852935791
x3104c0s1b0n0: [2024-03-29 16:14:26,787] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.989771127700806
x3104c0s1b0n0: [2024-03-29 16:14:26,788] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.990054368972778
x3104c0s25b0n0: [2024-03-29 16:14:26,789] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.991215467453003
x3104c0s1b0n0: [2024-03-29 16:14:26,797] [INFO] [stage3.py:2251:step] Full outer step loop took 4.999106407165527
x3104c0s1b0n0: [2024-03-29 16:14:26,797] [INFO] [stage3.py:2251:step] Full outer step loop took 4.999342441558838
x3104c0s25b0n0: [2024-03-29 16:14:26,804] [INFO] [stage3.py:2251:step] Full outer step loop took 5.006107568740845
x3104c0s25b0n0: [2024-03-29 16:14:26,937] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.139051914215088
x3104c0s25b0n0: [2024-03-29 16:14:26,953] [INFO] [stage3.py:2251:step] Full outer step loop took 5.15496826171875
x3104c0s25b0n0: [2024-03-29 16:14:26,959] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.1610729694366455
x3104c0s25b0n0: [2024-03-29 16:14:26,968] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.170105457305908
x3104c0s25b0n0: [2024-03-29 16:14:26,971] [INFO] [stage3.py:2251:step] Full outer step loop took 5.173228025436401
x3104c0s25b0n0: [2024-03-29 16:14:26,977] [INFO] [stage3.py:2251:step] Full outer step loop took 5.179260969161987
x3104c0s25b0n0: [2024-03-29 16:14:26,986] [INFO] [stage3.py:2277:step] End to end step took 5.188770055770874
x3104c0s1b0n0: [2024-03-29 16:14:26,987] [INFO] [stage3.py:2277:step] End to end step took 5.188838243484497
x3104c0s25b0n0: [2024-03-29 16:14:26,987] [INFO] [stage3.py:2277:step] End to end step took 5.188908815383911
x3104c0s25b0n0: [2024-03-29 16:14:26,987] [INFO] [stage3.py:2277:step] End to end step took 5.188863039016724
x3104c0s1b0n0: [2024-03-29 16:14:26,987] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4886.38
x3104c0s1b0n0: [2024-03-29 16:14:26,987] [INFO] [stage3.py:2277:step] End to end step took 5.1892311573028564
x3104c0s25b0n0: [2024-03-29 16:14:26,987] [INFO] [stage3.py:2277:step] End to end step took 5.189301252365112
x3104c0s1b0n0: [2024-03-29 16:14:26,987] [INFO] [stage3.py:2277:step] End to end step took 5.189249277114868
x3104c0s1b0n0: [2024-03-29 16:14:26,987] [INFO] [stage3.py:2277:step] End to end step took 5.189526557922363
x3104c0s1b0n0: [2024-03-29 16:14:26,987] [INFO] [logging.py:96:log_dist] [Rank 0] step=5, skipped=0, lr=[0.0001884425039850356], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 16:14:26,988] [INFO] [timer.py:260:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=1.101477702240868, CurrSamplesPerSec=1.0761517409903458, MemAllocated=10.26GB, MaxMemAllocated=12.14GB
x3104c0s1b0n0: [2024-03-29 16:14:26,988] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 7286.97 | bwd_microstep: 16880.78 | bwd_inner_microstep: 16800.00 | bwd_allreduce_microstep: 80.71 | step_microstep: 5213.48
x3104c0s1b0n0: [2024-03-29 16:14:26,988] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 7286.96 | bwd: 16880.78 | bwd_inner: 16800.00 | bwd_allreduce: 80.72 | step: 5213.48
x3104c0s1b0n0: [2024-03-29 16:14:27,080] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 16:14:27,081] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.14 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 16:14:27,081] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.79 GB, percent = 81.0%
x3104c0s1b0n0: <TIMER:interval-time,29.961042642593384><TIMER:interval-time,29.961044311523438><TIMER:interval-time,29.961039543151855>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,29.961053371429443>
x3104c0s25b0n0: <TIMER:interval-time,29.9610595703125><TIMER:interval-time,29.961076736450195><TIMER:interval-time,29.96108341217041>
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,29.96119260787964>
x3104c0s25b0n0:  elapsed_time 29.961060 | consumed samples:          160 | consumed tokens:       327680 | elapsed time per iteration (ms): 29961.1 | learning rate: 1.884E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.068 | TFLOPs: 73.90 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 16:14:27,221] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 16:14:27,222] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 16:14:27,222] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.78 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:14:33,622] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 16:14:33,623] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:14:33,623] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.79 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:14:33,701] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 16:14:33,701] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:14:33,701] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.79 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:14:50,788] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 16:14:50,789] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 23.87 GB         CA 12.34 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 16:14:50,789] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.79 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:14:50,860] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 16:14:50,860] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 12.14 GB         CA 12.34 GB         Max_CA 12 GB 
x3104c0s1b0n0: [2024-03-29 16:14:50,860] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.79 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:14:55,638] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.7540669441223145
x3104c0s1b0n0: [2024-03-29 16:14:55,646] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.76212215423584
x3104c0s1b0n0: [2024-03-29 16:14:55,648] [INFO] [stage3.py:2251:step] Full outer step loop took 4.763286352157593
x3104c0s1b0n0: [2024-03-29 16:14:55,658] [INFO] [stage3.py:2251:step] Full outer step loop took 4.773983001708984
x3104c0s1b0n0: [2024-03-29 16:14:55,755] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.871070146560669
x3104c0s1b0n0: [2024-03-29 16:14:55,756] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.8723485469818115
x3104c0s1b0n0: [2024-03-29 16:14:55,765] [INFO] [stage3.py:2251:step] Full outer step loop took 4.880911111831665
x3104c0s1b0n0: [2024-03-29 16:14:55,766] [INFO] [stage3.py:2251:step] Full outer step loop took 4.881638765335083
x3104c0s25b0n0: [2024-03-29 16:14:55,894] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.010063648223877
x3104c0s25b0n0: [2024-03-29 16:14:55,906] [INFO] [stage3.py:2251:step] Full outer step loop took 5.021856307983398
x3104c0s25b0n0: [2024-03-29 16:14:56,030] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.14617657661438
x3104c0s25b0n0: [2024-03-29 16:14:56,060] [INFO] [stage3.py:2251:step] Full outer step loop took 5.176119565963745
x3104c0s25b0n0: [2024-03-29 16:14:56,063] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.1790525913238525
x3104c0s25b0n0: [2024-03-29 16:14:56,064] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.179525852203369
x3104c0s25b0n0: [2024-03-29 16:14:56,073] [INFO] [stage3.py:2251:step] Full outer step loop took 5.188751459121704
x3104c0s25b0n0: [2024-03-29 16:14:56,073] [INFO] [stage3.py:2251:step] Full outer step loop took 5.188765525817871
x3104c0s25b0n0: [2024-03-29 16:14:56,083] [INFO] [stage3.py:2277:step] End to end step took 5.19861626625061
x3104c0s1b0n0: [2024-03-29 16:14:56,083] [INFO] [stage3.py:2277:step] End to end step took 5.198635101318359
x3104c0s25b0n0: [2024-03-29 16:14:56,083] [INFO] [stage3.py:2277:step] End to end step took 5.198666572570801
x3104c0s25b0n0: [2024-03-29 16:14:56,083] [INFO] [stage3.py:2277:step] End to end step took 5.1986000537872314
x3104c0s1b0n0: [2024-03-29 16:14:56,083] [INFO] [stage3.py:2277:step] End to end step took 5.198644638061523
x3104c0s1b0n0: [2024-03-29 16:14:56,083] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4763.44
x3104c0s25b0n0: [2024-03-29 16:14:56,083] [INFO] [stage3.py:2277:step] End to end step took 5.1990039348602295
x3104c0s1b0n0: [2024-03-29 16:14:56,083] [INFO] [stage3.py:2277:step] End to end step took 5.198990345001221
x3104c0s1b0n0: [2024-03-29 16:14:56,083] [INFO] [stage3.py:2277:step] End to end step took 5.199225902557373
x3104c0s1b0n0: [2024-03-29 16:14:56,084] [INFO] [logging.py:96:log_dist] [Rank 0] step=6, skipped=0, lr=[0.0001415574960149644], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 16:14:56,084] [INFO] [timer.py:260:stop] epoch=0/micro_step=6/global_step=6, RunningAvgSamplesPerSec=1.1032942126356189, CurrSamplesPerSec=1.108779871103108, MemAllocated=10.26GB, MaxMemAllocated=12.14GB
x3104c0s1b0n0: [2024-03-29 16:14:56,084] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6363.48 | bwd_microstep: 16922.39 | bwd_inner_microstep: 16840.14 | bwd_allreduce_microstep: 82.19 | step_microstep: 5223.40
x3104c0s1b0n0: [2024-03-29 16:14:56,084] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6363.46 | bwd: 16922.38 | bwd_inner: 16840.13 | bwd_allreduce: 82.20 | step: 5223.40
x3104c0s1b0n0: [2024-03-29 16:14:56,173] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 16:14:56,174] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.14 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 16:14:56,174] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.79 GB, percent = 81.0%
x3104c0s1b0n0: <TIMER:interval-time,29.09275484085083><TIMER:interval-time,29.09275484085083><TIMER:interval-time,29.09275460243225>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,29.092756986618042>
x3104c0s25b0n0: <TIMER:interval-time,29.092784643173218><TIMER:interval-time,29.092787504196167><TIMER:interval-time,29.092790365219116>
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,29.092899799346924>
x3104c0s25b0n0:  elapsed_time 29.092790 | consumed samples:          192 | consumed tokens:       393216 | elapsed time per iteration (ms): 29092.8 | learning rate: 1.416E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.100 | TFLOPs: 76.10 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 16:14:56,281] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 16:14:56,282] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 16:14:56,282] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.81 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:15:02,715] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 16:15:02,716] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:15:02,716] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.8 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:15:02,793] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 16:15:02,793] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:15:02,794] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.8 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:15:19,965] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 16:15:19,966] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 23.87 GB         CA 12.34 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 16:15:19,966] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.79 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:15:20,036] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 16:15:20,037] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 12.14 GB         CA 12.34 GB         Max_CA 12 GB 
x3104c0s1b0n0: [2024-03-29 16:15:20,037] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.79 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:15:24,895] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.833827257156372
x3104c0s1b0n0: [2024-03-29 16:15:24,906] [INFO] [stage3.py:2251:step] Full outer step loop took 4.845003366470337
x3104c0s25b0n0: [2024-03-29 16:15:24,958] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.897647857666016
x3104c0s25b0n0: [2024-03-29 16:15:24,968] [INFO] [stage3.py:2251:step] Full outer step loop took 4.907386541366577
x3104c0s1b0n0: [2024-03-29 16:15:25,058] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.997483015060425
x3104c0s1b0n0: [2024-03-29 16:15:25,062] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.001225233078003
x3104c0s25b0n0: [2024-03-29 16:15:25,064] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.003413200378418
x3104c0s1b0n0: [2024-03-29 16:15:25,069] [INFO] [stage3.py:2251:step] Full outer step loop took 5.008135557174683
x3104c0s1b0n0: [2024-03-29 16:15:25,088] [INFO] [stage3.py:2251:step] Full outer step loop took 5.027074575424194
x3104c0s25b0n0: [2024-03-29 16:15:25,095] [INFO] [stage3.py:2251:step] Full outer step loop took 5.034137010574341
x3104c0s1b0n0: [2024-03-29 16:15:25,100] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.039116382598877
x3104c0s1b0n0: [2024-03-29 16:15:25,109] [INFO] [stage3.py:2251:step] Full outer step loop took 5.048252582550049
x3104c0s25b0n0: [2024-03-29 16:15:25,153] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.091694355010986
x3104c0s25b0n0: [2024-03-29 16:15:25,168] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.106945037841797
x3104c0s25b0n0: [2024-03-29 16:15:25,168] [INFO] [stage3.py:2251:step] Full outer step loop took 5.107501029968262
x3104c0s25b0n0: [2024-03-29 16:15:25,177] [INFO] [stage3.py:2251:step] Full outer step loop took 5.116077661514282
x3104c0s25b0n0: [2024-03-29 16:15:25,186] [INFO] [stage3.py:2277:step] End to end step took 5.1256256103515625
x3104c0s1b0n0: [2024-03-29 16:15:25,186] [INFO] [stage3.py:2277:step] End to end step took 5.125591993331909
x3104c0s1b0n0: [2024-03-29 16:15:25,186] [INFO] [stage3.py:2277:step] End to end step took 5.125669002532959
x3104c0s1b0n0: [2024-03-29 16:15:25,186] [INFO] [stage3.py:2277:step] End to end step took 5.125664710998535
x3104c0s25b0n0: [2024-03-29 16:15:25,186] [INFO] [stage3.py:2277:step] End to end step took 5.125690698623657
x3104c0s1b0n0: [2024-03-29 16:15:25,187] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4845.16
x3104c0s25b0n0: [2024-03-29 16:15:25,187] [INFO] [stage3.py:2277:step] End to end step took 5.1259331703186035
x3104c0s1b0n0: [2024-03-29 16:15:25,187] [INFO] [stage3.py:2277:step] End to end step took 5.125971794128418
x3104c0s25b0n0: [2024-03-29 16:15:25,187] [INFO] [stage3.py:2277:step] End to end step took 5.126187324523926
x3104c0s1b0n0: [2024-03-29 16:15:25,187] [INFO] [logging.py:96:log_dist] [Rank 0] step=7, skipped=0, lr=[9.750000000000001e-05], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 16:15:25,188] [INFO] [timer.py:260:stop] epoch=0/micro_step=7/global_step=7, RunningAvgSamplesPerSec=1.1040552126053302, CurrSamplesPerSec=1.1071097395615115, MemAllocated=10.26GB, MaxMemAllocated=12.14GB
x3104c0s1b0n0: [2024-03-29 16:15:25,188] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6401.59 | bwd_microstep: 17007.98 | bwd_inner_microstep: 16926.01 | bwd_allreduce_microstep: 81.90 | step_microstep: 5150.44
x3104c0s1b0n0: [2024-03-29 16:15:25,188] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6401.58 | bwd: 17007.98 | bwd_inner: 16926.01 | bwd_allreduce: 81.92 | step: 5150.44
x3104c0s1b0n0: [2024-03-29 16:15:25,285] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 16:15:25,286] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.14 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 16:15:25,286] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.79 GB, percent = 81.0%
x3104c0s1b0n0: <TIMER:interval-time,29.111052751541138><TIMER:interval-time,29.111053943634033>
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,29.111056566238403>
x3104c0s25b0n0: <TIMER:interval-time,29.11106777191162><TIMER:interval-time,29.11108922958374><TIMER:interval-time,29.11108708381653>
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,29.11111092567444>
x3104c0s25b0n0: <TIMER:interval-time,29.111203908920288>
x3104c0s25b0n0:  elapsed_time 29.111087 | consumed samples:          224 | consumed tokens:       458752 | elapsed time per iteration (ms): 29111.1 | learning rate: 9.750E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.099 | TFLOPs: 76.06 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 16:15:25,427] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 16:15:25,427] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 16:15:25,428] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.78 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:15:31,887] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 16:15:31,887] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:15:31,888] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.78 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:15:31,972] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 16:15:31,973] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:15:31,973] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.78 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:15:49,171] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 16:15:49,171] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 23.87 GB         CA 12.34 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 16:15:49,171] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.78 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:15:49,244] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 16:15:49,245] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 12.14 GB         CA 12.34 GB         Max_CA 12 GB 
x3104c0s1b0n0: [2024-03-29 16:15:49,245] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.78 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:15:54,172] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.904134273529053
x3104c0s1b0n0: [2024-03-29 16:15:54,173] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.9044694900512695
x3104c0s1b0n0: [2024-03-29 16:15:54,182] [INFO] [stage3.py:2251:step] Full outer step loop took 4.913323402404785
x3104c0s1b0n0: [2024-03-29 16:15:54,184] [INFO] [stage3.py:2251:step] Full outer step loop took 4.916183948516846
x3104c0s1b0n0: [2024-03-29 16:15:54,283] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.0145583152771
x3104c0s1b0n0: [2024-03-29 16:15:54,283] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.0148701667785645
x3104c0s1b0n0: [2024-03-29 16:15:54,292] [INFO] [stage3.py:2251:step] Full outer step loop took 5.023845672607422
x3104c0s1b0n0: [2024-03-29 16:15:54,292] [INFO] [stage3.py:2251:step] Full outer step loop took 5.024274587631226
x3104c0s25b0n0: [2024-03-29 16:15:54,433] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.16497540473938
x3104c0s25b0n0: [2024-03-29 16:15:54,438] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.170205593109131
x3104c0s25b0n0: [2024-03-29 16:15:54,445] [INFO] [stage3.py:2251:step] Full outer step loop took 5.176230192184448
x3104c0s25b0n0: [2024-03-29 16:15:54,477] [INFO] [stage3.py:2251:step] Full outer step loop took 5.208049774169922
x3104c0s25b0n0: [2024-03-29 16:15:54,499] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.230873346328735
x3104c0s25b0n0: [2024-03-29 16:15:54,500] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.2316389083862305
x3104c0s25b0n0: [2024-03-29 16:15:54,508] [INFO] [stage3.py:2251:step] Full outer step loop took 5.240161418914795
x3104c0s25b0n0: [2024-03-29 16:15:54,509] [INFO] [stage3.py:2251:step] Full outer step loop took 5.240880012512207
x3104c0s25b0n0: [2024-03-29 16:15:54,519] [INFO] [stage3.py:2277:step] End to end step took 5.250448226928711
x3104c0s25b0n0: [2024-03-29 16:15:54,519] [INFO] [stage3.py:2277:step] End to end step took 5.25058650970459
x3104c0s1b0n0: [2024-03-29 16:15:54,519] [INFO] [stage3.py:2277:step] End to end step took 5.25063419342041
x3104c0s1b0n0: [2024-03-29 16:15:54,519] [INFO] [stage3.py:2277:step] End to end step took 5.250644683837891
x3104c0s25b0n0: [2024-03-29 16:15:54,519] [INFO] [stage3.py:2277:step] End to end step took 5.250672101974487
x3104c0s25b0n0: [2024-03-29 16:15:54,519] [INFO] [stage3.py:2277:step] End to end step took 5.250723838806152
x3104c0s1b0n0: [2024-03-29 16:15:54,519] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4913.45
x3104c0s1b0n0: [2024-03-29 16:15:54,519] [INFO] [stage3.py:2277:step] End to end step took 5.251074552536011
x3104c0s1b0n0: [2024-03-29 16:15:54,519] [INFO] [stage3.py:2277:step] End to end step took 5.251140832901001
x3104c0s1b0n0: [2024-03-29 16:15:54,520] [INFO] [logging.py:96:log_dist] [Rank 0] step=8, skipped=0, lr=[6.158400017893797e-05], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 16:15:54,520] [INFO] [timer.py:260:stop] epoch=0/micro_step=8/global_step=8, RunningAvgSamplesPerSec=1.1033756003090611, CurrSamplesPerSec=1.0999900505846634, MemAllocated=10.26GB, MaxMemAllocated=12.14GB
x3104c0s1b0n0: [2024-03-29 16:15:54,520] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6412.60 | bwd_microstep: 17033.54 | bwd_inner_microstep: 16952.03 | bwd_allreduce_microstep: 81.44 | step_microstep: 5275.14
x3104c0s1b0n0: [2024-03-29 16:15:54,520] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6412.59 | bwd: 17033.54 | bwd_inner: 16952.03 | bwd_allreduce: 81.46 | step: 5275.14
x3104c0s1b0n0: [2024-03-29 16:15:54,610] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 16:15:54,610] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.14 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 16:15:54,610] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.77 GB, percent = 81.0%
x3104c0s1b0n0: <TIMER:interval-time,29.32421350479126><TIMER:interval-time,29.324217081069946>
x3104c0s1b0n0: <TIMER:interval-time,29.32421875>
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,29.32421636581421>
x3104c0s25b0n0: <TIMER:interval-time,29.324226140975952><TIMER:interval-time,29.324208974838257><TIMER:interval-time,29.324230194091797>
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,29.324344396591187>
x3104c0s25b0n0:  elapsed_time 29.324344 | consumed samples:          256 | consumed tokens:       524288 | elapsed time per iteration (ms): 29324.3 | learning rate: 6.158E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.091 | TFLOPs: 75.50 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 16:15:54,754] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 16:15:54,754] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 16:15:54,755] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.78 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:16:01,228] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 16:16:01,229] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:16:01,229] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.77 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:16:01,324] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 16:16:01,325] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:16:01,325] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.77 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:16:18,488] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 16:16:18,489] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 23.87 GB         CA 12.34 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 16:16:18,489] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.77 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:16:18,558] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 16:16:18,559] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 12.14 GB         CA 12.34 GB         Max_CA 12 GB 
x3104c0s1b0n0: [2024-03-29 16:16:18,559] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.77 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:16:23,445] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.862413167953491
x3104c0s1b0n0: [2024-03-29 16:16:23,458] [INFO] [stage3.py:2251:step] Full outer step loop took 4.8746607303619385
x3104c0s1b0n0: [2024-03-29 16:16:23,544] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.961236476898193
x3104c0s1b0n0: [2024-03-29 16:16:23,560] [INFO] [stage3.py:2251:step] Full outer step loop took 4.9764769077301025
x3104c0s25b0n0: [2024-03-29 16:16:23,586] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.003355264663696
x3104c0s25b0n0: [2024-03-29 16:16:23,598] [INFO] [stage3.py:2251:step] Full outer step loop took 5.015266180038452
x3104c0s1b0n0: [2024-03-29 16:16:23,613] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.030165672302246
x3104c0s1b0n0: [2024-03-29 16:16:23,613] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.030237674713135
x3104c0s1b0n0: [2024-03-29 16:16:23,623] [INFO] [stage3.py:2251:step] Full outer step loop took 5.0395848751068115
x3104c0s1b0n0: [2024-03-29 16:16:23,623] [INFO] [stage3.py:2251:step] Full outer step loop took 5.040252923965454
x3104c0s25b0n0: [2024-03-29 16:16:23,772] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.188805818557739
x3104c0s25b0n0: [2024-03-29 16:16:23,773] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.1899940967559814
x3104c0s25b0n0: [2024-03-29 16:16:23,778] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.195220947265625
x3104c0s25b0n0: [2024-03-29 16:16:23,781] [INFO] [stage3.py:2251:step] Full outer step loop took 5.198241949081421
x3104c0s25b0n0: [2024-03-29 16:16:23,782] [INFO] [stage3.py:2251:step] Full outer step loop took 5.199237108230591
x3104c0s25b0n0: [2024-03-29 16:16:23,787] [INFO] [stage3.py:2251:step] Full outer step loop took 5.204376220703125
x3104c0s25b0n0: [2024-03-29 16:16:23,797] [INFO] [stage3.py:2277:step] End to end step took 5.213885545730591
x3104c0s25b0n0: [2024-03-29 16:16:23,797] [INFO] [stage3.py:2277:step] End to end step took 5.2139129638671875
x3104c0s1b0n0: [2024-03-29 16:16:23,797] [INFO] [stage3.py:2277:step] End to end step took 5.2139599323272705
x3104c0s25b0n0: [2024-03-29 16:16:23,797] [INFO] [stage3.py:2277:step] End to end step took 5.2137770652771
x3104c0s1b0n0: [2024-03-29 16:16:23,797] [INFO] [stage3.py:2277:step] End to end step took 5.214064121246338
x3104c0s1b0n0: [2024-03-29 16:16:23,797] [INFO] [stage3.py:2277:step] End to end step took 5.214060306549072
x3104c0s1b0n0: [2024-03-29 16:16:23,797] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4875.02
x3104c0s25b0n0: [2024-03-29 16:16:23,797] [INFO] [stage3.py:2277:step] End to end step took 5.214287757873535
x3104c0s1b0n0: [2024-03-29 16:16:23,797] [INFO] [stage3.py:2277:step] End to end step took 5.214272975921631
x3104c0s1b0n0: [2024-03-29 16:16:23,798] [INFO] [logging.py:96:log_dist] [Rank 0] step=9, skipped=0, lr=[3.814149619390238e-05], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 16:16:23,798] [INFO] [timer.py:260:stop] epoch=0/micro_step=9/global_step=9, RunningAvgSamplesPerSec=1.1031593145261953, CurrSamplesPerSec=1.1018633784006806, MemAllocated=10.26GB, MaxMemAllocated=12.14GB
x3104c0s1b0n0: [2024-03-29 16:16:23,798] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6441.40 | bwd_microstep: 16998.67 | bwd_inner_microstep: 16915.43 | bwd_allreduce_microstep: 83.15 | step_microstep: 5238.95
x3104c0s1b0n0: [2024-03-29 16:16:23,798] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6441.39 | bwd: 16998.66 | bwd_inner: 16915.41 | bwd_allreduce: 83.17 | step: 5238.95
x3104c0s1b0n0: [2024-03-29 16:16:23,889] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 16:16:23,890] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.14 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 16:16:23,890] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.77 GB, percent = 81.0%
x3104c0s1b0n0: <TIMER:interval-time,29.279372215270996><TIMER:interval-time,29.27937340736389><TIMER:interval-time,29.27936577796936>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,29.27941656112671><TIMER:interval-time,29.279420852661133>
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,29.279535055160522><TIMER:interval-time,29.279521465301514>
x3104c0s25b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,29.279469966888428>
x3104c0s25b0n0:  elapsed_time 29.279521 | consumed samples:          288 | consumed tokens:       589824 | elapsed time per iteration (ms): 29279.5 | learning rate: 3.814E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.093 | TFLOPs: 75.62 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 16:16:24,031] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 16:16:24,031] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 16:16:24,031] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.78 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:16:30,517] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 16:16:30,518] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:16:30,518] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.78 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:16:30,599] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 16:16:30,600] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 16:16:30,600] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.78 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:16:47,856] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 16:16:47,857] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 23.87 GB         CA 12.34 GB         Max_CA 32 GB 
x3104c0s1b0n0: [2024-03-29 16:16:47,857] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.78 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:16:47,929] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 16:16:47,929] [INFO] [utils.py:801:see_memory_usage] MA 12.14 GB         Max_MA 12.14 GB         CA 12.34 GB         Max_CA 12 GB 
x3104c0s1b0n0: [2024-03-29 16:16:47,929] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.78 GB, percent = 81.0%
x3104c0s1b0n0: [2024-03-29 16:16:52,742] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.789349555969238
x3104c0s1b0n0: [2024-03-29 16:16:52,760] [INFO] [stage3.py:2251:step] Full outer step loop took 4.806559085845947
x3104c0s1b0n0: [2024-03-29 16:16:52,906] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.953441858291626
x3104c0s1b0n0: [2024-03-29 16:16:52,914] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.96115779876709
x3104c0s1b0n0: [2024-03-29 16:16:52,923] [INFO] [stage3.py:2251:step] Full outer step loop took 4.969916582107544
x3104c0s1b0n0: [2024-03-29 16:16:52,925] [INFO] [stage3.py:2251:step] Full outer step loop took 4.972501754760742
x3104c0s25b0n0: [2024-03-29 16:16:52,992] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.0394134521484375
x3104c0s1b0n0: [2024-03-29 16:16:53,000] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.047347068786621
x3104c0s25b0n0: [2024-03-29 16:16:53,007] [INFO] [stage3.py:2251:step] Full outer step loop took 5.054606914520264
x3104c0s1b0n0: [2024-03-29 16:16:53,010] [INFO] [stage3.py:2251:step] Full outer step loop took 5.056490182876587
x3104c0s25b0n0: [2024-03-29 16:16:53,137] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.183901071548462
x3104c0s25b0n0: [2024-03-29 16:16:53,141] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.188355922698975
x3104c0s25b0n0: [2024-03-29 16:16:53,149] [INFO] [stage3.py:2251:step] Full outer step loop took 5.196091890335083
x3104c0s25b0n0: [2024-03-29 16:16:53,150] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.196760416030884
x3104c0s25b0n0: [2024-03-29 16:16:53,155] [INFO] [stage3.py:2251:step] Full outer step loop took 5.202484369277954
x3104c0s25b0n0: [2024-03-29 16:16:53,159] [INFO] [stage3.py:2251:step] Full outer step loop took 5.205896615982056
x3104c0s25b0n0: [2024-03-29 16:16:53,169] [INFO] [stage3.py:2277:step] End to end step took 5.215582370758057
x3104c0s25b0n0: [2024-03-29 16:16:53,169] [INFO] [stage3.py:2277:step] End to end step took 5.215786933898926
x3104c0s25b0n0: [2024-03-29 16:16:53,169] [INFO] [stage3.py:2277:step] End to end step took 5.215799808502197
x3104c0s1b0n0: [2024-03-29 16:16:53,169] [INFO] [stage3.py:2277:step] End to end step took 5.21579909324646
x3104c0s1b0n0: [2024-03-29 16:16:53,169] [INFO] [stage3.py:2277:step] End to end step took 5.215728998184204
x3104c0s1b0n0: [2024-03-29 16:16:53,169] [INFO] [stage3.py:2277:step] End to end step took 5.216046333312988
x3104c0s1b0n0: [2024-03-29 16:16:53,169] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4806.82
x3104c0s25b0n0: [2024-03-29 16:16:53,169] [INFO] [stage3.py:2277:step] End to end step took 5.216294288635254
x3104c0s1b0n0: [2024-03-29 16:16:53,169] [INFO] [stage3.py:2277:step] End to end step took 5.216385126113892
x3104c0s1b0n0: [2024-03-29 16:16:53,170] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[3e-05], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 16:16:53,170] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=1.1025419473479603, CurrSamplesPerSec=1.0982396496361468, MemAllocated=10.26GB, MaxMemAllocated=12.14GB
x3104c0s1b0n0: [2024-03-29 16:16:53,170] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6443.70 | bwd_microstep: 17093.60 | bwd_inner_microstep: 17011.15 | bwd_allreduce_microstep: 82.39 | step_microstep: 5240.66
x3104c0s1b0n0: [2024-03-29 16:16:53,170] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6443.69 | bwd: 17093.60 | bwd_inner: 17011.14 | bwd_allreduce: 82.40 | step: 5240.66
x3104c0s1b0n0: [2024-03-29 16:16:53,262] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 16:16:53,263] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.14 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 16:16:53,263] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 407.78 GB, percent = 81.0%
x3104c0s1b0n0: <TIMER:interval-time,29.37216329574585><TIMER:interval-time,29.372148752212524>
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,29.372167110443115>
x3104c0s25b0n0: <TIMER:interval-time,29.372178554534912><TIMER:interval-time,29.372191905975342><TIMER:interval-time,29.372194528579712><TIMER:interval-time,29.372179985046387>
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,29.372273445129395>
x3104c0s25b0n0:  elapsed_time 29.372179 | consumed samples:          320 | consumed tokens:       655360 | elapsed time per iteration (ms): 29372.2 | learning rate: 3.000E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.089 | TFLOPs: 75.38 |
x3104c0s25b0n0: <<<only_train:304.8542585372925>>>
x3104c0s25b0n0: <<<only_train:304.85445857048035>>>
x3104c0s1b0n0: <<<only_train:304.8541986942291>>>
x3104c0s1b0n0: <<<only_train:304.85446977615356>>>
x3104c0s1b0n0: <<<only_train:304.8543372154236>>>
x3104c0s1b0n0: <<<only_train:304.8544747829437>>>
x3104c0s25b0n0: <<<only_train:304.854287147522>>>
x3104c0s25b0n0: <<<only_train:304.85352659225464>>>
x3104c0s1b0n0: [after training ends] datetime: 2024-03-29 16:16:53 
x3104c0s1b0n0: <<<full_time:304.8546073436737>>>
x3104c0s1b0n0: <<<full_time:304.8547704219818>>>
x3104c0s1b0n0: <<<full_time:304.854749917984>>>
x3104c0s1b0n0: <<<full_time:304.8545114994049>>>
x3104c0s25b0n0: <<<full_time:304.854594707489>>><<<full_time:304.8547558784485>>>
x3104c0s25b0n0: 
x3104c0s25b0n0: <<<full_time:304.853773355484>>>
x3104c0s25b0n0: <<<full_time:304.854553937912>>>
x3104c0s25b0n0: [2024-03-29 16:17:02,004] [INFO] [launch.py:348:main] Process 27868 exits successfully.
x3104c0s1b0n0: [2024-03-29 16:17:02,257] [INFO] [launch.py:348:main] Process 2086 exits successfully.
x3104c0s25b0n0: [2024-03-29 16:17:10,012] [INFO] [launch.py:348:main] Process 27869 exits successfully.
x3104c0s25b0n0: [2024-03-29 16:17:10,012] [INFO] [launch.py:348:main] Process 27867 exits successfully.
x3104c0s25b0n0: [2024-03-29 16:17:10,013] [INFO] [launch.py:348:main] Process 27870 exits successfully.
x3104c0s1b0n0: [2024-03-29 16:17:10,266] [INFO] [launch.py:348:main] Process 2085 exits successfully.
x3104c0s1b0n0: [2024-03-29 16:17:10,266] [INFO] [launch.py:348:main] Process 2083 exits successfully.
x3104c0s1b0n0: [2024-03-29 16:17:10,266] [INFO] [launch.py:348:main] Process 2084 exits successfully.

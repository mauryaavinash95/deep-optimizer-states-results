[2024-03-29 16:36:43,857] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 16:36:47,010] [INFO] [runner.py:463:main] Using IP address of 10.140.57.107 for node x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-29 16:36:47,012] [INFO] [runner.py:557:main] deepspeed_env file = ./.deepspeed_env
[2024-03-29 16:36:47,012] [INFO] [multinode_runner.py:80:get_cmd] Running on the following workers: x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov,x3006c0s1b1n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-29 16:36:47,012] [INFO] [runner.py:568:main] cmd = pdsh -S -f 1024 -w x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov,x3006c0s1b1n0.hsn.cm.polaris.alcf.anl.gov export PYTHONUSERBASE=/home/am6429/.local/polaris/conda/2023-10-04; export PYTHONPATH=/home/am6429/dl-io/Megatron-DeepSpeed; export PATH=/home/am6429/.conda/envs/dspeed_env/bin:/soft/datascience/conda/2023-10-04/mconda3/condabin:/soft/compilers/cudatoolkit/cuda-11.8.0/bin:/soft/buildtools/cmake/cmake-3.23.2/cmake-3.23.2-linux-x86_64/bin:/opt/cray/pe/gcc/11.2.0/bin:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/include:/opt/cray/pe/pals/1.2.11/bin:/opt/cray/pe/craype/2.7.20/bin:/opt/cray/libfabric/1.15.2.0/bin:/home/am6429/.conda/envs/dspeed_env/bin:/opt/cray/pe/perftools/23.03.0/bin:/opt/cray/pe/papi/7.0.0.1/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/am6429/.local/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/am6429/veloc-build/include:/home/am6429/veloc-build/bin:/soft/datascience/conda/2023-01-10/mconda3/include:/opt/cray/pe/bin:/soft/datascience/conda/2023-01-10/mconda3/include; export LD_LIBRARY_PATH=/usr/lib64/:/soft/compilers/cudatoolkit/cuda-11.8.0/lib64:/opt/cray/pe/gcc/11.2.0/snos/lib64:/soft/compilers/cudatoolkit/cuda-11.8.0/extras/CUPTI/lib64:/soft/libraries/trt/TensorRT-8.5.3.1.Linux.x86_64-gnu.cuda-11.8.cudnn8.6/lib:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/lib:/soft/libraries/cudnn/cudnn-11-linux-x64-v8.6.0.163/lib:/opt/cray/libfabric/1.15.2.0/lib64:/usr/lib64:/opt/cray/pe/papi/7.0.0.1/lib64:/home/am6429/veloc-build/lib:/home/am6429/veloc-build/lib64:/home/am6429/nvcomp/lib:/soft/datascience/conda/2023-01-10/mconda3/lib:/soft/datascience/conda/2023-01-10/mconda3/lib/; export http_proxy=http://proxy.alcf.anl.gov:3128; export https_proxy=http://proxy.alcf.anl.gov:3128; export CC=gcc; export CXX=g++; export IBV_FORK_SAFE=1; export CFLAGS=-I/soft/datascience/conda/2023-01-10/mconda3/include/; export LDFLAGS=-L/soft/datascience/conda/2023-01-10/mconda3/lib/; export CUDA_DEVICE_MAX_CONNECTIONS=1; export TORCHSNAPSHOT_PER_RANK_MEMORY_BUDGET_BYTES=34359738368; export _DEFAULT_MAX_PER_RANK_IO_CONCURRENCY=1; export _MAX_PER_RANK_IO_CONCURRENCY=1; export NSYS_REPORT_DIR=/home/am6429/dl-io/dl-io-outputs/act-output-30B//rep-30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch0-flush_async1-opt_gaps0-%n;  cd /home/am6429/dl-io/Megatron-DeepSpeed; /home/am6429/.conda/envs/dspeed_env/bin/python -u -m deepspeed.launcher.launch --world_info=eyJ4MzAwNmMwczE5YjFuMC5oc24uY20ucG9sYXJpcy5hbGNmLmFubC5nb3YiOiBbMCwgMSwgMiwgM10sICJ4MzAwNmMwczFiMW4wLmhzbi5jbS5wb2xhcmlzLmFsY2YuYW5sLmdvdiI6IFswLCAxLCAyLCAzXX0= --node_rank=%n --master_addr=10.140.57.107 --master_port=29700 /home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py --tensor-model-parallel-size 1 --num-layers 60 --hidden-size 6656 --num-attention-heads 52 --micro-batch-size 4 --global-batch-size 32 --seq-length 2048 --max-position-embeddings 2048 --train-iters 10 --save /local/scratch/llama2/zero3-tp1}_dp8 --data-path /home/am6429/dl-io/datasets/meg-gpt2_text_document --vocab-file /home/am6429/dl-io/datasets/gpt2-vocab.json --merge-file /home/am6429/dl-io/datasets/gpt2-merges.txt --data-impl mmap --tokenizer-type GPTSentencePieceTokenizer --tokenizer-model /home/am6429/dl-io/datasets/tokenizer.model --split 949,50,1 --distributed-backend nccl --lr 3e-4 --lr-decay-style cosine --min-lr 3e-5 --weight-decay 0.1 --clip-grad 1 --lr-warmup-iters 1 --optimizer adam --adam-beta1 0.9 --adam-beta2 0.95 --log-interval 1 --save-interval 1000 --eval-interval 1000 --eval-iters 0 --bf16 --no-query-key-layer-scaling --attention-dropout 0 --hidden-dropout 0 --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization rmsnorm --disable-bias-linear --num-key-value-heads 4 --deepspeed --exit-interval 20 --deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch0-flush_async1-opt_gaps0.json --zero-stage=3 --no-pipeline-parallel --cpu-optimizer --checkpoint-activations --deepspeed-activation-checkpointing
x3006c0s19b1n0: [2024-03-29 16:36:49,032] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b1n0: [2024-03-29 16:36:49,051] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-29 16:36:50,881] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3006c0s19b1n0: [2024-03-29 16:36:50,881] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=0
x3006c0s19b1n0: [2024-03-29 16:36:50,882] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b1n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3006c0s19b1n0: [2024-03-29 16:36:50,882] [INFO] [launch.py:163:main] dist_world_size=8
x3006c0s19b1n0: [2024-03-29 16:36:50,882] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3006c0s19b1n0: [2024-03-29 16:36:50,882] [INFO] [launch.py:253:main] process 42894 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch0-flush_async1-opt_gaps0.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-29 16:36:50,883] [INFO] [launch.py:253:main] process 42895 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch0-flush_async1-opt_gaps0.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-29 16:36:50,883] [INFO] [launch.py:253:main] process 42896 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch0-flush_async1-opt_gaps0.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-29 16:36:50,884] [INFO] [launch.py:253:main] process 42897 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch0-flush_async1-opt_gaps0.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b1n0: [2024-03-29 16:36:51,452] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3006c0s1b1n0: [2024-03-29 16:36:51,452] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=1
x3006c0s1b1n0: [2024-03-29 16:36:51,452] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b1n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3006c0s1b1n0: [2024-03-29 16:36:51,452] [INFO] [launch.py:163:main] dist_world_size=8
x3006c0s1b1n0: [2024-03-29 16:36:51,452] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3006c0s1b1n0: [2024-03-29 16:36:51,453] [INFO] [launch.py:253:main] process 61588 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch0-flush_async1-opt_gaps0.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b1n0: [2024-03-29 16:36:51,453] [INFO] [launch.py:253:main] process 61589 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch0-flush_async1-opt_gaps0.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b1n0: [2024-03-29 16:36:51,454] [INFO] [launch.py:253:main] process 61590 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch0-flush_async1-opt_gaps0.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b1n0: [2024-03-29 16:36:51,455] [INFO] [launch.py:253:main] process 61591 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch0-flush_async1-opt_gaps0.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-29 16:36:52,666] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-29 16:36:52,701] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-29 16:36:52,713] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-29 16:36:52,718] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b1n0: [2024-03-29 16:36:53,255] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b1n0: [2024-03-29 16:36:53,258] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b1n0: [2024-03-29 16:36:53,264] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b1n0: [2024-03-29 16:36:53,283] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: [2024-03-29 16:36:55,597] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: [2024-03-29 16:36:55,663] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: INFO: overriding default arguments for tokenizer_type:GPTSentencePieceTokenizer                    with tokenizer_type:GPT2BPETokenizer
x3006c0s19b1n0: using world size: 8, data-parallel-size: 8, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
x3006c0s19b1n0: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
x3006c0s19b1n0: using torch.bfloat16 for parameters ...
x3006c0s19b1n0: ------------------------ arguments ------------------------
x3006c0s19b1n0:   accumulate_allreduce_grads_in_fp32 .............. True
x3006c0s19b1n0:   adam_beta1 ...................................... 0.9
x3006c0s19b1n0:   adam_beta2 ...................................... 0.95
x3006c0s19b1n0:   adam_eps ........................................ 1e-08
x3006c0s19b1n0:   add_bias_linear ................................. False
x3006c0s19b1n0:   add_position_embedding .......................... False
x3006c0s19b1n0:   adlr_autoresume ................................. False
x3006c0s19b1n0:   adlr_autoresume_interval ........................ 1000
x3006c0s19b1n0:   aml_data_download_path .......................... None
x3006c0s19b1n0:   apply_layernorm_1p .............................. False
x3006c0s19b1n0:   apply_query_key_layer_scaling ................... False
x3006c0s19b1n0:   apply_residual_connection_post_layernorm ........ False
x3006c0s19b1n0:   async_tensor_model_parallel_allreduce ........... False
x3006c0s19b1n0:   attention_dropout ............................... 0.0
x3006c0s19b1n0:   attention_softmax_in_fp32 ....................... False
x3006c0s19b1n0:   barrier_with_L1_time ............................ True
x3006c0s19b1n0:   bert_binary_head ................................ True
x3006c0s19b1n0:   bert_embedder_type .............................. megatron
x3006c0s19b1n0:   bert_load ....................................... None
x3006c0s19b1n0:   bf16 ............................................ True
x3006c0s19b1n0:   bias_dropout_fusion ............................. True
x3006c0s19b1n0:   bias_gelu_fusion ................................ False
x3006c0s19b1n0:   biencoder_projection_dim ........................ 0
x3006c0s19b1n0:   biencoder_shared_query_context_model ............ False
x3006c0s19b1n0:   block_data_path ................................. None
x3006c0s19b1n0:   checkpoint_activations .......................... True
x3006c0s19b1n0:   checkpoint_in_cpu ............................... False
x3006c0s19b1n0:   checkpoint_num_layers ........................... 1
x3006c0s19b1n0:   classes_fraction ................................ 1.0
x3006c0s19b1n0:   clip_grad ....................................... 1.0
x3006c0s19b1n0:   compression_training ............................ False
x3006c0s19b1n0:   consumed_train_samples .......................... 0
x3006c0s19b1n0:   consumed_train_tokens ........................... 0
x3006c0s19b1n0:   consumed_valid_samples .......................... 0
x3006c0s19b1n0:   contigious_checkpointing ........................ False
x3006c0s19b1n0:   cpu_optimizer ................................... True
x3006c0s19b1n0:   cpu_torch_adam .................................. False
x3006c0s19b1n0:   create_moe_param_group .......................... False
x3006c0s19b1n0:   curriculum_learning_legacy ...................... False
x3006c0s19b1n0:   data_cache_path ................................. None
x3006c0s19b1n0:   data_efficiency_curriculum_learning ............. False
x3006c0s19b1n0:   data_impl ....................................... mmap
x3006c0s19b1n0:   data_parallel_random_init ....................... False
x3006c0s19b1n0:   data_parallel_size .............................. 8
x3006c0s19b1n0:   data_path ....................................... ['/home/am6429/dl-io/datasets/meg-gpt2_text_document']
x3006c0s19b1n0:   data_per_class_fraction ......................... 1.0
x3006c0s19b1n0:   data_sharding ................................... True
x3006c0s19b1n0:   dataloader_type ................................. single
x3006c0s19b1n0:   DDP_impl ........................................ local
x3006c0s19b1n0:   decoder_num_layers .............................. None
x3006c0s19b1n0:   decoder_seq_length .............................. None
x3006c0s19b1n0:   deepscale ....................................... False
x3006c0s19b1n0:   deepscale_config ................................ None
x3006c0s19b1n0:   deepspeed ....................................... True
x3006c0s19b1n0:   deepspeed_activation_checkpointing .............. True
x3006c0s19b1n0:   deepspeed_config ................................ /home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch0-flush_async1-opt_gaps0.json
x3006c0s19b1n0:   dino_bottleneck_size ............................ 256
x3006c0s19b1n0:   dino_freeze_last_layer .......................... 1
x3006c0s19b1n0:   dino_head_hidden_size ........................... 2048
x3006c0s19b1n0:   dino_local_crops_number ......................... 10
x3006c0s19b1n0:   dino_local_img_size ............................. 96
x3006c0s19b1n0:   dino_norm_last_layer ............................ False
x3006c0s19b1n0:   dino_teacher_temp ............................... 0.07
x3006c0s19b1n0:   dino_warmup_teacher_temp ........................ 0.04
x3006c0s19b1n0:   dino_warmup_teacher_temp_epochs ................. 30
x3006c0s19b1n0:   distribute_checkpointed_activations ............. False
x3006c0s19b1n0:   distribute_saved_activations .................... False
x3006c0s19b1n0:   distributed_backend ............................. nccl
x3006c0s19b1n0:   distributed_timeout_minutes ..................... 10
x3006c0s19b1n0:   ds_inference .................................... False
x3006c0s19b1n0:   ds_pipeline_enabled ............................. False
x3006c0s19b1n0:   ds_sequence_parallel_size ....................... 1
x3006c0s19b1n0:   embedding_path .................................. None
x3006c0s19b1n0:   embedding_weights_in_fp32 ....................... False
x3006c0s19b1n0:   empty_unused_memory_level ....................... 0
x3006c0s19b1n0:   enable_expert_tensor_parallelism ................ False
x3006c0s19b1n0:   encoder_num_layers .............................. 60
x3006c0s19b1n0:   encoder_seq_length .............................. 2048
x3006c0s19b1n0:   end_weight_decay ................................ 0.1
x3006c0s19b1n0:   eod_mask_loss ................................... False
x3006c0s19b1n0:   eval_interval ................................... 1000
x3006c0s19b1n0:   eval_iters ...................................... 0
x3006c0s19b1n0:   evidence_data_path .............................. None
x3006c0s19b1n0:   exit_duration_in_mins ........................... None
x3006c0s19b1n0:   exit_interval ................................... 20
x3006c0s19b1n0:   exit_on_missing_checkpoint ...................... False
x3006c0s19b1n0:   exit_signal_handler ............................. False
x3006c0s19b1n0:   expert_interval ................................. 2
x3006c0s19b1n0:   ffn_hidden_size ................................. 17728
x3006c0s19b1n0:   finetune ........................................ False
x3006c0s19b1n0:   force_ds_sequence_parallel ...................... False
x3006c0s19b1n0:   fp16 ............................................ False
x3006c0s19b1n0:   fp16_lm_cross_entropy ........................... False
x3006c0s19b1n0:   fp32_residual_connection ........................ False
x3006c0s19b1n0:   fp8_amax_compute_algo ........................... most_recent
x3006c0s19b1n0:   fp8_amax_history_len ............................ 1
x3006c0s19b1n0:   fp8_e4m3 ........................................ False
x3006c0s19b1n0:   fp8_hybrid ...................................... False
x3006c0s19b1n0:   fp8_interval .................................... 1
x3006c0s19b1n0:   fp8_margin ...................................... 0
x3006c0s19b1n0:   fp8_wgrad ....................................... True
x3006c0s19b1n0:   global_batch_size ............................... 32
x3006c0s19b1n0:   gradient_accumulation_fusion .................... True
x3006c0s19b1n0:   head_lr_mult .................................... 1.0
x3006c0s19b1n0:   hidden_dropout .................................. 0.0
x3006c0s19b1n0:   hidden_size ..................................... 6656
x3006c0s19b1n0:   hidden_size_teacher ............................. None
x3006c0s19b1n0:   hysteresis ...................................... 2
x3006c0s19b1n0:   ict_head_size ................................... None
x3006c0s19b1n0:   ict_load ........................................ None
x3006c0s19b1n0:   img_h ........................................... 224
x3006c0s19b1n0:   img_w ........................................... 224
x3006c0s19b1n0:   indexer_batch_size .............................. 128
x3006c0s19b1n0:   indexer_log_interval ............................ 1000
x3006c0s19b1n0:   inference ....................................... False
x3006c0s19b1n0:   inference_batch_times_seqlen_threshold .......... 512
x3006c0s19b1n0:   init_method_std ................................. 0.02
x3006c0s19b1n0:   init_method_xavier_uniform ...................... False
x3006c0s19b1n0:   initial_loss_scale .............................. 4294967296
x3006c0s19b1n0:   iter_per_epoch .................................. 1250
x3006c0s19b1n0:   kd .............................................. False
x3006c0s19b1n0:   kd_alpha_ce ..................................... 1
x3006c0s19b1n0:   kd_beta_ce ...................................... 1
x3006c0s19b1n0:   kd_temp ......................................... 1.0
x3006c0s19b1n0:   kv_channels ..................................... 128
x3006c0s19b1n0:   layernorm_epsilon ............................... 1e-05
x3006c0s19b1n0:   lazy_mpu_init ................................... None
x3006c0s19b1n0:   load ............................................ None
x3006c0s19b1n0:   load_teacher .................................... None
x3006c0s19b1n0:   local_rank ...................................... 0
x3006c0s19b1n0:   log_batch_size_to_tensorboard ................... False
x3006c0s19b1n0:   log_interval .................................... 1
x3006c0s19b1n0:   log_learning_rate_to_tensorboard ................ True
x3006c0s19b1n0:   log_loss_scale_to_tensorboard ................... True
x3006c0s19b1n0:   log_memory_to_tensorboard ....................... False
x3006c0s19b1n0:   log_num_zeros_in_grad ........................... False
x3006c0s19b1n0:   log_optimizer_states_to_tensorboard ............. False
x3006c0s19b1n0:   log_params_norm ................................. False
x3006c0s19b1n0:   log_timers_to_tensorboard ....................... False
x3006c0s19b1n0:   log_validation_ppl_to_tensorboard ............... False
x3006c0s19b1n0:   log_world_size_to_tensorboard ................... False
x3006c0s19b1n0:   loss_scale ...................................... None
x3006c0s19b1n0:   loss_scale_window ............................... 1000
x3006c0s19b1n0:   lr .............................................. 0.0003
x3006c0s19b1n0:   lr_decay_iters .................................. None
x3006c0s19b1n0:   lr_decay_samples ................................ None
x3006c0s19b1n0:   lr_decay_style .................................. cosine
x3006c0s19b1n0:   lr_decay_tokens ................................. None
x3006c0s19b1n0:   lr_warmup_fraction .............................. None
x3006c0s19b1n0:   lr_warmup_iters ................................. 1
x3006c0s19b1n0:   lr_warmup_samples ............................... 0
x3006c0s19b1n0:   lr_warmup_tokens ................................ None
x3006c0s19b1n0:   make_vocab_size_divisible_by .................... 128
x3006c0s19b1n0:   mask_factor ..................................... 1.0
x3006c0s19b1n0:   mask_prob ....................................... 0.15
x3006c0s19b1n0:   mask_type ....................................... random
x3006c0s19b1n0:   masked_softmax_fusion ........................... True
x3006c0s19b1n0:   max_position_embeddings ......................... 2048
x3006c0s19b1n0:   max_tokens_to_oom ............................... 12000
x3006c0s19b1n0:   memory_centric_tiled_linear ..................... False
x3006c0s19b1n0:   merge_file ...................................... /home/am6429/dl-io/datasets/gpt2-merges.txt
x3006c0s19b1n0:   micro_batch_size ................................ 4
x3006c0s19b1n0:   min_loss_scale .................................. 1.0
x3006c0s19b1n0:   min_lr .......................................... 3e-05
x3006c0s19b1n0:   mlp_type ........................................ standard
x3006c0s19b1n0:   mmap_warmup ..................................... False
x3006c0s19b1n0:   moe_eval_capacity_factor ........................ 1.0
x3006c0s19b1n0:   moe_expert_parallel_size ........................ 1
x3006c0s19b1n0:   moe_loss_coeff .................................. 0.1
x3006c0s19b1n0:   moe_min_capacity ................................ 4
x3006c0s19b1n0:   moe_token_dropping .............................. True
x3006c0s19b1n0:   moe_train_capacity_factor ....................... 1.0
x3006c0s19b1n0:   mos ............................................. False
x3006c0s19b1n0:   no_load_lr_state ................................ False
x3006c0s19b1n0:   no_load_optim ................................... None
x3006c0s19b1n0:   no_load_rng ..................................... None
x3006c0s19b1n0:   no_persist_layer_norm ........................... False
x3006c0s19b1n0:   no_pipeline_parallel ............................ True
x3006c0s19b1n0:   no_save_optim ................................... None
x3006c0s19b1n0:   no_save_rng ..................................... None
x3006c0s19b1n0:   normalization ................................... rmsnorm
x3006c0s19b1n0:   num_attention_heads ............................. 52
x3006c0s19b1n0:   num_attention_heads_teacher ..................... None
x3006c0s19b1n0:   num_channels .................................... 3
x3006c0s19b1n0:   num_classes ..................................... 1000
x3006c0s19b1n0:   num_experts ..................................... [1]
x3006c0s19b1n0:   num_experts_switch .............................. None
x3006c0s19b1n0:   num_experts_teacher ............................. [1]
x3006c0s19b1n0:   num_key_value_heads ............................. 4
x3006c0s19b1n0:   num_layers ...................................... 60
x3006c0s19b1n0:   num_layers_per_virtual_pipeline_stage ........... None
x3006c0s19b1n0:   num_layers_teacher .............................. None
x3006c0s19b1n0:   num_workers ..................................... 2
x3006c0s19b1n0:   onnx_safe ....................................... None
x3006c0s19b1n0:   openai_gelu ..................................... False
x3006c0s19b1n0:   optimizer ....................................... adam
x3006c0s19b1n0:   output_bert_embeddings .......................... False
x3006c0s19b1n0:   overlap_p2p_comm ................................ False
x3006c0s19b1n0:   override_opt_param_scheduler .................... False
x3006c0s19b1n0:   params_dtype .................................... torch.bfloat16
x3006c0s19b1n0:   partition_activations ........................... False
x3006c0s19b1n0:   patch_dim ....................................... 16
x3006c0s19b1n0:   perform_initialization .......................... True
x3006c0s19b1n0:   pipeline_model_parallel_size .................... 1
x3006c0s19b1n0:   pipeline_model_parallel_split_rank .............. None
x3006c0s19b1n0:   profile_backward ................................ False
x3006c0s19b1n0:   query_in_block_prob ............................. 0.1
x3006c0s19b1n0:   rampup_batch_size ............................... None
x3006c0s19b1n0:   random_ltd ...................................... False
x3006c0s19b1n0:   rank ............................................ 0
x3006c0s19b1n0:   recompute_granularity ........................... None
x3006c0s19b1n0:   recompute_method ................................ None
x3006c0s19b1n0:   recompute_num_layers ............................ 1
x3006c0s19b1n0:   remote_device ................................... none
x3006c0s19b1n0:   reset_attention_mask ............................ False
x3006c0s19b1n0:   reset_iteration ................................. False
x3006c0s19b1n0:   reset_position_ids .............................. False
x3006c0s19b1n0:   retriever_report_topk_accuracies ................ []
x3006c0s19b1n0:   retriever_score_scaling ......................... False
x3006c0s19b1n0:   retriever_seq_length ............................ 256
x3006c0s19b1n0:   retro_add_retriever ............................. False
x3006c0s19b1n0:   retro_cyclic_train_iters ........................ None
x3006c0s19b1n0:   retro_encoder_attention_dropout ................. 0.1
x3006c0s19b1n0:   retro_encoder_hidden_dropout .................... 0.1
x3006c0s19b1n0:   retro_encoder_layers ............................ 2
x3006c0s19b1n0:   retro_num_neighbors ............................. 2
x3006c0s19b1n0:   retro_num_retrieved_chunks ...................... 2
x3006c0s19b1n0:   retro_return_doc_ids ............................ False
x3006c0s19b1n0:   retro_workdir ................................... None
x3006c0s19b1n0:   return_data_index ............................... False
x3006c0s19b1n0:   rotary_percent .................................. 1.0
x3006c0s19b1n0:   sample_rate ..................................... 1.0
x3006c0s19b1n0:   save ............................................ /local/scratch/llama2/zero3-tp1}_dp8
x3006c0s19b1n0:   save_interval ................................... 1000
x3006c0s19b1n0:   scatter_gather_tensors_in_pipeline .............. True
x3006c0s19b1n0:   scattered_embeddings ............................ False
x3006c0s19b1n0:   seed ............................................ 1234
x3006c0s19b1n0:   seq_length ...................................... 2048
x3006c0s19b1n0:   sequence_parallel ............................... False
x3006c0s19b1n0:   sgd_momentum .................................... 0.9
x3006c0s19b1n0:   short_seq_prob .................................. 0.1
x3006c0s19b1n0:   skip_train ...................................... False
x3006c0s19b1n0:   split ........................................... 949,50,1
x3006c0s19b1n0:   split_transformers .............................. False
x3006c0s19b1n0:   squared_relu .................................... False
x3006c0s19b1n0:   standalone_embedding_stage ...................... False
x3006c0s19b1n0:   start_weight_decay .............................. 0.1
x3006c0s19b1n0:   swiglu .......................................... True
x3006c0s19b1n0:   swin_backbone_type .............................. tiny
x3006c0s19b1n0:   synchronize_each_layer .......................... False
x3006c0s19b1n0:   tensor_model_parallel_size ...................... 1
x3006c0s19b1n0:   tensorboard_dir ................................. None
x3006c0s19b1n0:   tensorboard_log_interval ........................ 1
x3006c0s19b1n0:   tensorboard_queue_size .......................... 1000
x3006c0s19b1n0:   test_data_path .................................. None
x3006c0s19b1n0:   tile_factor ..................................... 1
x3006c0s19b1n0:   timing_log_level ................................ 0
x3006c0s19b1n0:   timing_log_option ............................... minmax
x3006c0s19b1n0:   titles_data_path ................................ None
x3006c0s19b1n0:   tokenizer_model ................................. /home/am6429/dl-io/datasets/tokenizer.model
x3006c0s19b1n0:   tokenizer_type .................................. GPT2BPETokenizer
x3006c0s19b1n0:   topk ............................................ 1
x3006c0s19b1n0:   train_data_exact_num_epochs ..................... None
x3006c0s19b1n0:   train_data_path ................................. None
x3006c0s19b1n0:   train_desc_path ................................. None
x3006c0s19b1n0:   train_doc_idx_path .............................. None
x3006c0s19b1n0:   train_idx_path .................................. None
x3006c0s19b1n0:   train_iters ..................................... 10
x3006c0s19b1n0:   train_sample_idx_path ........................... None
x3006c0s19b1n0:   train_samples ................................... None
x3006c0s19b1n0:   train_shuffle_idx_path .......................... None
x3006c0s19b1n0:   train_tokens .................................... None
x3006c0s19b1n0:   transformer_impl ................................ local
x3006c0s19b1n0:   transformer_pipeline_model_parallel_size ........ 1
x3006c0s19b1n0:   untie_embeddings_and_output_weights ............. True
x3006c0s19b1n0:   use_checkpoint_args ............................. False
x3006c0s19b1n0:   use_checkpoint_opt_param_scheduler .............. False
x3006c0s19b1n0:   use_contiguous_buffers_in_local_ddp ............. True
x3006c0s19b1n0:   use_cpu_initialization .......................... None
x3006c0s19b1n0:   use_dataset_only ................................ False
x3006c0s19b1n0:   use_distributed_optimizer ....................... False
x3006c0s19b1n0:   use_flash_attn .................................. False
x3006c0s19b1n0:   use_flash_attn_triton ........................... False
x3006c0s19b1n0:   use_flash_attn_v1 ............................... False
x3006c0s19b1n0:   use_flash_attn_v2 ............................... False
x3006c0s19b1n0:   use_one_sent_docs ............................... False
x3006c0s19b1n0:   use_pin_memory .................................. False
x3006c0s19b1n0:   use_ring_exchange_p2p ........................... False
x3006c0s19b1n0:   use_rotary_position_embeddings .................. True
x3006c0s19b1n0:   use_tutel ....................................... False
x3006c0s19b1n0:   valid_data_path ................................. None
x3006c0s19b1n0:   variable_seq_lengths ............................ False
x3006c0s19b1n0:   virtual_pipeline_model_parallel_size ............ None
x3006c0s19b1n0:   vision_backbone_type ............................ vit
x3006c0s19b1n0:   vision_pretraining .............................. False
x3006c0s19b1n0:   vision_pretraining_type ......................... classify
x3006c0s19b1n0:   vocab_extra_ids ................................. 0
x3006c0s19b1n0:   vocab_file ...................................... /home/am6429/dl-io/datasets/gpt2-vocab.json
x3006c0s19b1n0:   vocab_size ...................................... None
x3006c0s19b1n0:   weight_decay .................................... 0.1
x3006c0s19b1n0:   weight_decay_incr_style ......................... constant
x3006c0s19b1n0:   world_size ...................................... 8
x3006c0s19b1n0:   zero_allgather_bucket_size ...................... 0.0
x3006c0s19b1n0:   zero_contigious_gradients ....................... False
x3006c0s19b1n0:   zero_reduce_bucket_size ......................... 0.0
x3006c0s19b1n0:   zero_reduce_scatter ............................. False
x3006c0s19b1n0:   zero_stage ...................................... 3
x3006c0s19b1n0: -------------------- end of arguments ---------------------
x3006c0s19b1n0: setting number of micro-batches to constant 1
x3006c0s19b1n0: > building GPT2BPETokenizer tokenizer ...
x3006c0s19b1n0: [2024-03-29 16:36:55,703] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
x3006c0s19b1n0: > initializing torch distributed ...
x3006c0s19b1n0: [2024-03-29 16:36:55,732] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: [2024-03-29 16:36:55,732] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b1n0:       meet the required dependencies to JIT install the op.
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: JIT compiled ops requires ninja
x3006c0s1b1n0: ninja .................. [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: op name ................ installed .. compatible
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: DeepSpeed general environment info:
x3006c0s1b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b1n0: torch version .................... 2.0.1+cu118
x3006c0s1b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b1n0: torch cuda version ............... 11.8
x3006c0s1b1n0: torch hip version ................ None
x3006c0s1b1n0: nvcc version ..................... 11.8
x3006c0s1b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b1n0:       meet the required dependencies to JIT install the op.
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: JIT compiled ops requires ninja
x3006c0s1b1n0: ninja .................. [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: op name ................ installed .. compatible
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b1n0:       meet the required dependencies to JIT install the op.
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: JIT compiled ops requires ninja
x3006c0s1b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b1n0: ninja .................. [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: op name ................ installed .. compatible
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b1n0:       meet the required dependencies to JIT install the op.
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: JIT compiled ops requires ninja
x3006c0s1b1n0: ninja .................. [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: op name ................ installed .. compatible
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b1n0: [2024-03-29 16:36:56,120] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: DeepSpeed general environment info:
x3006c0s1b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b1n0: torch version .................... 2.0.1+cu118
x3006c0s1b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b1n0: torch cuda version ............... 11.8
x3006c0s1b1n0: torch hip version ................ None
x3006c0s1b1n0: nvcc version ..................... 11.8
x3006c0s1b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b1n0: DeepSpeed general environment info:
x3006c0s1b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b1n0: torch version .................... 2.0.1+cu118
x3006c0s1b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b1n0: torch cuda version ............... 11.8
x3006c0s1b1n0: torch hip version ................ None
x3006c0s1b1n0: nvcc version ..................... 11.8
x3006c0s1b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: DeepSpeed general environment info:
x3006c0s1b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b1n0: torch version .................... 2.0.1+cu118
x3006c0s1b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b1n0: torch cuda version ............... 11.8
x3006c0s1b1n0: torch hip version ................ None
x3006c0s1b1n0: nvcc version ..................... 11.8
x3006c0s1b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b1n0: [2024-03-29 16:36:56,287] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b1n0: [2024-03-29 16:36:56,290] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b1n0: [2024-03-29 16:36:56,296] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: > initialized tensor model parallel with size 1
x3006c0s19b1n0: > initialized pipeline model parallel with size 1
x3006c0s19b1n0: > setting random seeds to 1234 ...
x3006c0s19b1n0: [2024-03-29 16:36:57,169] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
x3006c0s19b1n0: > compiling dataset index builder ...
x3006c0s19b1n0: make: Entering directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3006c0s19b1n0: make: Nothing to be done for 'default'.
x3006c0s19b1n0: make: Leaving directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3006c0s19b1n0: >>> done with dataset index builder. Compilation time: 0.083 seconds
x3006c0s19b1n0: > compiling and loading fused kernels ...
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: >>> done with compiling and loading fused kernels. Compilation time: 3.478 seconds
x3006c0s19b1n0: <<<<<<<<<<< 2
x3006c0s19b1n0: <<<<<<<<<<< 1
x3006c0s1b1n0: <<<<<<<<<<< 4
x3006c0s19b1n0: <<<<<<<<<<< 3
x3006c0s1b1n0: <<<<<<<<<<< 5
x3006c0s19b1n0: initialize_megatron took 5.843274354934692
x3006c0s19b1n0: <<<<<<<<<<< 0
x3006c0s1b1n0: <<<<<<<<<<< 6
x3006c0s1b1n0: <<<<<<<<<<< 7
x3006c0s19b1n0: time to initialize megatron (seconds): 6.278
x3006c0s19b1n0: [after megatron is initialized] datetime: 2024-03-29 16:37:01 
x3006c0s19b1n0: get_accelerator and all_reduce  took 0.001348257064819336
x3006c0s19b1n0: building GPT model ...
x3006c0s19b1n0: [2024-03-29 16:37:01,601] [INFO] [utils.py:800:see_memory_usage] Before Building Model
x3006c0s19b1n0: [2024-03-29 16:37:01,601] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 4.62 GB         CA 0.0 GB         Max_CA 5 GB 
x3006c0s19b1n0: [2024-03-29 16:37:01,602] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 22.95 GB, percent = 4.6%
x3006c0s19b1n0: [2024-03-29 16:37:07,756] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 423, num_elems = 27.64B
x3006c0s19b1n0: [2024-03-29 16:37:07,821] [INFO] [utils.py:800:see_memory_usage] After Building Model
x3006c0s19b1n0: [2024-03-29 16:37:07,822] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 7.07 GB         CA 21.86 GB         Max_CA 38 GB 
x3006c0s19b1n0: [2024-03-29 16:37:07,822] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 23.38 GB, percent = 4.6%
x3006c0s19b1n0:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 27635239424
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: Time to load cpu_adam op: 2.462770700454712 seconds
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: Time to load cpu_adam op: 2.439394235610962 seconds
x3006c0s19b1n0: Time to load cpu_adam op: 2.5865907669067383 seconds
x3006c0s19b1n0: Time to load cpu_adam op: 2.4914965629577637 seconds
x3006c0s1b1n0: ninja: no work to do.
x3006c0s1b1n0: Time to load cpu_adam op: 2.449951410293579 seconds
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: > learning rate decay style: cosine
x3006c0s19b1n0: DeepSpeed is enabled.
x3006c0s19b1n0: [2024-03-29 16:37:12,264] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.3+8074cd62, git-hash=8074cd62, git-branch=hybrid_opt_offload
x3006c0s1b1n0: ninja: no work to do.
x3006c0s1b1n0: Time to load cpu_adam op: 2.5233826637268066 seconds
x3006c0s19b1n0: [2024-03-29 16:37:12,336] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After args sanity test
x3006c0s19b1n0: [2024-03-29 16:37:12,337] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:37:12,337] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.4 GB, percent = 6.0%
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s1b1n0: Time to load cpu_adam op: 2.4551827907562256 seconds
x3006c0s1b1n0: Time to load cpu_adam op: 2.456080675125122 seconds
x3006c0s19b1n0: [2024-03-29 16:37:12,394] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure distributed model
x3006c0s19b1n0: [2024-03-29 16:37:12,394] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:37:12,394] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.6 GB, percent = 6.1%
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-29 16:37:12,455] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After configure distributed model
x3006c0s19b1n0: [2024-03-29 16:37:12,455] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:37:12,455] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.79 GB, percent = 6.1%
x3006c0s19b1n0: [2024-03-29 16:37:12,456] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-29 16:37:12,508] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After setting model parameters
x3006c0s19b1n0: [2024-03-29 16:37:12,508] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:37:12,508] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.8 GB, percent = 6.1%
x3006c0s19b1n0: [2024-03-29 16:37:12,561] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure optimizer
x3006c0s19b1n0: [2024-03-29 16:37:12,562] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:37:12,562] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.8 GB, percent = 6.1%
x3006c0s19b1n0: [2024-03-29 16:37:12,562] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
x3006c0s19b1n0: [2024-03-29 16:37:12,562] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
x3006c0s19b1n0: [2024-03-29 16:37:12,580] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
x3006c0s19b1n0: [2024-03-29 16:37:12,580] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
x3006c0s19b1n0: [2024-03-29 16:37:12,580] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
x3006c0s19b1n0: [2024-03-29 16:37:12,580] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
x3006c0s19b1n0: [2024-03-29 16:37:12,632] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
x3006c0s19b1n0: [2024-03-29 16:37:12,633] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:37:12,633] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.8 GB, percent = 6.1%
x3006c0s19b1n0: [2024-03-29 16:37:12,635] [INFO] [stage3.py:137:__init__] Reduce bucket size 500,000,000
x3006c0s19b1n0: [2024-03-29 16:37:12,635] [INFO] [stage3.py:138:__init__] Prefetch bucket size 50,000,000
x3006c0s19b1n0: [2024-03-29 16:37:12,688] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
x3006c0s19b1n0: [2024-03-29 16:37:12,688] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:37:12,689] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.8 GB, percent = 6.1%
x3006c0s19b1n0: Parameter Offload: Total persistent parameters: 805376 in 121 params
x3006c0s19b1n0: [2024-03-29 16:37:12,766] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
x3006c0s19b1n0: [2024-03-29 16:37:12,767] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:37:12,767] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.8 GB, percent = 6.1%
x3006c0s19b1n0: [2024-03-29 16:37:12,822] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
x3006c0s19b1n0: [2024-03-29 16:37:12,823] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:37:12,823] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.8 GB, percent = 6.1%
x3006c0s1b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s1b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s1b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s1b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-29 16:37:13,222] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-29 16:37:13,222] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b1n0: [2024-03-29 16:37:13,222] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b1n0: [2024-03-29 16:37:13,222] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b1n0: [2024-03-29 16:37:13,222] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b1n0: [2024-03-29 16:37:13,222] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-29 16:37:13,222] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-29 16:37:13,222] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-29 16:37:13,222] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-29 16:37:13,222] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-29 16:37:13,222] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-29 16:37:13,222] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b1n0: [2024-03-29 16:37:13,222] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b1n0: [2024-03-29 16:37:13,222] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b1n0: [2024-03-29 16:37:13,222] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b1n0: [2024-03-29 16:37:13,222] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,223] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,224] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,225] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,226] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:37:13,227] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:37:13,228] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-29 16:37:13,228] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-29 16:37:13,228] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-29 16:37:13,228] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b1n0: [2024-03-29 16:37:13,228] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b1n0: [2024-03-29 16:37:13,228] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b1n0: [2024-03-29 16:37:13,228] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b1n0: [2024-03-29 16:37:13,228] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-29 16:37:15,438] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 31
x3006c0s19b1n0: [2024-03-29 16:37:15,439] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.44 GB         CA 6.44 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:37:15,439] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 50.15 GB, percent = 10.0%
x3006c0s19b1n0: [2024-03-29 16:37:15,505] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
x3006c0s19b1n0: [2024-03-29 16:37:15,506] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-29 16:37:15,506] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 50.15 GB, percent = 10.0%
x3006c0s19b1n0: [2024-03-29 16:37:18,221] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
x3006c0s19b1n0: [2024-03-29 16:37:18,222] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-29 16:37:18,222] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 64.77 GB, percent = 12.9%
x3006c0s19b1n0: [2024-03-29 16:37:19,801] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
x3006c0s19b1n0: [2024-03-29 16:37:19,801] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-29 16:37:19,802] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 85.71 GB, percent = 17.0%
x3006c0s19b1n0: [2024-03-29 16:37:26,411] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | init_optimizer_state: 6594.74
x3006c0s19b1n0: [2024-03-29 16:37:26,515] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
x3006c0s19b1n0: [2024-03-29 16:37:26,515] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-29 16:37:26,516] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 146.58 GB, percent = 29.1%
x3006c0s19b1n0: [2024-03-29 16:37:26,544] [INFO] [stage3.py:520:_setup_for_real_optimizer] optimizer state initialized
x3006c0s19b1n0: [2024-03-29 16:37:34,195] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
x3006c0s19b1n0: [2024-03-29 16:37:34,196] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 8.61 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-29 16:37:34,196] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 178.49 GB, percent = 35.5%
x3006c0s19b1n0: [2024-03-29 16:37:34,196] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
x3006c0s19b1n0: [2024-03-29 16:37:34,263] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure LR scheduler
x3006c0s19b1n0: [2024-03-29 16:37:34,264] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-29 16:37:34,264] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 178.51 GB, percent = 35.5%
x3006c0s19b1n0: [2024-03-29 16:37:34,264] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
x3006c0s19b1n0: [2024-03-29 16:37:34,264] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7ff190c5c1c0>
x3006c0s19b1n0: [2024-03-29 16:37:34,264] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-29 16:37:34,330] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before rewriting optimizer step
x3006c0s19b1n0: [2024-03-29 16:37:34,330] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-29 16:37:34,330] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 178.51 GB, percent = 35.5%
x3006c0s19b1n0: [2024-03-29 16:37:34,395] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure checkpointing
x3006c0s19b1n0: [2024-03-29 16:37:34,396] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-29 16:37:34,396] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 178.51 GB, percent = 35.5%
x3006c0s19b1n0: [2024-03-29 16:37:34,396] [INFO] [config.py:998:print] DeepSpeedEngine configuration:
x3006c0s19b1n0: [2024-03-29 16:37:34,396] [INFO] [config.py:1002:print]   activation_checkpointing_config  {
x3006c0s19b1n0:     "partition_activations": false, 
x3006c0s19b1n0:     "contiguous_memory_optimization": false, 
x3006c0s19b1n0:     "cpu_checkpointing": false, 
x3006c0s19b1n0:     "number_checkpoints": null, 
x3006c0s19b1n0:     "synchronize_checkpoint_boundary": false, 
x3006c0s19b1n0:     "profile": false
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-29 16:37:34,396] [INFO] [config.py:1002:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
x3006c0s19b1n0: [2024-03-29 16:37:34,396] [INFO] [config.py:1002:print]   amp_enabled .................. False
x3006c0s19b1n0: [2024-03-29 16:37:34,396] [INFO] [config.py:1002:print]   amp_params ................... False
x3006c0s19b1n0: [2024-03-29 16:37:34,396] [INFO] [config.py:1002:print]   autotuning_config ............ {
x3006c0s19b1n0:     "enabled": false, 
x3006c0s19b1n0:     "start_step": null, 
x3006c0s19b1n0:     "end_step": null, 
x3006c0s19b1n0:     "metric_path": null, 
x3006c0s19b1n0:     "arg_mappings": null, 
x3006c0s19b1n0:     "metric": "throughput", 
x3006c0s19b1n0:     "model_info": null, 
x3006c0s19b1n0:     "results_dir": "autotuning_results", 
x3006c0s19b1n0:     "exps_dir": "autotuning_exps", 
x3006c0s19b1n0:     "overwrite": true, 
x3006c0s19b1n0:     "fast": true, 
x3006c0s19b1n0:     "start_profile_step": 3, 
x3006c0s19b1n0:     "end_profile_step": 5, 
x3006c0s19b1n0:     "tuner_type": "gridsearch", 
x3006c0s19b1n0:     "tuner_early_stopping": 5, 
x3006c0s19b1n0:     "tuner_num_trials": 50, 
x3006c0s19b1n0:     "model_info_path": null, 
x3006c0s19b1n0:     "mp_size": 1, 
x3006c0s19b1n0:     "max_train_batch_size": null, 
x3006c0s19b1n0:     "min_train_batch_size": 1, 
x3006c0s19b1n0:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
x3006c0s19b1n0:     "min_train_micro_batch_size_per_gpu": 1, 
x3006c0s19b1n0:     "num_tuning_micro_batch_sizes": 3
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-29 16:37:34,396] [INFO] [config.py:1002:print]   bfloat16_enabled ............. True
x3006c0s19b1n0: [2024-03-29 16:37:34,396] [INFO] [config.py:1002:print]   bfloat16_immediate_grad_update  False
x3006c0s19b1n0: [2024-03-29 16:37:34,396] [INFO] [config.py:1002:print]   checkpoint_parallel_write_pipeline  False
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   checkpoint_tag_validation_enabled  True
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   checkpoint_tag_validation_fail  False
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ff190c5cb20>
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   communication_data_type ...... None
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   curriculum_enabled_legacy .... False
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   curriculum_params_legacy ..... False
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   data_efficiency_enabled ...... False
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   dataloader_drop_last ......... False
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   datastates_config ............ {
x3006c0s19b1n0:     "enabled": null, 
x3006c0s19b1n0:     "config": {
x3006c0s19b1n0:     }
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   disable_allgather ............ False
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   dump_state ................... False
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   dynamic_loss_scale_args ...... None
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   eigenvalue_enabled ........... False
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   eigenvalue_gas_boundary_resolution  1
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   eigenvalue_layer_name ........ bert.encoder.layer
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   eigenvalue_layer_num ......... 0
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   eigenvalue_max_iter .......... 100
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   eigenvalue_stability ......... 1e-06
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   eigenvalue_tol ............... 0.01
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   eigenvalue_verbose ........... False
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   elasticity_enabled ........... False
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   flops_profiler_config ........ {
x3006c0s19b1n0:     "enabled": false, 
x3006c0s19b1n0:     "recompute_fwd_factor": 0.0, 
x3006c0s19b1n0:     "profile_step": 1, 
x3006c0s19b1n0:     "module_depth": -1, 
x3006c0s19b1n0:     "top_modules": 1, 
x3006c0s19b1n0:     "detailed": true, 
x3006c0s19b1n0:     "output_file": null
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   fp16_auto_cast ............... None
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   fp16_enabled ................. False
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   fp16_master_weights_and_gradients  False
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   global_rank .................. 0
x3006c0s19b1n0: [2024-03-29 16:37:34,397] [INFO] [config.py:1002:print]   grad_accum_dtype ............. bf16
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   gradient_accumulation_steps .. 1
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   gradient_clipping ............ 0.0
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   gradient_predivide_factor .... 1.0
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   graph_harvesting ............. False
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   initial_dynamic_scale ........ 1
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   load_universal_checkpoint .... False
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   loss_scale ................... 1.0
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   memory_breakdown ............. True
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   mics_hierarchial_params_gather  False
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   mics_shard_size .............. -1
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   nebula_config ................ {
x3006c0s19b1n0:     "enabled": false, 
x3006c0s19b1n0:     "persistent_storage_path": null, 
x3006c0s19b1n0:     "persistent_time_interval": 100, 
x3006c0s19b1n0:     "num_of_version_in_retention": 2, 
x3006c0s19b1n0:     "enable_nebula_load": true, 
x3006c0s19b1n0:     "load_path": null
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   optimizer_legacy_fusion ...... False
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   optimizer_name ............... None
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   optimizer_params ............. None
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   pld_enabled .................. False
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   pld_params ................... False
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   prescale_gradients ........... False
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   scheduler_name ............... None
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   scheduler_params ............. None
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   seq_parallel_communication_data_type  torch.float32
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   sparse_attention ............. None
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   sparse_gradients_enabled ..... False
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   steps_per_print .............. 1
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   train_batch_size ............. 32
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   train_micro_batch_size_per_gpu  4
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   use_data_before_expert_parallel_  False
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   use_node_local_storage ....... False
x3006c0s19b1n0: [2024-03-29 16:37:34,398] [INFO] [config.py:1002:print]   wall_clock_breakdown ......... True
x3006c0s19b1n0: [2024-03-29 16:37:34,399] [INFO] [config.py:1002:print]   weight_quantization_config ... None
x3006c0s19b1n0: [2024-03-29 16:37:34,399] [INFO] [config.py:1002:print]   world_size ................... 8
x3006c0s19b1n0: [2024-03-29 16:37:34,399] [INFO] [config.py:1002:print]   zero_allow_untested_optimizer  False
x3006c0s19b1n0: [2024-03-29 16:37:34,399] [INFO] [config.py:1002:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0, prefetch_optimizer=False, part_grads_async=True, prefetch_optimizer_gap=0) sub_group_size=100000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
x3006c0s19b1n0: [2024-03-29 16:37:34,399] [INFO] [config.py:1002:print]   zero_enabled ................. True
x3006c0s19b1n0: [2024-03-29 16:37:34,399] [INFO] [config.py:1002:print]   zero_force_ds_cpu_optimizer .. True
x3006c0s19b1n0: [2024-03-29 16:37:34,399] [INFO] [config.py:1002:print]   zero_optimization_stage ...... 3
x3006c0s19b1n0: [2024-03-29 16:37:34,399] [INFO] [config.py:988:print_user_config]   json = {
x3006c0s19b1n0:     "train_batch_size": 32, 
x3006c0s19b1n0:     "train_micro_batch_size_per_gpu": 4, 
x3006c0s19b1n0:     "steps_per_print": 1, 
x3006c0s19b1n0:     "zero_optimization": {
x3006c0s19b1n0:         "stage": 3, 
x3006c0s19b1n0:         "offload_optimizer": {
x3006c0s19b1n0:             "device": "cpu", 
x3006c0s19b1n0:             "ratio": 1, 
x3006c0s19b1n0:             "pin_memory": true, 
x3006c0s19b1n0:             "prefetch_optimizer": 0, 
x3006c0s19b1n0:             "part_grads_async": 1, 
x3006c0s19b1n0:             "prefetch_optimizer_gap": 0
x3006c0s19b1n0:         }, 
x3006c0s19b1n0:         "sub_group_size": 1.000000e+08
x3006c0s19b1n0:     }, 
x3006c0s19b1n0:     "bf16": {
x3006c0s19b1n0:         "enabled": true
x3006c0s19b1n0:     }, 
x3006c0s19b1n0:     "data_types": {
x3006c0s19b1n0:         "grad_accum_dtype": "bf16"
x3006c0s19b1n0:     }, 
x3006c0s19b1n0:     "wall_clock_breakdown": true, 
x3006c0s19b1n0:     "memory_breakdown": true, 
x3006c0s19b1n0:     "flops_profiler": {
x3006c0s19b1n0:         "enabled": false
x3006c0s19b1n0:     }
x3006c0s19b1n0: }
x3006c0s1b1n0: <TIMER:model-and-optimizer-setup,33.980284214019775>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,33.9809730052948>
x3006c0s1b1n0: <TIMER:model-and-optimizer-setup,33.98153209686279>
x3006c0s1b1n0: <TIMER:model-and-optimizer-setup,33.98166012763977>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,33.98189067840576>
x3006c0s1b1n0: <TIMER:model-and-optimizer-setup,33.98193144798279>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,33.982460737228394>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,33.98275566101074>
x3006c0s19b1n0: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-29 16:37:35 
x3006c0s19b1n0: > building train, validation, and test datasets ...
x3006c0s19b1n0:  > datasets target sizes (minimum size):
x3006c0s19b1n0:     train:      320
x3006c0s19b1n0:     validation: 0
x3006c0s19b1n0:     test:       0
x3006c0s19b1n0: > building train, validation, and test datasets for GPT ...
x3006c0s19b1n0: Single data path provided for train, valid & test
x3006c0s19b1n0:  > building dataset index ...
x3006c0s19b1n0:     reading sizes...
x3006c0s19b1n0:     reading pointers...
x3006c0s19b1n0:     reading document index...
x3006c0s19b1n0:     creating numpy buffer of mmap...
x3006c0s19b1n0:     creating memory view of numpy buffer...
x3006c0s19b1n0:  > finished creating indexed dataset in 0.001156 seconds
x3006c0s19b1n0:     number of documents: 79000
x3006c0s19b1n0:  > dataset split:
x3006c0s19b1n0:     train:
x3006c0s19b1n0:      document indices in [0, 74971) total of 74971 documents
x3006c0s19b1n0:     validation:
x3006c0s19b1n0:      document indices in [74971, 78921) total of 3950 documents
x3006c0s19b1n0:     test:
x3006c0s19b1n0:      document indices in [78921, 79000) total of 79 documents
x3006c0s19b1n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_doc_idx.npy
x3006c0s19b1n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_sample_idx.npy
x3006c0s19b1n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_shuffle_idx.npy
x3006c0s19b1n0:     loaded indexed file in 0.003 seconds
x3006c0s19b1n0:     total number of samples: 108448
x3006c0s19b1n0:     total number of epochs: 1
x3006c0s19b1n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_doc_idx.npy
x3006c0s19b1n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_sample_idx.npy
x3006c0s19b1n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_shuffle_idx.npy
x3006c0s19b1n0:     loaded indexed file in 0.004 seconds
x3006c0s19b1n0:     total number of samples: 5792
x3006c0s19b1n0:     total number of epochs: 1
x3006c0s19b1n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_doc_idx.npy
x3006c0s19b1n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_sample_idx.npy
x3006c0s19b1n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_shuffle_idx.npy
x3006c0s19b1n0:     loaded indexed file in 0.003 seconds
x3006c0s19b1n0:     total number of samples: 185
x3006c0s19b1n0:     total number of epochs: 1
x3006c0s19b1n0: > finished creating GPT datasets ...
x3006c0s1b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5084011554718018>
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5210931301116943>
x3006c0s1b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5283098220825195>
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5453522205352783>
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5507044792175293>
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5974550247192383>
x3006c0s1b1n0: <TIMER:train/valid/test-data-iterators-setup,0.6405141353607178>
x3006c0s1b1n0: <TIMER:train/valid/test-data-iterators-setup,0.6600568294525146>
x3006c0s19b1n0: [after dataloaders are built] datetime: 2024-03-29 16:37:36 
x3006c0s19b1n0: done with setup ...
x3006c0s19b1n0: training ...
x3006c0s1b1n0: (min, max) time across ranks (ms):
x3006c0s1b1n0:     model-and-optimizer-setup ......................: (33980.29, 33982.76)
x3006c0s1b1n0:     train/valid/test-data-iterators-setup ..........: (508.40, 660.06)
x3006c0s19b1n0: [before training begins] datetime: 2024-03-29 16:37:36 
x3006c0s19b1n0: [before the start of training step] datetime: 2024-03-29 16:37:36 
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:37:36,370] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:37:36,370] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 7.45 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-29 16:37:36,370] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 181.44 GB, percent = 36.1%
x3006c0s19b1n0: [2024-03-29 16:37:36,490] [INFO] [checkpointing.py:539:forward] Activation Checkpointing Information
x3006c0s19b1n0: [2024-03-29 16:37:36,490] [INFO] [checkpointing.py:540:forward] ----Partition Activations False, CPU CHECKPOINTING False
x3006c0s19b1n0: [2024-03-29 16:37:36,490] [INFO] [checkpointing.py:541:forward] ----contiguous Memory Checkpointing False with 60 total layers
x3006c0s19b1n0: [2024-03-29 16:37:36,490] [INFO] [checkpointing.py:543:forward] ----Synchronization False
x3006c0s19b1n0: [2024-03-29 16:37:36,490] [INFO] [checkpointing.py:544:forward] ----Profiling time in checkpointing False
x3006c0s19b1n0: [2024-03-29 16:37:44,363] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:37:44,364] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 19.76 GB         CA 22.95 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-29 16:37:44,364] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 181.6 GB, percent = 36.1%
x3006c0s19b1n0: [2024-03-29 16:37:44,541] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:37:44,542] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 16.84 GB         CA 16.91 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-29 16:37:44,542] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 181.6 GB, percent = 36.1%
x3006c0s19b1n0: [2024-03-29 16:38:08,668] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:38:08,669] [INFO] [utils.py:801:see_memory_usage] MA 10.25 GB         Max_MA 20.43 GB         CA 10.33 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-29 16:38:08,669] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 181.62 GB, percent = 36.1%
x3006c0s19b1n0: [2024-03-29 16:38:08,739] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:38:08,739] [INFO] [utils.py:801:see_memory_usage] MA 10.25 GB         Max_MA 10.25 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:38:08,739] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 181.69 GB, percent = 36.1%
x3006c0s1b1n0: [2024-03-29 16:38:17,910] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.065183877944946
x3006c0s1b1n0: [2024-03-29 16:38:17,910] [INFO] [stage3.py:2251:step] Full outer step loop took 9.065470457077026
x3006c0s1b1n0: [2024-03-29 16:38:18,180] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.33510947227478
x3006c0s1b1n0: [2024-03-29 16:38:18,181] [INFO] [stage3.py:2251:step] Full outer step loop took 9.335978269577026
x3006c0s19b1n0: [2024-03-29 16:38:18,255] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.410258293151855
x3006c0s19b1n0: [2024-03-29 16:38:18,255] [INFO] [stage3.py:2251:step] Full outer step loop took 9.410637378692627
x3006c0s1b1n0: [2024-03-29 16:38:18,273] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.427959203720093
x3006c0s1b1n0: [2024-03-29 16:38:18,273] [INFO] [stage3.py:2251:step] Full outer step loop took 9.428210496902466
x3006c0s1b1n0: [2024-03-29 16:38:18,280] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.434955596923828
x3006c0s1b1n0: [2024-03-29 16:38:18,280] [INFO] [stage3.py:2251:step] Full outer step loop took 9.43516230583191
x3006c0s19b1n0: [2024-03-29 16:38:18,352] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.507299900054932
x3006c0s19b1n0: [2024-03-29 16:38:18,353] [INFO] [stage3.py:2251:step] Full outer step loop took 9.507817029953003
x3006c0s19b1n0: [2024-03-29 16:38:18,363] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.518327951431274
x3006c0s19b1n0: [2024-03-29 16:38:18,364] [INFO] [stage3.py:2251:step] Full outer step loop took 9.518538475036621
x3006c0s19b1n0: [2024-03-29 16:38:18,367] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.522114992141724
x3006c0s19b1n0: [2024-03-29 16:38:18,367] [INFO] [stage3.py:2251:step] Full outer step loop took 9.522299766540527
x3006c0s19b1n0: [2024-03-29 16:38:18,378] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 9518.80
x3006c0s1b1n0: [2024-03-29 16:38:18,378] [INFO] [stage3.py:2277:step] End to end step took 9.532799005508423
x3006c0s1b1n0: [2024-03-29 16:38:18,378] [INFO] [stage3.py:2277:step] End to end step took 9.532713890075684
x3006c0s19b1n0: [2024-03-29 16:38:18,378] [INFO] [stage3.py:2277:step] End to end step took 9.533088684082031
x3006c0s19b1n0: [2024-03-29 16:38:18,378] [INFO] [stage3.py:2277:step] End to end step took 9.53312373161316
x3006c0s19b1n0: [2024-03-29 16:38:18,378] [WARNING] [stage3.py:2267:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3006c0s19b1n0: [2024-03-29 16:38:18,378] [INFO] [stage3.py:2277:step] End to end step took 9.533045530319214
x3006c0s19b1n0: [2024-03-29 16:38:18,378] [INFO] [stage3.py:2277:step] End to end step took 9.533500909805298
x3006c0s19b1n0: [2024-03-29 16:38:18,378] [INFO] [logging.py:96:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
x3006c0s1b1n0: [2024-03-29 16:38:18,378] [INFO] [stage3.py:2277:step] End to end step took 9.533577680587769
x3006c0s1b1n0: [2024-03-29 16:38:18,378] [INFO] [stage3.py:2277:step] End to end step took 9.533595561981201
x3006c0s19b1n0: [2024-03-29 16:38:18,379] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 8078.28 | bwd_microstep: 23978.71 | bwd_inner_microstep: 23898.27 | bwd_allreduce_microstep: 80.33 | step_microstep: 9639.13
x3006c0s19b1n0: [2024-03-29 16:38:18,379] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 8078.28 | bwd: 23978.70 | bwd_inner: 23898.28 | bwd_allreduce: 80.33 | step: 9639.12
x3006c0s19b1n0: [2024-03-29 16:38:18,486] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:38:18,486] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.25 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:38:18,487] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.43 GB, percent = 66.9%
x3006c0s19b1n0: <TIMER:interval-time,42.298681020736694><TIMER:interval-time,42.2986786365509><TIMER:interval-time,42.29869103431702>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b1n0: <TIMER:interval-time,42.298702001571655><TIMER:interval-time,42.298693895339966><TIMER:interval-time,42.298694372177124>
x3006c0s1b1n0: 
x3006c0s1b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,42.29881453514099>
x3006c0s1b1n0: <TIMER:interval-time,42.29881453514099>
x3006c0s19b1n0: [Rank 0] (after 1 iterations) memory (MB) | allocated: 9224.5439453125 | max allocated: 9224.54443359375 | reserved: 9296.0 | max reserved: 9296.0
x3006c0s1b1n0:  elapsed_time 42.298694 | consumed samples:           32 | consumed tokens:        65536 | elapsed time per iteration (ms): 42298.7 | learning rate: 3.000E-04 | global batch size:    32 | lm loss: 1.215015E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.757 | TFLOPs: 52.34 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:38:18,630] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:38:18,631] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-29 16:38:18,631] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.51 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:38:25,318] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:38:25,319] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:38:25,319] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.51 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:38:25,398] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:38:25,399] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:38:25,399] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.51 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:38:43,021] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:38:43,022] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-29 16:38:43,022] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.5 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:38:43,094] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:38:43,094] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:38:43,094] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.5 GB, percent = 66.9%
x3006c0s1b1n0: [2024-03-29 16:38:49,787] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.669269800186157
x3006c0s1b1n0: [2024-03-29 16:38:49,788] [INFO] [stage3.py:2251:step] Full outer step loop took 6.66962456703186
x3006c0s1b1n0: [2024-03-29 16:38:50,204] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.086018323898315
x3006c0s1b1n0: [2024-03-29 16:38:50,204] [INFO] [stage3.py:2251:step] Full outer step loop took 7.086273670196533
x3006c0s19b1n0: [2024-03-29 16:38:50,262] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.143750190734863
x3006c0s19b1n0: [2024-03-29 16:38:50,262] [INFO] [stage3.py:2251:step] Full outer step loop took 7.143970727920532
x3006c0s1b1n0: [2024-03-29 16:38:50,302] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.184444427490234
x3006c0s1b1n0: [2024-03-29 16:38:50,303] [INFO] [stage3.py:2251:step] Full outer step loop took 7.184658765792847
x3006c0s1b1n0: [2024-03-29 16:38:50,308] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.190412998199463
x3006c0s1b1n0: [2024-03-29 16:38:50,309] [INFO] [stage3.py:2251:step] Full outer step loop took 7.190561771392822
x3006c0s19b1n0: [2024-03-29 16:38:50,384] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.265027284622192
x3006c0s19b1n0: [2024-03-29 16:38:50,388] [INFO] [stage3.py:2251:step] Full outer step loop took 7.269174098968506
x3006c0s19b1n0: [2024-03-29 16:38:50,408] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.289721965789795
x3006c0s19b1n0: [2024-03-29 16:38:50,408] [INFO] [stage3.py:2251:step] Full outer step loop took 7.290055751800537
x3006c0s19b1n0: [2024-03-29 16:38:50,481] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.362783432006836
x3006c0s19b1n0: [2024-03-29 16:38:50,481] [INFO] [stage3.py:2251:step] Full outer step loop took 7.362937927246094
x3006c0s1b1n0: [2024-03-29 16:38:50,493] [INFO] [stage3.py:2277:step] End to end step took 7.374612092971802
x3006c0s1b1n0: [2024-03-29 16:38:50,493] [INFO] [stage3.py:2277:step] End to end step took 7.374677419662476
x3006c0s19b1n0: [2024-03-29 16:38:50,493] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 7271.35
x3006c0s19b1n0: [2024-03-29 16:38:50,493] [INFO] [stage3.py:2277:step] End to end step took 7.375091075897217
x3006c0s19b1n0: [2024-03-29 16:38:50,493] [INFO] [stage3.py:2277:step] End to end step took 7.374863624572754
x3006c0s1b1n0: [2024-03-29 16:38:50,493] [INFO] [stage3.py:2277:step] End to end step took 7.3752381801605225
x3006c0s1b1n0: [2024-03-29 16:38:50,493] [INFO] [stage3.py:2277:step] End to end step took 7.375252723693848
x3006c0s19b1n0: [2024-03-29 16:38:50,493] [INFO] [stage3.py:2277:step] End to end step took 7.375272035598755
x3006c0s19b1n0: [2024-03-29 16:38:50,493] [INFO] [stage3.py:2277:step] End to end step took 7.375385522842407
x3006c0s19b1n0: [2024-03-29 16:38:50,493] [INFO] [logging.py:96:log_dist] [Rank 0] step=2, skipped=0, lr=[0.0002918585038060976], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-29 16:38:50,494] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6647.88 | bwd_microstep: 17458.61 | bwd_inner_microstep: 17374.13 | bwd_allreduce_microstep: 84.41 | step_microstep: 7399.18
x3006c0s19b1n0: [2024-03-29 16:38:50,494] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6647.86 | bwd: 17458.61 | bwd_inner: 17374.12 | bwd_allreduce: 84.42 | step: 7399.18
x3006c0s19b1n0: [2024-03-29 16:38:50,632] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:38:50,633] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:38:50,633] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.51 GB, percent = 66.9%
x3006c0s19b1n0: <TIMER:interval-time,32.14573669433594><TIMER:interval-time,32.145737409591675><TIMER:interval-time,32.145740270614624>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,32.145745038986206>
x3006c0s1b1n0: <TIMER:interval-time,32.145934104919434><TIMER:interval-time,32.14593577384949><TIMER:interval-time,32.14593744277954><TIMER:interval-time,32.145939111709595>
x3006c0s1b1n0: 
x3006c0s1b1n0: 
x3006c0s1b1n0: 
x3006c0s1b1n0:  elapsed_time 32.145939 | consumed samples:           64 | consumed tokens:       131072 | elapsed time per iteration (ms): 32145.9 | learning rate: 2.919E-04 | global batch size:    32 | lm loss: 1.216527E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.995 | TFLOPs: 68.88 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:38:50,784] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:38:50,784] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-29 16:38:50,784] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.51 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:38:57,304] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:38:57,305] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:38:57,305] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.49 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:38:57,384] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:38:57,385] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:38:57,385] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.49 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:39:14,906] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:39:14,907] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-29 16:39:14,907] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.47 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:39:14,978] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:39:14,979] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:39:14,979] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.47 GB, percent = 66.9%
x3006c0s1b1n0: [2024-03-29 16:39:21,997] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.9941840171813965
x3006c0s1b1n0: [2024-03-29 16:39:21,997] [INFO] [stage3.py:2251:step] Full outer step loop took 6.99437141418457
x3006c0s1b1n0: [2024-03-29 16:39:22,122] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.119011163711548
x3006c0s1b1n0: [2024-03-29 16:39:22,122] [INFO] [stage3.py:2251:step] Full outer step loop took 7.119274139404297
x3006c0s19b1n0: [2024-03-29 16:39:22,126] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.123234748840332
x3006c0s19b1n0: [2024-03-29 16:39:22,126] [INFO] [stage3.py:2251:step] Full outer step loop took 7.123612403869629
x3006c0s1b1n0: [2024-03-29 16:39:22,130] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.12711501121521
x3006c0s1b1n0: [2024-03-29 16:39:22,130] [INFO] [stage3.py:2251:step] Full outer step loop took 7.127558708190918
x3006c0s1b1n0: [2024-03-29 16:39:22,152] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.14934229850769
x3006c0s1b1n0: [2024-03-29 16:39:22,152] [INFO] [stage3.py:2251:step] Full outer step loop took 7.149488210678101
x3006c0s19b1n0: [2024-03-29 16:39:22,152] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.149522066116333
x3006c0s19b1n0: [2024-03-29 16:39:22,152] [INFO] [stage3.py:2251:step] Full outer step loop took 7.149693489074707
x3006c0s19b1n0: [2024-03-29 16:39:22,177] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.174013376235962
x3006c0s19b1n0: [2024-03-29 16:39:22,177] [INFO] [stage3.py:2251:step] Full outer step loop took 7.174252271652222
x3006c0s19b1n0: [2024-03-29 16:39:22,203] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.200177907943726
x3006c0s19b1n0: [2024-03-29 16:39:22,203] [INFO] [stage3.py:2251:step] Full outer step loop took 7.2003374099731445
x3006c0s19b1n0: [2024-03-29 16:39:22,214] [INFO] [stage3.py:2277:step] End to end step took 7.21126389503479
x3006c0s1b1n0: [2024-03-29 16:39:22,214] [INFO] [stage3.py:2277:step] End to end step took 7.21105170249939
x3006c0s19b1n0: [2024-03-29 16:39:22,214] [INFO] [stage3.py:2277:step] End to end step took 7.21136736869812
x3006c0s1b1n0: [2024-03-29 16:39:22,214] [INFO] [stage3.py:2277:step] End to end step took 7.211368083953857
x3006c0s19b1n0: [2024-03-29 16:39:22,214] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 7149.90
x3006c0s19b1n0: [2024-03-29 16:39:22,214] [INFO] [stage3.py:2277:step] End to end step took 7.211514711380005
x3006c0s1b1n0: [2024-03-29 16:39:22,214] [INFO] [stage3.py:2277:step] End to end step took 7.211515426635742
x3006c0s19b1n0: [2024-03-29 16:39:22,214] [INFO] [stage3.py:2277:step] End to end step took 7.211676597595215
x3006c0s1b1n0: [2024-03-29 16:39:22,215] [INFO] [stage3.py:2277:step] End to end step took 7.211771488189697
x3006c0s19b1n0: [2024-03-29 16:39:22,215] [INFO] [logging.py:96:log_dist] [Rank 0] step=3, skipped=0, lr=[0.00026841599982106197], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-29 16:39:22,215] [INFO] [timer.py:260:stop] epoch=0/micro_step=3/global_step=3, RunningAvgSamplesPerSec=1.018148952630755, CurrSamplesPerSec=1.018148952630755, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-29 16:39:22,215] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6482.36 | bwd_microstep: 17357.28 | bwd_inner_microstep: 17272.79 | bwd_allreduce_microstep: 84.42 | step_microstep: 7236.26
x3006c0s19b1n0: [2024-03-29 16:39:22,215] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6482.34 | bwd: 17357.28 | bwd_inner: 17272.78 | bwd_allreduce: 84.43 | step: 7236.26
x3006c0s19b1n0: [2024-03-29 16:39:22,336] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:39:22,336] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:39:22,337] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.51 GB, percent = 66.9%
x3006c0s19b1n0: <TIMER:interval-time,31.703300952911377><TIMER:interval-time,31.703303813934326><TIMER:interval-time,31.703307390213013>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,31.703320503234863>
x3006c0s1b1n0: <TIMER:interval-time,31.703301429748535><TIMER:interval-time,31.703304529190063><TIMER:interval-time,31.703302145004272>
x3006c0s1b1n0: 
x3006c0s1b1n0: 
x3006c0s1b1n0: <TIMER:interval-time,31.703309774398804>
x3006c0s1b1n0:  elapsed_time 31.703302 | consumed samples:           96 | consumed tokens:       196608 | elapsed time per iteration (ms): 31703.3 | learning rate: 2.684E-04 | global batch size:    32 | lm loss: 3.444557E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.009 | TFLOPs: 69.84 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:39:22,475] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:39:22,476] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-29 16:39:22,476] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.51 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:39:29,088] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:39:29,089] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:39:29,089] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.5 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:39:29,172] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:39:29,173] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:39:29,173] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.5 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:39:47,106] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:39:47,107] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-29 16:39:47,107] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.48 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:39:47,176] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:39:47,177] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:39:47,177] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.48 GB, percent = 66.9%
x3006c0s1b1n0: [2024-03-29 16:39:54,199] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.997815132141113
x3006c0s1b1n0: [2024-03-29 16:39:54,199] [INFO] [stage3.py:2251:step] Full outer step loop took 6.99833083152771
x3006c0s1b1n0: [2024-03-29 16:39:54,202] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.001417875289917
x3006c0s1b1n0: [2024-03-29 16:39:54,202] [INFO] [stage3.py:2251:step] Full outer step loop took 7.001610517501831
x3006c0s1b1n0: [2024-03-29 16:39:54,209] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.008487939834595
x3006c0s1b1n0: [2024-03-29 16:39:54,209] [INFO] [stage3.py:2251:step] Full outer step loop took 7.008665561676025
x3006c0s1b1n0: [2024-03-29 16:39:54,213] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.0120110511779785
x3006c0s1b1n0: [2024-03-29 16:39:54,213] [INFO] [stage3.py:2251:step] Full outer step loop took 7.0121729373931885
x3006c0s19b1n0: [2024-03-29 16:39:54,340] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.139462947845459
x3006c0s19b1n0: [2024-03-29 16:39:54,340] [INFO] [stage3.py:2251:step] Full outer step loop took 7.139693975448608
x3006c0s19b1n0: [2024-03-29 16:39:54,343] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.142544984817505
x3006c0s19b1n0: [2024-03-29 16:39:54,344] [INFO] [stage3.py:2251:step] Full outer step loop took 7.1430675983428955
x3006c0s19b1n0: [2024-03-29 16:39:54,410] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.185411214828491
x3006c0s19b1n0: [2024-03-29 16:39:54,411] [INFO] [stage3.py:2251:step] Full outer step loop took 7.185716390609741
x3006c0s19b1n0: [2024-03-29 16:39:54,423] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.221871852874756
x3006c0s19b1n0: [2024-03-29 16:39:54,423] [INFO] [stage3.py:2251:step] Full outer step loop took 7.222034692764282
x3006c0s19b1n0: [2024-03-29 16:39:54,434] [INFO] [stage3.py:2277:step] End to end step took 7.208548069000244
x3006c0s19b1n0: [2024-03-29 16:39:54,434] [INFO] [stage3.py:2277:step] End to end step took 7.232796907424927
x3006c0s1b1n0: [2024-03-29 16:39:54,434] [INFO] [stage3.py:2277:step] End to end step took 7.232771873474121
x3006c0s1b1n0: [2024-03-29 16:39:54,434] [INFO] [stage3.py:2277:step] End to end step took 7.23279595375061
x3006c0s1b1n0: [2024-03-29 16:39:54,434] [INFO] [stage3.py:2277:step] End to end step took 7.232833385467529
x3006c0s1b1n0: [2024-03-29 16:39:54,434] [INFO] [stage3.py:2277:step] End to end step took 7.232891321182251
x3006c0s19b1n0: [2024-03-29 16:39:54,434] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 7143.53
x3006c0s19b1n0: [2024-03-29 16:39:54,434] [INFO] [stage3.py:2277:step] End to end step took 7.233222007751465
x3006c0s19b1n0: [2024-03-29 16:39:54,434] [INFO] [stage3.py:2277:step] End to end step took 7.233313798904419
x3006c0s19b1n0: [2024-03-29 16:39:54,434] [INFO] [logging.py:96:log_dist] [Rank 0] step=4, skipped=0, lr=[0.00023249999999999996], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-29 16:39:54,435] [INFO] [timer.py:260:stop] epoch=0/micro_step=4/global_step=4, RunningAvgSamplesPerSec=1.0096726160111338, CurrSamplesPerSec=1.0013362492193387, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-29 16:39:54,435] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6576.48 | bwd_microstep: 17774.78 | bwd_inner_microstep: 17688.20 | bwd_allreduce_microstep: 86.51 | step_microstep: 7257.68
x3006c0s19b1n0: [2024-03-29 16:39:54,435] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6576.47 | bwd: 17774.78 | bwd_inner: 17688.19 | bwd_allreduce: 86.53 | step: 7257.68
x3006c0s19b1n0: [2024-03-29 16:39:54,551] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:39:54,552] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:39:54,552] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.52 GB, percent = 66.9%
x3006c0s19b1n0: <TIMER:interval-time,32.2150092124939><TIMER:interval-time,32.21500849723816><TIMER:interval-time,32.21500849723816>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,32.21501612663269>
x3006c0s1b1n0: <TIMER:interval-time,32.215017557144165><TIMER:interval-time,32.21502065658569>
x3006c0s1b1n0: <TIMER:interval-time,32.21502423286438>
x3006c0s1b1n0: 
x3006c0s1b1n0: <TIMER:interval-time,32.215025901794434>
x3006c0s1b1n0:  elapsed_time 32.215021 | consumed samples:          128 | consumed tokens:       262144 | elapsed time per iteration (ms): 32215.0 | learning rate: 2.325E-04 | global batch size:    32 | lm loss: 2.249373E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.993 | TFLOPs: 68.73 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:39:54,671] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:39:54,671] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-29 16:39:54,672] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.52 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:40:01,417] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:40:01,417] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:40:01,418] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.5 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:40:01,491] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:40:01,492] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:40:01,492] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.5 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:40:19,091] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:40:19,092] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-29 16:40:19,092] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.49 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:40:19,162] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:40:19,163] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:40:19,163] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.49 GB, percent = 66.9%
x3006c0s1b1n0: [2024-03-29 16:40:26,208] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.0211427211761475
x3006c0s1b1n0: [2024-03-29 16:40:26,208] [INFO] [stage3.py:2251:step] Full outer step loop took 7.021503686904907
x3006c0s1b1n0: [2024-03-29 16:40:26,214] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.027108430862427
x3006c0s1b1n0: [2024-03-29 16:40:26,214] [INFO] [stage3.py:2251:step] Full outer step loop took 7.0272979736328125
x3006c0s1b1n0: [2024-03-29 16:40:26,282] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.0952136516571045
x3006c0s1b1n0: [2024-03-29 16:40:26,282] [INFO] [stage3.py:2251:step] Full outer step loop took 7.09537672996521
x3006c0s1b1n0: [2024-03-29 16:40:26,292] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.10508918762207
x3006c0s1b1n0: [2024-03-29 16:40:26,292] [INFO] [stage3.py:2251:step] Full outer step loop took 7.105249881744385
x3006c0s19b1n0: [2024-03-29 16:40:26,357] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.170111894607544
x3006c0s19b1n0: [2024-03-29 16:40:26,357] [INFO] [stage3.py:2251:step] Full outer step loop took 7.170378684997559
x3006c0s19b1n0: [2024-03-29 16:40:26,426] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.2390525341033936
x3006c0s19b1n0: [2024-03-29 16:40:26,426] [INFO] [stage3.py:2251:step] Full outer step loop took 7.239259481430054
x3006c0s19b1n0: [2024-03-29 16:40:26,438] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.251122713088989
x3006c0s19b1n0: [2024-03-29 16:40:26,438] [INFO] [stage3.py:2251:step] Full outer step loop took 7.251350402832031
x3006c0s19b1n0: [2024-03-29 16:40:26,443] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.256118297576904
x3006c0s19b1n0: [2024-03-29 16:40:26,443] [INFO] [stage3.py:2251:step] Full outer step loop took 7.256284475326538
x3006c0s19b1n0: [2024-03-29 16:40:26,454] [INFO] [stage3.py:2277:step] End to end step took 7.2670159339904785
x3006c0s19b1n0: [2024-03-29 16:40:26,454] [INFO] [stage3.py:2277:step] End to end step took 7.267014741897583
x3006c0s19b1n0: [2024-03-29 16:40:26,454] [INFO] [stage3.py:2277:step] End to end step took 7.267060279846191
x3006c0s1b1n0: [2024-03-29 16:40:26,454] [INFO] [stage3.py:2277:step] End to end step took 7.267028093338013
x3006c0s1b1n0: [2024-03-29 16:40:26,454] [INFO] [stage3.py:2277:step] End to end step took 7.267001390457153
x3006c0s19b1n0: [2024-03-29 16:40:26,454] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 7170.69
x3006c0s1b1n0: [2024-03-29 16:40:26,454] [INFO] [stage3.py:2277:step] End to end step took 7.267385244369507
x3006c0s1b1n0: [2024-03-29 16:40:26,454] [INFO] [stage3.py:2277:step] End to end step took 7.267444849014282
x3006c0s19b1n0: [2024-03-29 16:40:26,454] [INFO] [stage3.py:2277:step] End to end step took 7.2674055099487305
x3006c0s19b1n0: [2024-03-29 16:40:26,455] [INFO] [logging.py:96:log_dist] [Rank 0] step=5, skipped=0, lr=[0.0001884425039850356], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-29 16:40:26,455] [INFO] [timer.py:260:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=1.0087328467325396, CurrSamplesPerSec=1.0068585466573785, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-29 16:40:26,455] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6716.05 | bwd_microstep: 17435.68 | bwd_inner_microstep: 17349.68 | bwd_allreduce_microstep: 85.92 | step_microstep: 7291.78
x3006c0s19b1n0: [2024-03-29 16:40:26,455] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6716.04 | bwd: 17435.67 | bwd_inner: 17349.68 | bwd_allreduce: 85.94 | step: 7291.78
x3006c0s19b1n0: [2024-03-29 16:40:26,586] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:40:26,587] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:40:26,587] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.51 GB, percent = 66.9%
x3006c0s19b1n0: <TIMER:interval-time,32.03488850593567><TIMER:interval-time,32.03489303588867>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,32.034894704818726>
x3006c0s19b1n0: <TIMER:interval-time,32.03489851951599>
x3006c0s1b1n0: <TIMER:interval-time,32.03523349761963><TIMER:interval-time,32.035234451293945><TIMER:interval-time,32.03523659706116>
x3006c0s1b1n0: 
x3006c0s1b1n0: 
x3006c0s1b1n0: <TIMER:interval-time,32.03532671928406>
x3006c0s1b1n0:  elapsed_time 32.035233 | consumed samples:          160 | consumed tokens:       327680 | elapsed time per iteration (ms): 32035.2 | learning rate: 1.884E-04 | global batch size:    32 | lm loss: 1.748318E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.999 | TFLOPs: 69.11 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:40:26,714] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:40:26,714] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-29 16:40:26,714] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.52 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:40:33,102] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:40:33,102] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:40:33,102] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.5 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:40:33,182] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:40:33,183] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:40:33,183] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.5 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:40:51,863] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:40:51,864] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-29 16:40:51,864] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.49 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:40:51,935] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:40:51,935] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:40:51,935] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.49 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:40:58,909] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.949566841125488
x3006c0s19b1n0: [2024-03-29 16:40:58,911] [INFO] [stage3.py:2251:step] Full outer step loop took 6.951491832733154
x3006c0s1b1n0: [2024-03-29 16:40:59,088] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.105870723724365
x3006c0s1b1n0: [2024-03-29 16:40:59,089] [INFO] [stage3.py:2251:step] Full outer step loop took 7.106703042984009
x3006c0s1b1n0: [2024-03-29 16:40:59,097] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.1376953125
x3006c0s1b1n0: [2024-03-29 16:40:59,097] [INFO] [stage3.py:2251:step] Full outer step loop took 7.1378819942474365
x3006c0s1b1n0: [2024-03-29 16:40:59,106] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.1462082862854
x3006c0s1b1n0: [2024-03-29 16:40:59,106] [INFO] [stage3.py:2251:step] Full outer step loop took 7.146361827850342
x3006c0s19b1n0: [2024-03-29 16:40:59,135] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.175694942474365
x3006c0s19b1n0: [2024-03-29 16:40:59,135] [INFO] [stage3.py:2251:step] Full outer step loop took 7.175918340682983
x3006c0s19b1n0: [2024-03-29 16:40:59,149] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.189294338226318
x3006c0s19b1n0: [2024-03-29 16:40:59,149] [INFO] [stage3.py:2251:step] Full outer step loop took 7.189458131790161
x3006c0s1b1n0: [2024-03-29 16:40:59,185] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.225281476974487
x3006c0s1b1n0: [2024-03-29 16:40:59,185] [INFO] [stage3.py:2251:step] Full outer step loop took 7.225442171096802
x3006c0s19b1n0: [2024-03-29 16:40:59,210] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.250256776809692
x3006c0s19b1n0: [2024-03-29 16:40:59,210] [INFO] [stage3.py:2251:step] Full outer step loop took 7.250413179397583
x3006c0s19b1n0: [2024-03-29 16:40:59,220] [INFO] [stage3.py:2277:step] End to end step took 7.260976314544678
x3006c0s1b1n0: [2024-03-29 16:40:59,220] [INFO] [stage3.py:2277:step] End to end step took 7.260968446731567
x3006c0s1b1n0: [2024-03-29 16:40:59,221] [INFO] [stage3.py:2277:step] End to end step took 7.261134147644043
x3006c0s1b1n0: [2024-03-29 16:40:59,221] [INFO] [stage3.py:2277:step] End to end step took 7.261200189590454
x3006c0s19b1n0: [2024-03-29 16:40:59,221] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6953.09
x3006c0s19b1n0: [2024-03-29 16:40:59,221] [INFO] [stage3.py:2277:step] End to end step took 7.261451005935669
x3006c0s19b1n0: [2024-03-29 16:40:59,221] [INFO] [stage3.py:2277:step] End to end step took 7.261601686477661
x3006c0s1b1n0: [2024-03-29 16:40:59,221] [INFO] [stage3.py:2277:step] End to end step took 7.23900032043457
x3006c0s19b1n0: [2024-03-29 16:40:59,221] [INFO] [stage3.py:2277:step] End to end step took 7.261761426925659
x3006c0s19b1n0: [2024-03-29 16:40:59,221] [INFO] [logging.py:96:log_dist] [Rank 0] step=6, skipped=0, lr=[0.0001415574960149644], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-29 16:40:59,222] [INFO] [timer.py:260:stop] epoch=0/micro_step=6/global_step=6, RunningAvgSamplesPerSec=1.0025456721503445, CurrSamplesPerSec=0.9844313167762611, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-29 16:40:59,222] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6351.43 | bwd_microstep: 18517.37 | bwd_inner_microstep: 18426.90 | bwd_allreduce_microstep: 90.39 | step_microstep: 7286.27
x3006c0s19b1n0: [2024-03-29 16:40:59,222] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6351.42 | bwd: 18517.37 | bwd_inner: 18426.89 | bwd_allreduce: 90.41 | step: 7286.27
x3006c0s19b1n0: [2024-03-29 16:40:59,339] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:40:59,339] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:40:59,340] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.51 GB, percent = 66.9%
x3006c0s19b1n0: <TIMER:interval-time,32.75211524963379><TIMER:interval-time,32.752117395401><TIMER:interval-time,32.75211977958679>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,32.752119064331055>
x3006c0s1b1n0: <TIMER:interval-time,32.75211215019226><TIMER:interval-time,32.75211763381958><TIMER:interval-time,32.75211954116821><TIMER:interval-time,32.75211954116821>
x3006c0s1b1n0: 
x3006c0s1b1n0: 
x3006c0s1b1n0: 
x3006c0s1b1n0:  elapsed_time 32.752118 | consumed samples:          192 | consumed tokens:       393216 | elapsed time per iteration (ms): 32752.1 | learning rate: 1.416E-04 | global batch size:    32 | lm loss: 1.497432E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.977 | TFLOPs: 67.60 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:40:59,464] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:40:59,465] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-29 16:40:59,465] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.51 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:41:06,385] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:41:06,386] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:41:06,386] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.49 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:41:06,466] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:41:06,467] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:41:06,467] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.49 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:41:24,565] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:41:24,566] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-29 16:41:24,566] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.48 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:41:24,635] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:41:24,636] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:41:24,636] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.48 GB, percent = 66.9%
x3006c0s1b1n0: [2024-03-29 16:41:31,507] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.847323417663574
x3006c0s1b1n0: [2024-03-29 16:41:31,507] [INFO] [stage3.py:2251:step] Full outer step loop took 6.847589731216431
x3006c0s1b1n0: [2024-03-29 16:41:31,653] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.9928624629974365
x3006c0s1b1n0: [2024-03-29 16:41:31,653] [INFO] [stage3.py:2251:step] Full outer step loop took 6.99324369430542
x3006c0s1b1n0: [2024-03-29 16:41:31,668] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.008148431777954
x3006c0s1b1n0: [2024-03-29 16:41:31,668] [INFO] [stage3.py:2251:step] Full outer step loop took 7.008328199386597
x3006c0s1b1n0: [2024-03-29 16:41:31,677] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.017231225967407
x3006c0s1b1n0: [2024-03-29 16:41:31,677] [INFO] [stage3.py:2251:step] Full outer step loop took 7.0173821449279785
x3006c0s19b1n0: [2024-03-29 16:41:31,709] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.048569679260254
x3006c0s19b1n0: [2024-03-29 16:41:31,709] [INFO] [stage3.py:2251:step] Full outer step loop took 7.049098491668701
x3006c0s19b1n0: [2024-03-29 16:41:31,753] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.092952013015747
x3006c0s19b1n0: [2024-03-29 16:41:31,753] [INFO] [stage3.py:2251:step] Full outer step loop took 7.093156337738037
x3006c0s19b1n0: [2024-03-29 16:41:31,850] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.1902947425842285
x3006c0s19b1n0: [2024-03-29 16:41:31,850] [INFO] [stage3.py:2251:step] Full outer step loop took 7.1904473304748535
x3006c0s19b1n0: [2024-03-29 16:41:31,972] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.312235116958618
x3006c0s19b1n0: [2024-03-29 16:41:31,972] [INFO] [stage3.py:2251:step] Full outer step loop took 7.312391996383667
x3006c0s19b1n0: [2024-03-29 16:41:31,983] [INFO] [stage3.py:2277:step] End to end step took 7.323033571243286
x3006c0s1b1n0: [2024-03-29 16:41:31,983] [INFO] [stage3.py:2277:step] End to end step took 7.322988510131836
x3006c0s1b1n0: [2024-03-29 16:41:31,983] [INFO] [stage3.py:2277:step] End to end step took 7.323076009750366
x3006c0s19b1n0: [2024-03-29 16:41:31,983] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 7190.63
x3006c0s19b1n0: [2024-03-29 16:41:31,983] [INFO] [stage3.py:2277:step] End to end step took 7.323578119277954
x3006c0s19b1n0: [2024-03-29 16:41:31,983] [INFO] [stage3.py:2277:step] End to end step took 7.3233091831207275
x3006c0s1b1n0: [2024-03-29 16:41:31,983] [INFO] [stage3.py:2277:step] End to end step took 7.323601961135864
x3006c0s19b1n0: [2024-03-29 16:41:31,983] [INFO] [stage3.py:2277:step] End to end step took 7.3237292766571045
x3006c0s1b1n0: [2024-03-29 16:41:31,984] [INFO] [stage3.py:2277:step] End to end step took 7.323721647262573
x3006c0s19b1n0: [2024-03-29 16:41:31,984] [INFO] [logging.py:96:log_dist] [Rank 0] step=7, skipped=0, lr=[9.750000000000001e-05], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-29 16:41:31,984] [INFO] [timer.py:260:stop] epoch=0/micro_step=7/global_step=7, RunningAvgSamplesPerSec=0.9987988671026959, CurrSamplesPerSec=0.9840875799688259, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-29 16:41:31,984] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6886.01 | bwd_microstep: 17936.29 | bwd_inner_microstep: 17857.22 | bwd_allreduce_microstep: 79.00 | step_microstep: 7348.08
x3006c0s19b1n0: [2024-03-29 16:41:31,984] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6886.00 | bwd: 17936.29 | bwd_inner: 17857.21 | bwd_allreduce: 79.02 | step: 7348.08
x3006c0s19b1n0: [2024-03-29 16:41:32,101] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:41:32,101] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:41:32,101] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.49 GB, percent = 66.9%
x3006c0s19b1n0: <TIMER:interval-time,32.76162648200989><TIMER:interval-time,32.76162505149841><TIMER:interval-time,32.761632204055786>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b1n0: <TIMER:interval-time,32.76167106628418><TIMER:interval-time,32.761666774749756>
x3006c0s1b1n0: <TIMER:interval-time,32.76167702674866>
x3006c0s1b1n0: 
x3006c0s1b1n0: <TIMER:interval-time,32.761679887771606>
x3006c0s19b1n0: <TIMER:interval-time,32.76173210144043>
x3006c0s1b1n0:  elapsed_time 32.761677 | consumed samples:          224 | consumed tokens:       458752 | elapsed time per iteration (ms): 32761.7 | learning rate: 9.750E-05 | global batch size:    32 | lm loss: 1.319906E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.977 | TFLOPs: 67.58 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:41:32,226] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:41:32,227] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-29 16:41:32,227] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.5 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:41:38,890] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:41:38,891] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:41:38,891] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.48 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:41:38,983] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:41:38,983] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:41:38,983] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.48 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:41:56,700] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:41:56,701] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-29 16:41:56,701] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.47 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:41:56,772] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:41:56,773] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:41:56,773] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.47 GB, percent = 66.9%
x3006c0s1b1n0: [2024-03-29 16:42:03,602] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.805104494094849
x3006c0s1b1n0: [2024-03-29 16:42:03,603] [INFO] [stage3.py:2251:step] Full outer step loop took 6.8054468631744385
x3006c0s1b1n0: [2024-03-29 16:42:03,693] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.896052360534668
x3006c0s1b1n0: [2024-03-29 16:42:03,694] [INFO] [stage3.py:2251:step] Full outer step loop took 6.896262884140015
x3006c0s1b1n0: [2024-03-29 16:42:03,787] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.989498615264893
x3006c0s1b1n0: [2024-03-29 16:42:03,787] [INFO] [stage3.py:2251:step] Full outer step loop took 6.98968243598938
x3006c0s1b1n0: [2024-03-29 16:42:03,812] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.0148491859436035
x3006c0s1b1n0: [2024-03-29 16:42:03,812] [INFO] [stage3.py:2251:step] Full outer step loop took 7.015007019042969
x3006c0s19b1n0: [2024-03-29 16:42:04,048] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.249850273132324
x3006c0s19b1n0: [2024-03-29 16:42:04,047] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.249842166900635
x3006c0s19b1n0: [2024-03-29 16:42:04,048] [INFO] [stage3.py:2251:step] Full outer step loop took 7.250375032424927
x3006c0s19b1n0: [2024-03-29 16:42:04,049] [INFO] [stage3.py:2251:step] Full outer step loop took 7.2506186962127686
x3006c0s19b1n0: [2024-03-29 16:42:04,070] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.272588491439819
x3006c0s19b1n0: [2024-03-29 16:42:04,070] [INFO] [stage3.py:2251:step] Full outer step loop took 7.272754907608032
x3006c0s19b1n0: [2024-03-29 16:42:04,070] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.273033142089844
x3006c0s19b1n0: [2024-03-29 16:42:04,070] [INFO] [stage3.py:2251:step] Full outer step loop took 7.2731616497039795
x3006c0s19b1n0: [2024-03-29 16:42:04,082] [INFO] [stage3.py:2277:step] End to end step took 7.284299612045288
x3006c0s19b1n0: [2024-03-29 16:42:04,082] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 7272.92
x3006c0s19b1n0: [2024-03-29 16:42:04,082] [INFO] [stage3.py:2277:step] End to end step took 7.2840564250946045
x3006c0s1b1n0: [2024-03-29 16:42:04,082] [INFO] [stage3.py:2277:step] End to end step took 7.284358024597168
x3006c0s19b1n0: [2024-03-29 16:42:04,082] [INFO] [stage3.py:2277:step] End to end step took 7.284422874450684
x3006c0s19b1n0: [2024-03-29 16:42:04,082] [INFO] [stage3.py:2277:step] End to end step took 7.284539699554443
x3006c0s1b1n0: [2024-03-29 16:42:04,082] [INFO] [stage3.py:2277:step] End to end step took 7.284493684768677
x3006c0s19b1n0: [2024-03-29 16:42:04,082] [INFO] [logging.py:96:log_dist] [Rank 0] step=8, skipped=0, lr=[6.158400017893797e-05], mom=[(0.9, 0.95)]
x3006c0s1b1n0: [2024-03-29 16:42:04,082] [INFO] [stage3.py:2277:step] End to end step took 7.284788370132446
x3006c0s1b1n0: [2024-03-29 16:42:04,082] [INFO] [stage3.py:2277:step] End to end step took 7.284826755523682
x3006c0s19b1n0: [2024-03-29 16:42:04,082] [INFO] [timer.py:260:stop] epoch=0/micro_step=8/global_step=8, RunningAvgSamplesPerSec=0.9997555143479722, CurrSamplesPerSec=1.0045663710842374, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-29 16:42:04,083] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6632.54 | bwd_microstep: 17552.75 | bwd_inner_microstep: 17466.89 | bwd_allreduce_microstep: 85.78 | step_microstep: 7309.52
x3006c0s19b1n0: [2024-03-29 16:42:04,083] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6632.53 | bwd: 17552.74 | bwd_inner: 17466.88 | bwd_allreduce: 85.79 | step: 7309.52
x3006c0s19b1n0: [2024-03-29 16:42:04,206] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:42:04,207] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:42:04,207] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.5 GB, percent = 66.9%
x3006c0s19b1n0: <TIMER:interval-time,32.10507941246033><TIMER:interval-time,32.10509133338928><TIMER:interval-time,32.10509657859802>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,32.1051242351532>
x3006c0s1b1n0: <TIMER:interval-time,32.10539937019348>
x3006c0s1b1n0: <TIMER:interval-time,32.10540699958801>
x3006c0s1b1n0: <TIMER:interval-time,32.10549211502075>
x3006c0s1b1n0: <TIMER:interval-time,32.105509757995605>
x3006c0s1b1n0:  elapsed_time 32.105492 | consumed samples:          256 | consumed tokens:       524288 | elapsed time per iteration (ms): 32105.5 | learning rate: 6.158E-05 | global batch size:    32 | lm loss: 1.228736E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.997 | TFLOPs: 68.96 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:42:04,343] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:42:04,344] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-29 16:42:04,344] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.5 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:42:10,549] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:42:10,550] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:42:10,550] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.51 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:42:10,631] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:42:10,632] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:42:10,632] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.51 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:42:27,799] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:42:27,800] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-29 16:42:27,800] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.49 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:42:27,871] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:42:27,872] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:42:27,872] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.49 GB, percent = 66.9%
x3006c0s1b1n0: [2024-03-29 16:42:34,963] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.06784725189209
x3006c0s1b1n0: [2024-03-29 16:42:34,964] [INFO] [stage3.py:2251:step] Full outer step loop took 7.068336725234985
x3006c0s1b1n0: [2024-03-29 16:42:34,969] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.0738184452056885
x3006c0s1b1n0: [2024-03-29 16:42:34,969] [INFO] [stage3.py:2251:step] Full outer step loop took 7.074196100234985
x3006c0s1b1n0: [2024-03-29 16:42:34,979] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.08433985710144
x3006c0s1b1n0: [2024-03-29 16:42:34,980] [INFO] [stage3.py:2251:step] Full outer step loop took 7.084507703781128
x3006c0s1b1n0: [2024-03-29 16:42:34,994] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.099062919616699
x3006c0s1b1n0: [2024-03-29 16:42:34,994] [INFO] [stage3.py:2251:step] Full outer step loop took 7.099212646484375
x3006c0s19b1n0: [2024-03-29 16:42:35,168] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.27231502532959
x3006c0s19b1n0: [2024-03-29 16:42:35,168] [INFO] [stage3.py:2251:step] Full outer step loop took 7.2728190422058105
x3006c0s19b1n0: [2024-03-29 16:42:35,229] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.333455324172974
x3006c0s19b1n0: [2024-03-29 16:42:35,230] [INFO] [stage3.py:2251:step] Full outer step loop took 7.334439754486084
x3006c0s19b1n0: [2024-03-29 16:42:35,265] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.369727849960327
x3006c0s19b1n0: [2024-03-29 16:42:35,265] [INFO] [stage3.py:2251:step] Full outer step loop took 7.369945764541626
x3006c0s19b1n0: [2024-03-29 16:42:35,306] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.411163330078125
x3006c0s19b1n0: [2024-03-29 16:42:35,306] [INFO] [stage3.py:2251:step] Full outer step loop took 7.411323308944702
x3006c0s19b1n0: [2024-03-29 16:42:35,317] [INFO] [stage3.py:2277:step] End to end step took 7.422243595123291
x3006c0s19b1n0: [2024-03-29 16:42:35,317] [INFO] [stage3.py:2277:step] End to end step took 7.4222517013549805
x3006c0s1b1n0: [2024-03-29 16:42:35,317] [INFO] [stage3.py:2277:step] End to end step took 7.422258377075195
x3006c0s19b1n0: [2024-03-29 16:42:35,317] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 7273.49
x3006c0s1b1n0: [2024-03-29 16:42:35,318] [INFO] [stage3.py:2277:step] End to end step took 7.422344446182251
x3006c0s1b1n0: [2024-03-29 16:42:35,318] [INFO] [stage3.py:2277:step] End to end step took 7.422553539276123
x3006c0s19b1n0: [2024-03-29 16:42:35,318] [INFO] [stage3.py:2277:step] End to end step took 7.422221422195435
x3006c0s19b1n0: [2024-03-29 16:42:35,318] [INFO] [stage3.py:2277:step] End to end step took 7.422688245773315
x3006c0s1b1n0: [2024-03-29 16:42:35,318] [INFO] [stage3.py:2277:step] End to end step took 7.422829866409302
x3006c0s19b1n0: [2024-03-29 16:42:35,318] [INFO] [logging.py:96:log_dist] [Rank 0] step=9, skipped=0, lr=[3.814149619390238e-05], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-29 16:42:35,318] [INFO] [timer.py:260:stop] epoch=0/micro_step=9/global_step=9, RunningAvgSamplesPerSec=1.0043934291759085, CurrSamplesPerSec=1.0331504423905462, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-29 16:42:35,319] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6164.78 | bwd_microstep: 17004.81 | bwd_inner_microstep: 16925.96 | bwd_allreduce_microstep: 78.79 | step_microstep: 7446.70
x3006c0s19b1n0: [2024-03-29 16:42:35,319] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6164.77 | bwd: 17004.81 | bwd_inner: 16925.95 | bwd_allreduce: 78.80 | step: 7446.70
x3006c0s19b1n0: [2024-03-29 16:42:35,458] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:42:35,458] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:42:35,458] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.49 GB, percent = 66.9%
x3006c0s19b1n0: <TIMER:interval-time,31.25088667869568><TIMER:interval-time,31.250892162322998>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,31.250895977020264>
x3006c0s1b1n0: <TIMER:interval-time,31.25102400779724><TIMER:interval-time,31.251020669937134><TIMER:interval-time,31.251027822494507>
x3006c0s1b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,31.25095248222351>
x3006c0s1b1n0: 
x3006c0s1b1n0: <TIMER:interval-time,31.251033782958984>
x3006c0s1b1n0:  elapsed_time 31.251021 | consumed samples:          288 | consumed tokens:       589824 | elapsed time per iteration (ms): 31251.0 | learning rate: 3.814E-05 | global batch size:    32 | lm loss: 1.242802E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.024 | TFLOPs: 70.85 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:42:35,608] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:42:35,608] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-29 16:42:35,609] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.5 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:42:41,829] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:42:41,829] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:42:41,830] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.48 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:42:41,907] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:42:41,908] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-29 16:42:41,908] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.48 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:42:58,789] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:42:58,790] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-29 16:42:58,790] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.47 GB, percent = 66.9%
x3006c0s19b1n0: [2024-03-29 16:42:58,860] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:42:58,860] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:42:58,861] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.47 GB, percent = 66.9%
x3006c0s1b1n0: [2024-03-29 16:43:05,711] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.82680082321167
x3006c0s1b1n0: [2024-03-29 16:43:05,711] [INFO] [stage3.py:2251:step] Full outer step loop took 6.827062129974365
x3006c0s1b1n0: [2024-03-29 16:43:05,923] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.03922963142395
x3006c0s1b1n0: [2024-03-29 16:43:05,924] [INFO] [stage3.py:2251:step] Full outer step loop took 7.039521217346191
x3006c0s19b1n0: [2024-03-29 16:43:06,021] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.1367247104644775
x3006c0s19b1n0: [2024-03-29 16:43:06,021] [INFO] [stage3.py:2251:step] Full outer step loop took 7.136949777603149
x3006c0s1b1n0: [2024-03-29 16:43:06,050] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.165953874588013
x3006c0s1b1n0: [2024-03-29 16:43:06,050] [INFO] [stage3.py:2251:step] Full outer step loop took 7.166118860244751
x3006c0s1b1n0: [2024-03-29 16:43:06,059] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.174752473831177
x3006c0s1b1n0: [2024-03-29 16:43:06,059] [INFO] [stage3.py:2251:step] Full outer step loop took 7.1749114990234375
x3006c0s19b1n0: [2024-03-29 16:43:06,101] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.216325521469116
x3006c0s19b1n0: [2024-03-29 16:43:06,101] [INFO] [stage3.py:2251:step] Full outer step loop took 7.2166054248809814
x3006c0s19b1n0: [2024-03-29 16:43:06,135] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.250982046127319
x3006c0s19b1n0: [2024-03-29 16:43:06,135] [INFO] [stage3.py:2251:step] Full outer step loop took 7.251159191131592
x3006c0s19b1n0: [2024-03-29 16:43:06,191] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.306533098220825
x3006c0s19b1n0: [2024-03-29 16:43:06,191] [INFO] [stage3.py:2251:step] Full outer step loop took 7.306685447692871
x3006c0s19b1n0: [2024-03-29 16:43:06,202] [INFO] [stage3.py:2277:step] End to end step took 7.317623853683472
x3006c0s1b1n0: [2024-03-29 16:43:06,202] [INFO] [stage3.py:2277:step] End to end step took 7.3176891803741455
x3006c0s1b1n0: [2024-03-29 16:43:06,202] [INFO] [stage3.py:2277:step] End to end step took 7.3177595138549805
x3006c0s19b1n0: [2024-03-29 16:43:06,202] [INFO] [stage3.py:2277:step] End to end step took 7.317984342575073
x3006c0s19b1n0: [2024-03-29 16:43:06,202] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 7217.04
x3006c0s1b1n0: [2024-03-29 16:43:06,202] [INFO] [stage3.py:2277:step] End to end step took 7.318042278289795
x3006c0s1b1n0: [2024-03-29 16:43:06,202] [INFO] [stage3.py:2277:step] End to end step took 7.3182053565979
x3006c0s19b1n0: [2024-03-29 16:43:06,202] [INFO] [stage3.py:2277:step] End to end step took 7.318177223205566
x3006c0s19b1n0: [2024-03-29 16:43:06,202] [INFO] [stage3.py:2277:step] End to end step took 7.317691802978516
x3006c0s19b1n0: [2024-03-29 16:43:06,203] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[3e-05], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-29 16:43:06,203] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=1.0094110494188955, CurrSamplesPerSec=1.0459889750802274, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-29 16:43:06,203] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6184.26 | bwd_microstep: 16718.76 | bwd_inner_microstep: 16637.96 | bwd_allreduce_microstep: 80.74 | step_microstep: 7342.58
x3006c0s19b1n0: [2024-03-29 16:43:06,203] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6184.25 | bwd: 16718.76 | bwd_inner: 16637.96 | bwd_allreduce: 80.75 | step: 7342.58
x3006c0s19b1n0: [2024-03-29 16:43:06,324] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:43:06,325] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:43:06,325] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.48 GB, percent = 66.9%
x3006c0s19b1n0: <TIMER:interval-time,30.865958213806152><TIMER:interval-time,30.86595368385315><TIMER:interval-time,30.8659610748291>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,30.86600375175476>
x3006c0s1b1n0: <TIMER:interval-time,30.865935564041138><TIMER:interval-time,30.865936994552612><TIMER:interval-time,30.865936994552612><TIMER:interval-time,30.865939378738403>
x3006c0s1b1n0: 
x3006c0s1b1n0: 
x3006c0s1b1n0: 
x3006c0s1b1n0:  elapsed_time 30.865939 | consumed samples:          320 | consumed tokens:       655360 | elapsed time per iteration (ms): 30865.9 | learning rate: 3.000E-05 | global batch size:    32 | lm loss: 1.122073E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.037 | TFLOPs: 71.73 |
x3006c0s1b1n0: <<<only_train:330.14579796791077>>>
x3006c0s19b1n0: <<<only_train:330.14578461647034>>>
x3006c0s19b1n0: <<<only_train:330.14577054977417>>>
x3006c0s19b1n0: <<<only_train:330.14579224586487>>>
x3006c0s1b1n0: <<<only_train:330.1452901363373>>>
x3006c0s19b1n0: <<<only_train:330.14566349983215>>>
x3006c0s1b1n0: <<<only_train:330.14582681655884>>>
x3006c0s1b1n0: <<<only_train:330.14565920829773>>>
x3006c0s19b1n0: [after training ends] datetime: 2024-03-29 16:43:06 
x3006c0s19b1n0: <<<full_time:330.1460645198822>>><<<full_time:330.1460511684418>>>
x3006c0s19b1n0: 
x3006c0s19b1n0: <<<full_time:330.14608931541443>>>
x3006c0s19b1n0: <<<full_time:330.14592242240906>>>
x3006c0s1b1n0: <<<full_time:330.14617013931274>>>
x3006c0s1b1n0: <<<full_time:330.14606976509094>>><<<full_time:330.1458888053894>>>
x3006c0s1b1n0: 
x3006c0s1b1n0: <<<full_time:330.145605802536>>>
x3006c0s1b1n0: [2024-03-29 16:43:09,943] [INFO] [launch.py:348:main] Process 61591 exits successfully.
x3006c0s19b1n0: [2024-03-29 16:43:10,397] [INFO] [launch.py:348:main] Process 42897 exits successfully.
x3006c0s1b1n0: [2024-03-29 16:43:12,947] [INFO] [launch.py:348:main] Process 61589 exits successfully.
x3006c0s1b1n0: [2024-03-29 16:43:12,947] [INFO] [launch.py:348:main] Process 61590 exits successfully.
x3006c0s1b1n0: [2024-03-29 16:43:12,947] [INFO] [launch.py:348:main] Process 61588 exits successfully.
x3006c0s19b1n0: [2024-03-29 16:43:13,401] [INFO] [launch.py:348:main] Process 42896 exits successfully.
x3006c0s19b1n0: [2024-03-29 16:43:13,401] [INFO] [launch.py:348:main] Process 42894 exits successfully.
x3006c0s19b1n0: [2024-03-29 16:43:13,401] [INFO] [launch.py:348:main] Process 42895 exits successfully.

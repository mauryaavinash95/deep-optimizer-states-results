[2024-03-29 16:50:13,591] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 16:50:16,719] [INFO] [runner.py:463:main] Using IP address of 10.140.57.107 for node x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-29 16:50:16,721] [INFO] [runner.py:557:main] deepspeed_env file = ./.deepspeed_env
[2024-03-29 16:50:16,721] [INFO] [multinode_runner.py:80:get_cmd] Running on the following workers: x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov,x3006c0s1b1n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-29 16:50:16,721] [INFO] [runner.py:568:main] cmd = pdsh -S -f 1024 -w x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov,x3006c0s1b1n0.hsn.cm.polaris.alcf.anl.gov export PYTHONUSERBASE=/home/am6429/.local/polaris/conda/2023-10-04; export PYTHONPATH=/home/am6429/dl-io/Megatron-DeepSpeed; export PATH=/home/am6429/.conda/envs/dspeed_env/bin:/soft/datascience/conda/2023-10-04/mconda3/condabin:/soft/compilers/cudatoolkit/cuda-11.8.0/bin:/soft/buildtools/cmake/cmake-3.23.2/cmake-3.23.2-linux-x86_64/bin:/opt/cray/pe/gcc/11.2.0/bin:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/include:/opt/cray/pe/pals/1.2.11/bin:/opt/cray/pe/craype/2.7.20/bin:/opt/cray/libfabric/1.15.2.0/bin:/home/am6429/.conda/envs/dspeed_env/bin:/opt/cray/pe/perftools/23.03.0/bin:/opt/cray/pe/papi/7.0.0.1/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/am6429/.local/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/am6429/veloc-build/include:/home/am6429/veloc-build/bin:/soft/datascience/conda/2023-01-10/mconda3/include:/opt/cray/pe/bin:/soft/datascience/conda/2023-01-10/mconda3/include; export LD_LIBRARY_PATH=/usr/lib64/:/soft/compilers/cudatoolkit/cuda-11.8.0/lib64:/opt/cray/pe/gcc/11.2.0/snos/lib64:/soft/compilers/cudatoolkit/cuda-11.8.0/extras/CUPTI/lib64:/soft/libraries/trt/TensorRT-8.5.3.1.Linux.x86_64-gnu.cuda-11.8.cudnn8.6/lib:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/lib:/soft/libraries/cudnn/cudnn-11-linux-x64-v8.6.0.163/lib:/opt/cray/libfabric/1.15.2.0/lib64:/usr/lib64:/opt/cray/pe/papi/7.0.0.1/lib64:/home/am6429/veloc-build/lib:/home/am6429/veloc-build/lib64:/home/am6429/nvcomp/lib:/soft/datascience/conda/2023-01-10/mconda3/lib:/soft/datascience/conda/2023-01-10/mconda3/lib/; export http_proxy=http://proxy.alcf.anl.gov:3128; export https_proxy=http://proxy.alcf.anl.gov:3128; export CC=gcc; export CXX=g++; export IBV_FORK_SAFE=1; export CFLAGS=-I/soft/datascience/conda/2023-01-10/mconda3/include/; export LDFLAGS=-L/soft/datascience/conda/2023-01-10/mconda3/lib/; export CUDA_DEVICE_MAX_CONNECTIONS=1; export TORCHSNAPSHOT_PER_RANK_MEMORY_BUDGET_BYTES=34359738368; export _DEFAULT_MAX_PER_RANK_IO_CONCURRENCY=1; export _MAX_PER_RANK_IO_CONCURRENCY=1; export NSYS_REPORT_DIR=/home/am6429/dl-io/dl-io-outputs/act-output-30B//rep-30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch1-flush_async1-opt_gaps2-%n;  cd /home/am6429/dl-io/Megatron-DeepSpeed; /home/am6429/.conda/envs/dspeed_env/bin/python -u -m deepspeed.launcher.launch --world_info=eyJ4MzAwNmMwczE5YjFuMC5oc24uY20ucG9sYXJpcy5hbGNmLmFubC5nb3YiOiBbMCwgMSwgMiwgM10sICJ4MzAwNmMwczFiMW4wLmhzbi5jbS5wb2xhcmlzLmFsY2YuYW5sLmdvdiI6IFswLCAxLCAyLCAzXX0= --node_rank=%n --master_addr=10.140.57.107 --master_port=29700 /home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py --tensor-model-parallel-size 1 --num-layers 60 --hidden-size 6656 --num-attention-heads 52 --micro-batch-size 4 --global-batch-size 32 --seq-length 2048 --max-position-embeddings 2048 --train-iters 10 --save /local/scratch/llama2/zero3-tp1}_dp8 --data-path /home/am6429/dl-io/datasets/meg-gpt2_text_document --vocab-file /home/am6429/dl-io/datasets/gpt2-vocab.json --merge-file /home/am6429/dl-io/datasets/gpt2-merges.txt --data-impl mmap --tokenizer-type GPTSentencePieceTokenizer --tokenizer-model /home/am6429/dl-io/datasets/tokenizer.model --split 949,50,1 --distributed-backend nccl --lr 3e-4 --lr-decay-style cosine --min-lr 3e-5 --weight-decay 0.1 --clip-grad 1 --lr-warmup-iters 1 --optimizer adam --adam-beta1 0.9 --adam-beta2 0.95 --log-interval 1 --save-interval 1000 --eval-interval 1000 --eval-iters 0 --bf16 --no-query-key-layer-scaling --attention-dropout 0 --hidden-dropout 0 --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization rmsnorm --disable-bias-linear --num-key-value-heads 4 --deepspeed --exit-interval 20 --deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch1-flush_async1-opt_gaps2.json --zero-stage=3 --no-pipeline-parallel --cpu-optimizer --checkpoint-activations --deepspeed-activation-checkpointing
x3006c0s19b1n0: [2024-03-29 16:50:18,766] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b1n0: [2024-03-29 16:50:18,767] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-29 16:50:20,599] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3006c0s19b1n0: [2024-03-29 16:50:20,599] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=0
x3006c0s19b1n0: [2024-03-29 16:50:20,599] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b1n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3006c0s19b1n0: [2024-03-29 16:50:20,599] [INFO] [launch.py:163:main] dist_world_size=8
x3006c0s19b1n0: [2024-03-29 16:50:20,599] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3006c0s19b1n0: [2024-03-29 16:50:20,600] [INFO] [launch.py:253:main] process 49737 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch1-flush_async1-opt_gaps2.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-29 16:50:20,601] [INFO] [launch.py:253:main] process 49738 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch1-flush_async1-opt_gaps2.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-29 16:50:20,601] [INFO] [launch.py:253:main] process 49739 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch1-flush_async1-opt_gaps2.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-29 16:50:20,602] [INFO] [launch.py:253:main] process 49740 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch1-flush_async1-opt_gaps2.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b1n0: [2024-03-29 16:50:21,152] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3006c0s1b1n0: [2024-03-29 16:50:21,152] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=1
x3006c0s1b1n0: [2024-03-29 16:50:21,152] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b1n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3006c0s1b1n0: [2024-03-29 16:50:21,152] [INFO] [launch.py:163:main] dist_world_size=8
x3006c0s1b1n0: [2024-03-29 16:50:21,152] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3006c0s1b1n0: [2024-03-29 16:50:21,153] [INFO] [launch.py:253:main] process 2260 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch1-flush_async1-opt_gaps2.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b1n0: [2024-03-29 16:50:21,153] [INFO] [launch.py:253:main] process 2261 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch1-flush_async1-opt_gaps2.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b1n0: [2024-03-29 16:50:21,154] [INFO] [launch.py:253:main] process 2262 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch1-flush_async1-opt_gaps2.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b1n0: [2024-03-29 16:50:21,154] [INFO] [launch.py:253:main] process 2263 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch1-flush_async1-opt_gaps2.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-29 16:50:22,396] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-29 16:50:22,413] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-29 16:50:22,423] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-29 16:50:22,426] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b1n0: [2024-03-29 16:50:22,933] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b1n0: [2024-03-29 16:50:22,953] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b1n0: [2024-03-29 16:50:22,968] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b1n0: [2024-03-29 16:50:22,971] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: [2024-03-29 16:50:25,401] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: INFO: overriding default arguments for tokenizer_type:GPTSentencePieceTokenizer                    with tokenizer_type:GPT2BPETokenizer
x3006c0s19b1n0: using world size: 8, data-parallel-size: 8, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
x3006c0s19b1n0: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
x3006c0s19b1n0: using torch.bfloat16 for parameters ...
x3006c0s19b1n0: ------------------------ arguments ------------------------
x3006c0s19b1n0:   accumulate_allreduce_grads_in_fp32 .............. True
x3006c0s19b1n0:   adam_beta1 ...................................... 0.9
x3006c0s19b1n0:   adam_beta2 ...................................... 0.95
x3006c0s19b1n0:   adam_eps ........................................ 1e-08
x3006c0s19b1n0:   add_bias_linear ................................. False
x3006c0s19b1n0:   add_position_embedding .......................... False
x3006c0s19b1n0:   adlr_autoresume ................................. False
x3006c0s19b1n0:   adlr_autoresume_interval ........................ 1000
x3006c0s19b1n0:   aml_data_download_path .......................... None
x3006c0s19b1n0:   apply_layernorm_1p .............................. False
x3006c0s19b1n0:   apply_query_key_layer_scaling ................... False
x3006c0s19b1n0:   apply_residual_connection_post_layernorm ........ False
x3006c0s19b1n0:   async_tensor_model_parallel_allreduce ........... False
x3006c0s19b1n0:   attention_dropout ............................... 0.0
x3006c0s19b1n0:   attention_softmax_in_fp32 ....................... False
x3006c0s19b1n0:   barrier_with_L1_time ............................ True
x3006c0s19b1n0:   bert_binary_head ................................ True
x3006c0s19b1n0:   bert_embedder_type .............................. megatron
x3006c0s19b1n0:   bert_load ....................................... None
x3006c0s19b1n0:   bf16 ............................................ True
x3006c0s19b1n0:   bias_dropout_fusion ............................. True
x3006c0s19b1n0:   bias_gelu_fusion ................................ False
x3006c0s19b1n0:   biencoder_projection_dim ........................ 0
x3006c0s19b1n0:   biencoder_shared_query_context_model ............ False
x3006c0s19b1n0:   block_data_path ................................. None
x3006c0s19b1n0:   checkpoint_activations .......................... True
x3006c0s19b1n0:   checkpoint_in_cpu ............................... False
x3006c0s19b1n0:   checkpoint_num_layers ........................... 1
x3006c0s19b1n0:   classes_fraction ................................ 1.0
x3006c0s19b1n0:   clip_grad ....................................... 1.0
x3006c0s19b1n0:   compression_training ............................ False
x3006c0s19b1n0:   consumed_train_samples .......................... 0
x3006c0s19b1n0:   consumed_train_tokens ........................... 0
x3006c0s19b1n0:   consumed_valid_samples .......................... 0
x3006c0s19b1n0:   contigious_checkpointing ........................ False
x3006c0s19b1n0:   cpu_optimizer ................................... True
x3006c0s19b1n0:   cpu_torch_adam .................................. False
x3006c0s19b1n0:   create_moe_param_group .......................... False
x3006c0s19b1n0:   curriculum_learning_legacy ...................... False
x3006c0s19b1n0:   data_cache_path ................................. None
x3006c0s19b1n0:   data_efficiency_curriculum_learning ............. False
x3006c0s19b1n0:   data_impl ....................................... mmap
x3006c0s19b1n0:   data_parallel_random_init ....................... False
x3006c0s19b1n0:   data_parallel_size .............................. 8
x3006c0s19b1n0:   data_path ....................................... ['/home/am6429/dl-io/datasets/meg-gpt2_text_document']
x3006c0s19b1n0:   data_per_class_fraction ......................... 1.0
x3006c0s19b1n0:   data_sharding ................................... True
x3006c0s19b1n0:   dataloader_type ................................. single
x3006c0s19b1n0:   DDP_impl ........................................ local
x3006c0s19b1n0:   decoder_num_layers .............................. None
x3006c0s19b1n0:   decoder_seq_length .............................. None
x3006c0s19b1n0:   deepscale ....................................... False
x3006c0s19b1n0:   deepscale_config ................................ None
x3006c0s19b1n0:   deepspeed ....................................... True
x3006c0s19b1n0:   deepspeed_activation_checkpointing .............. True
x3006c0s19b1n0:   deepspeed_config ................................ /home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config_30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch1-flush_async1-opt_gaps2.json
x3006c0s19b1n0:   dino_bottleneck_size ............................ 256
x3006c0s19b1n0:   dino_freeze_last_layer .......................... 1
x3006c0s19b1n0:   dino_head_hidden_size ........................... 2048
x3006c0s19b1n0:   dino_local_crops_number ......................... 10
x3006c0s19b1n0:   dino_local_img_size ............................. 96
x3006c0s19b1n0:   dino_norm_last_layer ............................ False
x3006c0s19b1n0:   dino_teacher_temp ............................... 0.07
x3006c0s19b1n0:   dino_warmup_teacher_temp ........................ 0.04
x3006c0s19b1n0:   dino_warmup_teacher_temp_epochs ................. 30
x3006c0s19b1n0:   distribute_checkpointed_activations ............. False
x3006c0s19b1n0:   distribute_saved_activations .................... False
x3006c0s19b1n0:   distributed_backend ............................. nccl
x3006c0s19b1n0:   distributed_timeout_minutes ..................... 10
x3006c0s19b1n0:   ds_inference .................................... False
x3006c0s19b1n0:   ds_pipeline_enabled ............................. False
x3006c0s19b1n0:   ds_sequence_parallel_size ....................... 1
x3006c0s19b1n0:   embedding_path .................................. None
x3006c0s19b1n0:   embedding_weights_in_fp32 ....................... False
x3006c0s19b1n0:   empty_unused_memory_level ....................... 0
x3006c0s19b1n0:   enable_expert_tensor_parallelism ................ False
x3006c0s19b1n0:   encoder_num_layers .............................. 60
x3006c0s19b1n0:   encoder_seq_length .............................. 2048
x3006c0s19b1n0:   end_weight_decay ................................ 0.1
x3006c0s19b1n0:   eod_mask_loss ................................... False
x3006c0s19b1n0:   eval_interval ................................... 1000
x3006c0s19b1n0:   eval_iters ...................................... 0
x3006c0s19b1n0:   evidence_data_path .............................. None
x3006c0s19b1n0:   exit_duration_in_mins ........................... None
x3006c0s19b1n0:   exit_interval ................................... 20
x3006c0s19b1n0:   exit_on_missing_checkpoint ...................... False
x3006c0s19b1n0:   exit_signal_handler ............................. False
x3006c0s19b1n0:   expert_interval ................................. 2
x3006c0s19b1n0:   ffn_hidden_size ................................. 17728
x3006c0s19b1n0:   finetune ........................................ False
x3006c0s19b1n0:   force_ds_sequence_parallel ...................... False
x3006c0s19b1n0:   fp16 ............................................ False
x3006c0s19b1n0:   fp16_lm_cross_entropy ........................... False
x3006c0s19b1n0:   fp32_residual_connection ........................ False
x3006c0s19b1n0:   fp8_amax_compute_algo ........................... most_recent
x3006c0s19b1n0:   fp8_amax_history_len ............................ 1
x3006c0s19b1n0:   fp8_e4m3 ........................................ False
x3006c0s19b1n0:   fp8_hybrid ...................................... False
x3006c0s19b1n0:   fp8_interval .................................... 1
x3006c0s19b1n0:   fp8_margin ...................................... 0
x3006c0s19b1n0:   fp8_wgrad ....................................... True
x3006c0s19b1n0:   global_batch_size ............................... 32
x3006c0s19b1n0:   gradient_accumulation_fusion .................... True
x3006c0s19b1n0:   head_lr_mult .................................... 1.0
x3006c0s19b1n0:   hidden_dropout .................................. 0.0
x3006c0s19b1n0:   hidden_size ..................................... 6656
x3006c0s19b1n0:   hidden_size_teacher ............................. None
x3006c0s19b1n0:   hysteresis ...................................... 2
x3006c0s19b1n0:   ict_head_size ................................... None
x3006c0s19b1n0:   ict_load ........................................ None
x3006c0s19b1n0:   img_h ........................................... 224
x3006c0s19b1n0:   img_w ........................................... 224
x3006c0s19b1n0:   indexer_batch_size .............................. 128
x3006c0s19b1n0:   indexer_log_interval ............................ 1000
x3006c0s19b1n0:   inference ....................................... False
x3006c0s19b1n0:   inference_batch_times_seqlen_threshold .......... 512
x3006c0s19b1n0:   init_method_std ................................. 0.02
x3006c0s19b1n0:   init_method_xavier_uniform ...................... False
x3006c0s19b1n0:   initial_loss_scale .............................. 4294967296
x3006c0s19b1n0:   iter_per_epoch .................................. 1250
x3006c0s19b1n0:   kd .............................................. False
x3006c0s19b1n0:   kd_alpha_ce ..................................... 1
x3006c0s19b1n0:   kd_beta_ce ...................................... 1
x3006c0s19b1n0:   kd_temp ......................................... 1.0
x3006c0s19b1n0:   kv_channels ..................................... 128
x3006c0s19b1n0:   layernorm_epsilon ............................... 1e-05
x3006c0s19b1n0:   lazy_mpu_init ................................... None
x3006c0s19b1n0:   load ............................................ None
x3006c0s19b1n0:   load_teacher .................................... None
x3006c0s19b1n0:   local_rank ...................................... 0
x3006c0s19b1n0:   log_batch_size_to_tensorboard ................... False
x3006c0s19b1n0:   log_interval .................................... 1
x3006c0s19b1n0:   log_learning_rate_to_tensorboard ................ True
x3006c0s19b1n0:   log_loss_scale_to_tensorboard ................... True
x3006c0s19b1n0:   log_memory_to_tensorboard ....................... False
x3006c0s19b1n0:   log_num_zeros_in_grad ........................... False
x3006c0s19b1n0:   log_optimizer_states_to_tensorboard ............. False
x3006c0s19b1n0:   log_params_norm ................................. False
x3006c0s19b1n0:   log_timers_to_tensorboard ....................... False
x3006c0s19b1n0:   log_validation_ppl_to_tensorboard ............... False
x3006c0s19b1n0:   log_world_size_to_tensorboard ................... False
x3006c0s19b1n0:   loss_scale ...................................... None
x3006c0s19b1n0:   loss_scale_window ............................... 1000
x3006c0s19b1n0:   lr .............................................. 0.0003
x3006c0s19b1n0:   lr_decay_iters .................................. None
x3006c0s19b1n0:   lr_decay_samples ................................ None
x3006c0s19b1n0:   lr_decay_style .................................. cosine
x3006c0s19b1n0:   lr_decay_tokens ................................. None
x3006c0s19b1n0:   lr_warmup_fraction .............................. None
x3006c0s19b1n0:   lr_warmup_iters ................................. 1
x3006c0s19b1n0:   lr_warmup_samples ............................... 0
x3006c0s19b1n0:   lr_warmup_tokens ................................ None
x3006c0s19b1n0:   make_vocab_size_divisible_by .................... 128
x3006c0s19b1n0:   mask_factor ..................................... 1.0
x3006c0s19b1n0:   mask_prob ....................................... 0.15
x3006c0s19b1n0:   mask_type ....................................... random
x3006c0s19b1n0:   masked_softmax_fusion ........................... True
x3006c0s19b1n0:   max_position_embeddings ......................... 2048
x3006c0s19b1n0:   max_tokens_to_oom ............................... 12000
x3006c0s19b1n0:   memory_centric_tiled_linear ..................... False
x3006c0s19b1n0:   merge_file ...................................... /home/am6429/dl-io/datasets/gpt2-merges.txt
x3006c0s19b1n0:   micro_batch_size ................................ 4
x3006c0s19b1n0:   min_loss_scale .................................. 1.0
x3006c0s19b1n0:   min_lr .......................................... 3e-05
x3006c0s19b1n0:   mlp_type ........................................ standard
x3006c0s19b1n0:   mmap_warmup ..................................... False
x3006c0s19b1n0:   moe_eval_capacity_factor ........................ 1.0
x3006c0s19b1n0:   moe_expert_parallel_size ........................ 1
x3006c0s19b1n0:   moe_loss_coeff .................................. 0.1
x3006c0s19b1n0:   moe_min_capacity ................................ 4
x3006c0s19b1n0:   moe_token_dropping .............................. True
x3006c0s19b1n0:   moe_train_capacity_factor ....................... 1.0
x3006c0s19b1n0:   mos ............................................. False
x3006c0s19b1n0:   no_load_lr_state ................................ False
x3006c0s19b1n0:   no_load_optim ................................... None
x3006c0s19b1n0:   no_load_rng ..................................... None
x3006c0s19b1n0:   no_persist_layer_norm ........................... False
x3006c0s19b1n0:   no_pipeline_parallel ............................ True
x3006c0s19b1n0:   no_save_optim ................................... None
x3006c0s19b1n0:   no_save_rng ..................................... None
x3006c0s19b1n0:   normalization ................................... rmsnorm
x3006c0s19b1n0:   num_attention_heads ............................. 52
x3006c0s19b1n0:   num_attention_heads_teacher ..................... None
x3006c0s19b1n0:   num_channels .................................... 3
x3006c0s19b1n0:   num_classes ..................................... 1000
x3006c0s19b1n0:   num_experts ..................................... [1]
x3006c0s19b1n0:   num_experts_switch .............................. None
x3006c0s19b1n0:   num_experts_teacher ............................. [1]
x3006c0s19b1n0:   num_key_value_heads ............................. 4
x3006c0s19b1n0:   num_layers ...................................... 60
x3006c0s19b1n0:   num_layers_per_virtual_pipeline_stage ........... None
x3006c0s19b1n0:   num_layers_teacher .............................. None
x3006c0s19b1n0:   num_workers ..................................... 2
x3006c0s19b1n0:   onnx_safe ....................................... None
x3006c0s19b1n0:   openai_gelu ..................................... False
x3006c0s19b1n0:   optimizer ....................................... adam
x3006c0s19b1n0:   output_bert_embeddings .......................... False
x3006c0s19b1n0:   overlap_p2p_comm ................................ False
x3006c0s19b1n0:   override_opt_param_scheduler .................... False
x3006c0s19b1n0:   params_dtype .................................... torch.bfloat16
x3006c0s19b1n0:   partition_activations ........................... False
x3006c0s19b1n0:   patch_dim ....................................... 16
x3006c0s19b1n0:   perform_initialization .......................... True
x3006c0s19b1n0:   pipeline_model_parallel_size .................... 1
x3006c0s19b1n0:   pipeline_model_parallel_split_rank .............. None
x3006c0s19b1n0:   profile_backward ................................ False
x3006c0s19b1n0:   query_in_block_prob ............................. 0.1
x3006c0s19b1n0:   rampup_batch_size ............................... None
x3006c0s19b1n0:   random_ltd ...................................... False
x3006c0s19b1n0:   rank ............................................ 0
x3006c0s19b1n0:   recompute_granularity ........................... None
x3006c0s19b1n0:   recompute_method ................................ None
x3006c0s19b1n0:   recompute_num_layers ............................ 1
x3006c0s19b1n0:   remote_device ................................... none
x3006c0s19b1n0:   reset_attention_mask ............................ False
x3006c0s19b1n0:   reset_iteration ................................. False
x3006c0s19b1n0:   reset_position_ids .............................. False
x3006c0s19b1n0:   retriever_report_topk_accuracies ................ []
x3006c0s19b1n0:   retriever_score_scaling ......................... False
x3006c0s19b1n0:   retriever_seq_length ............................ 256
x3006c0s19b1n0:   retro_add_retriever ............................. False
x3006c0s19b1n0:   retro_cyclic_train_iters ........................ None
x3006c0s19b1n0:   retro_encoder_attention_dropout ................. 0.1
x3006c0s19b1n0:   retro_encoder_hidden_dropout .................... 0.1
x3006c0s19b1n0:   retro_encoder_layers ............................ 2
x3006c0s19b1n0:   retro_num_neighbors ............................. 2
x3006c0s19b1n0:   retro_num_retrieved_chunks ...................... 2
x3006c0s19b1n0:   retro_return_doc_ids ............................ False
x3006c0s19b1n0:   retro_workdir ................................... None
x3006c0s19b1n0:   return_data_index ............................... False
x3006c0s19b1n0:   rotary_percent .................................. 1.0
x3006c0s19b1n0:   sample_rate ..................................... 1.0
x3006c0s19b1n0:   save ............................................ /local/scratch/llama2/zero3-tp1}_dp8
x3006c0s19b1n0:   save_interval ................................... 1000
x3006c0s19b1n0:   scatter_gather_tensors_in_pipeline .............. True
x3006c0s19b1n0:   scattered_embeddings ............................ False
x3006c0s19b1n0:   seed ............................................ 1234
x3006c0s19b1n0:   seq_length ...................................... 2048
x3006c0s19b1n0:   sequence_parallel ............................... False
x3006c0s19b1n0:   sgd_momentum .................................... 0.9
x3006c0s19b1n0:   short_seq_prob .................................. 0.1
x3006c0s19b1n0:   skip_train ...................................... False
x3006c0s19b1n0:   split ........................................... 949,50,1
x3006c0s19b1n0:   split_transformers .............................. False
x3006c0s19b1n0:   squared_relu .................................... False
x3006c0s19b1n0:   standalone_embedding_stage ...................... False
x3006c0s19b1n0:   start_weight_decay .............................. 0.1
x3006c0s19b1n0:   swiglu .......................................... True
x3006c0s19b1n0:   swin_backbone_type .............................. tiny
x3006c0s19b1n0:   synchronize_each_layer .......................... False
x3006c0s19b1n0:   tensor_model_parallel_size ...................... 1
x3006c0s19b1n0:   tensorboard_dir ................................. None
x3006c0s19b1n0:   tensorboard_log_interval ........................ 1
x3006c0s19b1n0:   tensorboard_queue_size .......................... 1000
x3006c0s19b1n0:   test_data_path .................................. None
x3006c0s19b1n0:   tile_factor ..................................... 1
x3006c0s19b1n0:   timing_log_level ................................ 0
x3006c0s19b1n0:   timing_log_option ............................... minmax
x3006c0s19b1n0:   titles_data_path ................................ None
x3006c0s19b1n0:   tokenizer_model ................................. /home/am6429/dl-io/datasets/tokenizer.model
x3006c0s19b1n0:   tokenizer_type .................................. GPT2BPETokenizer
x3006c0s19b1n0:   topk ............................................ 1
x3006c0s19b1n0:   train_data_exact_num_epochs ..................... None
x3006c0s19b1n0:   train_data_path ................................. None
x3006c0s19b1n0:   train_desc_path ................................. None
x3006c0s19b1n0:   train_doc_idx_path .............................. None
x3006c0s19b1n0:   train_idx_path .................................. None
x3006c0s19b1n0:   train_iters ..................................... 10
x3006c0s19b1n0:   train_sample_idx_path ........................... None
x3006c0s19b1n0:   train_samples ................................... None
x3006c0s19b1n0:   train_shuffle_idx_path .......................... None
x3006c0s19b1n0:   train_tokens .................................... None
x3006c0s19b1n0:   transformer_impl ................................ local
x3006c0s19b1n0:   transformer_pipeline_model_parallel_size ........ 1
x3006c0s19b1n0:   untie_embeddings_and_output_weights ............. True
x3006c0s19b1n0:   use_checkpoint_args ............................. False
x3006c0s19b1n0:   use_checkpoint_opt_param_scheduler .............. False
x3006c0s19b1n0:   use_contiguous_buffers_in_local_ddp ............. True
x3006c0s19b1n0:   use_cpu_initialization .......................... None
x3006c0s19b1n0:   use_dataset_only ................................ False
x3006c0s19b1n0:   use_distributed_optimizer ....................... False
x3006c0s19b1n0:   use_flash_attn .................................. False
x3006c0s19b1n0:   use_flash_attn_triton ........................... False
x3006c0s19b1n0:   use_flash_attn_v1 ............................... False
x3006c0s19b1n0:   use_flash_attn_v2 ............................... False
x3006c0s19b1n0:   use_one_sent_docs ............................... False
x3006c0s19b1n0:   use_pin_memory .................................. False
x3006c0s19b1n0:   use_ring_exchange_p2p ........................... False
x3006c0s19b1n0:   use_rotary_position_embeddings .................. True
x3006c0s19b1n0:   use_tutel ....................................... False
x3006c0s19b1n0:   valid_data_path ................................. None
x3006c0s19b1n0:   variable_seq_lengths ............................ False
x3006c0s19b1n0:   virtual_pipeline_model_parallel_size ............ None
x3006c0s19b1n0:   vision_backbone_type ............................ vit
x3006c0s19b1n0:   vision_pretraining .............................. False
x3006c0s19b1n0:   vision_pretraining_type ......................... classify
x3006c0s19b1n0:   vocab_extra_ids ................................. 0
x3006c0s19b1n0:   vocab_file ...................................... /home/am6429/dl-io/datasets/gpt2-vocab.json
x3006c0s19b1n0:   vocab_size ...................................... None
x3006c0s19b1n0:   weight_decay .................................... 0.1
x3006c0s19b1n0:   weight_decay_incr_style ......................... constant
x3006c0s19b1n0:   world_size ...................................... 8
x3006c0s19b1n0:   zero_allgather_bucket_size ...................... 0.0
x3006c0s19b1n0:   zero_contigious_gradients ....................... False
x3006c0s19b1n0:   zero_reduce_bucket_size ......................... 0.0
x3006c0s19b1n0:   zero_reduce_scatter ............................. False
x3006c0s19b1n0:   zero_stage ...................................... 3
x3006c0s19b1n0: -------------------- end of arguments ---------------------
x3006c0s19b1n0: setting number of micro-batches to constant 1
x3006c0s19b1n0: > building GPT2BPETokenizer tokenizer ...
x3006c0s19b1n0: [2024-03-29 16:50:25,435] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: [2024-03-29 16:50:25,460] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
x3006c0s19b1n0: > initializing torch distributed ...
x3006c0s19b1n0: [2024-03-29 16:50:25,462] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: [2024-03-29 16:50:25,462] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b1n0:       meet the required dependencies to JIT install the op.
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: JIT compiled ops requires ninja
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b1n0:       meet the required dependencies to JIT install the op.
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: JIT compiled ops requires ninja
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b1n0:       meet the required dependencies to JIT install the op.
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: JIT compiled ops requires ninja
x3006c0s1b1n0: ninja .................. [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: op name ................ installed .. compatible
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b1n0: ninja .................. [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: op name ................ installed .. compatible
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b1n0: ninja .................. [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: op name ................ installed .. compatible
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b1n0:       meet the required dependencies to JIT install the op.
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: JIT compiled ops requires ninja
x3006c0s1b1n0: ninja .................. [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: op name ................ installed .. compatible
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: DeepSpeed general environment info:
x3006c0s1b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b1n0: torch version .................... 2.0.1+cu118
x3006c0s1b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b1n0: torch cuda version ............... 11.8
x3006c0s1b1n0: torch hip version ................ None
x3006c0s1b1n0: nvcc version ..................... 11.8
x3006c0s1b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: DeepSpeed general environment info:
x3006c0s1b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b1n0: torch version .................... 2.0.1+cu118
x3006c0s1b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b1n0: torch cuda version ............... 11.8
x3006c0s1b1n0: torch hip version ................ None
x3006c0s1b1n0: nvcc version ..................... 11.8
x3006c0s1b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b1n0: DeepSpeed general environment info:
x3006c0s1b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b1n0: torch version .................... 2.0.1+cu118
x3006c0s1b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b1n0: torch cuda version ............... 11.8
x3006c0s1b1n0: torch hip version ................ None
x3006c0s1b1n0: nvcc version ..................... 11.8
x3006c0s1b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b1n0: --------------------------------------------------
x3006c0s1b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b1n0: DeepSpeed general environment info:
x3006c0s1b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b1n0: torch version .................... 2.0.1+cu118
x3006c0s1b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b1n0: torch cuda version ............... 11.8
x3006c0s1b1n0: torch hip version ................ None
x3006c0s1b1n0: nvcc version ..................... 11.8
x3006c0s1b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b1n0: [2024-03-29 16:50:25,957] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b1n0: [2024-03-29 16:50:25,963] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b1n0: [2024-03-29 16:50:25,964] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b1n0: [2024-03-29 16:50:25,981] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: > initialized tensor model parallel with size 1
x3006c0s19b1n0: > initialized pipeline model parallel with size 1
x3006c0s19b1n0: > setting random seeds to 1234 ...
x3006c0s19b1n0: [2024-03-29 16:50:26,926] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
x3006c0s19b1n0: > compiling dataset index builder ...
x3006c0s19b1n0: make: Entering directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3006c0s19b1n0: make: Nothing to be done for 'default'.
x3006c0s19b1n0: make: Leaving directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3006c0s19b1n0: >>> done with dataset index builder. Compilation time: 0.088 seconds
x3006c0s19b1n0: > compiling and loading fused kernels ...
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: >>> done with compiling and loading fused kernels. Compilation time: 3.462 seconds
x3006c0s1b1n0: <<<<<<<<<<< 4
x3006c0s19b1n0: initialize_megatron took 5.848187446594238
x3006c0s19b1n0: <<<<<<<<<<< 0
x3006c0s1b1n0: <<<<<<<<<<< 6
x3006c0s1b1n0: <<<<<<<<<<< 5
x3006c0s1b1n0: <<<<<<<<<<< 7
x3006c0s19b1n0: <<<<<<<<<<< 3
x3006c0s19b1n0: <<<<<<<<<<< 1
x3006c0s19b1n0: <<<<<<<<<<< 2
x3006c0s19b1n0: time to initialize megatron (seconds): 6.207
x3006c0s19b1n0: [after megatron is initialized] datetime: 2024-03-29 16:50:31 
x3006c0s19b1n0: get_accelerator and all_reduce  took 0.007471561431884766
x3006c0s19b1n0: building GPT model ...
x3006c0s19b1n0: [2024-03-29 16:50:31,342] [INFO] [utils.py:800:see_memory_usage] Before Building Model
x3006c0s19b1n0: [2024-03-29 16:50:31,342] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 4.62 GB         CA 0.0 GB         Max_CA 5 GB 
x3006c0s19b1n0: [2024-03-29 16:50:31,343] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 22.92 GB, percent = 4.6%
x3006c0s19b1n0: [2024-03-29 16:50:37,608] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 423, num_elems = 27.64B
x3006c0s19b1n0: [2024-03-29 16:50:37,673] [INFO] [utils.py:800:see_memory_usage] After Building Model
x3006c0s19b1n0: [2024-03-29 16:50:37,674] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 7.07 GB         CA 22.33 GB         Max_CA 38 GB 
x3006c0s19b1n0: [2024-03-29 16:50:37,674] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 23.33 GB, percent = 4.6%
x3006c0s19b1n0:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 27635239424
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: Time to load cpu_adam op: 2.447807550430298 seconds
x3006c0s1b1n0: ninja: no work to do.
x3006c0s1b1n0: Time to load cpu_adam op: 2.468625545501709 seconds
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: Time to load cpu_adam op: 2.4765193462371826 seconds
x3006c0s1b1n0: ninja: no work to do.
x3006c0s1b1n0: Time to load cpu_adam op: 2.436851978302002 seconds
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: Time to load cpu_adam op: 2.4639275074005127 seconds
x3006c0s1b1n0: ninja: no work to do.
x3006c0s1b1n0: Time to load cpu_adam op: 2.434511184692383 seconds
x3006c0s19b1n0: Time to load cpu_adam op: 2.6380858421325684 seconds
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: > learning rate decay style: cosine
x3006c0s19b1n0: DeepSpeed is enabled.
x3006c0s19b1n0: [2024-03-29 16:50:42,071] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.3+8074cd62, git-hash=8074cd62, git-branch=hybrid_opt_offload
x3006c0s19b1n0: [2024-03-29 16:50:42,144] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After args sanity test
x3006c0s19b1n0: [2024-03-29 16:50:42,145] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.33 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:50:42,145] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 28.09 GB, percent = 5.6%
x3006c0s19b1n0: [2024-03-29 16:50:42,203] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure distributed model
x3006c0s19b1n0: [2024-03-29 16:50:42,203] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.33 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:50:42,203] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 28.5 GB, percent = 5.7%
x3006c0s1b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-29 16:50:42,267] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After configure distributed model
x3006c0s19b1n0: [2024-03-29 16:50:42,267] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.33 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:50:42,267] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 28.97 GB, percent = 5.8%
x3006c0s19b1n0: [2024-03-29 16:50:42,268] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
x3006c0s1b1n0: ninja: no work to do.
x3006c0s1b1n0: Time to load cpu_adam op: 2.4627416133880615 seconds
x3006c0s19b1n0: [2024-03-29 16:50:42,320] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After setting model parameters
x3006c0s19b1n0: [2024-03-29 16:50:42,321] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.33 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:50:42,321] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.18 GB, percent = 5.8%
x3006c0s19b1n0: [2024-03-29 16:50:42,375] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure optimizer
x3006c0s19b1n0: [2024-03-29 16:50:42,375] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.33 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:50:42,375] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.42 GB, percent = 5.8%
x3006c0s19b1n0: [2024-03-29 16:50:42,376] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
x3006c0s19b1n0: [2024-03-29 16:50:42,376] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
x3006c0s19b1n0: [2024-03-29 16:50:42,394] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
x3006c0s19b1n0: [2024-03-29 16:50:42,394] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
x3006c0s19b1n0: [2024-03-29 16:50:42,394] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
x3006c0s19b1n0: [2024-03-29 16:50:42,394] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-29 16:50:42,448] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
x3006c0s19b1n0: [2024-03-29 16:50:42,448] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.33 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:50:42,449] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.73 GB, percent = 5.9%
x3006c0s19b1n0: [2024-03-29 16:50:42,450] [INFO] [stage3.py:137:__init__] Reduce bucket size 500,000,000
x3006c0s19b1n0: [2024-03-29 16:50:42,450] [INFO] [stage3.py:138:__init__] Prefetch bucket size 50,000,000
x3006c0s19b1n0: [2024-03-29 16:50:42,504] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
x3006c0s19b1n0: [2024-03-29 16:50:42,505] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.33 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:50:42,505] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.92 GB, percent = 5.9%
x3006c0s19b1n0: Parameter Offload: Total persistent parameters: 805376 in 121 params
x3006c0s1b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-29 16:50:42,583] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
x3006c0s19b1n0: [2024-03-29 16:50:42,584] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.33 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:50:42,584] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.17 GB, percent = 6.0%
x3006c0s19b1n0: [2024-03-29 16:50:42,641] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
x3006c0s19b1n0: [2024-03-29 16:50:42,642] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 22.33 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:50:42,642] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.36 GB, percent = 6.0%
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s1b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s1b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-29 16:50:43,116] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-29 16:50:43,116] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-29 16:50:43,116] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-29 16:50:43,116] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b1n0: [2024-03-29 16:50:43,116] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b1n0: [2024-03-29 16:50:43,116] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b1n0: [2024-03-29 16:50:43,116] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b1n0: [2024-03-29 16:50:43,116] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-29 16:50:43,116] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b1n0: [2024-03-29 16:50:43,116] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-29 16:50:43,116] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-29 16:50:43,116] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-29 16:50:43,116] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-29 16:50:43,116] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,116] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,116] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,116] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,116] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b1n0: [2024-03-29 16:50:43,116] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b1n0: [2024-03-29 16:50:43,116] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,117] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,118] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,119] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,120] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b1n0: [2024-03-29 16:50:43,121] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-29 16:50:45,381] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 31
x3006c0s19b1n0: [2024-03-29 16:50:45,382] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.44 GB         CA 6.44 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-29 16:50:45,382] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 50.11 GB, percent = 10.0%
x3006c0s19b1n0: [2024-03-29 16:50:45,474] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
x3006c0s19b1n0: [2024-03-29 16:50:45,474] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-29 16:50:45,474] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 50.11 GB, percent = 10.0%
x3006c0s19b1n0: [2024-03-29 16:51:03,676] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
x3006c0s19b1n0: [2024-03-29 16:51:03,677] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-29 16:51:03,677] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 208.9 GB, percent = 41.5%
x3006c0s19b1n0: [2024-03-29 16:51:05,568] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
x3006c0s19b1n0: [2024-03-29 16:51:05,568] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-29 16:51:05,568] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 224.26 GB, percent = 44.6%
x3006c0s19b1n0: [2024-03-29 16:51:12,018] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | init_optimizer_state: 6437.68
x3006c0s19b1n0: [2024-03-29 16:51:12,088] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
x3006c0s19b1n0: [2024-03-29 16:51:12,089] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-29 16:51:12,089] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 284.2 GB, percent = 56.5%
x3006c0s19b1n0: [2024-03-29 16:51:12,089] [INFO] [stage3.py:520:_setup_for_real_optimizer] optimizer state initialized
x3006c0s19b1n0: [2024-03-29 16:51:20,207] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
x3006c0s19b1n0: [2024-03-29 16:51:20,208] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 8.61 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-29 16:51:20,208] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 316.91 GB, percent = 63.0%
x3006c0s19b1n0: [2024-03-29 16:51:20,208] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
x3006c0s19b1n0: [2024-03-29 16:51:20,274] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure LR scheduler
x3006c0s19b1n0: [2024-03-29 16:51:20,274] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-29 16:51:20,274] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 316.9 GB, percent = 63.0%
x3006c0s19b1n0: [2024-03-29 16:51:20,274] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
x3006c0s19b1n0: [2024-03-29 16:51:20,274] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7f0de31601c0>
x3006c0s19b1n0: [2024-03-29 16:51:20,274] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-29 16:51:20,338] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before rewriting optimizer step
x3006c0s19b1n0: [2024-03-29 16:51:20,338] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-29 16:51:20,338] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 316.92 GB, percent = 63.0%
x3006c0s19b1n0: [2024-03-29 16:51:20,402] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure checkpointing
x3006c0s19b1n0: [2024-03-29 16:51:20,402] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-29 16:51:20,402] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 316.9 GB, percent = 63.0%
x3006c0s19b1n0: [2024-03-29 16:51:20,402] [INFO] [config.py:998:print] DeepSpeedEngine configuration:
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   activation_checkpointing_config  {
x3006c0s19b1n0:     "partition_activations": false, 
x3006c0s19b1n0:     "contiguous_memory_optimization": false, 
x3006c0s19b1n0:     "cpu_checkpointing": false, 
x3006c0s19b1n0:     "number_checkpoints": null, 
x3006c0s19b1n0:     "synchronize_checkpoint_boundary": false, 
x3006c0s19b1n0:     "profile": false
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   amp_enabled .................. False
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   amp_params ................... False
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   autotuning_config ............ {
x3006c0s19b1n0:     "enabled": false, 
x3006c0s19b1n0:     "start_step": null, 
x3006c0s19b1n0:     "end_step": null, 
x3006c0s19b1n0:     "metric_path": null, 
x3006c0s19b1n0:     "arg_mappings": null, 
x3006c0s19b1n0:     "metric": "throughput", 
x3006c0s19b1n0:     "model_info": null, 
x3006c0s19b1n0:     "results_dir": "autotuning_results", 
x3006c0s19b1n0:     "exps_dir": "autotuning_exps", 
x3006c0s19b1n0:     "overwrite": true, 
x3006c0s19b1n0:     "fast": true, 
x3006c0s19b1n0:     "start_profile_step": 3, 
x3006c0s19b1n0:     "end_profile_step": 5, 
x3006c0s19b1n0:     "tuner_type": "gridsearch", 
x3006c0s19b1n0:     "tuner_early_stopping": 5, 
x3006c0s19b1n0:     "tuner_num_trials": 50, 
x3006c0s19b1n0:     "model_info_path": null, 
x3006c0s19b1n0:     "mp_size": 1, 
x3006c0s19b1n0:     "max_train_batch_size": null, 
x3006c0s19b1n0:     "min_train_batch_size": 1, 
x3006c0s19b1n0:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
x3006c0s19b1n0:     "min_train_micro_batch_size_per_gpu": 1, 
x3006c0s19b1n0:     "num_tuning_micro_batch_sizes": 3
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   bfloat16_enabled ............. True
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   bfloat16_immediate_grad_update  False
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   checkpoint_parallel_write_pipeline  False
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   checkpoint_tag_validation_enabled  True
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   checkpoint_tag_validation_fail  False
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f0de3160b50>
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   communication_data_type ...... None
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   curriculum_enabled_legacy .... False
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   curriculum_params_legacy ..... False
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   data_efficiency_enabled ...... False
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   dataloader_drop_last ......... False
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   datastates_config ............ {
x3006c0s19b1n0:     "enabled": null, 
x3006c0s19b1n0:     "config": {
x3006c0s19b1n0:     }
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   disable_allgather ............ False
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   dump_state ................... False
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   dynamic_loss_scale_args ...... None
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   eigenvalue_enabled ........... False
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   eigenvalue_gas_boundary_resolution  1
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   eigenvalue_layer_name ........ bert.encoder.layer
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   eigenvalue_layer_num ......... 0
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   eigenvalue_max_iter .......... 100
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   eigenvalue_stability ......... 1e-06
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   eigenvalue_tol ............... 0.01
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   eigenvalue_verbose ........... False
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   elasticity_enabled ........... False
x3006c0s19b1n0: [2024-03-29 16:51:20,403] [INFO] [config.py:1002:print]   flops_profiler_config ........ {
x3006c0s19b1n0:     "enabled": false, 
x3006c0s19b1n0:     "recompute_fwd_factor": 0.0, 
x3006c0s19b1n0:     "profile_step": 1, 
x3006c0s19b1n0:     "module_depth": -1, 
x3006c0s19b1n0:     "top_modules": 1, 
x3006c0s19b1n0:     "detailed": true, 
x3006c0s19b1n0:     "output_file": null
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   fp16_auto_cast ............... None
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   fp16_enabled ................. False
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   fp16_master_weights_and_gradients  False
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   global_rank .................. 0
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   grad_accum_dtype ............. bf16
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   gradient_accumulation_steps .. 1
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   gradient_clipping ............ 0.0
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   gradient_predivide_factor .... 1.0
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   graph_harvesting ............. False
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   initial_dynamic_scale ........ 1
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   load_universal_checkpoint .... False
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   loss_scale ................... 1.0
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   memory_breakdown ............. True
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   mics_hierarchial_params_gather  False
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   mics_shard_size .............. -1
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   nebula_config ................ {
x3006c0s19b1n0:     "enabled": false, 
x3006c0s19b1n0:     "persistent_storage_path": null, 
x3006c0s19b1n0:     "persistent_time_interval": 100, 
x3006c0s19b1n0:     "num_of_version_in_retention": 2, 
x3006c0s19b1n0:     "enable_nebula_load": true, 
x3006c0s19b1n0:     "load_path": null
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   optimizer_legacy_fusion ...... False
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   optimizer_name ............... None
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   optimizer_params ............. None
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   pld_enabled .................. False
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   pld_params ................... False
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   prescale_gradients ........... False
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   scheduler_name ............... None
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   scheduler_params ............. None
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   seq_parallel_communication_data_type  torch.float32
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   sparse_attention ............. None
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   sparse_gradients_enabled ..... False
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   steps_per_print .............. 1
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   train_batch_size ............. 32
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   train_micro_batch_size_per_gpu  4
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   use_data_before_expert_parallel_  False
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   use_node_local_storage ....... False
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   wall_clock_breakdown ......... True
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   weight_quantization_config ... None
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   world_size ................... 8
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   zero_allow_untested_optimizer  False
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0, prefetch_optimizer=True, part_grads_async=True, prefetch_optimizer_gap=2) sub_group_size=100000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   zero_enabled ................. True
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   zero_force_ds_cpu_optimizer .. True
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:1002:print]   zero_optimization_stage ...... 3
x3006c0s19b1n0: [2024-03-29 16:51:20,404] [INFO] [config.py:988:print_user_config]   json = {
x3006c0s19b1n0:     "train_batch_size": 32, 
x3006c0s19b1n0:     "train_micro_batch_size_per_gpu": 4, 
x3006c0s19b1n0:     "steps_per_print": 1, 
x3006c0s19b1n0:     "zero_optimization": {
x3006c0s19b1n0:         "stage": 3, 
x3006c0s19b1n0:         "offload_optimizer": {
x3006c0s19b1n0:             "device": "cpu", 
x3006c0s19b1n0:             "ratio": 1, 
x3006c0s19b1n0:             "pin_memory": true, 
x3006c0s19b1n0:             "prefetch_optimizer": 1, 
x3006c0s19b1n0:             "part_grads_async": 1, 
x3006c0s19b1n0:             "prefetch_optimizer_gap": 2
x3006c0s19b1n0:         }, 
x3006c0s19b1n0:         "sub_group_size": 1.000000e+08
x3006c0s19b1n0:     }, 
x3006c0s19b1n0:     "bf16": {
x3006c0s19b1n0:         "enabled": true
x3006c0s19b1n0:     }, 
x3006c0s19b1n0:     "data_types": {
x3006c0s19b1n0:         "grad_accum_dtype": "bf16"
x3006c0s19b1n0:     }, 
x3006c0s19b1n0:     "wall_clock_breakdown": true, 
x3006c0s19b1n0:     "memory_breakdown": true, 
x3006c0s19b1n0:     "flops_profiler": {
x3006c0s19b1n0:         "enabled": false
x3006c0s19b1n0:     }
x3006c0s19b1n0: }
x3006c0s1b1n0: <TIMER:model-and-optimizer-setup,50.38402009010315>
x3006c0s1b1n0: <TIMER:model-and-optimizer-setup,50.38418698310852>
x3006c0s1b1n0: <TIMER:model-and-optimizer-setup,50.385398626327515>
x3006c0s1b1n0: <TIMER:model-and-optimizer-setup,50.3854546546936>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,50.385499238967896>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,50.386409759521484>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,50.38726615905762>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,50.387404918670654>
x3006c0s19b1n0: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-29 16:51:21 
x3006c0s19b1n0: > building train, validation, and test datasets ...
x3006c0s19b1n0:  > datasets target sizes (minimum size):
x3006c0s19b1n0:     train:      320
x3006c0s19b1n0:     validation: 0
x3006c0s19b1n0:     test:       0
x3006c0s19b1n0: > building train, validation, and test datasets for GPT ...
x3006c0s19b1n0: Single data path provided for train, valid & test
x3006c0s19b1n0:  > building dataset index ...
x3006c0s19b1n0:     reading sizes...
x3006c0s19b1n0:     reading pointers...
x3006c0s19b1n0:     reading document index...
x3006c0s19b1n0:     creating numpy buffer of mmap...
x3006c0s19b1n0:     creating memory view of numpy buffer...
x3006c0s19b1n0:  > finished creating indexed dataset in 0.001177 seconds
x3006c0s19b1n0:     number of documents: 79000
x3006c0s19b1n0:  > dataset split:
x3006c0s19b1n0:     train:
x3006c0s19b1n0:      document indices in [0, 74971) total of 74971 documents
x3006c0s19b1n0:     validation:
x3006c0s19b1n0:      document indices in [74971, 78921) total of 3950 documents
x3006c0s19b1n0:     test:
x3006c0s19b1n0:      document indices in [78921, 79000) total of 79 documents
x3006c0s19b1n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_doc_idx.npy
x3006c0s19b1n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_sample_idx.npy
x3006c0s19b1n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_shuffle_idx.npy
x3006c0s19b1n0:     loaded indexed file in 0.004 seconds
x3006c0s19b1n0:     total number of samples: 108448
x3006c0s19b1n0:     total number of epochs: 1
x3006c0s19b1n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_doc_idx.npy
x3006c0s19b1n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_sample_idx.npy
x3006c0s19b1n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_shuffle_idx.npy
x3006c0s19b1n0:     loaded indexed file in 0.004 seconds
x3006c0s19b1n0:     total number of samples: 5792
x3006c0s19b1n0:     total number of epochs: 1
x3006c0s19b1n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_doc_idx.npy
x3006c0s19b1n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_sample_idx.npy
x3006c0s19b1n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_shuffle_idx.npy
x3006c0s19b1n0:     loaded indexed file in 0.003 seconds
x3006c0s19b1n0:     total number of samples: 185
x3006c0s19b1n0:     total number of epochs: 1
x3006c0s19b1n0: > finished creating GPT datasets ...
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5071277618408203>
x3006c0s1b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5083656311035156>
x3006c0s1b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5100758075714111>
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5129613876342773>
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5153563022613525>
x3006c0s1b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5334677696228027>
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5562534332275391>
x3006c0s1b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5673456192016602>
x3006c0s19b1n0: [after dataloaders are built] datetime: 2024-03-29 16:51:22 
x3006c0s19b1n0: done with setup ...
x3006c0s19b1n0: training ...
x3006c0s1b1n0: (min, max) time across ranks (ms):
x3006c0s1b1n0:     model-and-optimizer-setup ......................: (50384.02, 50387.41)
x3006c0s1b1n0:     train/valid/test-data-iterators-setup ..........: (507.13, 567.35)
x3006c0s19b1n0: [before training begins] datetime: 2024-03-29 16:51:22 
x3006c0s19b1n0: [before the start of training step] datetime: 2024-03-29 16:51:22 
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:51:22,436] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:51:22,437] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 7.45 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-29 16:51:22,437] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 319.48 GB, percent = 63.5%
x3006c0s19b1n0: [2024-03-29 16:51:22,557] [INFO] [checkpointing.py:539:forward] Activation Checkpointing Information
x3006c0s19b1n0: [2024-03-29 16:51:22,557] [INFO] [checkpointing.py:540:forward] ----Partition Activations False, CPU CHECKPOINTING False
x3006c0s19b1n0: [2024-03-29 16:51:22,557] [INFO] [checkpointing.py:541:forward] ----contiguous Memory Checkpointing False with 60 total layers
x3006c0s19b1n0: [2024-03-29 16:51:22,557] [INFO] [checkpointing.py:543:forward] ----Synchronization False
x3006c0s19b1n0: [2024-03-29 16:51:22,557] [INFO] [checkpointing.py:544:forward] ----Profiling time in checkpointing False
x3006c0s19b1n0: [2024-03-29 16:51:30,861] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:51:30,862] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 19.76 GB         CA 22.95 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-29 16:51:30,862] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 319.64 GB, percent = 63.5%
x3006c0s19b1n0: [2024-03-29 16:51:31,049] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:51:31,049] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 16.84 GB         CA 16.91 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-29 16:51:31,049] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 319.65 GB, percent = 63.5%
x3006c0s19b1n0: [2024-03-29 16:51:56,541] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:51:56,541] [INFO] [utils.py:801:see_memory_usage] MA 13.38 GB         Max_MA 20.43 GB         CA 13.73 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-29 16:51:56,541] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 319.68 GB, percent = 63.5%
x3006c0s19b1n0: [2024-03-29 16:51:56,633] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:51:56,634] [INFO] [utils.py:801:see_memory_usage] MA 13.38 GB         Max_MA 13.38 GB         CA 13.73 GB         Max_CA 14 GB 
x3006c0s19b1n0: [2024-03-29 16:51:56,634] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 319.79 GB, percent = 63.5%
x3006c0s19b1n0: [2024-03-29 16:52:00,913] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.205458164215088
x3006c0s19b1n0: [2024-03-29 16:52:00,930] [INFO] [stage3.py:2251:step] Full outer step loop took 4.223076105117798
x3006c0s1b1n0: [2024-03-29 16:52:01,311] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.603530645370483
x3006c0s1b1n0: [2024-03-29 16:52:01,336] [INFO] [stage3.py:2251:step] Full outer step loop took 4.628756284713745
x3006c0s1b1n0: [2024-03-29 16:52:01,599] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.891281604766846
x3006c0s1b1n0: [2024-03-29 16:52:01,611] [INFO] [stage3.py:2251:step] Full outer step loop took 4.904155969619751
x3006c0s19b1n0: [2024-03-29 16:52:01,707] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.000025749206543
x3006c0s19b1n0: [2024-03-29 16:52:01,720] [INFO] [stage3.py:2251:step] Full outer step loop took 5.013168811798096
x3006c0s19b1n0: [2024-03-29 16:52:01,761] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.052832126617432
x3006c0s1b1n0: [2024-03-29 16:52:01,764] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.057227849960327
x3006c0s1b1n0: [2024-03-29 16:52:01,773] [INFO] [stage3.py:2251:step] Full outer step loop took 5.066070556640625
x3006c0s19b1n0: [2024-03-29 16:52:01,783] [INFO] [stage3.py:2251:step] Full outer step loop took 5.076284646987915
x3006c0s19b1n0: [2024-03-29 16:52:01,793] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.085553169250488
x3006c0s19b1n0: [2024-03-29 16:52:01,801] [INFO] [stage3.py:2251:step] Full outer step loop took 5.094241380691528
x3006c0s1b1n0: [2024-03-29 16:52:01,913] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.206161260604858
x3006c0s1b1n0: [2024-03-29 16:52:01,922] [INFO] [stage3.py:2251:step] Full outer step loop took 5.214859485626221
x3006c0s1b1n0: [2024-03-29 16:52:01,934] [INFO] [stage3.py:2277:step] End to end step took 5.226850509643555
x3006c0s19b1n0: [2024-03-29 16:52:01,934] [INFO] [stage3.py:2277:step] End to end step took 5.226911306381226
x3006c0s19b1n0: [2024-03-29 16:52:01,934] [INFO] [stage3.py:2277:step] End to end step took 5.227086067199707
x3006c0s19b1n0: [2024-03-29 16:52:01,934] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4223.96
x3006c0s1b1n0: [2024-03-29 16:52:01,934] [INFO] [stage3.py:2277:step] End to end step took 5.2272889614105225
x3006c0s19b1n0: [2024-03-29 16:52:01,935] [INFO] [stage3.py:2277:step] End to end step took 5.227534294128418
x3006c0s19b1n0: [2024-03-29 16:52:01,935] [WARNING] [stage3.py:2267:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3006c0s19b1n0: [2024-03-29 16:52:01,935] [INFO] [stage3.py:2277:step] End to end step took 5.2277069091796875
x3006c0s1b1n0: [2024-03-29 16:52:01,935] [INFO] [stage3.py:2277:step] End to end step took 5.227668762207031
x3006c0s1b1n0: [2024-03-29 16:52:01,935] [INFO] [stage3.py:2277:step] End to end step took 5.227456092834473
x3006c0s19b1n0: [2024-03-29 16:52:01,935] [INFO] [logging.py:96:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-29 16:52:01,935] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 8516.68 | bwd_microstep: 25340.98 | bwd_inner_microstep: 25255.99 | bwd_allreduce_microstep: 84.89 | step_microstep: 5301.24
x3006c0s19b1n0: [2024-03-29 16:52:01,935] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 8516.68 | bwd: 25340.97 | bwd_inner: 25256.00 | bwd_allreduce: 84.88 | step: 5301.24
x3006c0s19b1n0: [2024-03-29 16:52:02,042] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:52:02,042] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 14.81 GB         CA 10.34 GB         Max_CA 19 GB 
x3006c0s19b1n0: [2024-03-29 16:52:02,043] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.79 GB, percent = 74.1%
x3006c0s19b1n0: <TIMER:interval-time,39.801531076431274><TIMER:interval-time,39.80151987075806>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,39.801539182662964>
x3006c0s19b1n0: <TIMER:interval-time,39.80154609680176>
x3006c0s1b1n0: <TIMER:interval-time,39.801544189453125><TIMER:interval-time,39.801478147506714>
x3006c0s1b1n0: 
x3006c0s1b1n0: <TIMER:interval-time,39.801552534103394>
x3006c0s1b1n0: <TIMER:interval-time,39.8016562461853>
x3006c0s19b1n0: [Rank 0] (after 1 iterations) memory (MB) | allocated: 10510.36328125 | max allocated: 10510.36376953125 | reserved: 10586.0 | max reserved: 10586.0
x3006c0s1b1n0:  elapsed_time 39.801656 | consumed samples:           32 | consumed tokens:        65536 | elapsed time per iteration (ms): 39801.7 | learning rate: 3.000E-04 | global batch size:    32 | lm loss: 1.215015E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.804 | TFLOPs: 55.63 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:52:02,152] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:52:02,152] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:52:02,152] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.88 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:52:08,719] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:52:08,720] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-29 16:52:08,720] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.9 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:52:08,798] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:52:08,799] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-29 16:52:08,799] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.9 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:52:25,808] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:52:25,808] [INFO] [utils.py:801:see_memory_usage] MA 14.64 GB         Max_MA 23.87 GB         CA 14.99 GB         Max_CA 34 GB 
x3006c0s19b1n0: [2024-03-29 16:52:25,809] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.91 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:52:25,880] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:52:25,881] [INFO] [utils.py:801:see_memory_usage] MA 14.64 GB         Max_MA 14.64 GB         CA 14.99 GB         Max_CA 15 GB 
x3006c0s19b1n0: [2024-03-29 16:52:25,881] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.91 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:52:29,363] [INFO] [stage3.py:2243:step] With missing steps outer loop took 3.4570226669311523
x3006c0s19b1n0: [2024-03-29 16:52:29,377] [INFO] [stage3.py:2251:step] Full outer step loop took 3.4711790084838867
x3006c0s1b1n0: [2024-03-29 16:52:29,757] [INFO] [stage3.py:2243:step] With missing steps outer loop took 3.8514795303344727
x3006c0s1b1n0: [2024-03-29 16:52:29,782] [INFO] [stage3.py:2251:step] Full outer step loop took 3.8761982917785645
x3006c0s19b1n0: [2024-03-29 16:52:29,816] [INFO] [stage3.py:2243:step] With missing steps outer loop took 3.9108517169952393
x3006c0s19b1n0: [2024-03-29 16:52:29,825] [INFO] [stage3.py:2251:step] Full outer step loop took 3.9195311069488525
x3006c0s1b1n0: [2024-03-29 16:52:30,028] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.122807502746582
x3006c0s1b1n0: [2024-03-29 16:52:30,035] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.129639148712158
x3006c0s1b1n0: [2024-03-29 16:52:30,038] [INFO] [stage3.py:2251:step] Full outer step loop took 4.132659912109375
x3006c0s1b1n0: [2024-03-29 16:52:30,046] [INFO] [stage3.py:2251:step] Full outer step loop took 4.140135049819946
x3006c0s1b1n0: [2024-03-29 16:52:30,107] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.202037334442139
x3006c0s1b1n0: [2024-03-29 16:52:30,116] [INFO] [stage3.py:2251:step] Full outer step loop took 4.2106969356536865
x3006c0s19b1n0: [2024-03-29 16:52:30,197] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.291788578033447
x3006c0s19b1n0: [2024-03-29 16:52:30,203] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.297512054443359
x3006c0s19b1n0: [2024-03-29 16:52:30,206] [INFO] [stage3.py:2251:step] Full outer step loop took 4.300451755523682
x3006c0s19b1n0: [2024-03-29 16:52:30,212] [INFO] [stage3.py:2251:step] Full outer step loop took 4.306187868118286
x3006c0s19b1n0: [2024-03-29 16:52:30,222] [INFO] [stage3.py:2277:step] End to end step took 4.316984176635742
x3006c0s1b1n0: [2024-03-29 16:52:30,222] [INFO] [stage3.py:2277:step] End to end step took 4.316958665847778
x3006c0s19b1n0: [2024-03-29 16:52:30,222] [INFO] [stage3.py:2277:step] End to end step took 4.317029237747192
x3006c0s19b1n0: [2024-03-29 16:52:30,222] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 3471.36
x3006c0s19b1n0: [2024-03-29 16:52:30,223] [INFO] [stage3.py:2277:step] End to end step took 4.317300319671631
x3006c0s1b1n0: [2024-03-29 16:52:30,223] [INFO] [stage3.py:2277:step] End to end step took 4.317368507385254
x3006c0s1b1n0: [2024-03-29 16:52:30,223] [INFO] [stage3.py:2277:step] End to end step took 4.317412376403809
x3006c0s1b1n0: [2024-03-29 16:52:30,223] [INFO] [stage3.py:2277:step] End to end step took 4.317439317703247
x3006c0s19b1n0: [2024-03-29 16:52:30,223] [INFO] [stage3.py:2277:step] End to end step took 4.31758975982666
x3006c0s19b1n0: [2024-03-29 16:52:30,223] [INFO] [logging.py:96:log_dist] [Rank 0] step=2, skipped=0, lr=[0.0002918585038060976], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-29 16:52:30,223] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6535.26 | bwd_microstep: 16847.77 | bwd_inner_microstep: 16765.17 | bwd_allreduce_microstep: 82.53 | step_microstep: 4341.97
x3006c0s19b1n0: [2024-03-29 16:52:30,224] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6535.26 | bwd: 16847.76 | bwd_inner: 16765.16 | bwd_allreduce: 82.54 | step: 4341.97
x3006c0s19b1n0: [2024-03-29 16:52:30,337] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:52:30,337] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 15.23 GB         CA 10.34 GB         Max_CA 19 GB 
x3006c0s19b1n0: [2024-03-29 16:52:30,337] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.91 GB, percent = 74.1%
x3006c0s19b1n0: <TIMER:interval-time,28.29428219795227><TIMER:interval-time,28.294299602508545>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,28.29430389404297>
x3006c0s1b1n0: <TIMER:interval-time,28.294318914413452><TIMER:interval-time,28.294318914413452><TIMER:interval-time,28.294320583343506><TIMER:interval-time,28.294322967529297>
x3006c0s1b1n0: 
x3006c0s1b1n0: 
x3006c0s1b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,28.29440212249756>
x3006c0s1b1n0:  elapsed_time 28.294321 | consumed samples:           64 | consumed tokens:       131072 | elapsed time per iteration (ms): 28294.3 | learning rate: 2.919E-04 | global batch size:    32 | lm loss: 1.082593E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.131 | TFLOPs: 78.25 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:52:30,472] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:52:30,473] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:52:30,473] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.94 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:52:36,866] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:52:36,866] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-29 16:52:36,866] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.94 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:52:36,949] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:52:36,949] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-29 16:52:36,950] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.94 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:52:53,574] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:52:53,575] [INFO] [utils.py:801:see_memory_usage] MA 14.64 GB         Max_MA 23.87 GB         CA 14.99 GB         Max_CA 34 GB 
x3006c0s19b1n0: [2024-03-29 16:52:53,575] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.94 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:52:53,647] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:52:53,648] [INFO] [utils.py:801:see_memory_usage] MA 14.64 GB         Max_MA 14.64 GB         CA 14.99 GB         Max_CA 15 GB 
x3006c0s19b1n0: [2024-03-29 16:52:53,648] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.94 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:52:57,006] [INFO] [stage3.py:2243:step] With missing steps outer loop took 3.3348751068115234
x3006c0s19b1n0: [2024-03-29 16:52:57,033] [INFO] [stage3.py:2251:step] Full outer step loop took 3.361617088317871
x3006c0s1b1n0: [2024-03-29 16:52:57,524] [INFO] [stage3.py:2243:step] With missing steps outer loop took 3.8521628379821777
x3006c0s1b1n0: [2024-03-29 16:52:57,532] [INFO] [stage3.py:2251:step] Full outer step loop took 3.8608787059783936
x3006c0s19b1n0: [2024-03-29 16:52:57,768] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.096308946609497
x3006c0s19b1n0: [2024-03-29 16:52:57,777] [INFO] [stage3.py:2251:step] Full outer step loop took 4.1049981117248535
x3006c0s1b1n0: [2024-03-29 16:52:57,861] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.18945574760437
x3006c0s1b1n0: [2024-03-29 16:52:57,870] [INFO] [stage3.py:2251:step] Full outer step loop took 4.198122978210449
x3006c0s1b1n0: [2024-03-29 16:52:57,935] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.263390064239502
x3006c0s1b1n0: [2024-03-29 16:52:57,945] [INFO] [stage3.py:2251:step] Full outer step loop took 4.273877382278442
x3006c0s19b1n0: [2024-03-29 16:52:57,949] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.277495622634888
x3006c0s19b1n0: [2024-03-29 16:52:57,958] [INFO] [stage3.py:2251:step] Full outer step loop took 4.286127805709839
x3006c0s1b1n0: [2024-03-29 16:52:58,008] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.336063623428345
x3006c0s1b1n0: [2024-03-29 16:52:58,016] [INFO] [stage3.py:2251:step] Full outer step loop took 4.344708204269409
x3006c0s19b1n0: [2024-03-29 16:52:58,018] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.346768379211426
x3006c0s19b1n0: [2024-03-29 16:52:58,027] [INFO] [stage3.py:2251:step] Full outer step loop took 4.355434894561768
x3006c0s19b1n0: [2024-03-29 16:52:58,038] [INFO] [stage3.py:2277:step] End to end step took 4.366046905517578
x3006c0s1b1n0: [2024-03-29 16:52:58,038] [INFO] [stage3.py:2277:step] End to end step took 4.366041421890259
x3006c0s19b1n0: [2024-03-29 16:52:58,038] [INFO] [stage3.py:2277:step] End to end step took 4.36629843711853
x3006c0s1b1n0: [2024-03-29 16:52:58,038] [INFO] [stage3.py:2277:step] End to end step took 4.3663341999053955
x3006c0s1b1n0: [2024-03-29 16:52:58,038] [INFO] [stage3.py:2277:step] End to end step took 4.366482496261597
x3006c0s1b1n0: [2024-03-29 16:52:58,038] [INFO] [stage3.py:2277:step] End to end step took 4.366587400436401
x3006c0s19b1n0: [2024-03-29 16:52:58,038] [INFO] [stage3.py:2277:step] End to end step took 4.366348743438721
x3006c0s19b1n0: [2024-03-29 16:52:58,038] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 3363.03
x3006c0s19b1n0: [2024-03-29 16:52:58,038] [INFO] [stage3.py:2277:step] End to end step took 4.366972208023071
x3006c0s19b1n0: [2024-03-29 16:52:58,039] [INFO] [logging.py:96:log_dist] [Rank 0] step=3, skipped=0, lr=[0.00026841599982106197], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-29 16:52:58,039] [INFO] [timer.py:260:stop] epoch=0/micro_step=3/global_step=3, RunningAvgSamplesPerSec=1.1608817401550315, CurrSamplesPerSec=1.1608817401550315, MemAllocated=10.26GB, MaxMemAllocated=15.23GB
x3006c0s19b1n0: [2024-03-29 16:52:58,039] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6349.15 | bwd_microstep: 16456.00 | bwd_inner_microstep: 16375.28 | bwd_allreduce_microstep: 80.66 | step_microstep: 4391.19
x3006c0s19b1n0: [2024-03-29 16:52:58,040] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6349.14 | bwd: 16456.00 | bwd_inner: 16375.27 | bwd_allreduce: 80.67 | step: 4391.19
x3006c0s19b1n0: [2024-03-29 16:52:58,158] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:52:58,158] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 15.23 GB         CA 10.34 GB         Max_CA 19 GB 
x3006c0s19b1n0: [2024-03-29 16:52:58,159] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.94 GB, percent = 74.1%
x3006c0s19b1n0: <TIMER:interval-time,27.821046829223633><TIMER:interval-time,27.821009159088135>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,27.82105851173401><TIMER:interval-time,27.82105779647827>
x3006c0s19b1n0: 
x3006c0s1b1n0: <TIMER:interval-time,27.821061611175537>
x3006c0s1b1n0: <TIMER:interval-time,27.821064472198486><TIMER:interval-time,27.821067810058594><TIMER:interval-time,27.821067810058594>
x3006c0s1b1n0: 
x3006c0s1b1n0: 
x3006c0s1b1n0:  elapsed_time 27.821068 | consumed samples:           96 | consumed tokens:       196608 | elapsed time per iteration (ms): 27821.1 | learning rate: 2.684E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.150 | TFLOPs: 79.58 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:52:58,318] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:52:58,318] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:52:58,318] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.93 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:53:04,749] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:53:04,750] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-29 16:53:04,750] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.93 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:53:04,854] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:53:04,855] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-29 16:53:04,855] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.93 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:53:21,447] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:53:21,448] [INFO] [utils.py:801:see_memory_usage] MA 14.64 GB         Max_MA 23.87 GB         CA 14.99 GB         Max_CA 34 GB 
x3006c0s19b1n0: [2024-03-29 16:53:21,448] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.93 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:53:21,515] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:53:21,516] [INFO] [utils.py:801:see_memory_usage] MA 14.64 GB         Max_MA 14.64 GB         CA 14.99 GB         Max_CA 15 GB 
x3006c0s19b1n0: [2024-03-29 16:53:21,516] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.93 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:53:25,043] [INFO] [stage3.py:2243:step] With missing steps outer loop took 3.503729820251465
x3006c0s19b1n0: [2024-03-29 16:53:25,056] [INFO] [stage3.py:2251:step] Full outer step loop took 3.5163674354553223
x3006c0s1b1n0: [2024-03-29 16:53:25,456] [INFO] [stage3.py:2243:step] With missing steps outer loop took 3.916590690612793
x3006c0s1b1n0: [2024-03-29 16:53:25,479] [INFO] [stage3.py:2251:step] Full outer step loop took 3.939754009246826
x3006c0s19b1n0: [2024-03-29 16:53:25,683] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.143776178359985
x3006c0s19b1n0: [2024-03-29 16:53:25,692] [INFO] [stage3.py:2251:step] Full outer step loop took 4.1524646282196045
x3006c0s1b1n0: [2024-03-29 16:53:25,822] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.282617568969727
x3006c0s1b1n0: [2024-03-29 16:53:25,829] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.290121555328369
x3006c0s1b1n0: [2024-03-29 16:53:25,833] [INFO] [stage3.py:2251:step] Full outer step loop took 4.294167757034302
x3006c0s1b1n0: [2024-03-29 16:53:25,838] [INFO] [stage3.py:2251:step] Full outer step loop took 4.298791408538818
x3006c0s19b1n0: [2024-03-29 16:53:25,853] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.31325888633728
x3006c0s19b1n0: [2024-03-29 16:53:25,870] [INFO] [stage3.py:2251:step] Full outer step loop took 4.330824851989746
x3006c0s19b1n0: [2024-03-29 16:53:25,897] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.358116626739502
x3006c0s19b1n0: [2024-03-29 16:53:25,906] [INFO] [stage3.py:2251:step] Full outer step loop took 4.366765260696411
x3006c0s1b1n0: [2024-03-29 16:53:25,964] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.4245758056640625
x3006c0s1b1n0: [2024-03-29 16:53:25,972] [INFO] [stage3.py:2251:step] Full outer step loop took 4.433214902877808
x3006c0s1b1n0: [2024-03-29 16:53:25,983] [INFO] [stage3.py:2277:step] End to end step took 4.4439146518707275
x3006c0s19b1n0: [2024-03-29 16:53:25,983] [INFO] [stage3.py:2277:step] End to end step took 4.443818807601929
x3006c0s19b1n0: [2024-03-29 16:53:25,983] [INFO] [stage3.py:2277:step] End to end step took 4.443903684616089
x3006c0s1b1n0: [2024-03-29 16:53:25,983] [INFO] [stage3.py:2277:step] End to end step took 4.444040536880493
x3006c0s1b1n0: [2024-03-29 16:53:25,983] [INFO] [stage3.py:2277:step] End to end step took 4.4441914558410645
x3006c0s19b1n0: [2024-03-29 16:53:25,983] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 3516.56
x3006c0s19b1n0: [2024-03-29 16:53:25,984] [INFO] [stage3.py:2277:step] End to end step took 4.444376707077026
x3006c0s19b1n0: [2024-03-29 16:53:25,984] [INFO] [stage3.py:2277:step] End to end step took 4.444298028945923
x3006c0s1b1n0: [2024-03-29 16:53:25,984] [INFO] [stage3.py:2277:step] End to end step took 4.444504976272583
x3006c0s19b1n0: [2024-03-29 16:53:25,984] [INFO] [logging.py:96:log_dist] [Rank 0] step=4, skipped=0, lr=[0.00023249999999999996], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-29 16:53:25,984] [INFO] [timer.py:260:stop] epoch=0/micro_step=4/global_step=4, RunningAvgSamplesPerSec=1.1587924178830218, CurrSamplesPerSec=1.1567106027067555, MemAllocated=10.26GB, MaxMemAllocated=15.23GB
x3006c0s19b1n0: [2024-03-29 16:53:25,985] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6394.94 | bwd_microstep: 16426.93 | bwd_inner_microstep: 16346.46 | bwd_allreduce_microstep: 80.39 | step_microstep: 4468.69
x3006c0s19b1n0: [2024-03-29 16:53:25,985] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6394.92 | bwd: 16426.93 | bwd_inner: 16346.45 | bwd_allreduce: 80.41 | step: 4468.70
x3006c0s19b1n0: [2024-03-29 16:53:26,101] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:53:26,102] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 15.23 GB         CA 10.34 GB         Max_CA 19 GB 
x3006c0s19b1n0: [2024-03-29 16:53:26,102] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.92 GB, percent = 74.1%
x3006c0s19b1n0: <TIMER:interval-time,27.942923069000244><TIMER:interval-time,27.942930221557617>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,27.942930459976196>
x3006c0s19b1n0: <TIMER:interval-time,27.94297218322754>
x3006c0s1b1n0: <TIMER:interval-time,27.942945957183838><TIMER:interval-time,27.942944288253784>
x3006c0s1b1n0: 
x3006c0s1b1n0: <TIMER:interval-time,27.942951440811157>
x3006c0s1b1n0: <TIMER:interval-time,27.943061590194702>
x3006c0s1b1n0:  elapsed_time 27.942944 | consumed samples:          128 | consumed tokens:       262144 | elapsed time per iteration (ms): 27942.9 | learning rate: 2.325E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.145 | TFLOPs: 79.24 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:53:26,233] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:53:26,234] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:53:26,234] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.93 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:53:32,530] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:53:32,530] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-29 16:53:32,531] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.93 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:53:32,616] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:53:32,617] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-29 16:53:32,617] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.93 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:53:49,290] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:53:49,290] [INFO] [utils.py:801:see_memory_usage] MA 14.64 GB         Max_MA 23.87 GB         CA 14.99 GB         Max_CA 34 GB 
x3006c0s19b1n0: [2024-03-29 16:53:49,290] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.93 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:53:49,365] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:53:49,366] [INFO] [utils.py:801:see_memory_usage] MA 14.64 GB         Max_MA 14.64 GB         CA 14.99 GB         Max_CA 15 GB 
x3006c0s19b1n0: [2024-03-29 16:53:49,366] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.93 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:53:52,916] [INFO] [stage3.py:2243:step] With missing steps outer loop took 3.5257201194763184
x3006c0s19b1n0: [2024-03-29 16:53:52,929] [INFO] [stage3.py:2251:step] Full outer step loop took 3.5388522148132324
x3006c0s1b1n0: [2024-03-29 16:53:53,301] [INFO] [stage3.py:2243:step] With missing steps outer loop took 3.91095232963562
x3006c0s1b1n0: [2024-03-29 16:53:53,310] [INFO] [stage3.py:2251:step] Full outer step loop took 3.9197640419006348
x3006c0s1b1n0: [2024-03-29 16:53:53,476] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.085986614227295
x3006c0s1b1n0: [2024-03-29 16:53:53,486] [INFO] [stage3.py:2251:step] Full outer step loop took 4.095404624938965
x3006c0s19b1n0: [2024-03-29 16:53:53,553] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.162629842758179
x3006c0s1b1n0: [2024-03-29 16:53:53,557] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.166889667510986
x3006c0s19b1n0: [2024-03-29 16:53:53,562] [INFO] [stage3.py:2251:step] Full outer step loop took 4.171659469604492
x3006c0s1b1n0: [2024-03-29 16:53:53,566] [INFO] [stage3.py:2251:step] Full outer step loop took 4.175553560256958
x3006c0s1b1n0: [2024-03-29 16:53:53,631] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.240868330001831
x3006c0s1b1n0: [2024-03-29 16:53:53,640] [INFO] [stage3.py:2251:step] Full outer step loop took 4.249515056610107
x3006c0s19b1n0: [2024-03-29 16:53:53,728] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.3377392292022705
x3006c0s19b1n0: [2024-03-29 16:53:53,737] [INFO] [stage3.py:2251:step] Full outer step loop took 4.346388339996338
x3006c0s19b1n0: [2024-03-29 16:53:53,804] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.413698673248291
x3006c0s19b1n0: [2024-03-29 16:53:53,813] [INFO] [stage3.py:2251:step] Full outer step loop took 4.422362804412842
x3006c0s19b1n0: [2024-03-29 16:53:53,825] [INFO] [stage3.py:2277:step] End to end step took 4.4346373081207275
x3006c0s19b1n0: [2024-03-29 16:53:53,825] [INFO] [stage3.py:2277:step] End to end step took 4.4348015785217285
x3006c0s1b1n0: [2024-03-29 16:53:53,825] [INFO] [stage3.py:2277:step] End to end step took 4.4347827434539795
x3006c0s1b1n0: [2024-03-29 16:53:53,825] [INFO] [stage3.py:2277:step] End to end step took 4.4350926876068115
x3006c0s19b1n0: [2024-03-29 16:53:53,825] [INFO] [stage3.py:2277:step] End to end step took 4.435086488723755
x3006c0s1b1n0: [2024-03-29 16:53:53,826] [INFO] [stage3.py:2277:step] End to end step took 4.435296058654785
x3006c0s1b1n0: [2024-03-29 16:53:53,826] [INFO] [stage3.py:2277:step] End to end step took 4.435291528701782
x3006c0s19b1n0: [2024-03-29 16:53:53,825] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 3539.06
x3006c0s19b1n0: [2024-03-29 16:53:53,826] [INFO] [stage3.py:2277:step] End to end step took 4.435437202453613
x3006c0s19b1n0: [2024-03-29 16:53:53,826] [INFO] [logging.py:96:log_dist] [Rank 0] step=5, skipped=0, lr=[0.0001884425039850356], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-29 16:53:53,827] [INFO] [timer.py:260:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=1.1591211318006223, CurrSamplesPerSec=1.1597791194298401, MemAllocated=10.26GB, MaxMemAllocated=15.23GB
x3006c0s19b1n0: [2024-03-29 16:53:53,827] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6254.95 | bwd_microstep: 16501.89 | bwd_inner_microstep: 16419.64 | bwd_allreduce_microstep: 82.19 | step_microstep: 4460.61
x3006c0s19b1n0: [2024-03-29 16:53:53,827] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6254.94 | bwd: 16501.89 | bwd_inner: 16419.63 | bwd_allreduce: 82.20 | step: 4460.61
x3006c0s19b1n0: [2024-03-29 16:53:53,976] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:53:53,977] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 15.23 GB         CA 10.34 GB         Max_CA 19 GB 
x3006c0s19b1n0: [2024-03-29 16:53:53,977] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.93 GB, percent = 74.1%
x3006c0s19b1n0: <TIMER:interval-time,27.874640703201294><TIMER:interval-time,27.874639987945557>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,27.874642848968506>
x3006c0s19b1n0: <TIMER:interval-time,27.87464165687561>
x3006c0s1b1n0: <TIMER:interval-time,27.87464928627014><TIMER:interval-time,27.874670028686523><TIMER:interval-time,27.87467312812805>
x3006c0s1b1n0: 
x3006c0s1b1n0: 
x3006c0s1b1n0: <TIMER:interval-time,27.874675512313843>
x3006c0s1b1n0:  elapsed_time 27.874676 | consumed samples:          160 | consumed tokens:       327680 | elapsed time per iteration (ms): 27874.7 | learning rate: 1.884E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.148 | TFLOPs: 79.43 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:53:54,078] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:53:54,078] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:53:54,078] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.92 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:54:00,374] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:54:00,375] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-29 16:54:00,375] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.92 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:54:00,454] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:54:00,455] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-29 16:54:00,455] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.92 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:54:16,480] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:54:16,481] [INFO] [utils.py:801:see_memory_usage] MA 14.64 GB         Max_MA 23.87 GB         CA 14.99 GB         Max_CA 34 GB 
x3006c0s19b1n0: [2024-03-29 16:54:16,481] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.92 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:54:16,550] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:54:16,551] [INFO] [utils.py:801:see_memory_usage] MA 14.64 GB         Max_MA 14.64 GB         CA 14.99 GB         Max_CA 15 GB 
x3006c0s19b1n0: [2024-03-29 16:54:16,551] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.92 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:54:19,977] [INFO] [stage3.py:2243:step] With missing steps outer loop took 3.4027438163757324
x3006c0s19b1n0: [2024-03-29 16:54:19,991] [INFO] [stage3.py:2251:step] Full outer step loop took 3.416675567626953
x3006c0s19b1n0: [2024-03-29 16:54:20,447] [INFO] [stage3.py:2243:step] With missing steps outer loop took 3.8724782466888428
x3006c0s19b1n0: [2024-03-29 16:54:20,456] [INFO] [stage3.py:2251:step] Full outer step loop took 3.881175994873047
x3006c0s1b1n0: [2024-03-29 16:54:20,510] [INFO] [stage3.py:2243:step] With missing steps outer loop took 3.935476779937744
x3006c0s1b1n0: [2024-03-29 16:54:20,519] [INFO] [stage3.py:2251:step] Full outer step loop took 3.944237470626831
x3006c0s1b1n0: [2024-03-29 16:54:20,656] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.081749439239502
x3006c0s19b1n0: [2024-03-29 16:54:20,660] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.0858073234558105
x3006c0s1b1n0: [2024-03-29 16:54:20,667] [INFO] [stage3.py:2251:step] Full outer step loop took 4.092310428619385
x3006c0s19b1n0: [2024-03-29 16:54:20,669] [INFO] [stage3.py:2251:step] Full outer step loop took 4.094489574432373
x3006c0s1b1n0: [2024-03-29 16:54:20,739] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.16411828994751
x3006c0s1b1n0: [2024-03-29 16:54:20,747] [INFO] [stage3.py:2251:step] Full outer step loop took 4.172784090042114
x3006c0s19b1n0: [2024-03-29 16:54:20,842] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.266937255859375
x3006c0s1b1n0: [2024-03-29 16:54:20,842] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.267832517623901
x3006c0s19b1n0: [2024-03-29 16:54:20,850] [INFO] [stage3.py:2251:step] Full outer step loop took 4.275576591491699
x3006c0s1b1n0: [2024-03-29 16:54:20,851] [INFO] [stage3.py:2251:step] Full outer step loop took 4.2764952182769775
x3006c0s19b1n0: [2024-03-29 16:54:20,862] [INFO] [stage3.py:2277:step] End to end step took 4.287012815475464
x3006c0s1b1n0: [2024-03-29 16:54:20,862] [INFO] [stage3.py:2277:step] End to end step took 4.286978483200073
x3006c0s19b1n0: [2024-03-29 16:54:20,862] [INFO] [stage3.py:2277:step] End to end step took 4.28725528717041
x3006c0s19b1n0: [2024-03-29 16:54:20,862] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 3416.96
x3006c0s19b1n0: [2024-03-29 16:54:20,862] [INFO] [stage3.py:2277:step] End to end step took 4.287549734115601
x3006c0s1b1n0: [2024-03-29 16:54:20,862] [INFO] [stage3.py:2277:step] End to end step took 4.287498950958252
x3006c0s1b1n0: [2024-03-29 16:54:20,862] [INFO] [stage3.py:2277:step] End to end step took 4.287511587142944
x3006c0s1b1n0: [2024-03-29 16:54:20,862] [INFO] [stage3.py:2277:step] End to end step took 4.287587404251099
x3006c0s19b1n0: [2024-03-29 16:54:20,862] [INFO] [stage3.py:2277:step] End to end step took 4.287473201751709
x3006c0s19b1n0: [2024-03-29 16:54:20,862] [INFO] [logging.py:96:log_dist] [Rank 0] step=6, skipped=0, lr=[0.0001415574960149644], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-29 16:54:20,863] [INFO] [timer.py:260:stop] epoch=0/micro_step=6/global_step=6, RunningAvgSamplesPerSec=1.1678343572856056, CurrSamplesPerSec=1.1947781453841553, MemAllocated=10.26GB, MaxMemAllocated=15.23GB
x3006c0s19b1n0: [2024-03-29 16:54:20,863] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6256.91 | bwd_microstep: 15866.22 | bwd_inner_microstep: 15790.93 | bwd_allreduce_microstep: 75.23 | step_microstep: 4311.87
x3006c0s19b1n0: [2024-03-29 16:54:20,863] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6256.90 | bwd: 15866.22 | bwd_inner: 15790.92 | bwd_allreduce: 75.24 | step: 4311.88
x3006c0s19b1n0: [2024-03-29 16:54:20,972] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:54:20,973] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 15.23 GB         CA 10.34 GB         Max_CA 19 GB 
x3006c0s19b1n0: [2024-03-29 16:54:20,973] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.92 GB, percent = 74.1%
x3006c0s19b1n0: <TIMER:interval-time,26.995960235595703><TIMER:interval-time,26.995969772338867>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,26.995972633361816><TIMER:interval-time,26.995948791503906>
x3006c0s19b1n0: 
x3006c0s1b1n0: <TIMER:interval-time,26.995984077453613><TIMER:interval-time,26.99598526954651><TIMER:interval-time,26.99598741531372>
x3006c0s1b1n0: 
x3006c0s1b1n0: <TIMER:interval-time,26.99598979949951>
x3006c0s1b1n0: 
x3006c0s1b1n0:  elapsed_time 26.995985 | consumed samples:          192 | consumed tokens:       393216 | elapsed time per iteration (ms): 26996.0 | learning rate: 1.416E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.185 | TFLOPs: 82.02 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:54:21,097] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:54:21,097] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:54:21,097] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.94 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:54:27,129] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:54:27,129] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-29 16:54:27,129] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.94 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:54:27,203] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:54:27,203] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-29 16:54:27,203] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.94 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:54:43,255] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:54:43,255] [INFO] [utils.py:801:see_memory_usage] MA 14.64 GB         Max_MA 23.87 GB         CA 14.99 GB         Max_CA 34 GB 
x3006c0s19b1n0: [2024-03-29 16:54:43,255] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.94 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:54:43,324] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:54:43,324] [INFO] [utils.py:801:see_memory_usage] MA 14.64 GB         Max_MA 14.64 GB         CA 14.99 GB         Max_CA 15 GB 
x3006c0s19b1n0: [2024-03-29 16:54:43,325] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.94 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:54:46,938] [INFO] [stage3.py:2243:step] With missing steps outer loop took 3.589289903640747
x3006c0s19b1n0: [2024-03-29 16:54:46,953] [INFO] [stage3.py:2251:step] Full outer step loop took 3.604168653488159
x3006c0s1b1n0: [2024-03-29 16:54:47,309] [INFO] [stage3.py:2243:step] With missing steps outer loop took 3.9602298736572266
x3006c0s1b1n0: [2024-03-29 16:54:47,318] [INFO] [stage3.py:2251:step] Full outer step loop took 3.968968391418457
x3006c0s19b1n0: [2024-03-29 16:54:47,339] [INFO] [stage3.py:2243:step] With missing steps outer loop took 3.9907925128936768
x3006c0s19b1n0: [2024-03-29 16:54:47,348] [INFO] [stage3.py:2251:step] Full outer step loop took 3.9995505809783936
x3006c0s19b1n0: [2024-03-29 16:54:47,529] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.1800971031188965
x3006c0s1b1n0: [2024-03-29 16:54:47,534] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.185749530792236
x3006c0s1b1n0: [2024-03-29 16:54:47,535] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.185860633850098
x3006c0s19b1n0: [2024-03-29 16:54:47,537] [INFO] [stage3.py:2251:step] Full outer step loop took 4.188793659210205
x3006c0s1b1n0: [2024-03-29 16:54:47,543] [INFO] [stage3.py:2251:step] Full outer step loop took 4.194716453552246
x3006c0s1b1n0: [2024-03-29 16:54:47,543] [INFO] [stage3.py:2251:step] Full outer step loop took 4.194730758666992
x3006c0s1b1n0: [2024-03-29 16:54:47,650] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.301035165786743
x3006c0s1b1n0: [2024-03-29 16:54:47,658] [INFO] [stage3.py:2251:step] Full outer step loop took 4.309690475463867
x3006c0s19b1n0: [2024-03-29 16:54:47,741] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.392573118209839
x3006c0s19b1n0: [2024-03-29 16:54:47,750] [INFO] [stage3.py:2251:step] Full outer step loop took 4.401215553283691
x3006c0s1b1n0: [2024-03-29 16:54:47,760] [INFO] [stage3.py:2277:step] End to end step took 4.411251068115234
x3006c0s19b1n0: [2024-03-29 16:54:47,760] [INFO] [stage3.py:2277:step] End to end step took 4.411243200302124
x3006c0s1b1n0: [2024-03-29 16:54:47,760] [INFO] [stage3.py:2277:step] End to end step took 4.411376476287842
x3006c0s19b1n0: [2024-03-29 16:54:47,760] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 3604.33
x3006c0s19b1n0: [2024-03-29 16:54:47,760] [INFO] [stage3.py:2277:step] End to end step took 4.411819934844971
x3006c0s19b1n0: [2024-03-29 16:54:47,760] [INFO] [stage3.py:2277:step] End to end step took 4.4116692543029785
x3006c0s1b1n0: [2024-03-29 16:54:47,760] [INFO] [stage3.py:2277:step] End to end step took 4.4118945598602295
x3006c0s19b1n0: [2024-03-29 16:54:47,760] [INFO] [stage3.py:2277:step] End to end step took 4.411922931671143
x3006c0s19b1n0: [2024-03-29 16:54:47,761] [INFO] [logging.py:96:log_dist] [Rank 0] step=7, skipped=0, lr=[9.750000000000001e-05], mom=[(0.9, 0.95)]
x3006c0s1b1n0: [2024-03-29 16:54:47,761] [INFO] [stage3.py:2277:step] End to end step took 4.412142992019653
x3006c0s19b1n0: [2024-03-29 16:54:47,761] [INFO] [timer.py:260:stop] epoch=0/micro_step=7/global_step=7, RunningAvgSamplesPerSec=1.1741667978973223, CurrSamplesPerSec=1.2001985234571961, MemAllocated=10.26GB, MaxMemAllocated=15.23GB
x3006c0s19b1n0: [2024-03-29 16:54:47,761] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6000.20 | bwd_microstep: 15889.87 | bwd_inner_microstep: 15814.23 | bwd_allreduce_microstep: 75.58 | step_microstep: 4436.54
x3006c0s19b1n0: [2024-03-29 16:54:47,761] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6000.20 | bwd: 15889.87 | bwd_inner: 15814.22 | bwd_allreduce: 75.59 | step: 4436.54
x3006c0s19b1n0: [2024-03-29 16:54:47,872] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:54:47,872] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 15.23 GB         CA 10.34 GB         Max_CA 19 GB 
x3006c0s19b1n0: [2024-03-29 16:54:47,873] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.93 GB, percent = 74.1%
x3006c0s19b1n0: <TIMER:interval-time,26.899484395980835><TIMER:interval-time,26.899486780166626><TIMER:interval-time,26.8994882106781><TIMER:interval-time,26.899487495422363>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b1n0: <TIMER:interval-time,26.89951467514038><TIMER:interval-time,26.89951467514038><TIMER:interval-time,26.899516582489014>
x3006c0s1b1n0: 
x3006c0s1b1n0: 
x3006c0s1b1n0: <TIMER:interval-time,26.899521350860596>
x3006c0s1b1n0:  elapsed_time 26.899517 | consumed samples:          224 | consumed tokens:       458752 | elapsed time per iteration (ms): 26899.5 | learning rate: 9.750E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.190 | TFLOPs: 82.31 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:54:48,004] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:54:48,004] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:54:48,004] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.93 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:54:54,240] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:54:54,241] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-29 16:54:54,241] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.92 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:54:54,331] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:54:54,332] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-29 16:54:54,332] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.92 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:55:10,596] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:55:10,596] [INFO] [utils.py:801:see_memory_usage] MA 14.64 GB         Max_MA 23.87 GB         CA 14.99 GB         Max_CA 34 GB 
x3006c0s19b1n0: [2024-03-29 16:55:10,596] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.92 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:55:10,673] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:55:10,674] [INFO] [utils.py:801:see_memory_usage] MA 14.64 GB         Max_MA 14.64 GB         CA 14.99 GB         Max_CA 15 GB 
x3006c0s19b1n0: [2024-03-29 16:55:10,674] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.92 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:55:14,097] [INFO] [stage3.py:2243:step] With missing steps outer loop took 3.398686170578003
x3006c0s19b1n0: [2024-03-29 16:55:14,112] [INFO] [stage3.py:2251:step] Full outer step loop took 3.413759469985962
x3006c0s1b1n0: [2024-03-29 16:55:14,538] [INFO] [stage3.py:2243:step] With missing steps outer loop took 3.8392045497894287
x3006c0s1b1n0: [2024-03-29 16:55:14,547] [INFO] [stage3.py:2251:step] Full outer step loop took 3.8485658168792725
x3006c0s19b1n0: [2024-03-29 16:55:14,771] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.072374582290649
x3006c0s19b1n0: [2024-03-29 16:55:14,780] [INFO] [stage3.py:2251:step] Full outer step loop took 4.081048965454102
x3006c0s19b1n0: [2024-03-29 16:55:14,852] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.153338432312012
x3006c0s1b1n0: [2024-03-29 16:55:14,860] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.161394119262695
x3006c0s19b1n0: [2024-03-29 16:55:14,861] [INFO] [stage3.py:2251:step] Full outer step loop took 4.162014484405518
x3006c0s1b1n0: [2024-03-29 16:55:14,873] [INFO] [stage3.py:2251:step] Full outer step loop took 4.173973798751831
x3006c0s1b1n0: [2024-03-29 16:55:14,908] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.20969557762146
x3006c0s1b1n0: [2024-03-29 16:55:14,917] [INFO] [stage3.py:2251:step] Full outer step loop took 4.218356609344482
x3006c0s19b1n0: [2024-03-29 16:55:14,983] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.284059524536133
x3006c0s19b1n0: [2024-03-29 16:55:14,991] [INFO] [stage3.py:2251:step] Full outer step loop took 4.292699813842773
x3006c0s1b1n0: [2024-03-29 16:55:14,995] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.296796798706055
x3006c0s1b1n0: [2024-03-29 16:55:15,004] [INFO] [stage3.py:2251:step] Full outer step loop took 4.305439472198486
x3006c0s1b1n0: [2024-03-29 16:55:15,014] [INFO] [stage3.py:2277:step] End to end step took 4.31587290763855
x3006c0s19b1n0: [2024-03-29 16:55:15,014] [INFO] [stage3.py:2277:step] End to end step took 4.3159520626068115
x3006c0s1b1n0: [2024-03-29 16:55:15,015] [INFO] [stage3.py:2277:step] End to end step took 4.316035270690918
x3006c0s19b1n0: [2024-03-29 16:55:15,015] [INFO] [stage3.py:2277:step] End to end step took 4.316251993179321
x3006c0s19b1n0: [2024-03-29 16:55:15,015] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 3413.94
x3006c0s19b1n0: [2024-03-29 16:55:15,015] [INFO] [stage3.py:2277:step] End to end step took 4.316371202468872
x3006c0s1b1n0: [2024-03-29 16:55:15,015] [INFO] [stage3.py:2277:step] End to end step took 4.316434621810913
x3006c0s1b1n0: [2024-03-29 16:55:15,015] [INFO] [stage3.py:2277:step] End to end step took 4.316438436508179
x3006c0s19b1n0: [2024-03-29 16:55:15,015] [INFO] [stage3.py:2277:step] End to end step took 4.3164284229278564
x3006c0s19b1n0: [2024-03-29 16:55:15,016] [INFO] [logging.py:96:log_dist] [Rank 0] step=8, skipped=0, lr=[6.158400017893797e-05], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-29 16:55:15,016] [INFO] [timer.py:260:stop] epoch=0/micro_step=8/global_step=8, RunningAvgSamplesPerSec=1.1759147794824438, CurrSamplesPerSec=1.1847333394362873, MemAllocated=10.26GB, MaxMemAllocated=15.23GB
x3006c0s19b1n0: [2024-03-29 16:55:15,016] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6189.77 | bwd_microstep: 16093.70 | bwd_inner_microstep: 16011.41 | bwd_allreduce_microstep: 82.22 | step_microstep: 4341.66
x3006c0s19b1n0: [2024-03-29 16:55:15,016] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6189.76 | bwd: 16093.70 | bwd_inner: 16011.40 | bwd_allreduce: 82.24 | step: 4341.66
x3006c0s19b1n0: [2024-03-29 16:55:15,132] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:55:15,132] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 15.23 GB         CA 10.34 GB         Max_CA 19 GB 
x3006c0s19b1n0: [2024-03-29 16:55:15,132] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.92 GB, percent = 74.1%
x3006c0s19b1n0: <TIMER:interval-time,27.2597234249115><TIMER:interval-time,27.2597234249115>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,27.25972890853882><TIMER:interval-time,27.259730339050293>
x3006c0s19b1n0: 
x3006c0s1b1n0: <TIMER:interval-time,27.259755611419678><TIMER:interval-time,27.25976538658142><TIMER:interval-time,27.25976324081421><TIMER:interval-time,27.25976800918579>
x3006c0s1b1n0: 
x3006c0s1b1n0: 
x3006c0s1b1n0: 
x3006c0s1b1n0:  elapsed_time 27.259768 | consumed samples:          256 | consumed tokens:       524288 | elapsed time per iteration (ms): 27259.8 | learning rate: 6.158E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.174 | TFLOPs: 81.22 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:55:15,285] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:55:15,286] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:55:15,286] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.94 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:55:21,659] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:55:21,660] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-29 16:55:21,660] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.94 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:55:21,742] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:55:21,743] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-29 16:55:21,743] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.94 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:55:38,220] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:55:38,220] [INFO] [utils.py:801:see_memory_usage] MA 14.64 GB         Max_MA 23.87 GB         CA 14.99 GB         Max_CA 34 GB 
x3006c0s19b1n0: [2024-03-29 16:55:38,221] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.94 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:55:38,292] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:55:38,293] [INFO] [utils.py:801:see_memory_usage] MA 14.64 GB         Max_MA 14.64 GB         CA 14.99 GB         Max_CA 15 GB 
x3006c0s19b1n0: [2024-03-29 16:55:38,293] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.94 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:55:41,629] [INFO] [stage3.py:2243:step] With missing steps outer loop took 3.312640905380249
x3006c0s19b1n0: [2024-03-29 16:55:41,656] [INFO] [stage3.py:2251:step] Full outer step loop took 3.338740587234497
x3006c0s1b1n0: [2024-03-29 16:55:42,282] [INFO] [stage3.py:2243:step] With missing steps outer loop took 3.965620517730713
x3006c0s1b1n0: [2024-03-29 16:55:42,292] [INFO] [stage3.py:2251:step] Full outer step loop took 3.975069046020508
x3006c0s19b1n0: [2024-03-29 16:55:42,344] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.027558088302612
x3006c0s19b1n0: [2024-03-29 16:55:42,357] [INFO] [stage3.py:2251:step] Full outer step loop took 4.039685964584351
x3006c0s1b1n0: [2024-03-29 16:55:42,541] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.224217414855957
x3006c0s1b1n0: [2024-03-29 16:55:42,542] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.225491523742676
x3006c0s1b1n0: [2024-03-29 16:55:42,550] [INFO] [stage3.py:2251:step] Full outer step loop took 4.232962608337402
x3006c0s1b1n0: [2024-03-29 16:55:42,552] [INFO] [stage3.py:2251:step] Full outer step loop took 4.235687255859375
x3006c0s19b1n0: [2024-03-29 16:55:42,586] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.269291877746582
x3006c0s19b1n0: [2024-03-29 16:55:42,610] [INFO] [stage3.py:2251:step] Full outer step loop took 4.293579339981079
x3006c0s1b1n0: [2024-03-29 16:55:42,615] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.298742055892944
x3006c0s1b1n0: [2024-03-29 16:55:42,624] [INFO] [stage3.py:2251:step] Full outer step loop took 4.30742621421814
x3006c0s19b1n0: [2024-03-29 16:55:42,652] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.335022687911987
x3006c0s19b1n0: [2024-03-29 16:55:42,661] [INFO] [stage3.py:2251:step] Full outer step loop took 4.343978404998779
x3006c0s1b1n0: [2024-03-29 16:55:42,673] [INFO] [stage3.py:2277:step] End to end step took 4.356499671936035
x3006c0s1b1n0: [2024-03-29 16:55:42,673] [INFO] [stage3.py:2277:step] End to end step took 4.356548070907593
x3006c0s19b1n0: [2024-03-29 16:55:42,673] [INFO] [stage3.py:2277:step] End to end step took 4.3566601276397705
x3006c0s19b1n0: [2024-03-29 16:55:42,673] [INFO] [stage3.py:2277:step] End to end step took 4.356801748275757
x3006c0s1b1n0: [2024-03-29 16:55:42,674] [INFO] [stage3.py:2277:step] End to end step took 4.356877326965332
x3006c0s19b1n0: [2024-03-29 16:55:42,673] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 3339.24
x3006c0s1b1n0: [2024-03-29 16:55:42,674] [INFO] [stage3.py:2277:step] End to end step took 4.356996536254883
x3006c0s19b1n0: [2024-03-29 16:55:42,674] [INFO] [stage3.py:2277:step] End to end step took 4.357102155685425
x3006c0s19b1n0: [2024-03-29 16:55:42,674] [INFO] [stage3.py:2277:step] End to end step took 4.357043027877808
x3006c0s19b1n0: [2024-03-29 16:55:42,674] [INFO] [logging.py:96:log_dist] [Rank 0] step=9, skipped=0, lr=[3.814149619390238e-05], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-29 16:55:42,675] [INFO] [timer.py:260:stop] epoch=0/micro_step=9/global_step=9, RunningAvgSamplesPerSec=1.1748374337417746, CurrSamplesPerSec=1.168414588300906, MemAllocated=10.26GB, MaxMemAllocated=15.23GB
x3006c0s19b1n0: [2024-03-29 16:55:42,675] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6332.20 | bwd_microstep: 16312.41 | bwd_inner_microstep: 16227.81 | bwd_allreduce_microstep: 84.52 | step_microstep: 4381.67
x3006c0s19b1n0: [2024-03-29 16:55:42,675] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6332.19 | bwd: 16312.41 | bwd_inner: 16227.81 | bwd_allreduce: 84.53 | step: 4381.67
x3006c0s19b1n0: [2024-03-29 16:55:42,823] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:55:42,824] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 15.23 GB         CA 10.34 GB         Max_CA 19 GB 
x3006c0s19b1n0: [2024-03-29 16:55:42,824] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.94 GB, percent = 74.1%
x3006c0s19b1n0: <TIMER:interval-time,27.691493272781372><TIMER:interval-time,27.691497325897217>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,27.691508293151855>
x3006c0s19b1n0: <TIMER:interval-time,27.691542387008667>
x3006c0s1b1n0: <TIMER:interval-time,27.691503524780273><TIMER:interval-time,27.691503524780273><TIMER:interval-time,27.691504955291748>
x3006c0s1b1n0: 
x3006c0s1b1n0: 
x3006c0s1b1n0: <TIMER:interval-time,27.691598415374756>
x3006c0s1b1n0:  elapsed_time 27.691504 | consumed samples:          288 | consumed tokens:       589824 | elapsed time per iteration (ms): 27691.5 | learning rate: 3.814E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.156 | TFLOPs: 79.96 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-29 16:55:42,952] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-29 16:55:42,953] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-29 16:55:42,953] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.93 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:55:49,707] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-29 16:55:49,708] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-29 16:55:49,708] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.92 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:55:49,790] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-29 16:55:49,790] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-29 16:55:49,791] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.92 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:56:06,897] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-29 16:56:06,898] [INFO] [utils.py:801:see_memory_usage] MA 14.64 GB         Max_MA 23.87 GB         CA 14.99 GB         Max_CA 34 GB 
x3006c0s19b1n0: [2024-03-29 16:56:06,898] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.93 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:56:06,974] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-29 16:56:06,975] [INFO] [utils.py:801:see_memory_usage] MA 14.64 GB         Max_MA 14.64 GB         CA 14.99 GB         Max_CA 15 GB 
x3006c0s19b1n0: [2024-03-29 16:56:06,975] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.93 GB, percent = 74.1%
x3006c0s19b1n0: [2024-03-29 16:56:10,515] [INFO] [stage3.py:2243:step] With missing steps outer loop took 3.516270160675049
x3006c0s19b1n0: [2024-03-29 16:56:10,529] [INFO] [stage3.py:2251:step] Full outer step loop took 3.530200958251953
x3006c0s1b1n0: [2024-03-29 16:56:10,833] [INFO] [stage3.py:2243:step] With missing steps outer loop took 3.8336167335510254
x3006c0s1b1n0: [2024-03-29 16:56:10,842] [INFO] [stage3.py:2251:step] Full outer step loop took 3.8425416946411133
x3006c0s19b1n0: [2024-03-29 16:56:11,050] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.051448106765747
x3006c0s19b1n0: [2024-03-29 16:56:11,059] [INFO] [stage3.py:2251:step] Full outer step loop took 4.060161828994751
x3006c0s1b1n0: [2024-03-29 16:56:11,180] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.181400775909424
x3006c0s1b1n0: [2024-03-29 16:56:11,189] [INFO] [stage3.py:2251:step] Full outer step loop took 4.19010329246521
x3006c0s19b1n0: [2024-03-29 16:56:11,234] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.234611749649048
x3006c0s19b1n0: [2024-03-29 16:56:11,244] [INFO] [stage3.py:2251:step] Full outer step loop took 4.245436429977417
x3006c0s1b1n0: [2024-03-29 16:56:11,246] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.247068166732788
x3006c0s1b1n0: [2024-03-29 16:56:11,258] [INFO] [stage3.py:2251:step] Full outer step loop took 4.258665561676025
x3006c0s19b1n0: [2024-03-29 16:56:11,277] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.278445243835449
x3006c0s19b1n0: [2024-03-29 16:56:11,286] [INFO] [stage3.py:2251:step] Full outer step loop took 4.2871057987213135
x3006c0s1b1n0: [2024-03-29 16:56:11,316] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.3171772956848145
x3006c0s1b1n0: [2024-03-29 16:56:11,325] [INFO] [stage3.py:2251:step] Full outer step loop took 4.3258490562438965
x3006c0s1b1n0: [2024-03-29 16:56:11,335] [INFO] [stage3.py:2277:step] End to end step took 4.336252927780151
x3006c0s19b1n0: [2024-03-29 16:56:11,335] [INFO] [stage3.py:2277:step] End to end step took 4.336322546005249
x3006c0s1b1n0: [2024-03-29 16:56:11,335] [INFO] [stage3.py:2277:step] End to end step took 4.3363542556762695
x3006c0s19b1n0: [2024-03-29 16:56:11,335] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 3530.38
x3006c0s19b1n0: [2024-03-29 16:56:11,336] [INFO] [stage3.py:2277:step] End to end step took 4.336583137512207
x3006c0s19b1n0: [2024-03-29 16:56:11,336] [INFO] [stage3.py:2277:step] End to end step took 4.33669376373291
x3006c0s19b1n0: [2024-03-29 16:56:11,336] [INFO] [stage3.py:2277:step] End to end step took 4.336609840393066
x3006c0s1b1n0: [2024-03-29 16:56:11,336] [INFO] [stage3.py:2277:step] End to end step took 4.336760759353638
x3006c0s1b1n0: [2024-03-29 16:56:11,336] [INFO] [stage3.py:2277:step] End to end step took 4.336787223815918
x3006c0s19b1n0: [2024-03-29 16:56:11,336] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[3e-05], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-29 16:56:11,337] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=1.1686989973629107, CurrSamplesPerSec=1.1274626502354501, MemAllocated=10.26GB, MaxMemAllocated=15.23GB
x3006c0s19b1n0: [2024-03-29 16:56:11,337] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6711.33 | bwd_microstep: 16935.90 | bwd_inner_microstep: 16854.69 | bwd_allreduce_microstep: 81.12 | step_microstep: 4361.68
x3006c0s19b1n0: [2024-03-29 16:56:11,337] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6711.32 | bwd: 16935.89 | bwd_inner: 16854.69 | bwd_allreduce: 81.14 | step: 4361.68
x3006c0s19b1n0: [2024-03-29 16:56:11,447] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-29 16:56:11,447] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 15.23 GB         CA 10.34 GB         Max_CA 19 GB 
x3006c0s19b1n0: [2024-03-29 16:56:11,448] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 372.92 GB, percent = 74.1%
x3006c0s1b1n0: <TIMER:interval-time,28.622934818267822>
x3006c0s1b1n0: <TIMER:interval-time,28.62296152114868><TIMER:interval-time,28.622963428497314>
x3006c0s1b1n0: <TIMER:interval-time,28.622963428497314>
x3006c0s1b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,28.622960090637207><TIMER:interval-time,28.622949600219727>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,28.62296748161316>
x3006c0s19b1n0: <TIMER:interval-time,28.62298011779785>
x3006c0s1b1n0:  elapsed_time 28.622962 | consumed samples:          320 | consumed tokens:       655360 | elapsed time per iteration (ms): 28623.0 | learning rate: 3.000E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.118 | TFLOPs: 77.35 |
x3006c0s1b1n0: <<<only_train:289.2144148349762>>>
x3006c0s1b1n0: <<<only_train:289.2144865989685>>><<<only_train:289.21444606781006>>>
x3006c0s1b1n0: 
x3006c0s1b1n0: <<<only_train:289.21420526504517>>>
x3006c0s19b1n0: <<<only_train:289.2144522666931>>><<<only_train:289.2144010066986>>><<<only_train:289.214439868927>>>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: <<<only_train:289.21442794799805>>>
x3006c0s19b1n0: [after training ends] datetime: 2024-03-29 16:56:11 
x3006c0s19b1n0: <<<full_time:289.2146728038788>>>
x3006c0s19b1n0: <<<full_time:289.2147035598755>>>
x3006c0s19b1n0: <<<full_time:289.21472907066345>>>
x3006c0s19b1n0: <<<full_time:289.21467995643616>>>
x3006c0s1b1n0: <<<full_time:289.21476340293884>>>
x3006c0s1b1n0: <<<full_time:289.21472573280334>>><<<full_time:289.2144660949707>>><<<full_time:289.2147705554962>>>
x3006c0s1b1n0: 
x3006c0s1b1n0: 

[2024-03-29 15:38:03,346] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 15:38:06,549] [INFO] [runner.py:463:main] Using IP address of 10.140.58.110 for node x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-29 15:38:06,551] [INFO] [runner.py:557:main] deepspeed_env file = ./.deepspeed_env
[2024-03-29 15:38:06,551] [INFO] [multinode_runner.py:80:get_cmd] Running on the following workers: x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov,x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-29 15:38:06,551] [INFO] [runner.py:568:main] cmd = pdsh -S -f 1024 -w x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov,x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov export PYTHONUSERBASE=/home/am6429/.local/polaris/conda/2023-10-04; export PYTHONPATH=/home/am6429/dl-io/Megatron-DeepSpeed; export PATH=/home/am6429/.conda/envs/dspeed_env/bin:/soft/datascience/conda/2023-10-04/mconda3/condabin:/soft/compilers/cudatoolkit/cuda-11.8.0/bin:/soft/buildtools/cmake/cmake-3.23.2/cmake-3.23.2-linux-x86_64/bin:/opt/cray/pe/gcc/11.2.0/bin:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/include:/opt/cray/pe/pals/1.2.11/bin:/opt/cray/pe/craype/2.7.20/bin:/opt/cray/libfabric/1.15.2.0/bin:/home/am6429/.conda/envs/dspeed_env/bin:/opt/cray/pe/perftools/23.03.0/bin:/opt/cray/pe/papi/7.0.0.1/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/am6429/.local/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/am6429/veloc-build/include:/home/am6429/veloc-build/bin:/soft/datascience/conda/2023-01-10/mconda3/include:/opt/cray/pe/bin:/soft/datascience/conda/2023-01-10/mconda3/include; export LD_LIBRARY_PATH=/usr/lib64/:/soft/compilers/cudatoolkit/cuda-11.8.0/lib64:/opt/cray/pe/gcc/11.2.0/snos/lib64:/soft/compilers/cudatoolkit/cuda-11.8.0/extras/CUPTI/lib64:/soft/libraries/trt/TensorRT-8.5.3.1.Linux.x86_64-gnu.cuda-11.8.cudnn8.6/lib:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/lib:/soft/libraries/cudnn/cudnn-11-linux-x64-v8.6.0.163/lib:/opt/cray/libfabric/1.15.2.0/lib64:/usr/lib64:/opt/cray/pe/papi/7.0.0.1/lib64:/home/am6429/veloc-build/lib:/home/am6429/veloc-build/lib64:/home/am6429/nvcomp/lib:/soft/datascience/conda/2023-01-10/mconda3/lib:/soft/datascience/conda/2023-01-10/mconda3/lib/; export http_proxy=http://proxy.alcf.anl.gov:3128; export https_proxy=http://proxy.alcf.anl.gov:3128; export CC=gcc; export CXX=g++; export IBV_FORK_SAFE=1; export CFLAGS=-I/soft/datascience/conda/2023-01-10/mconda3/include/; export LDFLAGS=-L/soft/datascience/conda/2023-01-10/mconda3/lib/; export CUDA_DEVICE_MAX_CONNECTIONS=1; export TORCHSNAPSHOT_PER_RANK_MEMORY_BUDGET_BYTES=34359738368; export _DEFAULT_MAX_PER_RANK_IO_CONCURRENCY=1; export _MAX_PER_RANK_IO_CONCURRENCY=1; export NSYS_REPORT_DIR=/home/am6429/dl-io/dl-io-outputs/act-output-30B//rep-30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch1-flush_async1-opt_gaps3-%n;  cd /home/am6429/dl-io/Megatron-DeepSpeed; /home/am6429/.conda/envs/dspeed_env/bin/python -u -m deepspeed.launcher.launch --world_info=eyJ4MzEwNGMwczFiMG4wLmhzbi5jbS5wb2xhcmlzLmFsY2YuYW5sLmdvdiI6IFswLCAxLCAyLCAzXSwgIngzMTA0YzBzMjViMG4wLmhzbi5jbS5wb2xhcmlzLmFsY2YuYW5sLmdvdiI6IFswLCAxLCAyLCAzXX0= --node_rank=%n --master_addr=10.140.58.110 --master_port=29700 /home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py --tensor-model-parallel-size 1 --num-layers 60 --hidden-size 6656 --num-attention-heads 52 --micro-batch-size 4 --global-batch-size 32 --seq-length 2048 --max-position-embeddings 2048 --train-iters 10 --save /local/scratch/llama2/zero3-tp1}_dp8 --data-path /home/am6429/dl-io/datasets/meg-gpt2_text_document --vocab-file /home/am6429/dl-io/datasets/gpt2-vocab.json --merge-file /home/am6429/dl-io/datasets/gpt2-merges.txt --data-impl mmap --tokenizer-type GPTSentencePieceTokenizer --tokenizer-model /home/am6429/dl-io/datasets/tokenizer.model --split 949,50,1 --distributed-backend nccl --lr 3e-4 --lr-decay-style cosine --min-lr 3e-5 --weight-decay 0.1 --clip-grad 1 --lr-warmup-iters 1 --optimizer adam --adam-beta1 0.9 --adam-beta2 0.95 --log-interval 1 --save-interval 1000 --eval-interval 1000 --eval-iters 0 --bf16 --no-query-key-layer-scaling --attention-dropout 0 --hidden-dropout 0 --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization rmsnorm --disable-bias-linear --num-key-value-heads 4 --deepspeed --exit-interval 20 --deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json --zero-stage=3 --no-pipeline-parallel --cpu-optimizer --checkpoint-activations --deepspeed-activation-checkpointing
x3104c0s25b0n0: [2024-03-29 15:38:08,578] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 15:38:08,738] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s25b0n0: [2024-03-29 15:38:10,531] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3104c0s25b0n0: [2024-03-29 15:38:10,531] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=1
x3104c0s25b0n0: [2024-03-29 15:38:10,531] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3104c0s25b0n0: [2024-03-29 15:38:10,531] [INFO] [launch.py:163:main] dist_world_size=8
x3104c0s25b0n0: [2024-03-29 15:38:10,531] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3104c0s25b0n0: [2024-03-29 15:38:10,531] [INFO] [launch.py:253:main] process 13521 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s25b0n0: [2024-03-29 15:38:10,532] [INFO] [launch.py:253:main] process 13522 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s25b0n0: [2024-03-29 15:38:10,532] [INFO] [launch.py:253:main] process 13523 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s25b0n0: [2024-03-29 15:38:10,533] [INFO] [launch.py:253:main] process 13524 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s1b0n0: [2024-03-29 15:38:10,710] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3104c0s1b0n0: [2024-03-29 15:38:10,710] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=0
x3104c0s1b0n0: [2024-03-29 15:38:10,710] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3104c0s1b0n0: [2024-03-29 15:38:10,710] [INFO] [launch.py:163:main] dist_world_size=8
x3104c0s1b0n0: [2024-03-29 15:38:10,710] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3104c0s1b0n0: [2024-03-29 15:38:10,711] [INFO] [launch.py:253:main] process 50058 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s1b0n0: [2024-03-29 15:38:10,711] [INFO] [launch.py:253:main] process 50059 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s1b0n0: [2024-03-29 15:38:10,712] [INFO] [launch.py:253:main] process 50060 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s1b0n0: [2024-03-29 15:38:10,713] [INFO] [launch.py:253:main] process 50061 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s25b0n0: [2024-03-29 15:38:12,231] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s25b0n0: [2024-03-29 15:38:12,234] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s25b0n0: [2024-03-29 15:38:12,237] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s25b0n0: [2024-03-29 15:38:12,238] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 15:38:12,653] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 15:38:12,659] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 15:38:12,660] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 15:38:12,660] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s25b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s25b0n0:       meet the required dependencies to JIT install the op.
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: JIT compiled ops requires ninja
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s25b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s25b0n0:       meet the required dependencies to JIT install the op.
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: JIT compiled ops requires ninja
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s25b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s25b0n0:       meet the required dependencies to JIT install the op.
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: JIT compiled ops requires ninja
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s25b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s25b0n0:       meet the required dependencies to JIT install the op.
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: JIT compiled ops requires ninja
x3104c0s25b0n0: ninja .................. [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: op name ................ installed .. compatible
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s25b0n0: ninja .................. [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: op name ................ installed .. compatible
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s25b0n0: ninja .................. [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: op name ................ installed .. compatible
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s25b0n0: ninja .................. [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: op name ................ installed .. compatible
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s25b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s25b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s25b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s25b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s25b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s25b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s25b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s25b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s25b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: DeepSpeed general environment info:
x3104c0s25b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s25b0n0: torch version .................... 2.0.1+cu118
x3104c0s25b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s25b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s25b0n0: torch cuda version ............... 11.8
x3104c0s25b0n0: torch hip version ................ None
x3104c0s25b0n0: nvcc version ..................... 11.8
x3104c0s25b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s25b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s25b0n0: DeepSpeed general environment info:
x3104c0s25b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s25b0n0: torch version .................... 2.0.1+cu118
x3104c0s25b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s25b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s25b0n0: torch cuda version ............... 11.8
x3104c0s25b0n0: torch hip version ................ None
x3104c0s25b0n0: nvcc version ..................... 11.8
x3104c0s25b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s25b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s25b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s25b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s25b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s25b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s25b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed general environment info:
x3104c0s25b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s25b0n0: spatial_inferencetorch version  ..........................  [93m[NO][0m2.0.1+cu118 
x3104c0s25b0n0: .......deepspeed install path  [92m[OKAY][0m........... 
x3104c0s25b0n0: ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s25b0n0: deepspeed info transformer...................  ............0.13.3+8074cd62, 8074cd62, hybrid_opt_offload 
x3104c0s25b0n0: [93m[NO][0mtorch cuda version  ......................  [92m[OKAY][0m11.8
x3104c0s25b0n0: torch hip version ................ None
x3104c0s25b0n0: 
x3104c0s25b0n0: nvcc version ..................... stochastic_transformer11.8 
x3104c0s25b0n0: .deepspeed wheel compiled w.  [93m[NO][0m......  .......torch 2.0, cuda 11.8 
x3104c0s25b0n0: [92m[OKAY][0m
x3104c0s25b0n0: shared memory (/dev/shm) size .... --------------------------------------------------251.61 GB
x3104c0s25b0n0: 
x3104c0s25b0n0: DeepSpeed general environment info:
x3104c0s25b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s25b0n0: torch version .................... 2.0.1+cu118
x3104c0s25b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s25b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s25b0n0: torch cuda version ............... 11.8
x3104c0s25b0n0: torch hip version ................ None
x3104c0s25b0n0: nvcc version ..................... 11.8
x3104c0s25b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s25b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s25b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s25b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s25b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s25b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s25b0n0: [2024-03-29 15:38:14,645] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s25b0n0: [2024-03-29 15:38:14,645] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s25b0n0: [2024-03-29 15:38:14,648] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s25b0n0: [2024-03-29 15:38:14,649] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s1b0n0:       meet the required dependencies to JIT install the op.
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: JIT compiled ops requires ninja
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s1b0n0:       meet the required dependencies to JIT install the op.
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: JIT compiled ops requires ninja
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s1b0n0:       meet the required dependencies to JIT install the op.
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: JIT compiled ops requires ninja
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s1b0n0:       meet the required dependencies to JIT install the op.
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: JIT compiled ops requires ninja
x3104c0s1b0n0: ninja .................. [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: op name ................ installed .. compatible
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s1b0n0: ninja .................. [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: op name ................ installed .. compatible
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s1b0n0: ninja .................. [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: op name ................ installed .. compatible
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s1b0n0: ninja .................. [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: op name ................ installed .. compatible
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed general environment info:
x3104c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s1b0n0: torch version .................... 2.0.1+cu118
x3104c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s1b0n0: torch cuda version ............... 11.8
x3104c0s1b0n0: torch hip version ................ None
x3104c0s1b0n0: nvcc version ..................... 11.8
x3104c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed general environment info:
x3104c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s1b0n0: torch version .................... 2.0.1+cu118
x3104c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s1b0n0: torch cuda version ............... 11.8
x3104c0s1b0n0: torch hip version ................ None
x3104c0s1b0n0: nvcc version ..................... 11.8
x3104c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s1b0n0: DeepSpeed general environment info:
x3104c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s1b0n0: torch version .................... 2.0.1+cu118
x3104c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s1b0n0: torch cuda version ............... 11.8
x3104c0s1b0n0: torch hip version ................ None
x3104c0s1b0n0: nvcc version ..................... 11.8
x3104c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s1b0n0: DeepSpeed general environment info:
x3104c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s1b0n0: torch version .................... 2.0.1+cu118
x3104c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s1b0n0: torch cuda version ............... 11.8
x3104c0s1b0n0: torch hip version ................ None
x3104c0s1b0n0: nvcc version ..................... 11.8
x3104c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s1b0n0: INFO: overriding default arguments for tokenizer_type:GPTSentencePieceTokenizer                    with tokenizer_type:GPT2BPETokenizer
x3104c0s1b0n0: using world size: 8, data-parallel-size: 8, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
x3104c0s1b0n0: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
x3104c0s1b0n0: using torch.bfloat16 for parameters ...
x3104c0s1b0n0: ------------------------ arguments ------------------------
x3104c0s1b0n0:   accumulate_allreduce_grads_in_fp32 .............. True
x3104c0s1b0n0:   adam_beta1 ...................................... 0.9
x3104c0s1b0n0:   adam_beta2 ...................................... 0.95
x3104c0s1b0n0:   adam_eps ........................................ 1e-08
x3104c0s1b0n0:   add_bias_linear ................................. False
x3104c0s1b0n0:   add_position_embedding .......................... False
x3104c0s1b0n0:   adlr_autoresume ................................. False
x3104c0s1b0n0:   adlr_autoresume_interval ........................ 1000
x3104c0s1b0n0:   aml_data_download_path .......................... None
x3104c0s1b0n0:   apply_layernorm_1p .............................. False
x3104c0s1b0n0:   apply_query_key_layer_scaling ................... False
x3104c0s1b0n0:   apply_residual_connection_post_layernorm ........ False
x3104c0s1b0n0:   async_tensor_model_parallel_allreduce ........... False
x3104c0s1b0n0:   attention_dropout ............................... 0.0
x3104c0s1b0n0:   attention_softmax_in_fp32 ....................... False
x3104c0s1b0n0:   barrier_with_L1_time ............................ True
x3104c0s1b0n0:   bert_binary_head ................................ True
x3104c0s1b0n0:   bert_embedder_type .............................. megatron
x3104c0s1b0n0:   bert_load ....................................... None
x3104c0s1b0n0:   bf16 ............................................ True
x3104c0s1b0n0:   bias_dropout_fusion ............................. True
x3104c0s1b0n0:   bias_gelu_fusion ................................ False
x3104c0s1b0n0:   biencoder_projection_dim ........................ 0
x3104c0s1b0n0:   biencoder_shared_query_context_model ............ False
x3104c0s1b0n0:   block_data_path ................................. None
x3104c0s1b0n0:   checkpoint_activations .......................... True
x3104c0s1b0n0:   checkpoint_in_cpu ............................... False
x3104c0s1b0n0:   checkpoint_num_layers ........................... 1
x3104c0s1b0n0:   classes_fraction ................................ 1.0
x3104c0s1b0n0:   clip_grad ....................................... 1.0
x3104c0s1b0n0:   compression_training ............................ False
x3104c0s1b0n0:   consumed_train_samples .......................... 0
x3104c0s1b0n0:   consumed_train_tokens ........................... 0
x3104c0s1b0n0:   consumed_valid_samples .......................... 0
x3104c0s1b0n0:   contigious_checkpointing ........................ False
x3104c0s1b0n0:   cpu_optimizer ................................... True
x3104c0s1b0n0:   cpu_torch_adam .................................. False
x3104c0s1b0n0:   create_moe_param_group .......................... False
x3104c0s1b0n0:   curriculum_learning_legacy ...................... False
x3104c0s1b0n0:   data_cache_path ................................. None
x3104c0s1b0n0:   data_efficiency_curriculum_learning ............. False
x3104c0s1b0n0:   data_impl ....................................... mmap
x3104c0s1b0n0:   data_parallel_random_init ....................... False
x3104c0s1b0n0:   data_parallel_size .............................. 8
x3104c0s1b0n0:   data_path ....................................... ['/home/am6429/dl-io/datasets/meg-gpt2_text_document']
x3104c0s1b0n0:   data_per_class_fraction ......................... 1.0
x3104c0s1b0n0:   data_sharding ................................... True
x3104c0s1b0n0:   dataloader_type ................................. single
x3104c0s1b0n0:   DDP_impl ........................................ local
x3104c0s1b0n0:   decoder_num_layers .............................. None
x3104c0s1b0n0:   decoder_seq_length .............................. None
x3104c0s1b0n0:   deepscale ....................................... False
x3104c0s1b0n0:   deepscale_config ................................ None
x3104c0s1b0n0:   deepspeed ....................................... True
x3104c0s1b0n0:   deepspeed_activation_checkpointing .............. True
x3104c0s1b0n0:   deepspeed_config ................................ /home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json
x3104c0s1b0n0:   dino_bottleneck_size ............................ 256
x3104c0s1b0n0:   dino_freeze_last_layer .......................... 1
x3104c0s1b0n0:   dino_head_hidden_size ........................... 2048
x3104c0s1b0n0:   dino_local_crops_number ......................... 10
x3104c0s1b0n0:   dino_local_img_size ............................. 96
x3104c0s1b0n0:   dino_norm_last_layer ............................ False
x3104c0s1b0n0:   dino_teacher_temp ............................... 0.07
x3104c0s1b0n0:   dino_warmup_teacher_temp ........................ 0.04
x3104c0s1b0n0:   dino_warmup_teacher_temp_epochs ................. 30
x3104c0s1b0n0:   distribute_checkpointed_activations ............. False
x3104c0s1b0n0:   distribute_saved_activations .................... False
x3104c0s1b0n0:   distributed_backend ............................. nccl
x3104c0s1b0n0:   distributed_timeout_minutes ..................... 10
x3104c0s1b0n0:   ds_inference .................................... False
x3104c0s1b0n0:   ds_pipeline_enabled ............................. False
x3104c0s1b0n0:   ds_sequence_parallel_size ....................... 1
x3104c0s1b0n0:   embedding_path .................................. None
x3104c0s1b0n0:   embedding_weights_in_fp32 ....................... False
x3104c0s1b0n0:   empty_unused_memory_level ....................... 0
x3104c0s1b0n0:   enable_expert_tensor_parallelism ................ False
x3104c0s1b0n0:   encoder_num_layers .............................. 60
x3104c0s1b0n0:   encoder_seq_length .............................. 2048
x3104c0s1b0n0:   end_weight_decay ................................ 0.1
x3104c0s1b0n0:   eod_mask_loss ................................... False
x3104c0s1b0n0:   eval_interval ................................... 1000
x3104c0s1b0n0:   eval_iters ...................................... 0
x3104c0s1b0n0:   evidence_data_path .............................. None
x3104c0s1b0n0:   exit_duration_in_mins ........................... None
x3104c0s1b0n0:   exit_interval ................................... 20
x3104c0s1b0n0:   exit_on_missing_checkpoint ...................... False
x3104c0s1b0n0:   exit_signal_handler ............................. False
x3104c0s1b0n0:   expert_interval ................................. 2
x3104c0s1b0n0:   ffn_hidden_size ................................. 17728
x3104c0s1b0n0:   finetune ........................................ False
x3104c0s1b0n0:   force_ds_sequence_parallel ...................... False
x3104c0s1b0n0:   fp16 ............................................ False
x3104c0s1b0n0:   fp16_lm_cross_entropy ........................... False
x3104c0s1b0n0:   fp32_residual_connection ........................ False
x3104c0s1b0n0:   fp8_amax_compute_algo ........................... most_recent
x3104c0s1b0n0:   fp8_amax_history_len ............................ 1
x3104c0s1b0n0:   fp8_e4m3 ........................................ False
x3104c0s1b0n0:   fp8_hybrid ...................................... False
x3104c0s1b0n0:   fp8_interval .................................... 1
x3104c0s1b0n0:   fp8_margin ...................................... 0
x3104c0s1b0n0:   fp8_wgrad ....................................... True
x3104c0s1b0n0:   global_batch_size ............................... 32
x3104c0s1b0n0:   gradient_accumulation_fusion .................... True
x3104c0s1b0n0:   head_lr_mult .................................... 1.0
x3104c0s1b0n0:   hidden_dropout .................................. 0.0
x3104c0s1b0n0:   hidden_size ..................................... 6656
x3104c0s1b0n0:   hidden_size_teacher ............................. None
x3104c0s1b0n0:   hysteresis ...................................... 2
x3104c0s1b0n0:   ict_head_size ................................... None
x3104c0s1b0n0:   ict_load ........................................ None
x3104c0s1b0n0:   img_h ........................................... 224
x3104c0s1b0n0:   img_w ........................................... 224
x3104c0s1b0n0:   indexer_batch_size .............................. 128
x3104c0s1b0n0:   indexer_log_interval ............................ 1000
x3104c0s1b0n0:   inference ....................................... False
x3104c0s1b0n0:   inference_batch_times_seqlen_threshold .......... 512
x3104c0s1b0n0:   init_method_std ................................. 0.02
x3104c0s1b0n0:   init_method_xavier_uniform ...................... False
x3104c0s1b0n0:   initial_loss_scale .............................. 4294967296
x3104c0s1b0n0:   iter_per_epoch .................................. 1250
x3104c0s1b0n0:   kd .............................................. False
x3104c0s1b0n0:   kd_alpha_ce ..................................... 1
x3104c0s1b0n0:   kd_beta_ce ...................................... 1
x3104c0s1b0n0:   kd_temp ......................................... 1.0
x3104c0s1b0n0:   kv_channels ..................................... 128
x3104c0s1b0n0:   layernorm_epsilon ............................... 1e-05
x3104c0s1b0n0:   lazy_mpu_init ................................... None
x3104c0s1b0n0:   load ............................................ None
x3104c0s1b0n0:   load_teacher .................................... None
x3104c0s1b0n0:   local_rank ...................................... 0
x3104c0s1b0n0:   log_batch_size_to_tensorboard ................... False
x3104c0s1b0n0:   log_interval .................................... 1
x3104c0s1b0n0:   log_learning_rate_to_tensorboard ................ True
x3104c0s1b0n0:   log_loss_scale_to_tensorboard ................... True
x3104c0s1b0n0:   log_memory_to_tensorboard ....................... False
x3104c0s1b0n0:   log_num_zeros_in_grad ........................... False
x3104c0s1b0n0:   log_optimizer_states_to_tensorboard ............. False
x3104c0s1b0n0:   log_params_norm ................................. False
x3104c0s1b0n0:   log_timers_to_tensorboard ....................... False
x3104c0s1b0n0:   log_validation_ppl_to_tensorboard ............... False
x3104c0s1b0n0:   log_world_size_to_tensorboard ................... False
x3104c0s1b0n0:   loss_scale ...................................... None
x3104c0s1b0n0:   loss_scale_window ............................... 1000
x3104c0s1b0n0:   lr .............................................. 0.0003
x3104c0s1b0n0:   lr_decay_iters .................................. None
x3104c0s1b0n0:   lr_decay_samples ................................ None
x3104c0s1b0n0:   lr_decay_style .................................. cosine
x3104c0s1b0n0:   lr_decay_tokens ................................. None
x3104c0s1b0n0:   lr_warmup_fraction .............................. None
x3104c0s1b0n0:   lr_warmup_iters ................................. 1
x3104c0s1b0n0:   lr_warmup_samples ............................... 0
x3104c0s1b0n0:   lr_warmup_tokens ................................ None
x3104c0s1b0n0:   make_vocab_size_divisible_by .................... 128
x3104c0s1b0n0:   mask_factor ..................................... 1.0
x3104c0s1b0n0:   mask_prob ....................................... 0.15
x3104c0s1b0n0:   mask_type ....................................... random
x3104c0s1b0n0:   masked_softmax_fusion ........................... True
x3104c0s1b0n0:   max_position_embeddings ......................... 2048
x3104c0s1b0n0:   max_tokens_to_oom ............................... 12000
x3104c0s1b0n0:   memory_centric_tiled_linear ..................... False
x3104c0s1b0n0:   merge_file ...................................... /home/am6429/dl-io/datasets/gpt2-merges.txt
x3104c0s1b0n0:   micro_batch_size ................................ 4
x3104c0s1b0n0:   min_loss_scale .................................. 1.0
x3104c0s1b0n0:   min_lr .......................................... 3e-05
x3104c0s1b0n0:   mlp_type ........................................ standard
x3104c0s1b0n0:   mmap_warmup ..................................... False
x3104c0s1b0n0:   moe_eval_capacity_factor ........................ 1.0
x3104c0s1b0n0:   moe_expert_parallel_size ........................ 1
x3104c0s1b0n0:   moe_loss_coeff .................................. 0.1
x3104c0s1b0n0:   moe_min_capacity ................................ 4
x3104c0s1b0n0:   moe_token_dropping .............................. True
x3104c0s1b0n0:   moe_train_capacity_factor ....................... 1.0
x3104c0s1b0n0:   mos ............................................. False
x3104c0s1b0n0:   no_load_lr_state ................................ False
x3104c0s1b0n0:   no_load_optim ................................... None
x3104c0s1b0n0:   no_load_rng ..................................... None
x3104c0s1b0n0:   no_persist_layer_norm ........................... False
x3104c0s1b0n0:   no_pipeline_parallel ............................ True
x3104c0s1b0n0:   no_save_optim ................................... None
x3104c0s1b0n0:   no_save_rng ..................................... None
x3104c0s1b0n0:   normalization ................................... rmsnorm
x3104c0s1b0n0:   num_attention_heads ............................. 52
x3104c0s1b0n0:   num_attention_heads_teacher ..................... None
x3104c0s1b0n0:   num_channels .................................... 3
x3104c0s1b0n0:   num_classes ..................................... 1000
x3104c0s1b0n0:   num_experts ..................................... [1]
x3104c0s1b0n0:   num_experts_switch .............................. None
x3104c0s1b0n0:   num_experts_teacher ............................. [1]
x3104c0s1b0n0:   num_key_value_heads ............................. 4
x3104c0s1b0n0:   num_layers ...................................... 60
x3104c0s1b0n0:   num_layers_per_virtual_pipeline_stage ........... None
x3104c0s1b0n0:   num_layers_teacher .............................. None
x3104c0s1b0n0:   num_workers ..................................... 2
x3104c0s1b0n0:   onnx_safe ....................................... None
x3104c0s1b0n0:   openai_gelu ..................................... False
x3104c0s1b0n0:   optimizer ....................................... adam
x3104c0s1b0n0:   output_bert_embeddings .......................... False
x3104c0s1b0n0:   overlap_p2p_comm ................................ False
x3104c0s1b0n0:   override_opt_param_scheduler .................... False
x3104c0s1b0n0:   params_dtype .................................... torch.bfloat16
x3104c0s1b0n0:   partition_activations ........................... False
x3104c0s1b0n0:   patch_dim ....................................... 16
x3104c0s1b0n0:   perform_initialization .......................... True
x3104c0s1b0n0:   pipeline_model_parallel_size .................... 1
x3104c0s1b0n0:   pipeline_model_parallel_split_rank .............. None
x3104c0s1b0n0:   profile_backward ................................ False
x3104c0s1b0n0:   query_in_block_prob ............................. 0.1
x3104c0s1b0n0:   rampup_batch_size ............................... None
x3104c0s1b0n0:   random_ltd ...................................... False
x3104c0s1b0n0:   rank ............................................ 0
x3104c0s1b0n0:   recompute_granularity ........................... None
x3104c0s1b0n0:   recompute_method ................................ None
x3104c0s1b0n0:   recompute_num_layers ............................ 1
x3104c0s1b0n0:   remote_device ................................... none
x3104c0s1b0n0:   reset_attention_mask ............................ False
x3104c0s1b0n0:   reset_iteration ................................. False
x3104c0s1b0n0:   reset_position_ids .............................. False
x3104c0s1b0n0:   retriever_report_topk_accuracies ................ []
x3104c0s1b0n0:   retriever_score_scaling ......................... False
x3104c0s1b0n0:   retriever_seq_length ............................ 256
x3104c0s1b0n0:   retro_add_retriever ............................. False
x3104c0s1b0n0:   retro_cyclic_train_iters ........................ None
x3104c0s1b0n0:   retro_encoder_attention_dropout ................. 0.1
x3104c0s1b0n0:   retro_encoder_hidden_dropout .................... 0.1
x3104c0s1b0n0:   retro_encoder_layers ............................ 2
x3104c0s1b0n0:   retro_num_neighbors ............................. 2
x3104c0s1b0n0:   retro_num_retrieved_chunks ...................... 2
x3104c0s1b0n0:   retro_return_doc_ids ............................ False
x3104c0s1b0n0:   retro_workdir ................................... None
x3104c0s1b0n0:   return_data_index ............................... False
x3104c0s1b0n0:   rotary_percent .................................. 1.0
x3104c0s1b0n0:   sample_rate ..................................... 1.0
x3104c0s1b0n0:   save ............................................ /local/scratch/llama2/zero3-tp1}_dp8
x3104c0s1b0n0:   save_interval ................................... 1000
x3104c0s1b0n0:   scatter_gather_tensors_in_pipeline .............. True
x3104c0s1b0n0:   scattered_embeddings ............................ False
x3104c0s1b0n0:   seed ............................................ 1234
x3104c0s1b0n0:   seq_length ...................................... 2048
x3104c0s1b0n0:   sequence_parallel ............................... False
x3104c0s1b0n0:   sgd_momentum .................................... 0.9
x3104c0s1b0n0:   short_seq_prob .................................. 0.1
x3104c0s1b0n0:   skip_train ...................................... False
x3104c0s1b0n0:   split ........................................... 949,50,1
x3104c0s1b0n0:   split_transformers .............................. False
x3104c0s1b0n0:   squared_relu .................................... False
x3104c0s1b0n0:   standalone_embedding_stage ...................... False
x3104c0s1b0n0:   start_weight_decay .............................. 0.1
x3104c0s1b0n0:   swiglu .......................................... True
x3104c0s1b0n0:   swin_backbone_type .............................. tiny
x3104c0s1b0n0:   synchronize_each_layer .......................... False
x3104c0s1b0n0:   tensor_model_parallel_size ...................... 1
x3104c0s1b0n0:   tensorboard_dir ................................. None
x3104c0s1b0n0:   tensorboard_log_interval ........................ 1
x3104c0s1b0n0:   tensorboard_queue_size .......................... 1000
x3104c0s1b0n0:   test_data_path .................................. None
x3104c0s1b0n0:   tile_factor ..................................... 1
x3104c0s1b0n0:   timing_log_level ................................ 0
x3104c0s1b0n0:   timing_log_option ............................... minmax
x3104c0s1b0n0:   titles_data_path ................................ None
x3104c0s1b0n0:   tokenizer_model ................................. /home/am6429/dl-io/datasets/tokenizer.model
x3104c0s1b0n0:   tokenizer_type .................................. GPT2BPETokenizer
x3104c0s1b0n0:   topk ............................................ 1
x3104c0s1b0n0:   train_data_exact_num_epochs ..................... None
x3104c0s1b0n0:   train_data_path ................................. None
x3104c0s1b0n0:   train_desc_path ................................. None
x3104c0s1b0n0:   train_doc_idx_path .............................. None
x3104c0s1b0n0:   train_idx_path .................................. None
x3104c0s1b0n0:   train_iters ..................................... 10
x3104c0s1b0n0:   train_sample_idx_path ........................... None
x3104c0s1b0n0:   train_samples ................................... None
x3104c0s1b0n0:   train_shuffle_idx_path .......................... None
x3104c0s1b0n0:   train_tokens .................................... None
x3104c0s1b0n0:   transformer_impl ................................ local
x3104c0s1b0n0:   transformer_pipeline_model_parallel_size ........ 1
x3104c0s1b0n0:   untie_embeddings_and_output_weights ............. True
x3104c0s1b0n0:   use_checkpoint_args ............................. False
x3104c0s1b0n0:   use_checkpoint_opt_param_scheduler .............. False
x3104c0s1b0n0:   use_contiguous_buffers_in_local_ddp ............. True
x3104c0s1b0n0:   use_cpu_initialization .......................... None
x3104c0s1b0n0:   use_dataset_only ................................ False
x3104c0s1b0n0:   use_distributed_optimizer ....................... False
x3104c0s1b0n0:   use_flash_attn .................................. False
x3104c0s1b0n0:   use_flash_attn_triton ........................... False
x3104c0s1b0n0:   use_flash_attn_v1 ............................... False
x3104c0s1b0n0:   use_flash_attn_v2 ............................... False
x3104c0s1b0n0:   use_one_sent_docs ............................... False
x3104c0s1b0n0:   use_pin_memory .................................. False
x3104c0s1b0n0:   use_ring_exchange_p2p ........................... False
x3104c0s1b0n0:   use_rotary_position_embeddings .................. True
x3104c0s1b0n0:   use_tutel ....................................... False
x3104c0s1b0n0:   valid_data_path ................................. None
x3104c0s1b0n0:   variable_seq_lengths ............................ False
x3104c0s1b0n0:   virtual_pipeline_model_parallel_size ............ None
x3104c0s1b0n0:   vision_backbone_type ............................ vit
x3104c0s1b0n0:   vision_pretraining .............................. False
x3104c0s1b0n0:   vision_pretraining_type ......................... classify
x3104c0s1b0n0:   vocab_extra_ids ................................. 0
x3104c0s1b0n0:   vocab_file ...................................... /home/am6429/dl-io/datasets/gpt2-vocab.json
x3104c0s1b0n0:   vocab_size ...................................... None
x3104c0s1b0n0:   weight_decay .................................... 0.1
x3104c0s1b0n0:   weight_decay_incr_style ......................... constant
x3104c0s1b0n0:   world_size ...................................... 8
x3104c0s1b0n0:   zero_allgather_bucket_size ...................... 0.0
x3104c0s1b0n0:   zero_contigious_gradients ....................... False
x3104c0s1b0n0:   zero_reduce_bucket_size ......................... 0.0
x3104c0s1b0n0:   zero_reduce_scatter ............................. False
x3104c0s1b0n0:   zero_stage ...................................... 3
x3104c0s1b0n0: -------------------- end of arguments ---------------------
x3104c0s1b0n0: setting number of micro-batches to constant 1
x3104c0s1b0n0: > building GPT2BPETokenizer tokenizer ...
x3104c0s1b0n0: [2024-03-29 15:38:15,935] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
x3104c0s1b0n0: > initializing torch distributed ...
x3104c0s1b0n0: [2024-03-29 15:38:15,936] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0: [2024-03-29 15:38:15,936] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
x3104c0s1b0n0: [2024-03-29 15:38:15,938] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0: [2024-03-29 15:38:15,938] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0: > initialized tensor model parallel with size 1
x3104c0s1b0n0: > initialized pipeline model parallel with size 1
x3104c0s1b0n0: > setting random seeds to 1234 ...
x3104c0s1b0n0: [2024-03-29 15:38:17,395] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
x3104c0s1b0n0: > compiling dataset index builder ...
x3104c0s1b0n0: make: Entering directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3104c0s1b0n0: make: Nothing to be done for 'default'.
x3104c0s1b0n0: make: Leaving directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3104c0s1b0n0: >>> done with dataset index builder. Compilation time: 0.085 seconds
x3104c0s1b0n0: > compiling and loading fused kernels ...
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: >>> done with compiling and loading fused kernels. Compilation time: 3.289 seconds
x3104c0s25b0n0: <<<<<<<<<<< 7
x3104c0s25b0n0: <<<<<<<<<<< 6
x3104c0s25b0n0: <<<<<<<<<<< 5
x3104c0s25b0n0: <<<<<<<<<<< 4
x3104c0s1b0n0: initialize_megatron took 5.671037435531616
x3104c0s1b0n0: <<<<<<<<<<< 0
x3104c0s1b0n0: <<<<<<<<<<< 3
x3104c0s1b0n0: <<<<<<<<<<< 1
x3104c0s1b0n0: <<<<<<<<<<< 2
x3104c0s1b0n0: time to initialize megatron (seconds): 7.247
x3104c0s1b0n0: [after megatron is initialized] datetime: 2024-03-29 15:38:21 
x3104c0s1b0n0: get_accelerator and all_reduce  took 0.005152463912963867
x3104c0s1b0n0: building GPT model ...
x3104c0s1b0n0: [2024-03-29 15:38:21,634] [INFO] [utils.py:800:see_memory_usage] Before Building Model
x3104c0s1b0n0: [2024-03-29 15:38:21,634] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 4.62 GB         CA 0.0 GB         Max_CA 5 GB 
x3104c0s1b0n0: [2024-03-29 15:38:21,634] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 17.85 GB, percent = 3.5%
x3104c0s1b0n0: [2024-03-29 15:38:27,773] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 423, num_elems = 27.64B
x3104c0s1b0n0: [2024-03-29 15:38:27,844] [INFO] [utils.py:800:see_memory_usage] After Building Model
x3104c0s1b0n0: [2024-03-29 15:38:27,844] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 7.07 GB         CA 21.86 GB         Max_CA 38 GB 
x3104c0s1b0n0: [2024-03-29 15:38:27,845] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 18.28 GB, percent = 3.6%
x3104c0s1b0n0:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 27635239424
x3104c0s25b0n0: ninja: no work to do.
x3104c0s25b0n0: Time to load cpu_adam op: 2.2791483402252197 seconds
x3104c0s25b0n0: Time to load cpu_adam op: 2.322019577026367 seconds
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: Time to load cpu_adam op: 2.475968360900879 seconds
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: Time to load cpu_adam op: 2.441272258758545 seconds
x3104c0s25b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s25b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: Time to load cpu_adam op: 2.4338173866271973 seconds
x3104c0s25b0n0: Time to load cpu_adam op: 2.4118549823760986 seconds
x3104c0s1b0n0: Time to load cpu_adam op: 2.5807559490203857 seconds
x3104c0s25b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s25b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s25b0n0: ninja: no work to do.
x3104c0s25b0n0: Time to load cpu_adam op: 2.296074628829956 seconds
x3104c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: > learning rate decay style: cosine
x3104c0s1b0n0: DeepSpeed is enabled.
x3104c0s1b0n0: [2024-03-29 15:38:32,290] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.3+8074cd62, git-hash=8074cd62, git-branch=hybrid_opt_offload
x3104c0s1b0n0: [2024-03-29 15:38:32,365] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After args sanity test
x3104c0s1b0n0: [2024-03-29 15:38:32,365] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:38:32,365] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 24.51 GB, percent = 4.9%
x3104c0s1b0n0: [2024-03-29 15:38:32,425] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure distributed model
x3104c0s1b0n0: [2024-03-29 15:38:32,426] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:38:32,426] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 24.79 GB, percent = 4.9%
x3104c0s1b0n0: [2024-03-29 15:38:32,490] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After configure distributed model
x3104c0s1b0n0: [2024-03-29 15:38:32,491] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:38:32,491] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.14 GB, percent = 5.0%
x3104c0s1b0n0: [2024-03-29 15:38:32,491] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
x3104c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: [2024-03-29 15:38:32,545] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After setting model parameters
x3104c0s1b0n0: [2024-03-29 15:38:32,546] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:38:32,546] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.36 GB, percent = 5.0%
x3104c0s1b0n0: [2024-03-29 15:38:32,600] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure optimizer
x3104c0s1b0n0: [2024-03-29 15:38:32,601] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:38:32,601] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.53 GB, percent = 5.1%
x3104c0s1b0n0: [2024-03-29 15:38:32,601] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
x3104c0s1b0n0: [2024-03-29 15:38:32,601] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
x3104c0s25b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s25b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: [2024-03-29 15:38:32,620] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
x3104c0s1b0n0: [2024-03-29 15:38:32,620] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
x3104c0s1b0n0: [2024-03-29 15:38:32,620] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
x3104c0s1b0n0: [2024-03-29 15:38:32,620] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
x3104c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: [2024-03-29 15:38:32,673] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
x3104c0s1b0n0: [2024-03-29 15:38:32,674] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:38:32,674] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.7 GB, percent = 5.1%
x3104c0s1b0n0: [2024-03-29 15:38:32,676] [INFO] [stage3.py:137:__init__] Reduce bucket size 500,000,000
x3104c0s1b0n0: [2024-03-29 15:38:32,676] [INFO] [stage3.py:138:__init__] Prefetch bucket size 50,000,000
x3104c0s25b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s25b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: [2024-03-29 15:38:32,731] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
x3104c0s1b0n0: [2024-03-29 15:38:32,731] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:38:32,731] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.71 GB, percent = 5.1%
x3104c0s1b0n0: Parameter Offload: Total persistent parameters: 805376 in 121 params
x3104c0s1b0n0: [2024-03-29 15:38:32,813] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
x3104c0s1b0n0: [2024-03-29 15:38:32,813] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:38:32,813] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.71 GB, percent = 5.1%
x3104c0s1b0n0: [2024-03-29 15:38:32,872] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
x3104c0s1b0n0: [2024-03-29 15:38:32,872] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:38:32,872] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.71 GB, percent = 5.1%
x3104c0s1b0n0: [2024-03-29 15:38:32,873] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s1b0n0: [2024-03-29 15:38:32,873] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s1b0n0: [2024-03-29 15:38:32,873] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s1b0n0: [2024-03-29 15:38:32,873] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s25b0n0: [2024-03-29 15:38:32,873] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s25b0n0: [2024-03-29 15:38:32,873] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s25b0n0: [2024-03-29 15:38:32,873] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s25b0n0: [2024-03-29 15:38:32,873] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s1b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s1b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s25b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s1b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s25b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s25b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s25b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s1b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s1b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,874] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,875] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,876] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,877] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:38:32,879] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s1b0n0: [2024-03-29 15:38:32,879] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s1b0n0: [2024-03-29 15:38:32,879] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s1b0n0: [2024-03-29 15:38:32,879] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s25b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s25b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s25b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s25b0n0: [2024-03-29 15:38:32,878] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s1b0n0: [2024-03-29 15:38:35,126] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 31
x3104c0s1b0n0: [2024-03-29 15:38:35,126] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.44 GB         CA 6.44 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:38:35,127] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 45.05 GB, percent = 9.0%
x3104c0s1b0n0: [2024-03-29 15:38:35,195] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
x3104c0s1b0n0: [2024-03-29 15:38:35,195] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3104c0s1b0n0: [2024-03-29 15:38:35,195] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 45.05 GB, percent = 9.0%
x3104c0s1b0n0: [2024-03-29 15:38:54,764] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
x3104c0s1b0n0: [2024-03-29 15:38:54,765] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3104c0s1b0n0: [2024-03-29 15:38:54,765] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 214.88 GB, percent = 42.7%
x3104c0s1b0n0: [2024-03-29 15:38:55,211] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
x3104c0s1b0n0: [2024-03-29 15:38:55,212] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3104c0s1b0n0: [2024-03-29 15:38:55,212] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 219.28 GB, percent = 43.6%
x3104c0s1b0n0: [2024-03-29 15:39:01,687] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | init_optimizer_state: 6461.14
x3104c0s1b0n0: [2024-03-29 15:39:01,754] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
x3104c0s1b0n0: [2024-03-29 15:39:01,755] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3104c0s1b0n0: [2024-03-29 15:39:01,755] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 279.19 GB, percent = 55.5%
x3104c0s1b0n0: [2024-03-29 15:39:02,006] [INFO] [stage3.py:520:_setup_for_real_optimizer] optimizer state initialized
x3104c0s1b0n0: [2024-03-29 15:39:10,537] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
x3104c0s1b0n0: [2024-03-29 15:39:10,538] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 8.61 GB         CA 22.13 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:39:10,538] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.87 GB, percent = 62.0%
x3104c0s1b0n0: [2024-03-29 15:39:10,538] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
x3104c0s1b0n0: [2024-03-29 15:39:10,601] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure LR scheduler
x3104c0s1b0n0: [2024-03-29 15:39:10,601] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.13 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:39:10,601] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.89 GB, percent = 62.0%
x3104c0s1b0n0: [2024-03-29 15:39:10,602] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
x3104c0s1b0n0: [2024-03-29 15:39:10,602] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7fabf501c250>
x3104c0s1b0n0: [2024-03-29 15:39:10,602] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 15:39:10,662] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before rewriting optimizer step
x3104c0s1b0n0: [2024-03-29 15:39:10,662] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.13 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:39:10,663] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.87 GB, percent = 62.0%
x3104c0s1b0n0: [2024-03-29 15:39:10,724] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure checkpointing
x3104c0s1b0n0: [2024-03-29 15:39:10,725] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.13 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:39:10,725] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.89 GB, percent = 62.0%
x3104c0s1b0n0: [2024-03-29 15:39:10,725] [INFO] [config.py:998:print] DeepSpeedEngine configuration:
x3104c0s1b0n0: [2024-03-29 15:39:10,725] [INFO] [config.py:1002:print]   activation_checkpointing_config  {
x3104c0s1b0n0:     "partition_activations": false, 
x3104c0s1b0n0:     "contiguous_memory_optimization": false, 
x3104c0s1b0n0:     "cpu_checkpointing": false, 
x3104c0s1b0n0:     "number_checkpoints": null, 
x3104c0s1b0n0:     "synchronize_checkpoint_boundary": false, 
x3104c0s1b0n0:     "profile": false
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 15:39:10,725] [INFO] [config.py:1002:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
x3104c0s1b0n0: [2024-03-29 15:39:10,725] [INFO] [config.py:1002:print]   amp_enabled .................. False
x3104c0s1b0n0: [2024-03-29 15:39:10,725] [INFO] [config.py:1002:print]   amp_params ................... False
x3104c0s1b0n0: [2024-03-29 15:39:10,725] [INFO] [config.py:1002:print]   autotuning_config ............ {
x3104c0s1b0n0:     "enabled": false, 
x3104c0s1b0n0:     "start_step": null, 
x3104c0s1b0n0:     "end_step": null, 
x3104c0s1b0n0:     "metric_path": null, 
x3104c0s1b0n0:     "arg_mappings": null, 
x3104c0s1b0n0:     "metric": "throughput", 
x3104c0s1b0n0:     "model_info": null, 
x3104c0s1b0n0:     "results_dir": "autotuning_results", 
x3104c0s1b0n0:     "exps_dir": "autotuning_exps", 
x3104c0s1b0n0:     "overwrite": true, 
x3104c0s1b0n0:     "fast": true, 
x3104c0s1b0n0:     "start_profile_step": 3, 
x3104c0s1b0n0:     "end_profile_step": 5, 
x3104c0s1b0n0:     "tuner_type": "gridsearch", 
x3104c0s1b0n0:     "tuner_early_stopping": 5, 
x3104c0s1b0n0:     "tuner_num_trials": 50, 
x3104c0s1b0n0:     "model_info_path": null, 
x3104c0s1b0n0:     "mp_size": 1, 
x3104c0s1b0n0:     "max_train_batch_size": null, 
x3104c0s1b0n0:     "min_train_batch_size": 1, 
x3104c0s1b0n0:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
x3104c0s1b0n0:     "min_train_micro_batch_size_per_gpu": 1, 
x3104c0s1b0n0:     "num_tuning_micro_batch_sizes": 3
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 15:39:10,725] [INFO] [config.py:1002:print]   bfloat16_enabled ............. True
x3104c0s1b0n0: [2024-03-29 15:39:10,725] [INFO] [config.py:1002:print]   bfloat16_immediate_grad_update  False
x3104c0s1b0n0: [2024-03-29 15:39:10,725] [INFO] [config.py:1002:print]   checkpoint_parallel_write_pipeline  False
x3104c0s1b0n0: [2024-03-29 15:39:10,725] [INFO] [config.py:1002:print]   checkpoint_tag_validation_enabled  True
x3104c0s1b0n0: [2024-03-29 15:39:10,725] [INFO] [config.py:1002:print]   checkpoint_tag_validation_fail  False
x3104c0s1b0n0: [2024-03-29 15:39:10,725] [INFO] [config.py:1002:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fabf501cbb0>
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   communication_data_type ...... None
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   curriculum_enabled_legacy .... False
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   curriculum_params_legacy ..... False
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   data_efficiency_enabled ...... False
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   dataloader_drop_last ......... False
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   datastates_config ............ {
x3104c0s1b0n0:     "enabled": null, 
x3104c0s1b0n0:     "config": {
x3104c0s1b0n0:     }
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   disable_allgather ............ False
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   dump_state ................... False
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   dynamic_loss_scale_args ...... None
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   eigenvalue_enabled ........... False
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   eigenvalue_gas_boundary_resolution  1
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   eigenvalue_layer_name ........ bert.encoder.layer
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   eigenvalue_layer_num ......... 0
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   eigenvalue_max_iter .......... 100
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   eigenvalue_stability ......... 1e-06
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   eigenvalue_tol ............... 0.01
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   eigenvalue_verbose ........... False
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   elasticity_enabled ........... False
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   flops_profiler_config ........ {
x3104c0s1b0n0:     "enabled": false, 
x3104c0s1b0n0:     "recompute_fwd_factor": 0.0, 
x3104c0s1b0n0:     "profile_step": 1, 
x3104c0s1b0n0:     "module_depth": -1, 
x3104c0s1b0n0:     "top_modules": 1, 
x3104c0s1b0n0:     "detailed": true, 
x3104c0s1b0n0:     "output_file": null
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   fp16_auto_cast ............... None
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   fp16_enabled ................. False
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   fp16_master_weights_and_gradients  False
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   global_rank .................. 0
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   grad_accum_dtype ............. bf16
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   gradient_accumulation_steps .. 1
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   gradient_clipping ............ 0.0
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   gradient_predivide_factor .... 1.0
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   graph_harvesting ............. False
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   initial_dynamic_scale ........ 1
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   load_universal_checkpoint .... False
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   loss_scale ................... 1.0
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   memory_breakdown ............. True
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   mics_hierarchial_params_gather  False
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   mics_shard_size .............. -1
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   nebula_config ................ {
x3104c0s1b0n0:     "enabled": false, 
x3104c0s1b0n0:     "persistent_storage_path": null, 
x3104c0s1b0n0:     "persistent_time_interval": 100, 
x3104c0s1b0n0:     "num_of_version_in_retention": 2, 
x3104c0s1b0n0:     "enable_nebula_load": true, 
x3104c0s1b0n0:     "load_path": null
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   optimizer_legacy_fusion ...... False
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   optimizer_name ............... None
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   optimizer_params ............. None
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   pld_enabled .................. False
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   pld_params ................... False
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   prescale_gradients ........... False
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   scheduler_name ............... None
x3104c0s1b0n0: [2024-03-29 15:39:10,726] [INFO] [config.py:1002:print]   scheduler_params ............. None
x3104c0s1b0n0: [2024-03-29 15:39:10,727] [INFO] [config.py:1002:print]   seq_parallel_communication_data_type  torch.float32
x3104c0s1b0n0: [2024-03-29 15:39:10,727] [INFO] [config.py:1002:print]   sparse_attention ............. None
x3104c0s1b0n0: [2024-03-29 15:39:10,727] [INFO] [config.py:1002:print]   sparse_gradients_enabled ..... False
x3104c0s1b0n0: [2024-03-29 15:39:10,727] [INFO] [config.py:1002:print]   steps_per_print .............. 1
x3104c0s1b0n0: [2024-03-29 15:39:10,727] [INFO] [config.py:1002:print]   train_batch_size ............. 32
x3104c0s1b0n0: [2024-03-29 15:39:10,727] [INFO] [config.py:1002:print]   train_micro_batch_size_per_gpu  4
x3104c0s1b0n0: [2024-03-29 15:39:10,727] [INFO] [config.py:1002:print]   use_data_before_expert_parallel_  False
x3104c0s1b0n0: [2024-03-29 15:39:10,727] [INFO] [config.py:1002:print]   use_node_local_storage ....... False
x3104c0s1b0n0: [2024-03-29 15:39:10,727] [INFO] [config.py:1002:print]   wall_clock_breakdown ......... True
x3104c0s1b0n0: [2024-03-29 15:39:10,727] [INFO] [config.py:1002:print]   weight_quantization_config ... None
x3104c0s1b0n0: [2024-03-29 15:39:10,727] [INFO] [config.py:1002:print]   world_size ................... 8
x3104c0s1b0n0: [2024-03-29 15:39:10,727] [INFO] [config.py:1002:print]   zero_allow_untested_optimizer  False
x3104c0s1b0n0: [2024-03-29 15:39:10,727] [INFO] [config.py:1002:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0, prefetch_optimizer=True, part_grads_async=True, prefetch_optimizer_gap=3) sub_group_size=100000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
x3104c0s1b0n0: [2024-03-29 15:39:10,727] [INFO] [config.py:1002:print]   zero_enabled ................. True
x3104c0s1b0n0: [2024-03-29 15:39:10,727] [INFO] [config.py:1002:print]   zero_force_ds_cpu_optimizer .. True
x3104c0s1b0n0: [2024-03-29 15:39:10,727] [INFO] [config.py:1002:print]   zero_optimization_stage ...... 3
x3104c0s1b0n0: [2024-03-29 15:39:10,727] [INFO] [config.py:988:print_user_config]   json = {
x3104c0s1b0n0:     "train_batch_size": 32, 
x3104c0s1b0n0:     "train_micro_batch_size_per_gpu": 4, 
x3104c0s1b0n0:     "steps_per_print": 1, 
x3104c0s1b0n0:     "zero_optimization": {
x3104c0s1b0n0:         "stage": 3, 
x3104c0s1b0n0:         "offload_optimizer": {
x3104c0s1b0n0:             "device": "cpu", 
x3104c0s1b0n0:             "ratio": 1, 
x3104c0s1b0n0:             "pin_memory": true, 
x3104c0s1b0n0:             "prefetch_optimizer": 1, 
x3104c0s1b0n0:             "part_grads_async": 1, 
x3104c0s1b0n0:             "prefetch_optimizer_gap": 3
x3104c0s1b0n0:         }, 
x3104c0s1b0n0:         "sub_group_size": 1.000000e+08
x3104c0s1b0n0:     }, 
x3104c0s1b0n0:     "bf16": {
x3104c0s1b0n0:         "enabled": true
x3104c0s1b0n0:     }, 
x3104c0s1b0n0:     "data_types": {
x3104c0s1b0n0:         "grad_accum_dtype": "bf16"
x3104c0s1b0n0:     }, 
x3104c0s1b0n0:     "wall_clock_breakdown": true, 
x3104c0s1b0n0:     "memory_breakdown": true, 
x3104c0s1b0n0:     "flops_profiler": {
x3104c0s1b0n0:         "enabled": false
x3104c0s1b0n0:     }
x3104c0s1b0n0: }
x3104c0s25b0n0: <TIMER:model-and-optimizer-setup,50.451576471328735>
x3104c0s1b0n0: <TIMER:model-and-optimizer-setup,50.451934814453125>
x3104c0s1b0n0: <TIMER:model-and-optimizer-setup,50.45197820663452>
x3104c0s25b0n0: <TIMER:model-and-optimizer-setup,50.45286154747009>
x3104c0s25b0n0: <TIMER:model-and-optimizer-setup,50.45294785499573>
x3104c0s1b0n0: <TIMER:model-and-optimizer-setup,50.45349454879761>
x3104c0s25b0n0: <TIMER:model-and-optimizer-setup,50.45350933074951>
x3104c0s1b0n0: <TIMER:model-and-optimizer-setup,50.45358347892761>
x3104c0s1b0n0: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-29 15:39:12 
x3104c0s1b0n0: > building train, validation, and test datasets ...
x3104c0s1b0n0:  > datasets target sizes (minimum size):
x3104c0s1b0n0:     train:      320
x3104c0s1b0n0:     validation: 0
x3104c0s1b0n0:     test:       0
x3104c0s1b0n0: > building train, validation, and test datasets for GPT ...
x3104c0s1b0n0: Single data path provided for train, valid & test
x3104c0s1b0n0:  > building dataset index ...
x3104c0s1b0n0:     reading sizes...
x3104c0s1b0n0:     reading pointers...
x3104c0s1b0n0:     reading document index...
x3104c0s1b0n0:     creating numpy buffer of mmap...
x3104c0s1b0n0:     creating memory view of numpy buffer...
x3104c0s1b0n0:  > finished creating indexed dataset in 0.002367 seconds
x3104c0s1b0n0:     number of documents: 79000
x3104c0s1b0n0:  > dataset split:
x3104c0s1b0n0:     train:
x3104c0s1b0n0:      document indices in [0, 74971) total of 74971 documents
x3104c0s1b0n0:     validation:
x3104c0s1b0n0:      document indices in [74971, 78921) total of 3950 documents
x3104c0s1b0n0:     test:
x3104c0s1b0n0:      document indices in [78921, 79000) total of 79 documents
x3104c0s1b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_doc_idx.npy
x3104c0s1b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_sample_idx.npy
x3104c0s1b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_shuffle_idx.npy
x3104c0s1b0n0:     loaded indexed file in 0.002 seconds
x3104c0s1b0n0:     total number of samples: 108448
x3104c0s1b0n0:     total number of epochs: 1
x3104c0s1b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_doc_idx.npy
x3104c0s1b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_sample_idx.npy
x3104c0s1b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_shuffle_idx.npy
x3104c0s1b0n0:     loaded indexed file in 0.004 seconds
x3104c0s1b0n0:     total number of samples: 5792
x3104c0s1b0n0:     total number of epochs: 1
x3104c0s1b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_doc_idx.npy
x3104c0s1b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_sample_idx.npy
x3104c0s1b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_shuffle_idx.npy
x3104c0s1b0n0:     loaded indexed file in 0.004 seconds
x3104c0s1b0n0:     total number of samples: 185
x3104c0s1b0n0:     total number of epochs: 1
x3104c0s1b0n0: > finished creating GPT datasets ...
x3104c0s25b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5127739906311035>
x3104c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5179381370544434>
x3104c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.526360273361206>
x3104c0s25b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5268664360046387>
x3104c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5281150341033936>
x3104c0s25b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5505926609039307>
x3104c0s25b0n0: <TIMER:train/valid/test-data-iterators-setup,0.579796314239502>
x3104c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.6163313388824463>
x3104c0s1b0n0: [after dataloaders are built] datetime: 2024-03-29 15:39:12 
x3104c0s1b0n0: done with setup ...
x3104c0s1b0n0: training ...
x3104c0s25b0n0: (min, max) time across ranks (ms):
x3104c0s25b0n0:     model-and-optimizer-setup ......................: (50451.58, 50453.58)
x3104c0s25b0n0:     train/valid/test-data-iterators-setup ..........: (512.77, 616.33)
x3104c0s1b0n0: [before training begins] datetime: 2024-03-29 15:39:12 
x3104c0s1b0n0: [before the start of training step] datetime: 2024-03-29 15:39:12 
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:39:12,821] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:39:12,822] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 7.45 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:39:12,822] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.16 GB, percent = 62.4%
x3104c0s1b0n0: [2024-03-29 15:39:12,943] [INFO] [checkpointing.py:539:forward] Activation Checkpointing Information
x3104c0s1b0n0: [2024-03-29 15:39:12,943] [INFO] [checkpointing.py:540:forward] ----Partition Activations False, CPU CHECKPOINTING False
x3104c0s1b0n0: [2024-03-29 15:39:12,943] [INFO] [checkpointing.py:541:forward] ----contiguous Memory Checkpointing False with 60 total layers
x3104c0s1b0n0: [2024-03-29 15:39:12,943] [INFO] [checkpointing.py:543:forward] ----Synchronization False
x3104c0s1b0n0: [2024-03-29 15:39:12,943] [INFO] [checkpointing.py:544:forward] ----Profiling time in checkpointing False
x3104c0s1b0n0: [2024-03-29 15:39:21,525] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:39:21,526] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 19.76 GB         CA 22.95 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 15:39:21,526] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.35 GB, percent = 62.5%
x3104c0s1b0n0: [2024-03-29 15:39:21,721] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:39:21,722] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 16.84 GB         CA 16.91 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 15:39:21,722] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.36 GB, percent = 62.5%
x3104c0s1b0n0: [2024-03-29 15:39:47,282] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:39:47,283] [INFO] [utils.py:801:see_memory_usage] MA 12.35 GB         Max_MA 20.43 GB         CA 12.63 GB         Max_CA 29 GB 
x3104c0s1b0n0: [2024-03-29 15:39:47,283] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.45 GB, percent = 62.5%
x3104c0s1b0n0: [2024-03-29 15:39:47,356] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:39:47,357] [INFO] [utils.py:801:see_memory_usage] MA 12.35 GB         Max_MA 12.35 GB         CA 12.63 GB         Max_CA 13 GB 
x3104c0s1b0n0: [2024-03-29 15:39:47,357] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.5 GB, percent = 62.5%
x3104c0s1b0n0: [2024-03-29 15:39:53,141] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.7196221351623535
x3104c0s1b0n0: [2024-03-29 15:39:53,184] [INFO] [stage3.py:2251:step] Full outer step loop took 5.763388633728027
x3104c0s1b0n0: [2024-03-29 15:39:53,220] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.7992331981658936
x3104c0s1b0n0: [2024-03-29 15:39:53,232] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.811254978179932
x3104c0s1b0n0: [2024-03-29 15:39:53,238] [INFO] [stage3.py:2251:step] Full outer step loop took 5.8168861865997314
x3104c0s1b0n0: [2024-03-29 15:39:53,239] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.817732810974121
x3104c0s1b0n0: [2024-03-29 15:39:53,242] [INFO] [stage3.py:2251:step] Full outer step loop took 5.820660352706909
x3104c0s1b0n0: [2024-03-29 15:39:53,248] [INFO] [stage3.py:2251:step] Full outer step loop took 5.82661509513855
x3104c0s25b0n0: [2024-03-29 15:39:53,402] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.98071813583374
x3104c0s25b0n0: [2024-03-29 15:39:53,415] [INFO] [stage3.py:2251:step] Full outer step loop took 5.994503974914551
x3104c0s25b0n0: [2024-03-29 15:39:53,502] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.0807342529296875
x3104c0s25b0n0: [2024-03-29 15:39:53,504] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.0825395584106445
x3104c0s25b0n0: [2024-03-29 15:39:53,510] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.088619232177734
x3104c0s25b0n0: [2024-03-29 15:39:53,511] [INFO] [stage3.py:2251:step] Full outer step loop took 6.0901408195495605
x3104c0s25b0n0: [2024-03-29 15:39:53,512] [INFO] [stage3.py:2251:step] Full outer step loop took 6.091436386108398
x3104c0s25b0n0: [2024-03-29 15:39:53,518] [INFO] [stage3.py:2251:step] Full outer step loop took 6.097513198852539
x3104c0s1b0n0: [2024-03-29 15:39:53,528] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 5817.11
x3104c0s25b0n0: [2024-03-29 15:39:53,528] [INFO] [stage3.py:2277:step] End to end step took 6.106665849685669
x3104c0s25b0n0: [2024-03-29 15:39:53,528] [INFO] [stage3.py:2277:step] End to end step took 6.106659173965454
x3104c0s25b0n0: [2024-03-29 15:39:53,528] [INFO] [stage3.py:2277:step] End to end step took 6.106757640838623
x3104c0s1b0n0: [2024-03-29 15:39:53,528] [INFO] [stage3.py:2277:step] End to end step took 6.106475353240967
x3104c0s1b0n0: [2024-03-29 15:39:53,528] [INFO] [stage3.py:2277:step] End to end step took 6.106745004653931
x3104c0s1b0n0: [2024-03-29 15:39:53,528] [WARNING] [stage3.py:2267:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3104c0s1b0n0: [2024-03-29 15:39:53,528] [INFO] [stage3.py:2277:step] End to end step took 6.106929779052734
x3104c0s1b0n0: [2024-03-29 15:39:53,528] [INFO] [logging.py:96:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 15:39:53,528] [INFO] [stage3.py:2277:step] End to end step took 6.107301712036133
x3104c0s25b0n0: [2024-03-29 15:39:53,528] [INFO] [stage3.py:2277:step] End to end step took 6.107338905334473
x3104c0s1b0n0: [2024-03-29 15:39:53,529] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 8801.41 | bwd_microstep: 25410.46 | bwd_inner_microstep: 25322.57 | bwd_allreduce_microstep: 87.79 | step_microstep: 6171.60
x3104c0s1b0n0: [2024-03-29 15:39:53,529] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 8801.41 | bwd: 25410.46 | bwd_inner: 25322.58 | bwd_allreduce: 87.79 | step: 6171.60
x3104c0s1b0n0: [2024-03-29 15:39:53,623] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:39:53,624] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 13.4 GB         CA 10.34 GB         Max_CA 15 GB 
x3104c0s1b0n0: [2024-03-29 15:39:53,624] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.11 GB, percent = 76.3%
x3104c0s1b0n0: <TIMER:interval-time,40.973411560058594><TIMER:interval-time,40.973411321640015><TIMER:interval-time,40.97340750694275>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,40.9734148979187>
x3104c0s25b0n0: <TIMER:interval-time,40.973621129989624><TIMER:interval-time,40.97363305091858><TIMER:interval-time,40.973620653152466><TIMER:interval-time,40.97362756729126>
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0:  elapsed_time 40.973633 | consumed samples:           32 | consumed tokens:        65536 | elapsed time per iteration (ms): 40973.6 | learning rate: 3.000E-04 | global batch size:    32 | lm loss: 1.215015E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.781 | TFLOPs: 54.04 |
x3104c0s1b0n0: [Rank 0] (after 1 iterations) memory (MB) | allocated: 10510.36328125 | max allocated: 10510.36376953125 | reserved: 10586.0 | max reserved: 10586.0
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:39:53,755] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:39:53,755] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 15:39:53,755] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.19 GB, percent = 76.3%
x3104c0s1b0n0: [2024-03-29 15:40:00,398] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:40:00,398] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 26.67 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:40:00,398] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.19 GB, percent = 76.3%
x3104c0s1b0n0: [2024-03-29 15:40:00,491] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:40:00,492] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.83 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:40:00,492] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.19 GB, percent = 76.3%
x3104c0s1b0n0: [2024-03-29 15:40:17,802] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:40:17,803] [INFO] [utils.py:801:see_memory_usage] MA 13.6 GB         Max_MA 23.87 GB         CA 13.89 GB         Max_CA 33 GB 
x3104c0s1b0n0: [2024-03-29 15:40:17,803] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.19 GB, percent = 76.3%
x3104c0s1b0n0: [2024-03-29 15:40:17,872] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:40:17,873] [INFO] [utils.py:801:see_memory_usage] MA 13.6 GB         Max_MA 13.6 GB         CA 13.89 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:40:17,873] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.19 GB, percent = 76.3%
x3104c0s1b0n0: [2024-03-29 15:40:21,888] [INFO] [stage3.py:2243:step] With missing steps outer loop took 3.990927219390869
x3104c0s1b0n0: [2024-03-29 15:40:21,917] [INFO] [stage3.py:2251:step] Full outer step loop took 4.020758628845215
x3104c0s1b0n0: [2024-03-29 15:40:21,986] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.088836431503296
x3104c0s1b0n0: [2024-03-29 15:40:21,999] [INFO] [stage3.py:2251:step] Full outer step loop took 4.102493524551392
x3104c0s1b0n0: [2024-03-29 15:40:22,016] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.119389533996582
x3104c0s1b0n0: [2024-03-29 15:40:22,022] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.125605344772339
x3104c0s1b0n0: [2024-03-29 15:40:22,027] [INFO] [stage3.py:2251:step] Full outer step loop took 4.130024671554565
x3104c0s1b0n0: [2024-03-29 15:40:22,031] [INFO] [stage3.py:2251:step] Full outer step loop took 4.134479761123657
x3104c0s25b0n0: [2024-03-29 15:40:22,211] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.314742565155029
x3104c0s25b0n0: [2024-03-29 15:40:22,220] [INFO] [stage3.py:2251:step] Full outer step loop took 4.323633909225464
x3104c0s25b0n0: [2024-03-29 15:40:22,375] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.478853940963745
x3104c0s25b0n0: [2024-03-29 15:40:22,385] [INFO] [stage3.py:2251:step] Full outer step loop took 4.4883201122283936
x3104c0s25b0n0: [2024-03-29 15:40:22,385] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.488981008529663
x3104c0s25b0n0: [2024-03-29 15:40:22,389] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.4928905963897705
x3104c0s25b0n0: [2024-03-29 15:40:22,395] [INFO] [stage3.py:2251:step] Full outer step loop took 4.498171329498291
x3104c0s25b0n0: [2024-03-29 15:40:22,398] [INFO] [stage3.py:2251:step] Full outer step loop took 4.501757860183716
x3104c0s1b0n0: [2024-03-29 15:40:22,408] [INFO] [stage3.py:2277:step] End to end step took 4.511190891265869
x3104c0s25b0n0: [2024-03-29 15:40:22,408] [INFO] [stage3.py:2277:step] End to end step took 4.51116418838501
x3104c0s25b0n0: [2024-03-29 15:40:22,408] [INFO] [stage3.py:2277:step] End to end step took 4.511277675628662
x3104c0s25b0n0: [2024-03-29 15:40:22,408] [INFO] [stage3.py:2277:step] End to end step took 4.511445045471191
x3104c0s1b0n0: [2024-03-29 15:40:22,408] [INFO] [stage3.py:2277:step] End to end step took 4.511128664016724
x3104c0s1b0n0: [2024-03-29 15:40:22,408] [INFO] [stage3.py:2277:step] End to end step took 4.5113608837127686
x3104c0s1b0n0: [2024-03-29 15:40:22,408] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4021.50
x3104c0s1b0n0: [2024-03-29 15:40:22,408] [INFO] [stage3.py:2277:step] End to end step took 4.511760711669922
x3104c0s25b0n0: [2024-03-29 15:40:22,408] [INFO] [stage3.py:2277:step] End to end step took 4.511915922164917
x3104c0s1b0n0: [2024-03-29 15:40:22,409] [INFO] [logging.py:96:log_dist] [Rank 0] step=2, skipped=0, lr=[0.0002918585038060976], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 15:40:22,409] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6606.83 | bwd_microstep: 17145.51 | bwd_inner_microstep: 17058.70 | bwd_allreduce_microstep: 86.74 | step_microstep: 4535.98
x3104c0s1b0n0: [2024-03-29 15:40:22,409] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6606.82 | bwd: 17145.51 | bwd_inner: 17058.69 | bwd_allreduce: 86.76 | step: 4535.98
x3104c0s1b0n0: [2024-03-29 15:40:22,516] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:40:22,516] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 13.6 GB         CA 10.34 GB         Max_CA 15 GB 
x3104c0s1b0n0: [2024-03-29 15:40:22,517] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.19 GB, percent = 76.3%
x3104c0s1b0n0: <TIMER:interval-time,28.892301559448242><TIMER:interval-time,28.89230465888977><TIMER:interval-time,28.892303943634033><TIMER:interval-time,28.89230251312256>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,28.89231538772583>
x3104c0s25b0n0: <TIMER:interval-time,28.892318964004517>
x3104c0s25b0n0: <TIMER:interval-time,28.892321586608887>
x3104c0s25b0n0: <TIMER:interval-time,28.892436504364014>
x3104c0s25b0n0:  elapsed_time 28.892322 | consumed samples:           64 | consumed tokens:       131072 | elapsed time per iteration (ms): 28892.3 | learning rate: 2.919E-04 | global batch size:    32 | lm loss: 1.082593E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.108 | TFLOPs: 76.63 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:40:22,633] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:40:22,634] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 15:40:22,634] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.2 GB, percent = 76.3%
x3104c0s1b0n0: [2024-03-29 15:40:29,163] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:40:29,163] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:40:29,164] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.2 GB, percent = 76.3%
x3104c0s1b0n0: [2024-03-29 15:40:29,245] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:40:29,245] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:40:29,245] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.2 GB, percent = 76.3%
x3104c0s1b0n0: [2024-03-29 15:40:46,590] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:40:46,590] [INFO] [utils.py:801:see_memory_usage] MA 13.6 GB         Max_MA 23.87 GB         CA 13.89 GB         Max_CA 33 GB 
x3104c0s1b0n0: [2024-03-29 15:40:46,591] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.2 GB, percent = 76.3%
x3104c0s1b0n0: [2024-03-29 15:40:46,663] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:40:46,663] [INFO] [utils.py:801:see_memory_usage] MA 13.6 GB         Max_MA 13.6 GB         CA 13.89 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:40:46,663] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.2 GB, percent = 76.3%
x3104c0s1b0n0: [2024-03-29 15:40:50,737] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.050261497497559
x3104c0s1b0n0: [2024-03-29 15:40:50,746] [INFO] [stage3.py:2251:step] Full outer step loop took 4.05924391746521
x3104c0s1b0n0: [2024-03-29 15:40:50,841] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.15429949760437
x3104c0s1b0n0: [2024-03-29 15:40:50,841] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.1543800830841064
x3104c0s1b0n0: [2024-03-29 15:40:50,850] [INFO] [stage3.py:2251:step] Full outer step loop took 4.163374423980713
x3104c0s1b0n0: [2024-03-29 15:40:50,859] [INFO] [stage3.py:2251:step] Full outer step loop took 4.171557426452637
x3104c0s1b0n0: [2024-03-29 15:40:50,897] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.210021734237671
x3104c0s1b0n0: [2024-03-29 15:40:50,906] [INFO] [stage3.py:2251:step] Full outer step loop took 4.21887731552124
x3104c0s25b0n0: [2024-03-29 15:40:51,210] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.52278470993042
x3104c0s25b0n0: [2024-03-29 15:40:51,210] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.523287534713745
x3104c0s25b0n0: [2024-03-29 15:40:51,219] [INFO] [stage3.py:2251:step] Full outer step loop took 4.532261371612549
x3104c0s25b0n0: [2024-03-29 15:40:51,222] [INFO] [stage3.py:2251:step] Full outer step loop took 4.534597158432007
x3104c0s25b0n0: [2024-03-29 15:40:51,340] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.652673721313477
x3104c0s25b0n0: [2024-03-29 15:40:51,340] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.653012275695801
x3104c0s25b0n0: [2024-03-29 15:40:51,348] [INFO] [stage3.py:2251:step] Full outer step loop took 4.661538362503052
x3104c0s25b0n0: [2024-03-29 15:40:51,349] [INFO] [stage3.py:2251:step] Full outer step loop took 4.661905527114868
x3104c0s25b0n0: [2024-03-29 15:40:51,358] [INFO] [stage3.py:2277:step] End to end step took 4.671043634414673
x3104c0s25b0n0: [2024-03-29 15:40:51,358] [INFO] [stage3.py:2277:step] End to end step took 4.671009063720703
x3104c0s1b0n0: [2024-03-29 15:40:51,358] [INFO] [stage3.py:2277:step] End to end step took 4.6708595752716064
x3104c0s1b0n0: [2024-03-29 15:40:51,358] [INFO] [stage3.py:2277:step] End to end step took 4.671196699142456
x3104c0s1b0n0: [2024-03-29 15:40:51,358] [INFO] [stage3.py:2277:step] End to end step took 4.671236038208008
x3104c0s1b0n0: [2024-03-29 15:40:51,358] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4059.39
x3104c0s25b0n0: [2024-03-29 15:40:51,358] [INFO] [stage3.py:2277:step] End to end step took 4.671491622924805
x3104c0s1b0n0: [2024-03-29 15:40:51,359] [INFO] [stage3.py:2277:step] End to end step took 4.671688795089722
x3104c0s25b0n0: [2024-03-29 15:40:51,359] [INFO] [stage3.py:2277:step] End to end step took 4.671677827835083
x3104c0s1b0n0: [2024-03-29 15:40:51,359] [INFO] [logging.py:96:log_dist] [Rank 0] step=3, skipped=0, lr=[0.00026841599982106197], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 15:40:51,359] [INFO] [timer.py:260:stop] epoch=0/micro_step=3/global_step=3, RunningAvgSamplesPerSec=1.1140510426953885, CurrSamplesPerSec=1.1140510426953885, MemAllocated=10.26GB, MaxMemAllocated=13.6GB
x3104c0s1b0n0: [2024-03-29 15:40:51,360] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6489.37 | bwd_microstep: 17176.98 | bwd_inner_microstep: 17089.42 | bwd_allreduce_microstep: 87.49 | step_microstep: 4696.06
x3104c0s1b0n0: [2024-03-29 15:40:51,360] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6489.36 | bwd: 17176.97 | bwd_inner: 17089.42 | bwd_allreduce: 87.50 | step: 4696.07
x3104c0s1b0n0: [2024-03-29 15:40:51,461] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:40:51,461] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 13.6 GB         CA 10.34 GB         Max_CA 15 GB 
x3104c0s1b0n0: [2024-03-29 15:40:51,461] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.2 GB, percent = 76.3%
x3104c0s1b0n0: <TIMER:interval-time,28.944385051727295><TIMER:interval-time,28.944382190704346><TIMER:interval-time,28.94438362121582><TIMER:interval-time,28.944379568099976>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,28.944421529769897>
x3104c0s25b0n0: <TIMER:interval-time,28.94442582130432><TIMER:interval-time,28.944426774978638>
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,28.944430828094482>
x3104c0s25b0n0:  elapsed_time 28.944427 | consumed samples:           96 | consumed tokens:       196608 | elapsed time per iteration (ms): 28944.4 | learning rate: 2.684E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.106 | TFLOPs: 76.50 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:40:51,554] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:40:51,555] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 15:40:51,555] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.22 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:40:58,066] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:40:58,067] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:40:58,067] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.22 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:40:58,157] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:40:58,158] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:40:58,158] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.22 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:41:15,476] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:41:15,476] [INFO] [utils.py:801:see_memory_usage] MA 13.6 GB         Max_MA 23.87 GB         CA 13.89 GB         Max_CA 33 GB 
x3104c0s1b0n0: [2024-03-29 15:41:15,476] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.22 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:41:15,551] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:41:15,552] [INFO] [utils.py:801:see_memory_usage] MA 13.6 GB         Max_MA 13.6 GB         CA 13.89 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:41:15,552] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.22 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:41:19,664] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.087532997131348
x3104c0s1b0n0: [2024-03-29 15:41:19,675] [INFO] [stage3.py:2251:step] Full outer step loop took 4.099246978759766
x3104c0s1b0n0: [2024-03-29 15:41:19,722] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.145793199539185
x3104c0s1b0n0: [2024-03-29 15:41:19,731] [INFO] [stage3.py:2251:step] Full outer step loop took 4.154706954956055
x3104c0s1b0n0: [2024-03-29 15:41:19,785] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.208588123321533
x3104c0s1b0n0: [2024-03-29 15:41:19,794] [INFO] [stage3.py:2251:step] Full outer step loop took 4.21744704246521
x3104c0s1b0n0: [2024-03-29 15:41:19,883] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.306709289550781
x3104c0s1b0n0: [2024-03-29 15:41:19,892] [INFO] [stage3.py:2251:step] Full outer step loop took 4.315560579299927
x3104c0s25b0n0: [2024-03-29 15:41:20,002] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.426082611083984
x3104c0s25b0n0: [2024-03-29 15:41:20,011] [INFO] [stage3.py:2251:step] Full outer step loop took 4.434955358505249
x3104c0s25b0n0: [2024-03-29 15:41:20,096] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.519990921020508
x3104c0s25b0n0: [2024-03-29 15:41:20,107] [INFO] [stage3.py:2251:step] Full outer step loop took 4.531351089477539
x3104c0s25b0n0: [2024-03-29 15:41:20,141] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.564604997634888
x3104c0s25b0n0: [2024-03-29 15:41:20,154] [INFO] [stage3.py:2251:step] Full outer step loop took 4.57767128944397
x3104c0s25b0n0: [2024-03-29 15:41:20,193] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.6165854930877686
x3104c0s25b0n0: [2024-03-29 15:41:20,201] [INFO] [stage3.py:2251:step] Full outer step loop took 4.625462770462036
x3104c0s1b0n0: [2024-03-29 15:41:20,211] [INFO] [stage3.py:2277:step] End to end step took 4.634956121444702
x3104c0s25b0n0: [2024-03-29 15:41:20,211] [INFO] [stage3.py:2277:step] End to end step took 4.634931325912476
x3104c0s25b0n0: [2024-03-29 15:41:20,211] [INFO] [stage3.py:2277:step] End to end step took 4.635050058364868
x3104c0s25b0n0: [2024-03-29 15:41:20,211] [INFO] [stage3.py:2277:step] End to end step took 4.635103940963745
x3104c0s1b0n0: [2024-03-29 15:41:20,211] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4099.41
x3104c0s25b0n0: [2024-03-29 15:41:20,211] [INFO] [stage3.py:2277:step] End to end step took 4.635474920272827
x3104c0s1b0n0: [2024-03-29 15:41:20,212] [INFO] [stage3.py:2277:step] End to end step took 4.635411024093628
x3104c0s1b0n0: [2024-03-29 15:41:20,212] [INFO] [stage3.py:2277:step] End to end step took 4.635423183441162
x3104c0s1b0n0: [2024-03-29 15:41:20,212] [INFO] [stage3.py:2277:step] End to end step took 4.635414361953735
x3104c0s1b0n0: [2024-03-29 15:41:20,212] [INFO] [logging.py:96:log_dist] [Rank 0] step=4, skipped=0, lr=[0.00023249999999999996], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 15:41:20,212] [INFO] [timer.py:260:stop] epoch=0/micro_step=4/global_step=4, RunningAvgSamplesPerSec=1.1153661742571008, CurrSamplesPerSec=1.1166844145007695, MemAllocated=10.26GB, MaxMemAllocated=13.6GB
x3104c0s1b0n0: [2024-03-29 15:41:20,213] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6458.26 | bwd_microstep: 17145.28 | bwd_inner_microstep: 17061.82 | bwd_allreduce_microstep: 83.38 | step_microstep: 4660.51
x3104c0s1b0n0: [2024-03-29 15:41:20,213] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6458.25 | bwd: 17145.27 | bwd_inner: 17061.81 | bwd_allreduce: 83.39 | step: 4660.51
x3104c0s1b0n0: [2024-03-29 15:41:20,324] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:41:20,325] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 13.6 GB         CA 10.34 GB         Max_CA 15 GB 
x3104c0s1b0n0: [2024-03-29 15:41:20,325] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.22 GB, percent = 76.4%
x3104c0s1b0n0: <TIMER:interval-time,28.863479614257812><TIMER:interval-time,28.863479614257812>
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,28.863479137420654><TIMER:interval-time,28.8634831905365>
x3104c0s1b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,28.863518476486206><TIMER:interval-time,28.8635196685791>
x3104c0s25b0n0: <TIMER:interval-time,28.86352229118347>
x3104c0s25b0n0: <TIMER:interval-time,28.863520860671997>
x3104c0s25b0n0: 
x3104c0s25b0n0:  elapsed_time 28.863522 | consumed samples:          128 | consumed tokens:       262144 | elapsed time per iteration (ms): 28863.5 | learning rate: 2.325E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.109 | TFLOPs: 76.71 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:41:20,451] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:41:20,451] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 15:41:20,451] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.21 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:41:26,624] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:41:26,624] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:41:26,624] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.21 GB, percent = 76.3%
x3104c0s1b0n0: [2024-03-29 15:41:26,716] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:41:26,716] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:41:26,716] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.21 GB, percent = 76.3%
x3104c0s1b0n0: [2024-03-29 15:41:42,482] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:41:42,483] [INFO] [utils.py:801:see_memory_usage] MA 13.6 GB         Max_MA 23.87 GB         CA 13.89 GB         Max_CA 33 GB 
x3104c0s1b0n0: [2024-03-29 15:41:42,483] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.21 GB, percent = 76.3%
x3104c0s1b0n0: [2024-03-29 15:41:42,561] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:41:42,562] [INFO] [utils.py:801:see_memory_usage] MA 13.6 GB         Max_MA 13.6 GB         CA 13.89 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:41:42,562] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.21 GB, percent = 76.3%
x3104c0s1b0n0: [2024-03-29 15:41:46,617] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.0299201011657715
x3104c0s1b0n0: [2024-03-29 15:41:46,631] [INFO] [stage3.py:2251:step] Full outer step loop took 4.0447962284088135
x3104c0s1b0n0: [2024-03-29 15:41:46,649] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.062816619873047
x3104c0s1b0n0: [2024-03-29 15:41:46,671] [INFO] [stage3.py:2251:step] Full outer step loop took 4.084545135498047
x3104c0s1b0n0: [2024-03-29 15:41:46,718] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.1311023235321045
x3104c0s1b0n0: [2024-03-29 15:41:46,721] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.134047031402588
x3104c0s1b0n0: [2024-03-29 15:41:46,727] [INFO] [stage3.py:2251:step] Full outer step loop took 4.140321731567383
x3104c0s1b0n0: [2024-03-29 15:41:46,730] [INFO] [stage3.py:2251:step] Full outer step loop took 4.142917156219482
x3104c0s25b0n0: [2024-03-29 15:41:47,043] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.4567551612854
x3104c0s25b0n0: [2024-03-29 15:41:47,055] [INFO] [stage3.py:2251:step] Full outer step loop took 4.468587398529053
x3104c0s25b0n0: [2024-03-29 15:41:47,057] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.470117807388306
x3104c0s25b0n0: [2024-03-29 15:41:47,080] [INFO] [stage3.py:2251:step] Full outer step loop took 4.49312686920166
x3104c0s25b0n0: [2024-03-29 15:41:47,092] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.505754709243774
x3104c0s25b0n0: [2024-03-29 15:41:47,101] [INFO] [stage3.py:2251:step] Full outer step loop took 4.514647960662842
x3104c0s25b0n0: [2024-03-29 15:41:47,179] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.592539310455322
x3104c0s25b0n0: [2024-03-29 15:41:47,188] [INFO] [stage3.py:2251:step] Full outer step loop took 4.601412534713745
x3104c0s1b0n0: [2024-03-29 15:41:47,197] [INFO] [stage3.py:2277:step] End to end step took 4.610625982284546
x3104c0s1b0n0: [2024-03-29 15:41:47,197] [INFO] [stage3.py:2277:step] End to end step took 4.610901355743408
x3104c0s25b0n0: [2024-03-29 15:41:47,197] [INFO] [stage3.py:2277:step] End to end step took 4.610800504684448
x3104c0s25b0n0: [2024-03-29 15:41:47,197] [INFO] [stage3.py:2277:step] End to end step took 4.610976696014404
x3104c0s25b0n0: [2024-03-29 15:41:47,197] [INFO] [stage3.py:2277:step] End to end step took 4.611124277114868
x3104c0s25b0n0: [2024-03-29 15:41:47,198] [INFO] [stage3.py:2277:step] End to end step took 4.611179351806641
x3104c0s1b0n0: [2024-03-29 15:41:47,198] [INFO] [stage3.py:2277:step] End to end step took 4.611205577850342
x3104c0s1b0n0: [2024-03-29 15:41:47,198] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4045.80
x3104c0s1b0n0: [2024-03-29 15:41:47,198] [INFO] [stage3.py:2277:step] End to end step took 4.611388206481934
x3104c0s1b0n0: [2024-03-29 15:41:47,198] [INFO] [logging.py:96:log_dist] [Rank 0] step=5, skipped=0, lr=[0.0001884425039850356], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 15:41:47,199] [INFO] [timer.py:260:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=1.1411424115479922, CurrSamplesPerSec=1.1964422400644674, MemAllocated=10.26GB, MaxMemAllocated=13.6GB
x3104c0s1b0n0: [2024-03-29 15:41:47,199] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6133.95 | bwd_microstep: 15589.79 | bwd_inner_microstep: 15513.84 | bwd_allreduce_microstep: 75.88 | step_microstep: 4636.47
x3104c0s1b0n0: [2024-03-29 15:41:47,199] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6133.94 | bwd: 15589.78 | bwd_inner: 15513.83 | bwd_allreduce: 75.89 | step: 4636.47
x3104c0s1b0n0: [2024-03-29 15:41:47,300] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:41:47,300] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 13.6 GB         CA 10.34 GB         Max_CA 15 GB 
x3104c0s1b0n0: [2024-03-29 15:41:47,300] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.21 GB, percent = 76.3%
x3104c0s1b0n0: <TIMER:interval-time,26.975181818008423><TIMER:interval-time,26.975184202194214><TIMER:interval-time,26.975184679031372><TIMER:interval-time,26.975184679031372>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,26.975196361541748><TIMER:interval-time,26.9751980304718><TIMER:interval-time,26.975196361541748><TIMER:interval-time,26.975199937820435>
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0:  elapsed_time 26.975196 | consumed samples:          160 | consumed tokens:       327680 | elapsed time per iteration (ms): 26975.2 | learning rate: 1.884E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.186 | TFLOPs: 82.08 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:41:47,436] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:41:47,436] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 15:41:47,436] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.23 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:41:53,473] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:41:53,473] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:41:53,473] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.22 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:41:53,550] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:41:53,550] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:41:53,550] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.22 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:42:09,785] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:42:09,785] [INFO] [utils.py:801:see_memory_usage] MA 13.6 GB         Max_MA 23.87 GB         CA 13.89 GB         Max_CA 33 GB 
x3104c0s1b0n0: [2024-03-29 15:42:09,786] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.23 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:42:09,857] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:42:09,858] [INFO] [utils.py:801:see_memory_usage] MA 13.6 GB         Max_MA 13.6 GB         CA 13.89 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:42:09,858] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.23 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:42:13,998] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.116115093231201
x3104c0s1b0n0: [2024-03-29 15:42:14,002] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.120423793792725
x3104c0s1b0n0: [2024-03-29 15:42:14,007] [INFO] [stage3.py:2251:step] Full outer step loop took 4.125343561172485
x3104c0s1b0n0: [2024-03-29 15:42:14,008] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.126251459121704
x3104c0s1b0n0: [2024-03-29 15:42:14,011] [INFO] [stage3.py:2251:step] Full outer step loop took 4.129328012466431
x3104c0s1b0n0: [2024-03-29 15:42:14,017] [INFO] [stage3.py:2251:step] Full outer step loop took 4.13513708114624
x3104c0s1b0n0: [2024-03-29 15:42:14,136] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.254598379135132
x3104c0s1b0n0: [2024-03-29 15:42:14,145] [INFO] [stage3.py:2251:step] Full outer step loop took 4.263443470001221
x3104c0s25b0n0: [2024-03-29 15:42:14,335] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.453177452087402
x3104c0s25b0n0: [2024-03-29 15:42:14,337] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.45564341545105
x3104c0s25b0n0: [2024-03-29 15:42:14,346] [INFO] [stage3.py:2251:step] Full outer step loop took 4.463953971862793
x3104c0s25b0n0: [2024-03-29 15:42:14,346] [INFO] [stage3.py:2251:step] Full outer step loop took 4.464582443237305
x3104c0s25b0n0: [2024-03-29 15:42:14,439] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.556921720504761
x3104c0s25b0n0: [2024-03-29 15:42:14,450] [INFO] [stage3.py:2251:step] Full outer step loop took 4.568633556365967
x3104c0s25b0n0: [2024-03-29 15:42:14,479] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.597153663635254
x3104c0s25b0n0: [2024-03-29 15:42:14,488] [INFO] [stage3.py:2251:step] Full outer step loop took 4.606004953384399
x3104c0s25b0n0: [2024-03-29 15:42:14,497] [INFO] [stage3.py:2277:step] End to end step took 4.615315198898315
x3104c0s25b0n0: [2024-03-29 15:42:14,497] [INFO] [stage3.py:2277:step] End to end step took 4.615239381790161
x3104c0s1b0n0: [2024-03-29 15:42:14,497] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4125.52
x3104c0s1b0n0: [2024-03-29 15:42:14,497] [INFO] [stage3.py:2277:step] End to end step took 4.615351915359497
x3104c0s1b0n0: [2024-03-29 15:42:14,497] [INFO] [stage3.py:2277:step] End to end step took 4.615488529205322
x3104c0s25b0n0: [2024-03-29 15:42:14,497] [INFO] [stage3.py:2277:step] End to end step took 4.615813970565796
x3104c0s25b0n0: [2024-03-29 15:42:14,498] [INFO] [stage3.py:2277:step] End to end step took 4.615880012512207
x3104c0s1b0n0: [2024-03-29 15:42:14,497] [INFO] [stage3.py:2277:step] End to end step took 4.615738153457642
x3104c0s1b0n0: [2024-03-29 15:42:14,497] [INFO] [stage3.py:2277:step] End to end step took 4.615724563598633
x3104c0s1b0n0: [2024-03-29 15:42:14,498] [INFO] [logging.py:96:log_dist] [Rank 0] step=6, skipped=0, lr=[0.0001415574960149644], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 15:42:14,498] [INFO] [timer.py:260:stop] epoch=0/micro_step=6/global_step=6, RunningAvgSamplesPerSec=1.1512182850215384, CurrSamplesPerSec=1.1825425500496567, MemAllocated=10.26GB, MaxMemAllocated=13.6GB
x3104c0s1b0n0: [2024-03-29 15:42:14,498] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6003.52 | bwd_microstep: 16064.16 | bwd_inner_microstep: 15986.96 | bwd_allreduce_microstep: 77.12 | step_microstep: 4639.94
x3104c0s1b0n0: [2024-03-29 15:42:14,498] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6003.51 | bwd: 16064.16 | bwd_inner: 15986.96 | bwd_allreduce: 77.14 | step: 4639.94
x3104c0s1b0n0: [2024-03-29 15:42:14,600] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:42:14,600] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 13.6 GB         CA 10.34 GB         Max_CA 15 GB 
x3104c0s1b0n0: [2024-03-29 15:42:14,601] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.23 GB, percent = 76.4%
x3104c0s1b0n0: <TIMER:interval-time,27.30002450942993><TIMER:interval-time,27.300031423568726>
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,27.300040245056152><TIMER:interval-time,27.300041913986206>
x3104c0s1b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,27.300071716308594><TIMER:interval-time,27.300073385238647>
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,27.30007576942444><TIMER:interval-time,27.300076723098755>
x3104c0s25b0n0: 
x3104c0s25b0n0:  elapsed_time 27.300073 | consumed samples:          192 | consumed tokens:       393216 | elapsed time per iteration (ms): 27300.1 | learning rate: 1.416E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.172 | TFLOPs: 81.10 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:42:14,730] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:42:14,731] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 15:42:14,731] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.25 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:42:21,360] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:42:21,360] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:42:21,360] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.24 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:42:21,435] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:42:21,436] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:42:21,436] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.24 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:42:38,857] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:42:38,857] [INFO] [utils.py:801:see_memory_usage] MA 13.6 GB         Max_MA 23.87 GB         CA 13.89 GB         Max_CA 33 GB 
x3104c0s1b0n0: [2024-03-29 15:42:38,857] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.25 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:42:38,926] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:42:38,927] [INFO] [utils.py:801:see_memory_usage] MA 13.6 GB         Max_MA 13.6 GB         CA 13.89 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:42:38,927] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.25 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:42:43,105] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.153617858886719
x3104c0s1b0n0: [2024-03-29 15:42:43,108] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.156713247299194
x3104c0s1b0n0: [2024-03-29 15:42:43,110] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.1588969230651855
x3104c0s1b0n0: [2024-03-29 15:42:43,113] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.162430047988892
x3104c0s1b0n0: [2024-03-29 15:42:43,114] [INFO] [stage3.py:2251:step] Full outer step loop took 4.1629860401153564
x3104c0s1b0n0: [2024-03-29 15:42:43,117] [INFO] [stage3.py:2251:step] Full outer step loop took 4.165694236755371
x3104c0s1b0n0: [2024-03-29 15:42:43,119] [INFO] [stage3.py:2251:step] Full outer step loop took 4.167783737182617
x3104c0s1b0n0: [2024-03-29 15:42:43,122] [INFO] [stage3.py:2251:step] Full outer step loop took 4.171306610107422
x3104c0s25b0n0: [2024-03-29 15:42:43,329] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.37844443321228
x3104c0s25b0n0: [2024-03-29 15:42:43,338] [INFO] [stage3.py:2251:step] Full outer step loop took 4.3873841762542725
x3104c0s25b0n0: [2024-03-29 15:42:43,414] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.463155269622803
x3104c0s25b0n0: [2024-03-29 15:42:43,438] [INFO] [stage3.py:2251:step] Full outer step loop took 4.486727476119995
x3104c0s25b0n0: [2024-03-29 15:42:43,503] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.552473783493042
x3104c0s25b0n0: [2024-03-29 15:42:43,514] [INFO] [stage3.py:2251:step] Full outer step loop took 4.563417911529541
x3104c0s25b0n0: [2024-03-29 15:42:43,514] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.563397645950317
x3104c0s25b0n0: [2024-03-29 15:42:43,523] [INFO] [stage3.py:2251:step] Full outer step loop took 4.572254419326782
x3104c0s1b0n0: [2024-03-29 15:42:43,533] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4167.94
x3104c0s1b0n0: [2024-03-29 15:42:43,533] [INFO] [stage3.py:2277:step] End to end step took 4.5818493366241455
x3104c0s25b0n0: [2024-03-29 15:42:43,533] [INFO] [stage3.py:2277:step] End to end step took 4.581918239593506
x3104c0s1b0n0: [2024-03-29 15:42:43,533] [INFO] [stage3.py:2277:step] End to end step took 4.5818400382995605
x3104c0s25b0n0: [2024-03-29 15:42:43,533] [INFO] [stage3.py:2277:step] End to end step took 4.582028150558472
x3104c0s1b0n0: [2024-03-29 15:42:43,533] [INFO] [stage3.py:2277:step] End to end step took 4.582062005996704
x3104c0s1b0n0: [2024-03-29 15:42:43,533] [INFO] [logging.py:96:log_dist] [Rank 0] step=7, skipped=0, lr=[9.750000000000001e-05], mom=[(0.9, 0.95)]
x3104c0s25b0n0: [2024-03-29 15:42:43,533] [INFO] [stage3.py:2277:step] End to end step took 4.582052707672119
x3104c0s25b0n0: [2024-03-29 15:42:43,533] [INFO] [stage3.py:2277:step] End to end step took 4.582415580749512
x3104c0s1b0n0: [2024-03-29 15:42:43,533] [INFO] [stage3.py:2277:step] End to end step took 4.582545757293701
x3104c0s1b0n0: [2024-03-29 15:42:43,533] [INFO] [timer.py:260:stop] epoch=0/micro_step=7/global_step=7, RunningAvgSamplesPerSec=1.142956968190109, CurrSamplesPerSec=1.1110643050402775, MemAllocated=10.26GB, MaxMemAllocated=13.6GB
x3104c0s1b0n0: [2024-03-29 15:42:43,534] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6593.55 | bwd_microstep: 17256.14 | bwd_inner_microstep: 17168.21 | bwd_allreduce_microstep: 87.86 | step_microstep: 4606.49
x3104c0s1b0n0: [2024-03-29 15:42:43,534] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6593.54 | bwd: 17256.15 | bwd_inner: 17168.21 | bwd_allreduce: 87.87 | step: 4606.49
x3104c0s1b0n0: [2024-03-29 15:42:43,655] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:42:43,655] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 13.6 GB         CA 10.34 GB         Max_CA 15 GB 
x3104c0s1b0n0: [2024-03-29 15:42:43,656] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.25 GB, percent = 76.4%
x3104c0s1b0n0: <TIMER:interval-time,29.054795742034912><TIMER:interval-time,29.05479621887207><TIMER:interval-time,29.05479121208191>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,29.054836750030518><TIMER:interval-time,29.054837465286255><TIMER:interval-time,29.05483889579773><TIMER:interval-time,29.054840326309204>
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,29.054869651794434>
x3104c0s25b0n0:  elapsed_time 29.054839 | consumed samples:          224 | consumed tokens:       458752 | elapsed time per iteration (ms): 29054.8 | learning rate: 9.750E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.101 | TFLOPs: 76.20 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:42:43,786] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:42:43,787] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 15:42:43,787] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.22 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:42:50,465] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:42:50,465] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:42:50,465] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.22 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:42:50,547] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:42:50,548] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:42:50,548] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.22 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:43:08,104] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:43:08,104] [INFO] [utils.py:801:see_memory_usage] MA 13.6 GB         Max_MA 23.87 GB         CA 13.89 GB         Max_CA 33 GB 
x3104c0s1b0n0: [2024-03-29 15:43:08,104] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.22 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:43:08,179] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:43:08,179] [INFO] [utils.py:801:see_memory_usage] MA 13.6 GB         Max_MA 13.6 GB         CA 13.89 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:43:08,180] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.22 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:43:12,274] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.070378065109253
x3104c0s1b0n0: [2024-03-29 15:43:12,277] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.073613166809082
x3104c0s1b0n0: [2024-03-29 15:43:12,283] [INFO] [stage3.py:2251:step] Full outer step loop took 4.0798022747039795
x3104c0s1b0n0: [2024-03-29 15:43:12,292] [INFO] [stage3.py:2251:step] Full outer step loop took 4.089066028594971
x3104c0s1b0n0: [2024-03-29 15:43:12,323] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.120138168334961
x3104c0s1b0n0: [2024-03-29 15:43:12,332] [INFO] [stage3.py:2251:step] Full outer step loop took 4.1290059089660645
x3104c0s1b0n0: [2024-03-29 15:43:12,383] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.179738759994507
x3104c0s1b0n0: [2024-03-29 15:43:12,392] [INFO] [stage3.py:2251:step] Full outer step loop took 4.188624382019043
x3104c0s25b0n0: [2024-03-29 15:43:12,634] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.430651426315308
x3104c0s25b0n0: [2024-03-29 15:43:12,643] [INFO] [stage3.py:2251:step] Full outer step loop took 4.440185785293579
x3104c0s25b0n0: [2024-03-29 15:43:12,752] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.548319339752197
x3104c0s25b0n0: [2024-03-29 15:43:12,762] [INFO] [stage3.py:2251:step] Full outer step loop took 4.558339357376099
x3104c0s25b0n0: [2024-03-29 15:43:12,815] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.611127138137817
x3104c0s25b0n0: [2024-03-29 15:43:12,835] [INFO] [stage3.py:2251:step] Full outer step loop took 4.63171911239624
x3104c0s25b0n0: [2024-03-29 15:43:12,837] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.633740186691284
x3104c0s25b0n0: [2024-03-29 15:43:12,846] [INFO] [stage3.py:2251:step] Full outer step loop took 4.6426355838775635
x3104c0s25b0n0: [2024-03-29 15:43:12,855] [INFO] [stage3.py:2277:step] End to end step took 4.652062892913818
x3104c0s1b0n0: [2024-03-29 15:43:12,855] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4129.16
x3104c0s25b0n0: [2024-03-29 15:43:12,855] [INFO] [stage3.py:2277:step] End to end step took 4.65219783782959
x3104c0s1b0n0: [2024-03-29 15:43:12,855] [INFO] [stage3.py:2277:step] End to end step took 4.652165412902832
x3104c0s1b0n0: [2024-03-29 15:43:12,855] [INFO] [stage3.py:2277:step] End to end step took 4.652137994766235
x3104c0s1b0n0: [2024-03-29 15:43:12,856] [INFO] [logging.py:96:log_dist] [Rank 0] step=8, skipped=0, lr=[6.158400017893797e-05], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 15:43:12,856] [INFO] [stage3.py:2277:step] End to end step took 4.6526031494140625
x3104c0s25b0n0: [2024-03-29 15:43:12,856] [INFO] [stage3.py:2277:step] End to end step took 4.65263295173645
x3104c0s1b0n0: [2024-03-29 15:43:12,856] [INFO] [stage3.py:2277:step] End to end step took 4.652606964111328
x3104c0s1b0n0: [2024-03-29 15:43:12,856] [INFO] [timer.py:260:stop] epoch=0/micro_step=8/global_step=8, RunningAvgSamplesPerSec=1.1357222756691623, CurrSamplesPerSec=1.1008804921010897, MemAllocated=10.26GB, MaxMemAllocated=13.6GB
x3104c0s1b0n0: [2024-03-29 15:43:12,856] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6635.32 | bwd_microstep: 17385.90 | bwd_inner_microstep: 17293.76 | bwd_allreduce_microstep: 92.07 | step_microstep: 4676.45
x3104c0s25b0n0: [2024-03-29 15:43:12,856] [INFO] [stage3.py:2277:step] End to end step took 4.653027057647705
x3104c0s1b0n0: [2024-03-29 15:43:12,856] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6635.31 | bwd: 17385.90 | bwd_inner: 17293.75 | bwd_allreduce: 92.09 | step: 4676.45
x3104c0s1b0n0: [2024-03-29 15:43:12,961] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:43:12,961] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 13.6 GB         CA 10.34 GB         Max_CA 15 GB 
x3104c0s1b0n0: [2024-03-29 15:43:12,962] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.22 GB, percent = 76.4%
x3104c0s1b0n0: <TIMER:interval-time,29.305655479431152><TIMER:interval-time,29.305651664733887><TIMER:interval-time,29.305640697479248>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,29.305656909942627>
x3104c0s25b0n0: <TIMER:interval-time,29.305667638778687>
x3104c0s25b0n0: <TIMER:interval-time,29.305676460266113><TIMER:interval-time,29.305676698684692>
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,29.305784225463867>
x3104c0s25b0n0:  elapsed_time 29.305676 | consumed samples:          256 | consumed tokens:       524288 | elapsed time per iteration (ms): 29305.7 | learning rate: 6.158E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.092 | TFLOPs: 75.55 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:43:13,111] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:43:13,112] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 15:43:13,112] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.23 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:43:19,954] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:43:19,955] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:43:19,955] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.22 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:43:20,036] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:43:20,037] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:43:20,037] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.22 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:43:37,434] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:43:37,434] [INFO] [utils.py:801:see_memory_usage] MA 13.6 GB         Max_MA 23.87 GB         CA 13.89 GB         Max_CA 33 GB 
x3104c0s1b0n0: [2024-03-29 15:43:37,435] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.23 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:43:37,506] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:43:37,507] [INFO] [utils.py:801:see_memory_usage] MA 13.6 GB         Max_MA 13.6 GB         CA 13.89 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:43:37,507] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.23 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:43:41,554] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.022161483764648
x3104c0s1b0n0: [2024-03-29 15:43:41,586] [INFO] [stage3.py:2251:step] Full outer step loop took 4.054664850234985
x3104c0s1b0n0: [2024-03-29 15:43:41,657] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.125104188919067
x3104c0s1b0n0: [2024-03-29 15:43:41,657] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.12523341178894
x3104c0s1b0n0: [2024-03-29 15:43:41,671] [INFO] [stage3.py:2251:step] Full outer step loop took 4.13935399055481
x3104c0s1b0n0: [2024-03-29 15:43:41,679] [INFO] [stage3.py:2251:step] Full outer step loop took 4.1475584506988525
x3104c0s1b0n0: [2024-03-29 15:43:41,689] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.15765905380249
x3104c0s1b0n0: [2024-03-29 15:43:41,698] [INFO] [stage3.py:2251:step] Full outer step loop took 4.166523218154907
x3104c0s25b0n0: [2024-03-29 15:43:41,953] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.421676397323608
x3104c0s25b0n0: [2024-03-29 15:43:41,982] [INFO] [stage3.py:2251:step] Full outer step loop took 4.450139045715332
x3104c0s25b0n0: [2024-03-29 15:43:41,992] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.460724115371704
x3104c0s25b0n0: [2024-03-29 15:43:42,005] [INFO] [stage3.py:2251:step] Full outer step loop took 4.472894906997681
x3104c0s25b0n0: [2024-03-29 15:43:42,108] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.5761847496032715
x3104c0s25b0n0: [2024-03-29 15:43:42,128] [INFO] [stage3.py:2251:step] Full outer step loop took 4.596765518188477
x3104c0s25b0n0: [2024-03-29 15:43:42,130] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.598593473434448
x3104c0s25b0n0: [2024-03-29 15:43:42,139] [INFO] [stage3.py:2251:step] Full outer step loop took 4.60746955871582
x3104c0s25b0n0: [2024-03-29 15:43:42,149] [INFO] [stage3.py:2277:step] End to end step took 4.617205619812012
x3104c0s1b0n0: [2024-03-29 15:43:42,149] [INFO] [stage3.py:2277:step] End to end step took 4.617347002029419
x3104c0s1b0n0: [2024-03-29 15:43:42,149] [INFO] [stage3.py:2277:step] End to end step took 4.617388963699341
x3104c0s1b0n0: [2024-03-29 15:43:42,149] [INFO] [stage3.py:2277:step] End to end step took 4.617451190948486
x3104c0s25b0n0: [2024-03-29 15:43:42,149] [INFO] [stage3.py:2277:step] End to end step took 4.617471933364868
x3104c0s25b0n0: [2024-03-29 15:43:42,149] [INFO] [stage3.py:2277:step] End to end step took 4.617748260498047
x3104c0s1b0n0: [2024-03-29 15:43:42,149] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4054.98
x3104c0s25b0n0: [2024-03-29 15:43:42,149] [INFO] [stage3.py:2277:step] End to end step took 4.617811918258667
x3104c0s1b0n0: [2024-03-29 15:43:42,150] [INFO] [stage3.py:2277:step] End to end step took 4.618038177490234
x3104c0s1b0n0: [2024-03-29 15:43:42,150] [INFO] [logging.py:96:log_dist] [Rank 0] step=9, skipped=0, lr=[3.814149619390238e-05], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 15:43:42,150] [INFO] [timer.py:260:stop] epoch=0/micro_step=9/global_step=9, RunningAvgSamplesPerSec=1.1307851697141282, CurrSamplesPerSec=1.102041029808514, MemAllocated=10.26GB, MaxMemAllocated=13.6GB
x3104c0s1b0n0: [2024-03-29 15:43:42,151] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6805.79 | bwd_microstep: 17231.67 | bwd_inner_microstep: 17147.69 | bwd_allreduce_microstep: 83.92 | step_microstep: 4643.28
x3104c0s1b0n0: [2024-03-29 15:43:42,151] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6805.78 | bwd: 17231.67 | bwd_inner: 17147.68 | bwd_allreduce: 83.93 | step: 4643.28
x3104c0s1b0n0: [2024-03-29 15:43:42,259] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:43:42,260] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 13.6 GB         CA 10.34 GB         Max_CA 15 GB 
x3104c0s1b0n0: [2024-03-29 15:43:42,260] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.22 GB, percent = 76.4%
x3104c0s1b0n0: <TIMER:interval-time,29.29799175262451><TIMER:interval-time,29.297992944717407><TIMER:interval-time,29.297994136810303>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,29.29799795150757>
x3104c0s25b0n0: <TIMER:interval-time,29.298000812530518>
x3104c0s25b0n0: <TIMER:interval-time,29.29802107810974><TIMER:interval-time,29.298018217086792>
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,29.29813861846924>
x3104c0s25b0n0:  elapsed_time 29.298139 | consumed samples:          288 | consumed tokens:       589824 | elapsed time per iteration (ms): 29298.1 | learning rate: 3.814E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.092 | TFLOPs: 75.57 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:43:42,391] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:43:42,392] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 15:43:42,392] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.23 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:43:49,064] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:43:49,065] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:43:49,065] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.23 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:43:49,143] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:43:49,144] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:43:49,144] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.22 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:44:07,141] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:44:07,142] [INFO] [utils.py:801:see_memory_usage] MA 13.6 GB         Max_MA 23.87 GB         CA 13.89 GB         Max_CA 33 GB 
x3104c0s1b0n0: [2024-03-29 15:44:07,142] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.23 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:44:07,212] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:44:07,212] [INFO] [utils.py:801:see_memory_usage] MA 13.6 GB         Max_MA 13.6 GB         CA 13.89 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:44:07,212] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.23 GB, percent = 76.4%
x3104c0s1b0n0: [2024-03-29 15:44:11,343] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.10717248916626
x3104c0s1b0n0: [2024-03-29 15:44:11,343] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.107487678527832
x3104c0s1b0n0: [2024-03-29 15:44:11,352] [INFO] [stage3.py:2251:step] Full outer step loop took 4.116397380828857
x3104c0s1b0n0: [2024-03-29 15:44:11,352] [INFO] [stage3.py:2251:step] Full outer step loop took 4.116432428359985
x3104c0s1b0n0: [2024-03-29 15:44:11,436] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.2004554271698
x3104c0s1b0n0: [2024-03-29 15:44:11,442] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.205617904663086
x3104c0s1b0n0: [2024-03-29 15:44:11,446] [INFO] [stage3.py:2251:step] Full outer step loop took 4.209838628768921
x3104c0s1b0n0: [2024-03-29 15:44:11,450] [INFO] [stage3.py:2251:step] Full outer step loop took 4.21448016166687
x3104c0s25b0n0: [2024-03-29 15:44:11,670] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.434214353561401
x3104c0s25b0n0: [2024-03-29 15:44:11,680] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.444517374038696
x3104c0s25b0n0: [2024-03-29 15:44:11,680] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.4445929527282715
x3104c0s25b0n0: [2024-03-29 15:44:11,681] [INFO] [stage3.py:2251:step] Full outer step loop took 4.445429563522339
x3104c0s25b0n0: [2024-03-29 15:44:11,689] [INFO] [stage3.py:2251:step] Full outer step loop took 4.4534056186676025
x3104c0s25b0n0: [2024-03-29 15:44:11,689] [INFO] [stage3.py:2251:step] Full outer step loop took 4.453509092330933
x3104c0s25b0n0: [2024-03-29 15:44:11,753] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.517265319824219
x3104c0s25b0n0: [2024-03-29 15:44:11,762] [INFO] [stage3.py:2251:step] Full outer step loop took 4.526153802871704
x3104c0s25b0n0: [2024-03-29 15:44:11,771] [INFO] [stage3.py:2277:step] End to end step took 4.535276651382446
x3104c0s25b0n0: [2024-03-29 15:44:11,772] [INFO] [stage3.py:2277:step] End to end step took 4.5357091426849365
x3104c0s1b0n0: [2024-03-29 15:44:11,772] [INFO] [stage3.py:2277:step] End to end step took 4.535864591598511
x3104c0s1b0n0: [2024-03-29 15:44:11,772] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4116.61
x3104c0s25b0n0: [2024-03-29 15:44:11,772] [INFO] [stage3.py:2277:step] End to end step took 4.535919666290283
x3104c0s25b0n0: [2024-03-29 15:44:11,772] [INFO] [stage3.py:2277:step] End to end step took 4.536010265350342
x3104c0s1b0n0: [2024-03-29 15:44:11,772] [INFO] [stage3.py:2277:step] End to end step took 4.536023139953613
x3104c0s1b0n0: [2024-03-29 15:44:11,772] [INFO] [stage3.py:2277:step] End to end step took 4.536001682281494
x3104c0s1b0n0: [2024-03-29 15:44:11,772] [INFO] [stage3.py:2277:step] End to end step took 4.53606915473938
x3104c0s1b0n0: [2024-03-29 15:44:11,772] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[3e-05], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 15:44:11,773] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=1.125413841010057, CurrSamplesPerSec=1.089197360270663, MemAllocated=10.26GB, MaxMemAllocated=13.6GB
x3104c0s1b0n0: [2024-03-29 15:44:11,773] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6637.77 | bwd_microstep: 17831.90 | bwd_inner_microstep: 17741.38 | bwd_allreduce_microstep: 90.45 | step_microstep: 4560.41
x3104c0s1b0n0: [2024-03-29 15:44:11,773] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6637.76 | bwd: 17831.89 | bwd_inner: 17741.38 | bwd_allreduce: 90.46 | step: 4560.42
x3104c0s1b0n0: [2024-03-29 15:44:11,879] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:44:11,880] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 13.6 GB         CA 10.34 GB         Max_CA 15 GB 
x3104c0s1b0n0: [2024-03-29 15:44:11,880] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 384.23 GB, percent = 76.4%
x3104c0s1b0n0: <TIMER:interval-time,29.619688034057617><TIMER:interval-time,29.619689464569092>
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,29.619693994522095><TIMER:interval-time,29.619691848754883>
x3104c0s1b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,29.61972665786743>
x3104c0s25b0n0: <TIMER:interval-time,29.619733333587646>
x3104c0s25b0n0: <TIMER:interval-time,29.619733572006226><TIMER:interval-time,29.61972451210022>
x3104c0s25b0n0: 
x3104c0s25b0n0:  elapsed_time 29.619725 | consumed samples:          320 | consumed tokens:       655360 | elapsed time per iteration (ms): 29619.7 | learning rate: 3.000E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.080 | TFLOPs: 74.75 |
x3104c0s25b0n0: <<<only_train:299.237993478775>>>
x3104c0s25b0n0: <<<only_train:299.2377564907074>>>
x3104c0s25b0n0: <<<only_train:299.23774337768555>>>
x3104c0s25b0n0: <<<only_train:299.23787689208984>>>
x3104c0s1b0n0: <<<only_train:299.23777174949646>>>
x3104c0s1b0n0: <<<only_train:299.2377338409424>>>
x3104c0s1b0n0: <<<only_train:299.23761081695557>>>
x3104c0s1b0n0: <<<only_train:299.23758697509766>>>
x3104c0s1b0n0: [after training ends] datetime: 2024-03-29 15:44:11 
x3104c0s1b0n0: <<<full_time:299.2379949092865>>><<<full_time:299.23785281181335>>>
x3104c0s1b0n0: 
x3104c0s1b0n0: <<<full_time:299.2378249168396>>>
x3104c0s1b0n0: <<<full_time:299.2381041049957>>>
x3104c0s25b0n0: <<<full_time:299.23834109306335>>>
x3104c0s25b0n0: <<<full_time:299.23806977272034>>>
x3104c0s25b0n0: <<<full_time:299.2381258010864>>>
x3104c0s25b0n0: <<<full_time:299.23810148239136>>>
x3104c0s25b0n0: [2024-03-29 15:44:21,010] [INFO] [launch.py:348:main] Process 13522 exits successfully.
x3104c0s1b0n0: [2024-03-29 15:44:21,151] [INFO] [launch.py:348:main] Process 50059 exits successfully.
x3104c0s25b0n0: [2024-03-29 15:44:29,019] [INFO] [launch.py:348:main] Process 13523 exits successfully.
x3104c0s25b0n0: [2024-03-29 15:44:29,019] [INFO] [launch.py:348:main] Process 13521 exits successfully.
x3104c0s25b0n0: [2024-03-29 15:44:29,019] [INFO] [launch.py:348:main] Process 13524 exits successfully.
x3104c0s1b0n0: [2024-03-29 15:44:29,160] [INFO] [launch.py:348:main] Process 50061 exits successfully.
x3104c0s1b0n0: [2024-03-29 15:44:29,160] [INFO] [launch.py:348:main] Process 50060 exits successfully.
x3104c0s1b0n0: [2024-03-29 15:44:29,160] [INFO] [launch.py:348:main] Process 50058 exits successfully.

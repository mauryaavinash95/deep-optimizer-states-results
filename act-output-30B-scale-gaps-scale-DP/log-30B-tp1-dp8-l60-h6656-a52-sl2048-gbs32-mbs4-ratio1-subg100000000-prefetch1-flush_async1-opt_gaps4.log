[2024-03-29 15:44:34,751] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-29 15:44:37,875] [INFO] [runner.py:463:main] Using IP address of 10.140.58.110 for node x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-29 15:44:37,878] [INFO] [runner.py:557:main] deepspeed_env file = ./.deepspeed_env
[2024-03-29 15:44:37,878] [INFO] [multinode_runner.py:80:get_cmd] Running on the following workers: x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov,x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-29 15:44:37,878] [INFO] [runner.py:568:main] cmd = pdsh -S -f 1024 -w x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov,x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov export PYTHONUSERBASE=/home/am6429/.local/polaris/conda/2023-10-04; export PYTHONPATH=/home/am6429/dl-io/Megatron-DeepSpeed; export PATH=/home/am6429/.conda/envs/dspeed_env/bin:/soft/datascience/conda/2023-10-04/mconda3/condabin:/soft/compilers/cudatoolkit/cuda-11.8.0/bin:/soft/buildtools/cmake/cmake-3.23.2/cmake-3.23.2-linux-x86_64/bin:/opt/cray/pe/gcc/11.2.0/bin:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/include:/opt/cray/pe/pals/1.2.11/bin:/opt/cray/pe/craype/2.7.20/bin:/opt/cray/libfabric/1.15.2.0/bin:/home/am6429/.conda/envs/dspeed_env/bin:/opt/cray/pe/perftools/23.03.0/bin:/opt/cray/pe/papi/7.0.0.1/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/am6429/.local/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/am6429/veloc-build/include:/home/am6429/veloc-build/bin:/soft/datascience/conda/2023-01-10/mconda3/include:/opt/cray/pe/bin:/soft/datascience/conda/2023-01-10/mconda3/include; export LD_LIBRARY_PATH=/usr/lib64/:/soft/compilers/cudatoolkit/cuda-11.8.0/lib64:/opt/cray/pe/gcc/11.2.0/snos/lib64:/soft/compilers/cudatoolkit/cuda-11.8.0/extras/CUPTI/lib64:/soft/libraries/trt/TensorRT-8.5.3.1.Linux.x86_64-gnu.cuda-11.8.cudnn8.6/lib:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/lib:/soft/libraries/cudnn/cudnn-11-linux-x64-v8.6.0.163/lib:/opt/cray/libfabric/1.15.2.0/lib64:/usr/lib64:/opt/cray/pe/papi/7.0.0.1/lib64:/home/am6429/veloc-build/lib:/home/am6429/veloc-build/lib64:/home/am6429/nvcomp/lib:/soft/datascience/conda/2023-01-10/mconda3/lib:/soft/datascience/conda/2023-01-10/mconda3/lib/; export http_proxy=http://proxy.alcf.anl.gov:3128; export https_proxy=http://proxy.alcf.anl.gov:3128; export CC=gcc; export CXX=g++; export IBV_FORK_SAFE=1; export CFLAGS=-I/soft/datascience/conda/2023-01-10/mconda3/include/; export LDFLAGS=-L/soft/datascience/conda/2023-01-10/mconda3/lib/; export CUDA_DEVICE_MAX_CONNECTIONS=1; export TORCHSNAPSHOT_PER_RANK_MEMORY_BUDGET_BYTES=34359738368; export _DEFAULT_MAX_PER_RANK_IO_CONCURRENCY=1; export _MAX_PER_RANK_IO_CONCURRENCY=1; export NSYS_REPORT_DIR=/home/am6429/dl-io/dl-io-outputs/act-output-30B//rep-30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch1-flush_async1-opt_gaps4-%n;  cd /home/am6429/dl-io/Megatron-DeepSpeed; /home/am6429/.conda/envs/dspeed_env/bin/python -u -m deepspeed.launcher.launch --world_info=eyJ4MzEwNGMwczFiMG4wLmhzbi5jbS5wb2xhcmlzLmFsY2YuYW5sLmdvdiI6IFswLCAxLCAyLCAzXSwgIngzMTA0YzBzMjViMG4wLmhzbi5jbS5wb2xhcmlzLmFsY2YuYW5sLmdvdiI6IFswLCAxLCAyLCAzXX0= --node_rank=%n --master_addr=10.140.58.110 --master_port=29700 /home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py --tensor-model-parallel-size 1 --num-layers 60 --hidden-size 6656 --num-attention-heads 52 --micro-batch-size 4 --global-batch-size 32 --seq-length 2048 --max-position-embeddings 2048 --train-iters 10 --save /local/scratch/llama2/zero3-tp1}_dp8 --data-path /home/am6429/dl-io/datasets/meg-gpt2_text_document --vocab-file /home/am6429/dl-io/datasets/gpt2-vocab.json --merge-file /home/am6429/dl-io/datasets/gpt2-merges.txt --data-impl mmap --tokenizer-type GPTSentencePieceTokenizer --tokenizer-model /home/am6429/dl-io/datasets/tokenizer.model --split 949,50,1 --distributed-backend nccl --lr 3e-4 --lr-decay-style cosine --min-lr 3e-5 --weight-decay 0.1 --clip-grad 1 --lr-warmup-iters 1 --optimizer adam --adam-beta1 0.9 --adam-beta2 0.95 --log-interval 1 --save-interval 1000 --eval-interval 1000 --eval-iters 0 --bf16 --no-query-key-layer-scaling --attention-dropout 0 --hidden-dropout 0 --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization rmsnorm --disable-bias-linear --num-key-value-heads 4 --deepspeed --exit-interval 20 --deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json --zero-stage=3 --no-pipeline-parallel --cpu-optimizer --checkpoint-activations --deepspeed-activation-checkpointing
x3104c0s25b0n0: [2024-03-29 15:44:39,715] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 15:44:39,906] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s25b0n0: [2024-03-29 15:44:41,540] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3104c0s25b0n0: [2024-03-29 15:44:41,540] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=1
x3104c0s25b0n0: [2024-03-29 15:44:41,540] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3104c0s25b0n0: [2024-03-29 15:44:41,540] [INFO] [launch.py:163:main] dist_world_size=8
x3104c0s25b0n0: [2024-03-29 15:44:41,540] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3104c0s25b0n0: [2024-03-29 15:44:41,541] [INFO] [launch.py:253:main] process 16375 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s25b0n0: [2024-03-29 15:44:41,541] [INFO] [launch.py:253:main] process 16376 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s25b0n0: [2024-03-29 15:44:41,542] [INFO] [launch.py:253:main] process 16377 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s25b0n0: [2024-03-29 15:44:41,542] [INFO] [launch.py:253:main] process 16378 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s1b0n0: [2024-03-29 15:44:41,769] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3104c0s1b0n0: [2024-03-29 15:44:41,769] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=0
x3104c0s1b0n0: [2024-03-29 15:44:41,769] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3104c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3104c0s25b0n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3104c0s1b0n0: [2024-03-29 15:44:41,769] [INFO] [launch.py:163:main] dist_world_size=8
x3104c0s1b0n0: [2024-03-29 15:44:41,769] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3104c0s1b0n0: [2024-03-29 15:44:41,770] [INFO] [launch.py:253:main] process 53497 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s1b0n0: [2024-03-29 15:44:41,770] [INFO] [launch.py:253:main] process 53498 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s1b0n0: [2024-03-29 15:44:41,771] [INFO] [launch.py:253:main] process 53499 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s1b0n0: [2024-03-29 15:44:41,771] [INFO] [launch.py:253:main] process 53500 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3104c0s25b0n0: [2024-03-29 15:44:43,104] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s25b0n0: [2024-03-29 15:44:43,106] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s25b0n0: [2024-03-29 15:44:43,108] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s25b0n0: [2024-03-29 15:44:43,111] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 15:44:43,508] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 15:44:43,593] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 15:44:43,597] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s1b0n0: [2024-03-29 15:44:43,601] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s25b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s25b0n0:       meet the required dependencies to JIT install the op.
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: JIT compiled ops requires ninja
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s25b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s25b0n0:       meet the required dependencies to JIT install the op.
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: JIT compiled ops requires ninja
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s25b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s25b0n0:       meet the required dependencies to JIT install the op.
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: JIT compiled ops requires ninja
x3104c0s25b0n0: ninja .................. [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: op name ................ installed .. compatible
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s25b0n0: ninja .................. [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: op name ................ installed .. compatible
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s25b0n0: ninja .................. [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: op name ................ installed .. compatible
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s25b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s25b0n0:       meet the required dependencies to JIT install the op.
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: JIT compiled ops requires ninja
x3104c0s25b0n0: ninja .................. [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: op name ................ installed .. compatible
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s25b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s25b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s25b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s25b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s25b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s25b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s25b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s25b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s25b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s25b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s25b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed general environment info:
x3104c0s25b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s25b0n0: torch version .................... 2.0.1+cu118
x3104c0s25b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s25b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s25b0n0: torch cuda version ............... 11.8
x3104c0s25b0n0: torch hip version ................ None
x3104c0s25b0n0: nvcc version ..................... 11.8
x3104c0s25b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s25b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s25b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s25b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s25b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s25b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed general environment info:
x3104c0s25b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s25b0n0: torch version .................... 2.0.1+cu118
x3104c0s25b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s25b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s25b0n0: torch cuda version ............... 11.8
x3104c0s25b0n0: torch hip version ................ None
x3104c0s25b0n0: nvcc version ..................... 11.8
x3104c0s25b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s25b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s25b0n0: DeepSpeed general environment info:
x3104c0s25b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s25b0n0: torch version .................... 2.0.1+cu118
x3104c0s25b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s25b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s25b0n0: torch cuda version ............... 11.8
x3104c0s25b0n0: torch hip version ................ None
x3104c0s25b0n0: nvcc version ..................... 11.8
x3104c0s25b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s25b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s25b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s25b0n0: --------------------------------------------------
x3104c0s25b0n0: DeepSpeed general environment info:
x3104c0s25b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s25b0n0: torch version .................... 2.0.1+cu118
x3104c0s25b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s25b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s25b0n0: torch cuda version ............... 11.8
x3104c0s25b0n0: torch hip version ................ None
x3104c0s25b0n0: nvcc version ..................... 11.8
x3104c0s25b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s25b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s25b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s25b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s25b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s25b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s25b0n0: [2024-03-29 15:44:45,375] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s25b0n0: [2024-03-29 15:44:45,375] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s25b0n0: [2024-03-29 15:44:45,377] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s25b0n0: [2024-03-29 15:44:45,379] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s1b0n0:       meet the required dependencies to JIT install the op.
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: JIT compiled ops requires ninja
x3104c0s1b0n0: ninja .................. [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: op name ................ installed .. compatible
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed general environment info:
x3104c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s1b0n0: torch version .................... 2.0.1+cu118
x3104c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s1b0n0: torch cuda version ............... 11.8
x3104c0s1b0n0: torch hip version ................ None
x3104c0s1b0n0: nvcc version ..................... 11.8
x3104c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s1b0n0:       meet the required dependencies to JIT install the op.
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: JIT compiled ops requires ninja
x3104c0s1b0n0: ninja .................. [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: op name ................ installed .. compatible
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s1b0n0:       meet the required dependencies to JIT install the op.
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: --------------------------------------------------JIT compiled ops requires ninja
x3104c0s1b0n0: 
x3104c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3104c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3104c0s1b0n0:       meet the required dependencies to JIT install the op.
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: JIT compiled ops requires ninja
x3104c0s1b0n0: ninja .................. [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: op name ................ installed .. compatible
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s1b0n0: ninja .................. [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: op name ................ installed .. compatible
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3104c0s1b0n0: [2024-03-29 15:44:46,390] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3104c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: DeepSpeed general environment info:
x3104c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s1b0n0: torch version .................... 2.0.1+cu118
x3104c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s1b0n0: torch cuda version ............... 11.8
x3104c0s1b0n0: torch hip version ................ None
x3104c0s1b0n0: nvcc version ..................... 11.8
x3104c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3104c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3104c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3104c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3104c0s1b0n0: --------------------------------------------------
x3104c0s1b0n0: DeepSpeed general environment info:
x3104c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s1b0n0: torch version .................... 2.0.1+cu118
x3104c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s1b0n0: torch cuda version ............... 11.8
x3104c0s1b0n0: torch hip version ................ None
x3104c0s1b0n0: nvcc version ..................... 11.8
x3104c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s1b0n0: DeepSpeed general environment info:
x3104c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3104c0s1b0n0: torch version .................... 2.0.1+cu118
x3104c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3104c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3104c0s1b0n0: torch cuda version ............... 11.8
x3104c0s1b0n0: torch hip version ................ None
x3104c0s1b0n0: nvcc version ..................... 11.8
x3104c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3104c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3104c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3104c0s1b0n0: INFO: overriding default arguments for tokenizer_type:GPTSentencePieceTokenizer                    with tokenizer_type:GPT2BPETokenizer
x3104c0s1b0n0: using world size: 8, data-parallel-size: 8, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
x3104c0s1b0n0: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
x3104c0s1b0n0: using torch.bfloat16 for parameters ...
x3104c0s1b0n0: ------------------------ arguments ------------------------
x3104c0s1b0n0:   accumulate_allreduce_grads_in_fp32 .............. True
x3104c0s1b0n0:   adam_beta1 ...................................... 0.9
x3104c0s1b0n0:   adam_beta2 ...................................... 0.95
x3104c0s1b0n0:   adam_eps ........................................ 1e-08
x3104c0s1b0n0:   add_bias_linear ................................. False
x3104c0s1b0n0:   add_position_embedding .......................... False
x3104c0s1b0n0:   adlr_autoresume ................................. False
x3104c0s1b0n0:   adlr_autoresume_interval ........................ 1000
x3104c0s1b0n0:   aml_data_download_path .......................... None
x3104c0s1b0n0:   apply_layernorm_1p .............................. False
x3104c0s1b0n0:   apply_query_key_layer_scaling ................... False
x3104c0s1b0n0:   apply_residual_connection_post_layernorm ........ False
x3104c0s1b0n0:   async_tensor_model_parallel_allreduce ........... False
x3104c0s1b0n0:   attention_dropout ............................... 0.0
x3104c0s1b0n0:   attention_softmax_in_fp32 ....................... False
x3104c0s1b0n0:   barrier_with_L1_time ............................ True
x3104c0s1b0n0:   bert_binary_head ................................ True
x3104c0s1b0n0:   bert_embedder_type .............................. megatron
x3104c0s1b0n0:   bert_load ....................................... None
x3104c0s1b0n0:   bf16 ............................................ True
x3104c0s1b0n0:   bias_dropout_fusion ............................. True
x3104c0s1b0n0:   bias_gelu_fusion ................................ False
x3104c0s1b0n0:   biencoder_projection_dim ........................ 0
x3104c0s1b0n0:   biencoder_shared_query_context_model ............ False
x3104c0s1b0n0:   block_data_path ................................. None
x3104c0s1b0n0:   checkpoint_activations .......................... True
x3104c0s1b0n0:   checkpoint_in_cpu ............................... False
x3104c0s1b0n0:   checkpoint_num_layers ........................... 1
x3104c0s1b0n0:   classes_fraction ................................ 1.0
x3104c0s1b0n0:   clip_grad ....................................... 1.0
x3104c0s1b0n0:   compression_training ............................ False
x3104c0s1b0n0:   consumed_train_samples .......................... 0
x3104c0s1b0n0:   consumed_train_tokens ........................... 0
x3104c0s1b0n0:   consumed_valid_samples .......................... 0
x3104c0s1b0n0:   contigious_checkpointing ........................ False
x3104c0s1b0n0:   cpu_optimizer ................................... True
x3104c0s1b0n0:   cpu_torch_adam .................................. False
x3104c0s1b0n0:   create_moe_param_group .......................... False
x3104c0s1b0n0:   curriculum_learning_legacy ...................... False
x3104c0s1b0n0:   data_cache_path ................................. None
x3104c0s1b0n0:   data_efficiency_curriculum_learning ............. False
x3104c0s1b0n0:   data_impl ....................................... mmap
x3104c0s1b0n0:   data_parallel_random_init ....................... False
x3104c0s1b0n0:   data_parallel_size .............................. 8
x3104c0s1b0n0:   data_path ....................................... ['/home/am6429/dl-io/datasets/meg-gpt2_text_document']
x3104c0s1b0n0:   data_per_class_fraction ......................... 1.0
x3104c0s1b0n0:   data_sharding ................................... True
x3104c0s1b0n0:   dataloader_type ................................. single
x3104c0s1b0n0:   DDP_impl ........................................ local
x3104c0s1b0n0:   decoder_num_layers .............................. None
x3104c0s1b0n0:   decoder_seq_length .............................. None
x3104c0s1b0n0:   deepscale ....................................... False
x3104c0s1b0n0:   deepscale_config ................................ None
x3104c0s1b0n0:   deepspeed ....................................... True
x3104c0s1b0n0:   deepspeed_activation_checkpointing .............. True
x3104c0s1b0n0:   deepspeed_config ................................ /home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json
x3104c0s1b0n0:   dino_bottleneck_size ............................ 256
x3104c0s1b0n0:   dino_freeze_last_layer .......................... 1
x3104c0s1b0n0:   dino_head_hidden_size ........................... 2048
x3104c0s1b0n0:   dino_local_crops_number ......................... 10
x3104c0s1b0n0:   dino_local_img_size ............................. 96
x3104c0s1b0n0:   dino_norm_last_layer ............................ False
x3104c0s1b0n0:   dino_teacher_temp ............................... 0.07
x3104c0s1b0n0:   dino_warmup_teacher_temp ........................ 0.04
x3104c0s1b0n0:   dino_warmup_teacher_temp_epochs ................. 30
x3104c0s1b0n0:   distribute_checkpointed_activations ............. False
x3104c0s1b0n0:   distribute_saved_activations .................... False
x3104c0s1b0n0:   distributed_backend ............................. nccl
x3104c0s1b0n0:   distributed_timeout_minutes ..................... 10
x3104c0s1b0n0:   ds_inference .................................... False
x3104c0s1b0n0:   ds_pipeline_enabled ............................. False
x3104c0s1b0n0:   ds_sequence_parallel_size ....................... 1
x3104c0s1b0n0:   embedding_path .................................. None
x3104c0s1b0n0:   embedding_weights_in_fp32 ....................... False
x3104c0s1b0n0:   empty_unused_memory_level ....................... 0
x3104c0s1b0n0:   enable_expert_tensor_parallelism ................ False
x3104c0s1b0n0:   encoder_num_layers .............................. 60
x3104c0s1b0n0:   encoder_seq_length .............................. 2048
x3104c0s1b0n0:   end_weight_decay ................................ 0.1
x3104c0s1b0n0:   eod_mask_loss ................................... False
x3104c0s1b0n0:   eval_interval ................................... 1000
x3104c0s1b0n0:   eval_iters ...................................... 0
x3104c0s1b0n0:   evidence_data_path .............................. None
x3104c0s1b0n0:   exit_duration_in_mins ........................... None
x3104c0s1b0n0:   exit_interval ................................... 20
x3104c0s1b0n0:   exit_on_missing_checkpoint ...................... False
x3104c0s1b0n0:   exit_signal_handler ............................. False
x3104c0s1b0n0:   expert_interval ................................. 2
x3104c0s1b0n0:   ffn_hidden_size ................................. 17728
x3104c0s1b0n0:   finetune ........................................ False
x3104c0s1b0n0:   force_ds_sequence_parallel ...................... False
x3104c0s1b0n0:   fp16 ............................................ False
x3104c0s1b0n0:   fp16_lm_cross_entropy ........................... False
x3104c0s1b0n0:   fp32_residual_connection ........................ False
x3104c0s1b0n0:   fp8_amax_compute_algo ........................... most_recent
x3104c0s1b0n0:   fp8_amax_history_len ............................ 1
x3104c0s1b0n0:   fp8_e4m3 ........................................ False
x3104c0s1b0n0:   fp8_hybrid ...................................... False
x3104c0s1b0n0:   fp8_interval .................................... 1
x3104c0s1b0n0:   fp8_margin ...................................... 0
x3104c0s1b0n0:   fp8_wgrad ....................................... True
x3104c0s1b0n0:   global_batch_size ............................... 32
x3104c0s1b0n0:   gradient_accumulation_fusion .................... True
x3104c0s1b0n0:   head_lr_mult .................................... 1.0
x3104c0s1b0n0:   hidden_dropout .................................. 0.0
x3104c0s1b0n0:   hidden_size ..................................... 6656
x3104c0s1b0n0:   hidden_size_teacher ............................. None
x3104c0s1b0n0:   hysteresis ...................................... 2
x3104c0s1b0n0:   ict_head_size ................................... None
x3104c0s1b0n0:   ict_load ........................................ None
x3104c0s1b0n0:   img_h ........................................... 224
x3104c0s1b0n0:   img_w ........................................... 224
x3104c0s1b0n0:   indexer_batch_size .............................. 128
x3104c0s1b0n0:   indexer_log_interval ............................ 1000
x3104c0s1b0n0:   inference ....................................... False
x3104c0s1b0n0:   inference_batch_times_seqlen_threshold .......... 512
x3104c0s1b0n0:   init_method_std ................................. 0.02
x3104c0s1b0n0:   init_method_xavier_uniform ...................... False
x3104c0s1b0n0:   initial_loss_scale .............................. 4294967296
x3104c0s1b0n0:   iter_per_epoch .................................. 1250
x3104c0s1b0n0:   kd .............................................. False
x3104c0s1b0n0:   kd_alpha_ce ..................................... 1
x3104c0s1b0n0:   kd_beta_ce ...................................... 1
x3104c0s1b0n0:   kd_temp ......................................... 1.0
x3104c0s1b0n0:   kv_channels ..................................... 128
x3104c0s1b0n0:   layernorm_epsilon ............................... 1e-05
x3104c0s1b0n0:   lazy_mpu_init ................................... None
x3104c0s1b0n0:   load ............................................ None
x3104c0s1b0n0:   load_teacher .................................... None
x3104c0s1b0n0:   local_rank ...................................... 0
x3104c0s1b0n0:   log_batch_size_to_tensorboard ................... False
x3104c0s1b0n0:   log_interval .................................... 1
x3104c0s1b0n0:   log_learning_rate_to_tensorboard ................ True
x3104c0s1b0n0:   log_loss_scale_to_tensorboard ................... True
x3104c0s1b0n0:   log_memory_to_tensorboard ....................... False
x3104c0s1b0n0:   log_num_zeros_in_grad ........................... False
x3104c0s1b0n0:   log_optimizer_states_to_tensorboard ............. False
x3104c0s1b0n0:   log_params_norm ................................. False
x3104c0s1b0n0:   log_timers_to_tensorboard ....................... False
x3104c0s1b0n0:   log_validation_ppl_to_tensorboard ............... False
x3104c0s1b0n0:   log_world_size_to_tensorboard ................... False
x3104c0s1b0n0:   loss_scale ...................................... None
x3104c0s1b0n0:   loss_scale_window ............................... 1000
x3104c0s1b0n0:   lr .............................................. 0.0003
x3104c0s1b0n0:   lr_decay_iters .................................. None
x3104c0s1b0n0:   lr_decay_samples ................................ None
x3104c0s1b0n0:   lr_decay_style .................................. cosine
x3104c0s1b0n0:   lr_decay_tokens ................................. None
x3104c0s1b0n0:   lr_warmup_fraction .............................. None
x3104c0s1b0n0:   lr_warmup_iters ................................. 1
x3104c0s1b0n0:   lr_warmup_samples ............................... 0
x3104c0s1b0n0:   lr_warmup_tokens ................................ None
x3104c0s1b0n0:   make_vocab_size_divisible_by .................... 128
x3104c0s1b0n0:   mask_factor ..................................... 1.0
x3104c0s1b0n0:   mask_prob ....................................... 0.15
x3104c0s1b0n0:   mask_type ....................................... random
x3104c0s1b0n0:   masked_softmax_fusion ........................... True
x3104c0s1b0n0:   max_position_embeddings ......................... 2048
x3104c0s1b0n0:   max_tokens_to_oom ............................... 12000
x3104c0s1b0n0:   memory_centric_tiled_linear ..................... False
x3104c0s1b0n0:   merge_file ...................................... /home/am6429/dl-io/datasets/gpt2-merges.txt
x3104c0s1b0n0:   micro_batch_size ................................ 4
x3104c0s1b0n0:   min_loss_scale .................................. 1.0
x3104c0s1b0n0:   min_lr .......................................... 3e-05
x3104c0s1b0n0:   mlp_type ........................................ standard
x3104c0s1b0n0:   mmap_warmup ..................................... False
x3104c0s1b0n0:   moe_eval_capacity_factor ........................ 1.0
x3104c0s1b0n0:   moe_expert_parallel_size ........................ 1
x3104c0s1b0n0:   moe_loss_coeff .................................. 0.1
x3104c0s1b0n0:   moe_min_capacity ................................ 4
x3104c0s1b0n0:   moe_token_dropping .............................. True
x3104c0s1b0n0:   moe_train_capacity_factor ....................... 1.0
x3104c0s1b0n0:   mos ............................................. False
x3104c0s1b0n0:   no_load_lr_state ................................ False
x3104c0s1b0n0:   no_load_optim ................................... None
x3104c0s1b0n0:   no_load_rng ..................................... None
x3104c0s1b0n0:   no_persist_layer_norm ........................... False
x3104c0s1b0n0:   no_pipeline_parallel ............................ True
x3104c0s1b0n0:   no_save_optim ................................... None
x3104c0s1b0n0:   no_save_rng ..................................... None
x3104c0s1b0n0:   normalization ................................... rmsnorm
x3104c0s1b0n0:   num_attention_heads ............................. 52
x3104c0s1b0n0:   num_attention_heads_teacher ..................... None
x3104c0s1b0n0:   num_channels .................................... 3
x3104c0s1b0n0:   num_classes ..................................... 1000
x3104c0s1b0n0:   num_experts ..................................... [1]
x3104c0s1b0n0:   num_experts_switch .............................. None
x3104c0s1b0n0:   num_experts_teacher ............................. [1]
x3104c0s1b0n0:   num_key_value_heads ............................. 4
x3104c0s1b0n0:   num_layers ...................................... 60
x3104c0s1b0n0:   num_layers_per_virtual_pipeline_stage ........... None
x3104c0s1b0n0:   num_layers_teacher .............................. None
x3104c0s1b0n0:   num_workers ..................................... 2
x3104c0s1b0n0:   onnx_safe ....................................... None
x3104c0s1b0n0:   openai_gelu ..................................... False
x3104c0s1b0n0:   optimizer ....................................... adam
x3104c0s1b0n0:   output_bert_embeddings .......................... False
x3104c0s1b0n0:   overlap_p2p_comm ................................ False
x3104c0s1b0n0:   override_opt_param_scheduler .................... False
x3104c0s1b0n0:   params_dtype .................................... torch.bfloat16
x3104c0s1b0n0:   partition_activations ........................... False
x3104c0s1b0n0:   patch_dim ....................................... 16
x3104c0s1b0n0:   perform_initialization .......................... True
x3104c0s1b0n0:   pipeline_model_parallel_size .................... 1
x3104c0s1b0n0:   pipeline_model_parallel_split_rank .............. None
x3104c0s1b0n0:   profile_backward ................................ False
x3104c0s1b0n0:   query_in_block_prob ............................. 0.1
x3104c0s1b0n0:   rampup_batch_size ............................... None
x3104c0s1b0n0:   random_ltd ...................................... False
x3104c0s1b0n0:   rank ............................................ 0
x3104c0s1b0n0:   recompute_granularity ........................... None
x3104c0s1b0n0:   recompute_method ................................ None
x3104c0s1b0n0:   recompute_num_layers ............................ 1
x3104c0s1b0n0:   remote_device ................................... none
x3104c0s1b0n0:   reset_attention_mask ............................ False
x3104c0s1b0n0:   reset_iteration ................................. False
x3104c0s1b0n0:   reset_position_ids .............................. False
x3104c0s1b0n0:   retriever_report_topk_accuracies ................ []
x3104c0s1b0n0:   retriever_score_scaling ......................... False
x3104c0s1b0n0:   retriever_seq_length ............................ 256
x3104c0s1b0n0:   retro_add_retriever ............................. False
x3104c0s1b0n0:   retro_cyclic_train_iters ........................ None
x3104c0s1b0n0:   retro_encoder_attention_dropout ................. 0.1
x3104c0s1b0n0:   retro_encoder_hidden_dropout .................... 0.1
x3104c0s1b0n0:   retro_encoder_layers ............................ 2
x3104c0s1b0n0:   retro_num_neighbors ............................. 2
x3104c0s1b0n0:   retro_num_retrieved_chunks ...................... 2
x3104c0s1b0n0:   retro_return_doc_ids ............................ False
x3104c0s1b0n0:   retro_workdir ................................... None
x3104c0s1b0n0:   return_data_index ............................... False
x3104c0s1b0n0:   rotary_percent .................................. 1.0
x3104c0s1b0n0:   sample_rate ..................................... 1.0
x3104c0s1b0n0:   save ............................................ /local/scratch/llama2/zero3-tp1}_dp8
x3104c0s1b0n0:   save_interval ................................... 1000
x3104c0s1b0n0:   scatter_gather_tensors_in_pipeline .............. True
x3104c0s1b0n0:   scattered_embeddings ............................ False
x3104c0s1b0n0:   seed ............................................ 1234
x3104c0s1b0n0:   seq_length ...................................... 2048
x3104c0s1b0n0:   sequence_parallel ............................... False
x3104c0s1b0n0:   sgd_momentum .................................... 0.9
x3104c0s1b0n0:   short_seq_prob .................................. 0.1
x3104c0s1b0n0:   skip_train ...................................... False
x3104c0s1b0n0:   split ........................................... 949,50,1
x3104c0s1b0n0:   split_transformers .............................. False
x3104c0s1b0n0:   squared_relu .................................... False
x3104c0s1b0n0:   standalone_embedding_stage ...................... False
x3104c0s1b0n0:   start_weight_decay .............................. 0.1
x3104c0s1b0n0:   swiglu .......................................... True
x3104c0s1b0n0:   swin_backbone_type .............................. tiny
x3104c0s1b0n0:   synchronize_each_layer .......................... False
x3104c0s1b0n0:   tensor_model_parallel_size ...................... 1
x3104c0s1b0n0:   tensorboard_dir ................................. None
x3104c0s1b0n0:   tensorboard_log_interval ........................ 1
x3104c0s1b0n0:   tensorboard_queue_size .......................... 1000
x3104c0s1b0n0:   test_data_path .................................. None
x3104c0s1b0n0:   tile_factor ..................................... 1
x3104c0s1b0n0:   timing_log_level ................................ 0
x3104c0s1b0n0:   timing_log_option ............................... minmax
x3104c0s1b0n0:   titles_data_path ................................ None
x3104c0s1b0n0:   tokenizer_model ................................. /home/am6429/dl-io/datasets/tokenizer.model
x3104c0s1b0n0:   tokenizer_type .................................. GPT2BPETokenizer
x3104c0s1b0n0:   topk ............................................ 1
x3104c0s1b0n0:   train_data_exact_num_epochs ..................... None
x3104c0s1b0n0:   train_data_path ................................. None
x3104c0s1b0n0:   train_desc_path ................................. None
x3104c0s1b0n0:   train_doc_idx_path .............................. None
x3104c0s1b0n0:   train_idx_path .................................. None
x3104c0s1b0n0:   train_iters ..................................... 10
x3104c0s1b0n0:   train_sample_idx_path ........................... None
x3104c0s1b0n0:   train_samples ................................... None
x3104c0s1b0n0:   train_shuffle_idx_path .......................... None
x3104c0s1b0n0:   train_tokens .................................... None
x3104c0s1b0n0:   transformer_impl ................................ local
x3104c0s1b0n0:   transformer_pipeline_model_parallel_size ........ 1
x3104c0s1b0n0:   untie_embeddings_and_output_weights ............. True
x3104c0s1b0n0:   use_checkpoint_args ............................. False
x3104c0s1b0n0:   use_checkpoint_opt_param_scheduler .............. False
x3104c0s1b0n0:   use_contiguous_buffers_in_local_ddp ............. True
x3104c0s1b0n0:   use_cpu_initialization .......................... None
x3104c0s1b0n0:   use_dataset_only ................................ False
x3104c0s1b0n0:   use_distributed_optimizer ....................... False
x3104c0s1b0n0:   use_flash_attn .................................. False
x3104c0s1b0n0:   use_flash_attn_triton ........................... False
x3104c0s1b0n0:   use_flash_attn_v1 ............................... False
x3104c0s1b0n0:   use_flash_attn_v2 ............................... False
x3104c0s1b0n0:   use_one_sent_docs ............................... False
x3104c0s1b0n0:   use_pin_memory .................................. False
x3104c0s1b0n0:   use_ring_exchange_p2p ........................... False
x3104c0s1b0n0:   use_rotary_position_embeddings .................. True
x3104c0s1b0n0:   use_tutel ....................................... False
x3104c0s1b0n0:   valid_data_path ................................. None
x3104c0s1b0n0:   variable_seq_lengths ............................ False
x3104c0s1b0n0:   virtual_pipeline_model_parallel_size ............ None
x3104c0s1b0n0:   vision_backbone_type ............................ vit
x3104c0s1b0n0:   vision_pretraining .............................. False
x3104c0s1b0n0:   vision_pretraining_type ......................... classify
x3104c0s1b0n0:   vocab_extra_ids ................................. 0
x3104c0s1b0n0:   vocab_file ...................................... /home/am6429/dl-io/datasets/gpt2-vocab.json
x3104c0s1b0n0:   vocab_size ...................................... None
x3104c0s1b0n0:   weight_decay .................................... 0.1
x3104c0s1b0n0:   weight_decay_incr_style ......................... constant
x3104c0s1b0n0:   world_size ...................................... 8
x3104c0s1b0n0:   zero_allgather_bucket_size ...................... 0.0
x3104c0s1b0n0:   zero_contigious_gradients ....................... False
x3104c0s1b0n0:   zero_reduce_bucket_size ......................... 0.0
x3104c0s1b0n0:   zero_reduce_scatter ............................. False
x3104c0s1b0n0:   zero_stage ...................................... 3
x3104c0s1b0n0: -------------------- end of arguments ---------------------
x3104c0s1b0n0: setting number of micro-batches to constant 1
x3104c0s1b0n0: > building GPT2BPETokenizer tokenizer ...
x3104c0s1b0n0: [2024-03-29 15:44:46,582] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0: [2024-03-29 15:44:46,590] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
x3104c0s1b0n0: > initializing torch distributed ...
x3104c0s1b0n0: [2024-03-29 15:44:46,591] [INFO] [comm.py:637:init_distributed] cdb=None
x3104c0s1b0n0: [2024-03-29 15:44:46,592] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
x3104c0s1b0n0: > initialized tensor model parallel with size 1
x3104c0s1b0n0: > initialized pipeline model parallel with size 1
x3104c0s1b0n0: > setting random seeds to 1234 ...
x3104c0s1b0n0: [2024-03-29 15:44:48,052] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
x3104c0s1b0n0: > compiling dataset index builder ...
x3104c0s1b0n0: make: Entering directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3104c0s1b0n0: make: Nothing to be done for 'default'.
x3104c0s1b0n0: make: Leaving directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3104c0s1b0n0: >>> done with dataset index builder. Compilation time: 0.083 seconds
x3104c0s1b0n0: > compiling and loading fused kernels ...
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: >>> done with compiling and loading fused kernels. Compilation time: 3.247 seconds
x3104c0s25b0n0: <<<<<<<<<<< 6
x3104c0s25b0n0: <<<<<<<<<<< 7
x3104c0s25b0n0: <<<<<<<<<<< 5
x3104c0s1b0n0: <<<<<<<<<<< 2
x3104c0s1b0n0: initialize_megatron took 5.628961801528931
x3104c0s1b0n0: <<<<<<<<<<< 0
x3104c0s1b0n0: <<<<<<<<<<< 3
x3104c0s1b0n0: <<<<<<<<<<< 1
x3104c0s25b0n0: <<<<<<<<<<< 4
x3104c0s1b0n0: time to initialize megatron (seconds): 7.097
x3104c0s1b0n0: [after megatron is initialized] datetime: 2024-03-29 15:44:52 
x3104c0s1b0n0: get_accelerator and all_reduce  took 0.01680922508239746
x3104c0s1b0n0: building GPT model ...
x3104c0s1b0n0: [2024-03-29 15:44:52,264] [INFO] [utils.py:800:see_memory_usage] Before Building Model
x3104c0s1b0n0: [2024-03-29 15:44:52,265] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 4.62 GB         CA 0.0 GB         Max_CA 5 GB 
x3104c0s1b0n0: [2024-03-29 15:44:52,265] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 17.85 GB, percent = 3.5%
x3104c0s1b0n0: [2024-03-29 15:44:57,480] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 423, num_elems = 27.64B
x3104c0s1b0n0: [2024-03-29 15:44:57,548] [INFO] [utils.py:800:see_memory_usage] After Building Model
x3104c0s1b0n0: [2024-03-29 15:44:57,548] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 7.07 GB         CA 21.5 GB         Max_CA 38 GB 
x3104c0s1b0n0: [2024-03-29 15:44:57,548] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 18.29 GB, percent = 3.6%
x3104c0s1b0n0:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 27635239424
x3104c0s25b0n0: ninja: no work to do.
x3104c0s25b0n0: Time to load cpu_adam op: 2.3103866577148438 seconds
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: Time to load cpu_adam op: 2.4405901432037354 seconds
x3104c0s25b0n0: ninja: no work to do.
x3104c0s25b0n0: Time to load cpu_adam op: 2.278395652770996 seconds
x3104c0s1b0n0: Time to load cpu_adam op: 2.4399359226226807 seconds
x3104c0s25b0n0: ninja: no work to do.
x3104c0s25b0n0: Time to load cpu_adam op: 2.2919771671295166 seconds
x3104c0s25b0n0: Time to load cpu_adam op: 2.325284481048584 seconds
x3104c0s1b0n0: Time to load cpu_adam op: 2.5393357276916504 seconds
x3104c0s1b0n0: ninja: no work to do.
x3104c0s1b0n0: Time to load cpu_adam op: 2.4870526790618896 seconds
x3104c0s25b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s25b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: > learning rate decay style: cosine
x3104c0s1b0n0: DeepSpeed is enabled.
x3104c0s1b0n0: [2024-03-29 15:45:01,942] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.3+8074cd62, git-hash=8074cd62, git-branch=hybrid_opt_offload
x3104c0s1b0n0: [2024-03-29 15:45:02,011] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After args sanity test
x3104c0s1b0n0: [2024-03-29 15:45:02,012] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.5 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:45:02,012] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 24.05 GB, percent = 4.8%
x3104c0s1b0n0: [2024-03-29 15:45:02,068] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure distributed model
x3104c0s1b0n0: [2024-03-29 15:45:02,068] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.5 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:45:02,068] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 24.31 GB, percent = 4.8%
x3104c0s25b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s25b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: [2024-03-29 15:45:02,131] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After configure distributed model
x3104c0s1b0n0: [2024-03-29 15:45:02,132] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.5 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:45:02,132] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 24.63 GB, percent = 4.9%
x3104c0s1b0n0: [2024-03-29 15:45:02,132] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
x3104c0s1b0n0: [2024-03-29 15:45:02,185] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After setting model parameters
x3104c0s1b0n0: [2024-03-29 15:45:02,186] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.5 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:45:02,186] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 24.89 GB, percent = 4.9%
x3104c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: [2024-03-29 15:45:02,241] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure optimizer
x3104c0s1b0n0: [2024-03-29 15:45:02,241] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.5 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:45:02,241] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.14 GB, percent = 5.0%
x3104c0s1b0n0: [2024-03-29 15:45:02,242] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
x3104c0s1b0n0: [2024-03-29 15:45:02,242] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
x3104c0s25b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s25b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: [2024-03-29 15:45:02,261] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
x3104c0s1b0n0: [2024-03-29 15:45:02,261] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
x3104c0s1b0n0: [2024-03-29 15:45:02,261] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
x3104c0s1b0n0: [2024-03-29 15:45:02,261] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
x3104c0s25b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s25b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: [2024-03-29 15:45:02,314] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
x3104c0s1b0n0: [2024-03-29 15:45:02,314] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.5 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:45:02,314] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.39 GB, percent = 5.0%
x3104c0s1b0n0: [2024-03-29 15:45:02,316] [INFO] [stage3.py:137:__init__] Reduce bucket size 500,000,000
x3104c0s1b0n0: [2024-03-29 15:45:02,316] [INFO] [stage3.py:138:__init__] Prefetch bucket size 50,000,000
x3104c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: [2024-03-29 15:45:02,371] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
x3104c0s1b0n0: [2024-03-29 15:45:02,371] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.5 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:45:02,372] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.52 GB, percent = 5.1%
x3104c0s1b0n0: Parameter Offload: Total persistent parameters: 805376 in 121 params
x3104c0s1b0n0: [2024-03-29 15:45:02,451] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
x3104c0s1b0n0: [2024-03-29 15:45:02,451] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.5 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:45:02,452] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.65 GB, percent = 5.1%
x3104c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3104c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3104c0s1b0n0: [2024-03-29 15:45:02,509] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
x3104c0s1b0n0: [2024-03-29 15:45:02,509] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.5 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:45:02,509] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.71 GB, percent = 5.1%
x3104c0s1b0n0: [2024-03-29 15:45:02,547] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s1b0n0: [2024-03-29 15:45:02,547] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s1b0n0: [2024-03-29 15:45:02,547] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s1b0n0: [2024-03-29 15:45:02,547] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s25b0n0: [2024-03-29 15:45:02,547] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s25b0n0: [2024-03-29 15:45:02,547] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s25b0n0: [2024-03-29 15:45:02,547] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s25b0n0: [2024-03-29 15:45:02,547] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 0, numel: 103569856
x3104c0s1b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s1b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s25b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s1b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s1b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s25b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s25b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s25b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 1, numel: 106817984
x3104c0s25b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 2, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 3, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,548] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 4, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 5, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 6, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 7, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 8, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,549] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 9, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 10, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 11, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 12, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 13, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 14, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 15, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,550] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 16, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 17, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 18, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 19, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 20, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 21, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,551] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 22, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 23, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 24, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 25, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 26, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 27, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 28, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s1b0n0: [2024-03-29 15:45:02,552] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 29, numel: 112356608
x3104c0s25b0n0: [2024-03-29 15:45:02,553] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s25b0n0: [2024-03-29 15:45:02,553] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s25b0n0: [2024-03-29 15:45:02,553] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s25b0n0: [2024-03-29 15:45:02,553] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s1b0n0: [2024-03-29 15:45:02,553] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s1b0n0: [2024-03-29 15:45:02,553] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s1b0n0: [2024-03-29 15:45:02,553] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s1b0n0: [2024-03-29 15:45:02,553] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 30, numel: 98032064
x3104c0s1b0n0: [2024-03-29 15:45:04,537] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 31
x3104c0s1b0n0: [2024-03-29 15:45:04,538] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.44 GB         CA 6.44 GB         Max_CA 22 GB 
x3104c0s1b0n0: [2024-03-29 15:45:04,538] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 45.06 GB, percent = 9.0%
x3104c0s1b0n0: [2024-03-29 15:45:04,600] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
x3104c0s1b0n0: [2024-03-29 15:45:04,601] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3104c0s1b0n0: [2024-03-29 15:45:04,601] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 45.06 GB, percent = 9.0%
x3104c0s1b0n0: [2024-03-29 15:45:23,621] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
x3104c0s1b0n0: [2024-03-29 15:45:23,621] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3104c0s1b0n0: [2024-03-29 15:45:23,622] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 210.12 GB, percent = 41.8%
x3104c0s1b0n0: [2024-03-29 15:45:24,970] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
x3104c0s1b0n0: [2024-03-29 15:45:24,970] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3104c0s1b0n0: [2024-03-29 15:45:24,970] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 219.08 GB, percent = 43.5%
x3104c0s1b0n0: [2024-03-29 15:45:31,148] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | init_optimizer_state: 6166.60
x3104c0s1b0n0: [2024-03-29 15:45:31,244] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
x3104c0s1b0n0: [2024-03-29 15:45:31,245] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3104c0s1b0n0: [2024-03-29 15:45:31,245] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 279.39 GB, percent = 55.5%
x3104c0s1b0n0: [2024-03-29 15:45:31,796] [INFO] [stage3.py:520:_setup_for_real_optimizer] optimizer state initialized
x3104c0s1b0n0: [2024-03-29 15:45:39,857] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
x3104c0s1b0n0: [2024-03-29 15:45:39,857] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 8.61 GB         CA 22.75 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 15:45:39,857] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.85 GB, percent = 62.0%
x3104c0s1b0n0: [2024-03-29 15:45:39,858] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
x3104c0s1b0n0: [2024-03-29 15:45:39,927] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure LR scheduler
x3104c0s1b0n0: [2024-03-29 15:45:39,928] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 15:45:39,928] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.87 GB, percent = 62.0%
x3104c0s1b0n0: [2024-03-29 15:45:39,928] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
x3104c0s1b0n0: [2024-03-29 15:45:39,928] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7fb4d68781f0>
x3104c0s1b0n0: [2024-03-29 15:45:39,928] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 15:45:39,996] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before rewriting optimizer step
x3104c0s1b0n0: [2024-03-29 15:45:39,997] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 15:45:39,997] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.86 GB, percent = 62.0%
x3104c0s1b0n0: [2024-03-29 15:45:40,064] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure checkpointing
x3104c0s1b0n0: [2024-03-29 15:45:40,064] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 15:45:40,064] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 311.85 GB, percent = 62.0%
x3104c0s1b0n0: [2024-03-29 15:45:40,064] [INFO] [config.py:998:print] DeepSpeedEngine configuration:
x3104c0s1b0n0: [2024-03-29 15:45:40,065] [INFO] [config.py:1002:print]   activation_checkpointing_config  {
x3104c0s1b0n0:     "partition_activations": false, 
x3104c0s1b0n0:     "contiguous_memory_optimization": false, 
x3104c0s1b0n0:     "cpu_checkpointing": false, 
x3104c0s1b0n0:     "number_checkpoints": null, 
x3104c0s1b0n0:     "synchronize_checkpoint_boundary": false, 
x3104c0s1b0n0:     "profile": false
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 15:45:40,065] [INFO] [config.py:1002:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
x3104c0s1b0n0: [2024-03-29 15:45:40,065] [INFO] [config.py:1002:print]   amp_enabled .................. False
x3104c0s1b0n0: [2024-03-29 15:45:40,065] [INFO] [config.py:1002:print]   amp_params ................... False
x3104c0s1b0n0: [2024-03-29 15:45:40,065] [INFO] [config.py:1002:print]   autotuning_config ............ {
x3104c0s1b0n0:     "enabled": false, 
x3104c0s1b0n0:     "start_step": null, 
x3104c0s1b0n0:     "end_step": null, 
x3104c0s1b0n0:     "metric_path": null, 
x3104c0s1b0n0:     "arg_mappings": null, 
x3104c0s1b0n0:     "metric": "throughput", 
x3104c0s1b0n0:     "model_info": null, 
x3104c0s1b0n0:     "results_dir": "autotuning_results", 
x3104c0s1b0n0:     "exps_dir": "autotuning_exps", 
x3104c0s1b0n0:     "overwrite": true, 
x3104c0s1b0n0:     "fast": true, 
x3104c0s1b0n0:     "start_profile_step": 3, 
x3104c0s1b0n0:     "end_profile_step": 5, 
x3104c0s1b0n0:     "tuner_type": "gridsearch", 
x3104c0s1b0n0:     "tuner_early_stopping": 5, 
x3104c0s1b0n0:     "tuner_num_trials": 50, 
x3104c0s1b0n0:     "model_info_path": null, 
x3104c0s1b0n0:     "mp_size": 1, 
x3104c0s1b0n0:     "max_train_batch_size": null, 
x3104c0s1b0n0:     "min_train_batch_size": 1, 
x3104c0s1b0n0:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
x3104c0s1b0n0:     "min_train_micro_batch_size_per_gpu": 1, 
x3104c0s1b0n0:     "num_tuning_micro_batch_sizes": 3
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 15:45:40,065] [INFO] [config.py:1002:print]   bfloat16_enabled ............. True
x3104c0s1b0n0: [2024-03-29 15:45:40,065] [INFO] [config.py:1002:print]   bfloat16_immediate_grad_update  False
x3104c0s1b0n0: [2024-03-29 15:45:40,065] [INFO] [config.py:1002:print]   checkpoint_parallel_write_pipeline  False
x3104c0s1b0n0: [2024-03-29 15:45:40,065] [INFO] [config.py:1002:print]   checkpoint_tag_validation_enabled  True
x3104c0s1b0n0: [2024-03-29 15:45:40,065] [INFO] [config.py:1002:print]   checkpoint_tag_validation_fail  False
x3104c0s1b0n0: [2024-03-29 15:45:40,065] [INFO] [config.py:1002:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb4d6878bb0>
x3104c0s1b0n0: [2024-03-29 15:45:40,065] [INFO] [config.py:1002:print]   communication_data_type ...... None
x3104c0s1b0n0: [2024-03-29 15:45:40,065] [INFO] [config.py:1002:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
x3104c0s1b0n0: [2024-03-29 15:45:40,065] [INFO] [config.py:1002:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
x3104c0s1b0n0: [2024-03-29 15:45:40,065] [INFO] [config.py:1002:print]   curriculum_enabled_legacy .... False
x3104c0s1b0n0: [2024-03-29 15:45:40,065] [INFO] [config.py:1002:print]   curriculum_params_legacy ..... False
x3104c0s1b0n0: [2024-03-29 15:45:40,065] [INFO] [config.py:1002:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
x3104c0s1b0n0: [2024-03-29 15:45:40,065] [INFO] [config.py:1002:print]   data_efficiency_enabled ...... False
x3104c0s1b0n0: [2024-03-29 15:45:40,065] [INFO] [config.py:1002:print]   dataloader_drop_last ......... False
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   datastates_config ............ {
x3104c0s1b0n0:     "enabled": null, 
x3104c0s1b0n0:     "config": {
x3104c0s1b0n0:     }
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   disable_allgather ............ False
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   dump_state ................... False
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   dynamic_loss_scale_args ...... None
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   eigenvalue_enabled ........... False
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   eigenvalue_gas_boundary_resolution  1
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   eigenvalue_layer_name ........ bert.encoder.layer
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   eigenvalue_layer_num ......... 0
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   eigenvalue_max_iter .......... 100
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   eigenvalue_stability ......... 1e-06
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   eigenvalue_tol ............... 0.01
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   eigenvalue_verbose ........... False
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   elasticity_enabled ........... False
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   flops_profiler_config ........ {
x3104c0s1b0n0:     "enabled": false, 
x3104c0s1b0n0:     "recompute_fwd_factor": 0.0, 
x3104c0s1b0n0:     "profile_step": 1, 
x3104c0s1b0n0:     "module_depth": -1, 
x3104c0s1b0n0:     "top_modules": 1, 
x3104c0s1b0n0:     "detailed": true, 
x3104c0s1b0n0:     "output_file": null
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   fp16_auto_cast ............... None
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   fp16_enabled ................. False
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   fp16_master_weights_and_gradients  False
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   global_rank .................. 0
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   grad_accum_dtype ............. bf16
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   gradient_accumulation_steps .. 1
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   gradient_clipping ............ 0.0
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   gradient_predivide_factor .... 1.0
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   graph_harvesting ............. False
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   initial_dynamic_scale ........ 1
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   load_universal_checkpoint .... False
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   loss_scale ................... 1.0
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   memory_breakdown ............. True
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   mics_hierarchial_params_gather  False
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   mics_shard_size .............. -1
x3104c0s1b0n0: [2024-03-29 15:45:40,066] [INFO] [config.py:1002:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
x3104c0s1b0n0: [2024-03-29 15:45:40,067] [INFO] [config.py:1002:print]   nebula_config ................ {
x3104c0s1b0n0:     "enabled": false, 
x3104c0s1b0n0:     "persistent_storage_path": null, 
x3104c0s1b0n0:     "persistent_time_interval": 100, 
x3104c0s1b0n0:     "num_of_version_in_retention": 2, 
x3104c0s1b0n0:     "enable_nebula_load": true, 
x3104c0s1b0n0:     "load_path": null
x3104c0s1b0n0: }
x3104c0s1b0n0: [2024-03-29 15:45:40,067] [INFO] [config.py:1002:print]   optimizer_legacy_fusion ...... False
x3104c0s1b0n0: [2024-03-29 15:45:40,067] [INFO] [config.py:1002:print]   optimizer_name ............... None
x3104c0s1b0n0: [2024-03-29 15:45:40,067] [INFO] [config.py:1002:print]   optimizer_params ............. None
x3104c0s1b0n0: [2024-03-29 15:45:40,067] [INFO] [config.py:1002:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
x3104c0s1b0n0: [2024-03-29 15:45:40,067] [INFO] [config.py:1002:print]   pld_enabled .................. False
x3104c0s1b0n0: [2024-03-29 15:45:40,067] [INFO] [config.py:1002:print]   pld_params ................... False
x3104c0s1b0n0: [2024-03-29 15:45:40,067] [INFO] [config.py:1002:print]   prescale_gradients ........... False
x3104c0s1b0n0: [2024-03-29 15:45:40,067] [INFO] [config.py:1002:print]   scheduler_name ............... None
x3104c0s1b0n0: [2024-03-29 15:45:40,067] [INFO] [config.py:1002:print]   scheduler_params ............. None
x3104c0s1b0n0: [2024-03-29 15:45:40,067] [INFO] [config.py:1002:print]   seq_parallel_communication_data_type  torch.float32
x3104c0s1b0n0: [2024-03-29 15:45:40,067] [INFO] [config.py:1002:print]   sparse_attention ............. None
x3104c0s1b0n0: [2024-03-29 15:45:40,067] [INFO] [config.py:1002:print]   sparse_gradients_enabled ..... False
x3104c0s1b0n0: [2024-03-29 15:45:40,067] [INFO] [config.py:1002:print]   steps_per_print .............. 1
x3104c0s1b0n0: [2024-03-29 15:45:40,067] [INFO] [config.py:1002:print]   train_batch_size ............. 32
x3104c0s1b0n0: [2024-03-29 15:45:40,067] [INFO] [config.py:1002:print]   train_micro_batch_size_per_gpu  4
x3104c0s1b0n0: [2024-03-29 15:45:40,067] [INFO] [config.py:1002:print]   use_data_before_expert_parallel_  False
x3104c0s1b0n0: [2024-03-29 15:45:40,067] [INFO] [config.py:1002:print]   use_node_local_storage ....... False
x3104c0s1b0n0: [2024-03-29 15:45:40,067] [INFO] [config.py:1002:print]   wall_clock_breakdown ......... True
x3104c0s1b0n0: [2024-03-29 15:45:40,067] [INFO] [config.py:1002:print]   weight_quantization_config ... None
x3104c0s1b0n0: [2024-03-29 15:45:40,067] [INFO] [config.py:1002:print]   world_size ................... 8
x3104c0s1b0n0: [2024-03-29 15:45:40,067] [INFO] [config.py:1002:print]   zero_allow_untested_optimizer  False
x3104c0s1b0n0: [2024-03-29 15:45:40,067] [INFO] [config.py:1002:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0, prefetch_optimizer=True, part_grads_async=True, prefetch_optimizer_gap=4) sub_group_size=100000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
x3104c0s1b0n0: [2024-03-29 15:45:40,067] [INFO] [config.py:1002:print]   zero_enabled ................. True
x3104c0s1b0n0: [2024-03-29 15:45:40,067] [INFO] [config.py:1002:print]   zero_force_ds_cpu_optimizer .. True
x3104c0s1b0n0: [2024-03-29 15:45:40,067] [INFO] [config.py:1002:print]   zero_optimization_stage ...... 3
x3104c0s1b0n0: [2024-03-29 15:45:40,067] [INFO] [config.py:988:print_user_config]   json = {
x3104c0s1b0n0:     "train_batch_size": 32, 
x3104c0s1b0n0:     "train_micro_batch_size_per_gpu": 4, 
x3104c0s1b0n0:     "steps_per_print": 1, 
x3104c0s1b0n0:     "zero_optimization": {
x3104c0s1b0n0:         "stage": 3, 
x3104c0s1b0n0:         "offload_optimizer": {
x3104c0s1b0n0:             "device": "cpu", 
x3104c0s1b0n0:             "ratio": 1, 
x3104c0s1b0n0:             "pin_memory": true, 
x3104c0s1b0n0:             "prefetch_optimizer": 1, 
x3104c0s1b0n0:             "part_grads_async": 1, 
x3104c0s1b0n0:             "prefetch_optimizer_gap": 4
x3104c0s1b0n0:         }, 
x3104c0s1b0n0:         "sub_group_size": 1.000000e+08
x3104c0s1b0n0:     }, 
x3104c0s1b0n0:     "bf16": {
x3104c0s1b0n0:         "enabled": true
x3104c0s1b0n0:     }, 
x3104c0s1b0n0:     "data_types": {
x3104c0s1b0n0:         "grad_accum_dtype": "bf16"
x3104c0s1b0n0:     }, 
x3104c0s1b0n0:     "wall_clock_breakdown": true, 
x3104c0s1b0n0:     "memory_breakdown": true, 
x3104c0s1b0n0:     "flops_profiler": {
x3104c0s1b0n0:         "enabled": false
x3104c0s1b0n0:     }
x3104c0s1b0n0: }
x3104c0s25b0n0: <TIMER:model-and-optimizer-setup,49.092602014541626>
x3104c0s25b0n0: <TIMER:model-and-optimizer-setup,49.092655181884766>
x3104c0s25b0n0: <TIMER:model-and-optimizer-setup,49.093886613845825>
x3104c0s1b0n0: <TIMER:model-and-optimizer-setup,49.094319581985474>
x3104c0s1b0n0: <TIMER:model-and-optimizer-setup,49.09491968154907>
x3104c0s25b0n0: <TIMER:model-and-optimizer-setup,49.09507727622986>
x3104c0s1b0n0: <TIMER:model-and-optimizer-setup,49.09597659111023>
x3104c0s1b0n0: <TIMER:model-and-optimizer-setup,49.09658193588257>
x3104c0s1b0n0: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-29 15:45:41 
x3104c0s1b0n0: > building train, validation, and test datasets ...
x3104c0s1b0n0:  > datasets target sizes (minimum size):
x3104c0s1b0n0:     train:      320
x3104c0s1b0n0:     validation: 0
x3104c0s1b0n0:     test:       0
x3104c0s1b0n0: > building train, validation, and test datasets for GPT ...
x3104c0s1b0n0: Single data path provided for train, valid & test
x3104c0s1b0n0:  > building dataset index ...
x3104c0s1b0n0:     reading sizes...
x3104c0s1b0n0:     reading pointers...
x3104c0s1b0n0:     reading document index...
x3104c0s1b0n0:     creating numpy buffer of mmap...
x3104c0s1b0n0:     creating memory view of numpy buffer...
x3104c0s1b0n0:  > finished creating indexed dataset in 0.002182 seconds
x3104c0s1b0n0:     number of documents: 79000
x3104c0s1b0n0:  > dataset split:
x3104c0s1b0n0:     train:
x3104c0s1b0n0:      document indices in [0, 74971) total of 74971 documents
x3104c0s1b0n0:     validation:
x3104c0s1b0n0:      document indices in [74971, 78921) total of 3950 documents
x3104c0s1b0n0:     test:
x3104c0s1b0n0:      document indices in [78921, 79000) total of 79 documents
x3104c0s1b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_doc_idx.npy
x3104c0s1b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_sample_idx.npy
x3104c0s1b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_shuffle_idx.npy
x3104c0s1b0n0:     loaded indexed file in 0.003 seconds
x3104c0s1b0n0:     total number of samples: 108448
x3104c0s1b0n0:     total number of epochs: 1
x3104c0s1b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_doc_idx.npy
x3104c0s1b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_sample_idx.npy
x3104c0s1b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_shuffle_idx.npy
x3104c0s1b0n0:     loaded indexed file in 0.003 seconds
x3104c0s1b0n0:     total number of samples: 5792
x3104c0s1b0n0:     total number of epochs: 1
x3104c0s1b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_doc_idx.npy
x3104c0s1b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_sample_idx.npy
x3104c0s1b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_shuffle_idx.npy
x3104c0s1b0n0:     loaded indexed file in 0.003 seconds
x3104c0s1b0n0:     total number of samples: 185
x3104c0s1b0n0:     total number of epochs: 1
x3104c0s1b0n0: > finished creating GPT datasets ...
x3104c0s25b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5035860538482666>
x3104c0s25b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5071446895599365>
x3104c0s25b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5229442119598389>
x3104c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.525261640548706>
x3104c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5263853073120117>
x3104c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.538693904876709>
x3104c0s25b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5429658889770508>
x3104c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5746049880981445>
x3104c0s1b0n0: [after dataloaders are built] datetime: 2024-03-29 15:45:41 
x3104c0s1b0n0: done with setup ...
x3104c0s25b0n0: (min, max) time across ranks (ms):
x3104c0s25b0n0:     model-and-optimizer-setup ......................: (49092.60, 49096.58)
x3104c0s25b0n0:     train/valid/test-data-iterators-setup ..........: (503.59, 574.60)
x3104c0s1b0n0: training ...
x3104c0s1b0n0: [before training begins] datetime: 2024-03-29 15:45:41 
x3104c0s1b0n0: [before the start of training step] datetime: 2024-03-29 15:45:41 
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:45:42,064] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:45:42,065] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 7.45 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 15:45:42,065] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.16 GB, percent = 62.4%
x3104c0s1b0n0: [2024-03-29 15:45:42,193] [INFO] [checkpointing.py:539:forward] Activation Checkpointing Information
x3104c0s1b0n0: [2024-03-29 15:45:42,193] [INFO] [checkpointing.py:540:forward] ----Partition Activations False, CPU CHECKPOINTING False
x3104c0s1b0n0: [2024-03-29 15:45:42,193] [INFO] [checkpointing.py:541:forward] ----contiguous Memory Checkpointing False with 60 total layers
x3104c0s1b0n0: [2024-03-29 15:45:42,193] [INFO] [checkpointing.py:543:forward] ----Synchronization False
x3104c0s1b0n0: [2024-03-29 15:45:42,193] [INFO] [checkpointing.py:544:forward] ----Profiling time in checkpointing False
x3104c0s1b0n0: [2024-03-29 15:45:49,818] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:45:49,818] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 19.76 GB         CA 22.95 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 15:45:49,818] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.35 GB, percent = 62.5%
x3104c0s1b0n0: [2024-03-29 15:45:49,984] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:45:49,985] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 16.84 GB         CA 16.91 GB         Max_CA 23 GB 
x3104c0s1b0n0: [2024-03-29 15:45:49,985] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.35 GB, percent = 62.5%
x3104c0s1b0n0: [2024-03-29 15:46:14,855] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:46:14,856] [INFO] [utils.py:801:see_memory_usage] MA 11.72 GB         Max_MA 20.43 GB         CA 11.97 GB         Max_CA 28 GB 
x3104c0s1b0n0: [2024-03-29 15:46:14,856] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.41 GB, percent = 62.5%
x3104c0s1b0n0: [2024-03-29 15:46:14,930] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:46:14,931] [INFO] [utils.py:801:see_memory_usage] MA 11.72 GB         Max_MA 11.72 GB         CA 11.97 GB         Max_CA 12 GB 
x3104c0s1b0n0: [2024-03-29 15:46:14,931] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 314.48 GB, percent = 62.5%
x3104c0s25b0n0: [2024-03-29 15:46:21,115] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.120978355407715
x3104c0s25b0n0: [2024-03-29 15:46:21,143] [INFO] [stage3.py:2251:step] Full outer step loop took 6.149759769439697
x3104c0s1b0n0: [2024-03-29 15:46:21,182] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.187474012374878
x3104c0s1b0n0: [2024-03-29 15:46:21,215] [INFO] [stage3.py:2251:step] Full outer step loop took 6.221145391464233
x3104c0s1b0n0: [2024-03-29 15:46:21,248] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.2539918422698975
x3104c0s1b0n0: [2024-03-29 15:46:21,256] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.2622435092926025
x3104c0s1b0n0: [2024-03-29 15:46:21,260] [INFO] [stage3.py:2251:step] Full outer step loop took 6.266008615493774
x3104c0s1b0n0: [2024-03-29 15:46:21,267] [INFO] [stage3.py:2251:step] Full outer step loop took 6.273950815200806
x3104c0s1b0n0: [2024-03-29 15:46:21,302] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.307957887649536
x3104c0s1b0n0: [2024-03-29 15:46:21,311] [INFO] [stage3.py:2251:step] Full outer step loop took 6.316980600357056
x3104c0s25b0n0: [2024-03-29 15:46:21,320] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.325795650482178
x3104c0s25b0n0: [2024-03-29 15:46:21,331] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.336878776550293
x3104c0s25b0n0: [2024-03-29 15:46:21,336] [INFO] [stage3.py:2251:step] Full outer step loop took 6.342674970626831
x3104c0s25b0n0: [2024-03-29 15:46:21,336] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.342597246170044
x3104c0s25b0n0: [2024-03-29 15:46:21,339] [INFO] [stage3.py:2251:step] Full outer step loop took 6.3459672927856445
x3104c0s25b0n0: [2024-03-29 15:46:21,345] [INFO] [stage3.py:2251:step] Full outer step loop took 6.351598024368286
x3104c0s25b0n0: [2024-03-29 15:46:21,355] [INFO] [stage3.py:2277:step] End to end step took 6.361121892929077
x3104c0s25b0n0: [2024-03-29 15:46:21,355] [INFO] [stage3.py:2277:step] End to end step took 6.361344814300537
x3104c0s1b0n0: [2024-03-29 15:46:21,355] [INFO] [stage3.py:2277:step] End to end step took 6.361314296722412
x3104c0s25b0n0: [2024-03-29 15:46:21,355] [INFO] [stage3.py:2277:step] End to end step took 6.36128568649292
x3104c0s1b0n0: [2024-03-29 15:46:21,355] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6274.24
x3104c0s1b0n0: [2024-03-29 15:46:21,355] [WARNING] [stage3.py:2267:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3104c0s1b0n0: [2024-03-29 15:46:21,355] [INFO] [stage3.py:2277:step] End to end step took 6.361974716186523
x3104c0s25b0n0: [2024-03-29 15:46:21,355] [INFO] [stage3.py:2277:step] End to end step took 6.361932754516602
x3104c0s1b0n0: [2024-03-29 15:46:21,355] [INFO] [stage3.py:2277:step] End to end step took 6.361912727355957
x3104c0s1b0n0: [2024-03-29 15:46:21,356] [INFO] [stage3.py:2277:step] End to end step took 6.362062931060791
x3104c0s1b0n0: [2024-03-29 15:46:21,356] [INFO] [logging.py:96:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 15:46:21,356] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 7817.40 | bwd_microstep: 24719.96 | bwd_inner_microstep: 24631.88 | bwd_allreduce_microstep: 87.97 | step_microstep: 6424.96
x3104c0s1b0n0: [2024-03-29 15:46:21,356] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 7817.40 | bwd: 24719.95 | bwd_inner: 24631.90 | bwd_allreduce: 87.97 | step: 6424.95
x3104c0s1b0n0: [2024-03-29 15:46:21,452] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:46:21,452] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.77 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:46:21,452] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.16 GB, percent = 78.3%
x3104c0s1b0n0: <TIMER:interval-time,39.5755889415741><TIMER:interval-time,39.57569885253906><TIMER:interval-time,39.575692892074585>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,39.57573747634888>
x3104c0s25b0n0: <TIMER:interval-time,39.575719118118286><TIMER:interval-time,39.57571744918823><TIMER:interval-time,39.57572364807129><TIMER:interval-time,39.575716972351074>
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0:  elapsed_time 39.575717 | consumed samples:           32 | consumed tokens:        65536 | elapsed time per iteration (ms): 39575.7 | learning rate: 3.000E-04 | global batch size:    32 | lm loss: 1.215015E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.809 | TFLOPs: 55.95 |
x3104c0s1b0n0: [Rank 0] (after 1 iterations) memory (MB) | allocated: 10510.36328125 | max allocated: 10510.36376953125 | reserved: 10586.0 | max reserved: 10586.0
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:46:21,596] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:46:21,597] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 15:46:21,597] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.25 GB, percent = 78.3%
x3104c0s1b0n0: [2024-03-29 15:46:27,868] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:46:27,868] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:46:27,868] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.27 GB, percent = 78.3%
x3104c0s1b0n0: [2024-03-29 15:46:27,963] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:46:27,964] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:46:27,964] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.27 GB, percent = 78.3%
x3104c0s1b0n0: [2024-03-29 15:46:44,811] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:46:44,811] [INFO] [utils.py:801:see_memory_usage] MA 12.98 GB         Max_MA 23.77 GB         CA 13.23 GB         Max_CA 33 GB 
x3104c0s1b0n0: [2024-03-29 15:46:44,812] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.27 GB, percent = 78.3%
x3104c0s1b0n0: [2024-03-29 15:46:44,886] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:46:44,887] [INFO] [utils.py:801:see_memory_usage] MA 12.98 GB         Max_MA 12.98 GB         CA 13.23 GB         Max_CA 13 GB 
x3104c0s1b0n0: [2024-03-29 15:46:44,887] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.27 GB, percent = 78.3%
x3104c0s25b0n0: [2024-03-29 15:46:49,169] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.257979154586792
x3104c0s25b0n0: [2024-03-29 15:46:49,231] [INFO] [stage3.py:2251:step] Full outer step loop took 4.319649934768677
x3104c0s1b0n0: [2024-03-29 15:46:49,328] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.415844440460205
x3104c0s1b0n0: [2024-03-29 15:46:49,372] [INFO] [stage3.py:2251:step] Full outer step loop took 4.460366487503052
x3104c0s25b0n0: [2024-03-29 15:46:49,386] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.474999904632568
x3104c0s1b0n0: [2024-03-29 15:46:49,387] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.475717306137085
x3104c0s1b0n0: [2024-03-29 15:46:49,388] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.476292610168457
x3104c0s25b0n0: [2024-03-29 15:46:49,389] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.477512359619141
x3104c0s1b0n0: [2024-03-29 15:46:49,393] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.481813192367554
x3104c0s25b0n0: [2024-03-29 15:46:49,396] [INFO] [stage3.py:2251:step] Full outer step loop took 4.485174655914307
x3104c0s1b0n0: [2024-03-29 15:46:49,397] [INFO] [stage3.py:2251:step] Full outer step loop took 4.485118865966797
x3104c0s1b0n0: [2024-03-29 15:46:49,397] [INFO] [stage3.py:2251:step] Full outer step loop took 4.4857001304626465
x3104c0s1b0n0: [2024-03-29 15:46:49,402] [INFO] [stage3.py:2251:step] Full outer step loop took 4.49078106880188
x3104c0s25b0n0: [2024-03-29 15:46:49,416] [INFO] [stage3.py:2251:step] Full outer step loop took 4.504812955856323
x3104c0s25b0n0: [2024-03-29 15:46:49,443] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.531406879425049
x3104c0s25b0n0: [2024-03-29 15:46:49,452] [INFO] [stage3.py:2251:step] Full outer step loop took 4.540358304977417
x3104c0s25b0n0: [2024-03-29 15:46:49,461] [INFO] [stage3.py:2277:step] End to end step took 4.549536228179932
x3104c0s1b0n0: [2024-03-29 15:46:49,461] [INFO] [stage3.py:2277:step] End to end step took 4.549573183059692
x3104c0s1b0n0: [2024-03-29 15:46:49,461] [INFO] [stage3.py:2277:step] End to end step took 4.549590349197388
x3104c0s25b0n0: [2024-03-29 15:46:49,461] [INFO] [stage3.py:2277:step] End to end step took 4.549717426300049
x3104c0s25b0n0: [2024-03-29 15:46:49,461] [INFO] [stage3.py:2277:step] End to end step took 4.549750566482544
x3104c0s1b0n0: [2024-03-29 15:46:49,461] [INFO] [stage3.py:2277:step] End to end step took 4.549778699874878
x3104c0s1b0n0: [2024-03-29 15:46:49,461] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4485.29
x3104c0s25b0n0: [2024-03-29 15:46:49,461] [INFO] [stage3.py:2277:step] End to end step took 4.550092697143555
x3104c0s1b0n0: [2024-03-29 15:46:49,462] [INFO] [stage3.py:2277:step] End to end step took 4.550173282623291
x3104c0s1b0n0: [2024-03-29 15:46:49,462] [INFO] [logging.py:96:log_dist] [Rank 0] step=2, skipped=0, lr=[0.0002918585038060976], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 15:46:49,462] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6234.05 | bwd_microstep: 16683.14 | bwd_inner_microstep: 16599.93 | bwd_allreduce_microstep: 83.13 | step_microstep: 4574.85
x3104c0s1b0n0: [2024-03-29 15:46:49,462] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6234.03 | bwd: 16683.14 | bwd_inner: 16599.93 | bwd_allreduce: 83.15 | step: 4574.85
x3104c0s1b0n0: [2024-03-29 15:46:49,559] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:46:49,560] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.98 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:46:49,560] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.27 GB, percent = 78.3%
x3104c0s1b0n0: <TIMER:interval-time,28.107431650161743><TIMER:interval-time,28.10742950439453>
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,28.10742688179016><TIMER:interval-time,28.107441186904907>
x3104c0s1b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,28.107467889785767><TIMER:interval-time,28.107470273971558>
x3104c0s25b0n0: <TIMER:interval-time,28.107472896575928>
x3104c0s25b0n0: <TIMER:interval-time,28.10747504234314>
x3104c0s25b0n0: 
x3104c0s25b0n0:  elapsed_time 28.107468 | consumed samples:           64 | consumed tokens:       131072 | elapsed time per iteration (ms): 28107.5 | learning rate: 2.919E-04 | global batch size:    32 | lm loss: 1.082593E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.138 | TFLOPs: 78.77 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:46:49,668] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:46:49,669] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 15:46:49,669] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.27 GB, percent = 78.3%
x3104c0s1b0n0: [2024-03-29 15:46:55,958] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:46:55,958] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:46:55,958] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.27 GB, percent = 78.3%
x3104c0s1b0n0: [2024-03-29 15:46:56,051] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:46:56,052] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:46:56,052] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.27 GB, percent = 78.3%
x3104c0s1b0n0: [2024-03-29 15:47:12,869] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:47:12,869] [INFO] [utils.py:801:see_memory_usage] MA 12.98 GB         Max_MA 23.77 GB         CA 13.23 GB         Max_CA 33 GB 
x3104c0s1b0n0: [2024-03-29 15:47:12,869] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.28 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:47:12,941] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:47:12,942] [INFO] [utils.py:801:see_memory_usage] MA 12.98 GB         Max_MA 12.98 GB         CA 13.23 GB         Max_CA 13 GB 
x3104c0s1b0n0: [2024-03-29 15:47:12,942] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.28 GB, percent = 78.4%
x3104c0s25b0n0: [2024-03-29 15:47:17,261] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.294886589050293
x3104c0s25b0n0: [2024-03-29 15:47:17,274] [INFO] [stage3.py:2251:step] Full outer step loop took 4.307898283004761
x3104c0s1b0n0: [2024-03-29 15:47:17,337] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.371083736419678
x3104c0s1b0n0: [2024-03-29 15:47:17,355] [INFO] [stage3.py:2251:step] Full outer step loop took 4.38880181312561
x3104c0s25b0n0: [2024-03-29 15:47:17,431] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.4645891189575195
x3104c0s1b0n0: [2024-03-29 15:47:17,443] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.477293491363525
x3104c0s25b0n0: [2024-03-29 15:47:17,450] [INFO] [stage3.py:2251:step] Full outer step loop took 4.483530521392822
x3104c0s1b0n0: [2024-03-29 15:47:17,454] [INFO] [stage3.py:2251:step] Full outer step loop took 4.487879753112793
x3104c0s25b0n0: [2024-03-29 15:47:17,509] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.542837381362915
x3104c0s25b0n0: [2024-03-29 15:47:17,535] [INFO] [stage3.py:2251:step] Full outer step loop took 4.56905460357666
x3104c0s25b0n0: [2024-03-29 15:47:17,541] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.574572563171387
x3104c0s25b0n0: [2024-03-29 15:47:17,550] [INFO] [stage3.py:2251:step] Full outer step loop took 4.5835535526275635
x3104c0s1b0n0: [2024-03-29 15:47:17,561] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.595394849777222
x3104c0s1b0n0: [2024-03-29 15:47:17,561] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.595425367355347
x3104c0s1b0n0: [2024-03-29 15:47:17,570] [INFO] [stage3.py:2251:step] Full outer step loop took 4.604429721832275
x3104c0s1b0n0: [2024-03-29 15:47:17,571] [INFO] [stage3.py:2251:step] Full outer step loop took 4.605071306228638
x3104c0s1b0n0: [2024-03-29 15:47:17,581] [INFO] [stage3.py:2277:step] End to end step took 4.615262746810913
x3104c0s25b0n0: [2024-03-29 15:47:17,581] [INFO] [stage3.py:2277:step] End to end step took 4.615353584289551
x3104c0s1b0n0: [2024-03-29 15:47:17,581] [INFO] [stage3.py:2277:step] End to end step took 4.615413427352905
x3104c0s25b0n0: [2024-03-29 15:47:17,581] [INFO] [stage3.py:2277:step] End to end step took 4.615503311157227
x3104c0s25b0n0: [2024-03-29 15:47:17,582] [INFO] [stage3.py:2277:step] End to end step took 4.615668773651123
x3104c0s1b0n0: [2024-03-29 15:47:17,582] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4488.07
x3104c0s1b0n0: [2024-03-29 15:47:17,582] [INFO] [stage3.py:2277:step] End to end step took 4.615999937057495
x3104c0s1b0n0: [2024-03-29 15:47:17,582] [INFO] [stage3.py:2277:step] End to end step took 4.615952968597412
x3104c0s25b0n0: [2024-03-29 15:47:17,582] [INFO] [stage3.py:2277:step] End to end step took 4.616144418716431
x3104c0s1b0n0: [2024-03-29 15:47:17,583] [INFO] [logging.py:96:log_dist] [Rank 0] step=3, skipped=0, lr=[0.00026841599982106197], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 15:47:17,583] [INFO] [timer.py:260:stop] epoch=0/micro_step=3/global_step=3, RunningAvgSamplesPerSec=1.1464260323644229, CurrSamplesPerSec=1.1464260323644229, MemAllocated=10.26GB, MaxMemAllocated=12.98GB
x3104c0s1b0n0: [2024-03-29 15:47:17,583] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6250.98 | bwd_microstep: 16652.56 | bwd_inner_microstep: 16568.13 | bwd_allreduce_microstep: 84.35 | step_microstep: 4640.83
x3104c0s1b0n0: [2024-03-29 15:47:17,583] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6250.97 | bwd: 16652.56 | bwd_inner: 16568.13 | bwd_allreduce: 84.37 | step: 4640.84
x3104c0s1b0n0: [2024-03-29 15:47:17,686] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:47:17,687] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.98 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:47:17,687] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.28 GB, percent = 78.3%
x3104c0s1b0n0: <TIMER:interval-time,28.126582384109497>
x3104c0s1b0n0: <TIMER:interval-time,28.126589059829712>
x3104c0s1b0n0: <TIMER:interval-time,28.126598596572876><TIMER:interval-time,28.12659215927124>
x3104c0s1b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,28.126614332199097><TIMER:interval-time,28.126615047454834><TIMER:interval-time,28.126617431640625>
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,28.126624822616577>
x3104c0s25b0n0:  elapsed_time 28.126617 | consumed samples:           96 | consumed tokens:       196608 | elapsed time per iteration (ms): 28126.6 | learning rate: 2.684E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.138 | TFLOPs: 78.72 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:47:17,830] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:47:17,830] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 15:47:17,831] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.3 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:47:24,128] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:47:24,129] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:47:24,129] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.29 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:47:24,210] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:47:24,211] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:47:24,211] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.29 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:47:41,035] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:47:41,035] [INFO] [utils.py:801:see_memory_usage] MA 12.98 GB         Max_MA 23.77 GB         CA 13.23 GB         Max_CA 33 GB 
x3104c0s1b0n0: [2024-03-29 15:47:41,035] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.3 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:47:41,107] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:47:41,108] [INFO] [utils.py:801:see_memory_usage] MA 12.98 GB         Max_MA 12.98 GB         CA 13.23 GB         Max_CA 13 GB 
x3104c0s1b0n0: [2024-03-29 15:47:41,108] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.3 GB, percent = 78.4%
x3104c0s25b0n0: [2024-03-29 15:47:45,356] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.225099325180054
x3104c0s25b0n0: [2024-03-29 15:47:45,370] [INFO] [stage3.py:2251:step] Full outer step loop took 4.239525556564331
x3104c0s1b0n0: [2024-03-29 15:47:45,623] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.492064952850342
x3104c0s25b0n0: [2024-03-29 15:47:45,629] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.49793815612793
x3104c0s1b0n0: [2024-03-29 15:47:45,629] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.498415470123291
x3104c0s1b0n0: [2024-03-29 15:47:45,634] [INFO] [stage3.py:2251:step] Full outer step loop took 4.502557754516602
x3104c0s1b0n0: [2024-03-29 15:47:45,638] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.506762981414795
x3104c0s1b0n0: [2024-03-29 15:47:45,639] [INFO] [stage3.py:2251:step] Full outer step loop took 4.50798487663269
x3104c0s1b0n0: [2024-03-29 15:47:45,642] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.51121973991394
x3104c0s1b0n0: [2024-03-29 15:47:45,647] [INFO] [stage3.py:2251:step] Full outer step loop took 4.51577615737915
x3104c0s1b0n0: [2024-03-29 15:47:45,651] [INFO] [stage3.py:2251:step] Full outer step loop took 4.520218849182129
x3104c0s25b0n0: [2024-03-29 15:47:45,659] [INFO] [stage3.py:2251:step] Full outer step loop took 4.527831792831421
x3104c0s25b0n0: [2024-03-29 15:47:45,690] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.559013843536377
x3104c0s25b0n0: [2024-03-29 15:47:45,699] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.568052291870117
x3104c0s25b0n0: [2024-03-29 15:47:45,700] [INFO] [stage3.py:2251:step] Full outer step loop took 4.569425344467163
x3104c0s25b0n0: [2024-03-29 15:47:45,708] [INFO] [stage3.py:2251:step] Full outer step loop took 4.577033519744873
x3104c0s1b0n0: [2024-03-29 15:47:45,717] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4520.37
x3104c0s25b0n0: [2024-03-29 15:47:45,717] [INFO] [stage3.py:2277:step] End to end step took 4.586329936981201
x3104c0s1b0n0: [2024-03-29 15:47:45,717] [INFO] [stage3.py:2277:step] End to end step took 4.586203098297119
x3104c0s25b0n0: [2024-03-29 15:47:45,717] [INFO] [stage3.py:2277:step] End to end step took 4.586374044418335
x3104c0s1b0n0: [2024-03-29 15:47:45,717] [INFO] [stage3.py:2277:step] End to end step took 4.586251497268677
x3104c0s1b0n0: [2024-03-29 15:47:45,717] [INFO] [stage3.py:2277:step] End to end step took 4.586331129074097
x3104c0s1b0n0: [2024-03-29 15:47:45,717] [INFO] [stage3.py:2277:step] End to end step took 4.586344480514526
x3104c0s25b0n0: [2024-03-29 15:47:45,717] [INFO] [stage3.py:2277:step] End to end step took 4.586592435836792
x3104c0s1b0n0: [2024-03-29 15:47:45,718] [INFO] [logging.py:96:log_dist] [Rank 0] step=4, skipped=0, lr=[0.00023249999999999996], mom=[(0.9, 0.95)]
x3104c0s25b0n0: [2024-03-29 15:47:45,718] [INFO] [stage3.py:2277:step] End to end step took 4.586820125579834
x3104c0s1b0n0: [2024-03-29 15:47:45,718] [INFO] [timer.py:260:stop] epoch=0/micro_step=4/global_step=4, RunningAvgSamplesPerSec=1.1469774097727556, CurrSamplesPerSec=1.147529317809888, MemAllocated=10.26GB, MaxMemAllocated=12.98GB
x3104c0s1b0n0: [2024-03-29 15:47:45,718] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6256.20 | bwd_microstep: 16661.22 | bwd_inner_microstep: 16577.62 | bwd_allreduce_microstep: 83.54 | step_microstep: 4610.02
x3104c0s1b0n0: [2024-03-29 15:47:45,718] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6256.19 | bwd: 16661.21 | bwd_inner: 16577.61 | bwd_allreduce: 83.55 | step: 4610.02
x3104c0s1b0n0: [2024-03-29 15:47:45,820] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:47:45,821] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.98 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:47:45,821] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.29 GB, percent = 78.4%
x3104c0s1b0n0: <TIMER:interval-time,28.133865118026733><TIMER:interval-time,28.13387107849121>
x3104c0s1b0n0: <TIMER:interval-time,28.13387393951416>
x3104c0s1b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,28.13387155532837><TIMER:interval-time,28.133873224258423>
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,28.13387942314148>
x3104c0s1b0n0: <TIMER:interval-time,28.133975982666016>
x3104c0s25b0n0: <TIMER:interval-time,28.13399910926819>
x3104c0s25b0n0:  elapsed_time 28.133873 | consumed samples:          128 | consumed tokens:       262144 | elapsed time per iteration (ms): 28133.9 | learning rate: 2.325E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.137 | TFLOPs: 78.70 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:47:45,971] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:47:45,972] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 15:47:45,972] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.3 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:47:52,725] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:47:52,725] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:47:52,726] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.29 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:47:52,809] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:47:52,809] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:47:52,810] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.29 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:48:10,053] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:48:10,054] [INFO] [utils.py:801:see_memory_usage] MA 12.98 GB         Max_MA 23.77 GB         CA 13.23 GB         Max_CA 33 GB 
x3104c0s1b0n0: [2024-03-29 15:48:10,054] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.29 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:48:10,128] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:48:10,129] [INFO] [utils.py:801:see_memory_usage] MA 12.98 GB         Max_MA 12.98 GB         CA 13.23 GB         Max_CA 13 GB 
x3104c0s1b0n0: [2024-03-29 15:48:10,129] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.29 GB, percent = 78.4%
x3104c0s25b0n0: [2024-03-29 15:48:14,336] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.182990550994873
x3104c0s25b0n0: [2024-03-29 15:48:14,345] [INFO] [stage3.py:2251:step] Full outer step loop took 4.1920106410980225
x3104c0s1b0n0: [2024-03-29 15:48:14,492] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.338911056518555
x3104c0s1b0n0: [2024-03-29 15:48:14,514] [INFO] [stage3.py:2251:step] Full outer step loop took 4.361101865768433
x3104c0s1b0n0: [2024-03-29 15:48:14,525] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.371613502502441
x3104c0s1b0n0: [2024-03-29 15:48:14,555] [INFO] [stage3.py:2251:step] Full outer step loop took 4.401785612106323
x3104c0s1b0n0: [2024-03-29 15:48:14,657] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.504375457763672
x3104c0s1b0n0: [2024-03-29 15:48:14,657] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.5042290687561035
x3104c0s1b0n0: [2024-03-29 15:48:14,666] [INFO] [stage3.py:2251:step] Full outer step loop took 4.513375282287598
x3104c0s1b0n0: [2024-03-29 15:48:14,666] [INFO] [stage3.py:2251:step] Full outer step loop took 4.513219356536865
x3104c0s25b0n0: [2024-03-29 15:48:14,737] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.583971261978149
x3104c0s25b0n0: [2024-03-29 15:48:14,753] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.600613355636597
x3104c0s25b0n0: [2024-03-29 15:48:14,757] [INFO] [stage3.py:2251:step] Full outer step loop took 4.604819059371948
x3104c0s25b0n0: [2024-03-29 15:48:14,783] [INFO] [stage3.py:2251:step] Full outer step loop took 4.630652666091919
x3104c0s25b0n0: [2024-03-29 15:48:14,786] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.633688449859619
x3104c0s25b0n0: [2024-03-29 15:48:14,795] [INFO] [stage3.py:2251:step] Full outer step loop took 4.642676591873169
x3104c0s1b0n0: [2024-03-29 15:48:14,805] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4513.52
x3104c0s25b0n0: [2024-03-29 15:48:14,805] [INFO] [stage3.py:2277:step] End to end step took 4.6520867347717285
x3104c0s1b0n0: [2024-03-29 15:48:14,805] [INFO] [stage3.py:2277:step] End to end step took 4.651897192001343
x3104c0s1b0n0: [2024-03-29 15:48:14,805] [INFO] [stage3.py:2277:step] End to end step took 4.652202129364014
x3104c0s25b0n0: [2024-03-29 15:48:14,805] [INFO] [stage3.py:2277:step] End to end step took 4.652194499969482
x3104c0s25b0n0: [2024-03-29 15:48:14,805] [INFO] [stage3.py:2277:step] End to end step took 4.652256011962891
x3104c0s1b0n0: [2024-03-29 15:48:14,805] [INFO] [logging.py:96:log_dist] [Rank 0] step=5, skipped=0, lr=[0.0001884425039850356], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 15:48:14,805] [INFO] [stage3.py:2277:step] End to end step took 4.65237832069397
x3104c0s25b0n0: [2024-03-29 15:48:14,805] [INFO] [stage3.py:2277:step] End to end step took 4.652637243270874
x3104c0s1b0n0: [2024-03-29 15:48:14,805] [INFO] [timer.py:260:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=1.134333644019692, CurrSamplesPerSec=1.1098643480853494, MemAllocated=10.26GB, MaxMemAllocated=12.98GB
x3104c0s1b0n0: [2024-03-29 15:48:14,805] [INFO] [stage3.py:2277:step] End to end step took 4.652695655822754
x3104c0s1b0n0: [2024-03-29 15:48:14,806] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6706.90 | bwd_microstep: 17076.14 | bwd_inner_microstep: 16988.25 | bwd_allreduce_microstep: 87.82 | step_microstep: 4676.65
x3104c0s1b0n0: [2024-03-29 15:48:14,806] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6706.89 | bwd: 17076.14 | bwd_inner: 16988.25 | bwd_allreduce: 87.83 | step: 4676.65
x3104c0s1b0n0: [2024-03-29 15:48:14,900] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:48:14,901] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.98 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:48:14,901] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.29 GB, percent = 78.4%
x3104c0s1b0n0: <TIMER:interval-time,29.07980966567993>
x3104c0s1b0n0: <TIMER:interval-time,29.079800844192505>
x3104c0s25b0n0: <TIMER:interval-time,29.07982850074768><TIMER:interval-time,29.07983136177063>
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,29.079833030700684><TIMER:interval-time,29.079819917678833>
x3104c0s25b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,29.07988429069519>
x3104c0s1b0n0: <TIMER:interval-time,29.079919576644897>
x3104c0s25b0n0:  elapsed_time 29.079831 | consumed samples:          160 | consumed tokens:       327680 | elapsed time per iteration (ms): 29079.8 | learning rate: 1.884E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.100 | TFLOPs: 76.14 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:48:15,041] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:48:15,042] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 15:48:15,042] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.3 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:48:21,548] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:48:21,549] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:48:21,549] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.29 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:48:21,627] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:48:21,628] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:48:21,628] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.29 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:48:39,352] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:48:39,353] [INFO] [utils.py:801:see_memory_usage] MA 12.98 GB         Max_MA 23.77 GB         CA 13.23 GB         Max_CA 33 GB 
x3104c0s1b0n0: [2024-03-29 15:48:39,353] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.3 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:48:39,425] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:48:39,425] [INFO] [utils.py:801:see_memory_usage] MA 12.98 GB         Max_MA 12.98 GB         CA 13.23 GB         Max_CA 13 GB 
x3104c0s1b0n0: [2024-03-29 15:48:39,425] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.29 GB, percent = 78.4%
x3104c0s25b0n0: [2024-03-29 15:48:43,665] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.215328693389893
x3104c0s25b0n0: [2024-03-29 15:48:43,688] [INFO] [stage3.py:2251:step] Full outer step loop took 4.238319635391235
x3104c0s1b0n0: [2024-03-29 15:48:43,881] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.430886268615723
x3104c0s1b0n0: [2024-03-29 15:48:43,891] [INFO] [stage3.py:2251:step] Full outer step loop took 4.441201686859131
x3104c0s1b0n0: [2024-03-29 15:48:43,892] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.441682577133179
x3104c0s1b0n0: [2024-03-29 15:48:43,892] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.442167043685913
x3104c0s1b0n0: [2024-03-29 15:48:43,902] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.451761484146118
x3104c0s1b0n0: [2024-03-29 15:48:43,902] [INFO] [stage3.py:2251:step] Full outer step loop took 4.45216178894043
x3104c0s1b0n0: [2024-03-29 15:48:43,904] [INFO] [stage3.py:2251:step] Full outer step loop took 4.453317880630493
x3104c0s1b0n0: [2024-03-29 15:48:43,911] [INFO] [stage3.py:2251:step] Full outer step loop took 4.460756778717041
x3104c0s25b0n0: [2024-03-29 15:48:43,936] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.485801458358765
x3104c0s25b0n0: [2024-03-29 15:48:43,966] [INFO] [stage3.py:2251:step] Full outer step loop took 4.51540207862854
x3104c0s25b0n0: [2024-03-29 15:48:43,988] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.538302898406982
x3104c0s25b0n0: [2024-03-29 15:48:43,989] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.538722276687622
x3104c0s25b0n0: [2024-03-29 15:48:43,997] [INFO] [stage3.py:2251:step] Full outer step loop took 4.547318696975708
x3104c0s25b0n0: [2024-03-29 15:48:43,998] [INFO] [stage3.py:2251:step] Full outer step loop took 4.547736883163452
x3104c0s1b0n0: [2024-03-29 15:48:44,007] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4453.48
x3104c0s1b0n0: [2024-03-29 15:48:44,007] [INFO] [stage3.py:2277:step] End to end step took 4.557198524475098
x3104c0s1b0n0: [2024-03-29 15:48:44,007] [INFO] [stage3.py:2277:step] End to end step took 4.557231664657593
x3104c0s25b0n0: [2024-03-29 15:48:44,007] [INFO] [stage3.py:2277:step] End to end step took 4.557276964187622
x3104c0s25b0n0: [2024-03-29 15:48:44,007] [INFO] [stage3.py:2277:step] End to end step took 4.55730938911438
x3104c0s1b0n0: [2024-03-29 15:48:44,007] [INFO] [stage3.py:2277:step] End to end step took 4.557121753692627
x3104c0s25b0n0: [2024-03-29 15:48:44,007] [INFO] [stage3.py:2277:step] End to end step took 4.5573577880859375
x3104c0s1b0n0: [2024-03-29 15:48:44,007] [INFO] [stage3.py:2277:step] End to end step took 4.557419776916504
x3104c0s1b0n0: [2024-03-29 15:48:44,008] [INFO] [logging.py:96:log_dist] [Rank 0] step=6, skipped=0, lr=[0.0001415574960149644], mom=[(0.9, 0.95)]
x3104c0s25b0n0: [2024-03-29 15:48:44,008] [INFO] [stage3.py:2277:step] End to end step took 4.557727575302124
x3104c0s1b0n0: [2024-03-29 15:48:44,008] [INFO] [timer.py:260:stop] epoch=0/micro_step=6/global_step=6, RunningAvgSamplesPerSec=1.1268005047559004, CurrSamplesPerSec=1.10478969340036, MemAllocated=10.26GB, MaxMemAllocated=12.98GB
x3104c0s1b0n0: [2024-03-29 15:48:44,008] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6471.19 | bwd_microstep: 17561.37 | bwd_inner_microstep: 17466.32 | bwd_allreduce_microstep: 94.97 | step_microstep: 4582.47
x3104c0s1b0n0: [2024-03-29 15:48:44,008] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6471.18 | bwd: 17561.36 | bwd_inner: 17466.32 | bwd_allreduce: 94.98 | step: 4582.48
x3104c0s1b0n0: [2024-03-29 15:48:44,107] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:48:44,107] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.98 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:48:44,107] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.29 GB, percent = 78.4%
x3104c0s1b0n0: <TIMER:interval-time,29.205909729003906><TIMER:interval-time,29.205945253372192><TIMER:interval-time,29.205944061279297>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,29.20595908164978>
x3104c0s25b0n0: <TIMER:interval-time,29.20596933364868><TIMER:interval-time,29.205973625183105>
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,29.206083059310913>
x3104c0s25b0n0: <TIMER:interval-time,29.206093549728394>
x3104c0s25b0n0:  elapsed_time 29.205969 | consumed samples:          192 | consumed tokens:       393216 | elapsed time per iteration (ms): 29206.0 | learning rate: 1.416E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.096 | TFLOPs: 75.81 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:48:44,223] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:48:44,224] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 15:48:44,224] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.3 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:48:50,737] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:48:50,738] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:48:50,738] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.29 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:48:50,818] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:48:50,818] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:48:50,819] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.29 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:49:08,645] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:49:08,646] [INFO] [utils.py:801:see_memory_usage] MA 12.98 GB         Max_MA 23.77 GB         CA 13.23 GB         Max_CA 33 GB 
x3104c0s1b0n0: [2024-03-29 15:49:08,646] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.3 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:49:08,718] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:49:08,719] [INFO] [utils.py:801:see_memory_usage] MA 12.98 GB         Max_MA 12.98 GB         CA 13.23 GB         Max_CA 13 GB 
x3104c0s1b0n0: [2024-03-29 15:49:08,719] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.3 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:49:13,188] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.445624113082886
x3104c0s25b0n0: [2024-03-29 15:49:13,202] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.459275484085083
x3104c0s25b0n0: [2024-03-29 15:49:13,206] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.462924480438232
x3104c0s1b0n0: [2024-03-29 15:49:13,220] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.476904630661011
x3104c0s1b0n0: [2024-03-29 15:49:13,225] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.482541799545288
x3104c0s1b0n0: [2024-03-29 15:49:13,229] [INFO] [stage3.py:2251:step] Full outer step loop took 4.4863951206207275
x3104c0s25b0n0: [2024-03-29 15:49:13,229] [INFO] [stage3.py:2251:step] Full outer step loop took 4.48573112487793
x3104c0s1b0n0: [2024-03-29 15:49:13,231] [INFO] [stage3.py:2251:step] Full outer step loop took 4.4882495403289795
x3104c0s1b0n0: [2024-03-29 15:49:13,238] [INFO] [stage3.py:2251:step] Full outer step loop took 4.4947381019592285
x3104c0s1b0n0: [2024-03-29 15:49:13,238] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.494654893875122
x3104c0s25b0n0: [2024-03-29 15:49:13,240] [INFO] [stage3.py:2251:step] Full outer step loop took 4.497109889984131
x3104c0s1b0n0: [2024-03-29 15:49:13,247] [INFO] [stage3.py:2251:step] Full outer step loop took 4.5036301612854
x3104c0s25b0n0: [2024-03-29 15:49:13,325] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.581451892852783
x3104c0s25b0n0: [2024-03-29 15:49:13,344] [INFO] [stage3.py:2251:step] Full outer step loop took 4.600973844528198
x3104c0s25b0n0: [2024-03-29 15:49:13,352] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.609076976776123
x3104c0s25b0n0: [2024-03-29 15:49:13,361] [INFO] [stage3.py:2251:step] Full outer step loop took 4.618049383163452
x3104c0s25b0n0: [2024-03-29 15:49:13,370] [INFO] [stage3.py:2277:step] End to end step took 4.627137184143066
x3104c0s1b0n0: [2024-03-29 15:49:13,370] [INFO] [stage3.py:2277:step] End to end step took 4.6271116733551025
x3104c0s1b0n0: [2024-03-29 15:49:13,370] [INFO] [stage3.py:2277:step] End to end step took 4.627155780792236
x3104c0s25b0n0: [2024-03-29 15:49:13,370] [INFO] [stage3.py:2277:step] End to end step took 4.627257347106934
x3104c0s1b0n0: [2024-03-29 15:49:13,370] [INFO] [stage3.py:2277:step] End to end step took 4.627315282821655
x3104c0s1b0n0: [2024-03-29 15:49:13,370] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4503.76
x3104c0s25b0n0: [2024-03-29 15:49:13,370] [INFO] [stage3.py:2277:step] End to end step took 4.627731800079346
x3104c0s25b0n0: [2024-03-29 15:49:13,370] [INFO] [stage3.py:2277:step] End to end step took 4.627774715423584
x3104c0s1b0n0: [2024-03-29 15:49:13,371] [INFO] [stage3.py:2277:step] End to end step took 4.627737760543823
x3104c0s1b0n0: [2024-03-29 15:49:13,371] [INFO] [logging.py:96:log_dist] [Rank 0] step=7, skipped=0, lr=[9.750000000000001e-05], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 15:49:13,371] [INFO] [timer.py:260:stop] epoch=0/micro_step=7/global_step=7, RunningAvgSamplesPerSec=1.1209041101361599, CurrSamplesPerSec=1.0979229803726054, MemAllocated=10.26GB, MaxMemAllocated=12.98GB
x3104c0s1b0n0: [2024-03-29 15:49:13,372] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6481.45 | bwd_microstep: 17661.91 | bwd_inner_microstep: 17567.74 | bwd_allreduce_microstep: 94.11 | step_microstep: 4652.51
x3104c0s1b0n0: [2024-03-29 15:49:13,372] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6481.44 | bwd: 17661.91 | bwd_inner: 17567.73 | bwd_allreduce: 94.12 | step: 4652.51
x3104c0s1b0n0: [2024-03-29 15:49:13,464] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:49:13,464] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.98 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:49:13,464] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.29 GB, percent = 78.4%
x3104c0s1b0n0: <TIMER:interval-time,29.356855154037476><TIMER:interval-time,29.356856107711792><TIMER:interval-time,29.35685658454895><TIMER:interval-time,29.35685658454895>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,29.35686945915222><TIMER:interval-time,29.356868743896484>
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,29.356887817382812><TIMER:interval-time,29.35689067840576>
x3104c0s25b0n0: 
x3104c0s25b0n0:  elapsed_time 29.356891 | consumed samples:          224 | consumed tokens:       458752 | elapsed time per iteration (ms): 29356.9 | learning rate: 9.750E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.090 | TFLOPs: 75.42 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:49:13,605] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:49:13,605] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 15:49:13,605] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.3 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:49:19,578] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:49:19,579] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:49:19,579] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.3 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:49:19,658] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:49:19,659] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:49:19,659] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.3 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:49:36,647] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:49:36,648] [INFO] [utils.py:801:see_memory_usage] MA 12.98 GB         Max_MA 23.77 GB         CA 13.23 GB         Max_CA 33 GB 
x3104c0s1b0n0: [2024-03-29 15:49:36,648] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.3 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:49:36,721] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:49:36,722] [INFO] [utils.py:801:see_memory_usage] MA 12.98 GB         Max_MA 12.98 GB         CA 13.23 GB         Max_CA 13 GB 
x3104c0s1b0n0: [2024-03-29 15:49:36,722] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.3 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:49:41,222] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.475016832351685
x3104c0s1b0n0: [2024-03-29 15:49:41,233] [INFO] [stage3.py:2251:step] Full outer step loop took 4.4871416091918945
x3104c0s1b0n0: [2024-03-29 15:49:41,237] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.4902663230896
x3104c0s1b0n0: [2024-03-29 15:49:41,237] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.490382194519043
x3104c0s1b0n0: [2024-03-29 15:49:41,247] [INFO] [stage3.py:2251:step] Full outer step loop took 4.500389337539673
x3104c0s1b0n0: [2024-03-29 15:49:41,260] [INFO] [stage3.py:2251:step] Full outer step loop took 4.513877630233765
x3104c0s25b0n0: [2024-03-29 15:49:41,280] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.533475637435913
x3104c0s1b0n0: [2024-03-29 15:49:41,305] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.558621168136597
x3104c0s25b0n0: [2024-03-29 15:49:41,304] [INFO] [stage3.py:2251:step] Full outer step loop took 4.557382106781006
x3104c0s1b0n0: [2024-03-29 15:49:41,314] [INFO] [stage3.py:2251:step] Full outer step loop took 4.5676140785217285
x3104c0s25b0n0: [2024-03-29 15:49:41,319] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.573083400726318
x3104c0s25b0n0: [2024-03-29 15:49:41,322] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.575935363769531
x3104c0s25b0n0: [2024-03-29 15:49:41,322] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.575979948043823
x3104c0s25b0n0: [2024-03-29 15:49:41,329] [INFO] [stage3.py:2251:step] Full outer step loop took 4.582728147506714
x3104c0s25b0n0: [2024-03-29 15:49:41,331] [INFO] [stage3.py:2251:step] Full outer step loop took 4.585015535354614
x3104c0s25b0n0: [2024-03-29 15:49:41,332] [INFO] [stage3.py:2251:step] Full outer step loop took 4.585341215133667
x3104c0s1b0n0: [2024-03-29 15:49:41,341] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4567.77
x3104c0s25b0n0: [2024-03-29 15:49:41,341] [INFO] [stage3.py:2277:step] End to end step took 4.5946831703186035
x3104c0s25b0n0: [2024-03-29 15:49:41,341] [INFO] [stage3.py:2277:step] End to end step took 4.594721555709839
x3104c0s25b0n0: [2024-03-29 15:49:41,341] [INFO] [stage3.py:2277:step] End to end step took 4.5947816371917725
x3104c0s1b0n0: [2024-03-29 15:49:41,341] [INFO] [stage3.py:2277:step] End to end step took 4.5947558879852295
x3104c0s1b0n0: [2024-03-29 15:49:41,341] [INFO] [stage3.py:2277:step] End to end step took 4.594656229019165
x3104c0s25b0n0: [2024-03-29 15:49:41,341] [INFO] [stage3.py:2277:step] End to end step took 4.594992637634277
x3104c0s1b0n0: [2024-03-29 15:49:41,341] [INFO] [stage3.py:2277:step] End to end step took 4.5950775146484375
x3104c0s1b0n0: [2024-03-29 15:49:41,341] [INFO] [logging.py:96:log_dist] [Rank 0] step=8, skipped=0, lr=[6.158400017893797e-05], mom=[(0.9, 0.95)]
x3104c0s1b0n0: [2024-03-29 15:49:41,342] [INFO] [stage3.py:2277:step] End to end step took 4.59525203704834
x3104c0s1b0n0: [2024-03-29 15:49:41,342] [INFO] [timer.py:260:stop] epoch=0/micro_step=8/global_step=8, RunningAvgSamplesPerSec=1.1262519013234809, CurrSamplesPerSec=1.1537749857933355, MemAllocated=10.26GB, MaxMemAllocated=12.98GB
x3104c0s1b0n0: [2024-03-29 15:49:41,342] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 5935.79 | bwd_microstep: 16822.43 | bwd_inner_microstep: 16737.06 | bwd_allreduce_microstep: 85.30 | step_microstep: 4619.40
x3104c0s1b0n0: [2024-03-29 15:49:41,342] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 5935.77 | bwd: 16822.43 | bwd_inner: 16737.06 | bwd_allreduce: 85.31 | step: 4619.40
x3104c0s1b0n0: [2024-03-29 15:49:41,438] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:49:41,439] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.98 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:49:41,439] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.3 GB, percent = 78.4%
x3104c0s1b0n0: <TIMER:interval-time,27.974219799041748><TIMER:interval-time,27.974223852157593><TIMER:interval-time,27.97422194480896>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,27.974233865737915>
x3104c0s25b0n0: <TIMER:interval-time,27.974252462387085><TIMER:interval-time,27.974251985549927><TIMER:interval-time,27.97425627708435>
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,27.974377870559692>
x3104c0s25b0n0:  elapsed_time 27.974252 | consumed samples:          256 | consumed tokens:       524288 | elapsed time per iteration (ms): 27974.3 | learning rate: 6.158E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.144 | TFLOPs: 79.15 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:49:41,570] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:49:41,571] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 15:49:41,571] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.32 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:49:47,325] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:49:47,326] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:49:47,326] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.32 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:49:47,402] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:49:47,403] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:49:47,403] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.32 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:50:04,323] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:50:04,324] [INFO] [utils.py:801:see_memory_usage] MA 12.98 GB         Max_MA 23.77 GB         CA 13.23 GB         Max_CA 33 GB 
x3104c0s1b0n0: [2024-03-29 15:50:04,324] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.32 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:50:04,398] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:50:04,399] [INFO] [utils.py:801:see_memory_usage] MA 12.98 GB         Max_MA 12.98 GB         CA 13.23 GB         Max_CA 13 GB 
x3104c0s1b0n0: [2024-03-29 15:50:04,399] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.32 GB, percent = 78.4%
x3104c0s25b0n0: [2024-03-29 15:50:08,671] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.247755289077759
x3104c0s25b0n0: [2024-03-29 15:50:08,696] [INFO] [stage3.py:2251:step] Full outer step loop took 4.272809743881226
x3104c0s1b0n0: [2024-03-29 15:50:08,765] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.341682195663452
x3104c0s1b0n0: [2024-03-29 15:50:08,777] [INFO] [stage3.py:2251:step] Full outer step loop took 4.35442328453064
x3104c0s1b0n0: [2024-03-29 15:50:08,827] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.404041767120361
x3104c0s1b0n0: [2024-03-29 15:50:08,838] [INFO] [stage3.py:2251:step] Full outer step loop took 4.415585041046143
x3104c0s1b0n0: [2024-03-29 15:50:08,842] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.419314384460449
x3104c0s1b0n0: [2024-03-29 15:50:08,861] [INFO] [stage3.py:2251:step] Full outer step loop took 4.4378557205200195
x3104c0s1b0n0: [2024-03-29 15:50:08,910] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.487067222595215
x3104c0s1b0n0: [2024-03-29 15:50:08,919] [INFO] [stage3.py:2251:step] Full outer step loop took 4.496051073074341
x3104c0s25b0n0: [2024-03-29 15:50:08,919] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.496119976043701
x3104c0s25b0n0: [2024-03-29 15:50:08,922] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.498706340789795
x3104c0s25b0n0: [2024-03-29 15:50:08,941] [INFO] [stage3.py:2251:step] Full outer step loop took 4.517674684524536
x3104c0s25b0n0: [2024-03-29 15:50:08,966] [INFO] [stage3.py:2251:step] Full outer step loop took 4.542747497558594
x3104c0s25b0n0: [2024-03-29 15:50:08,970] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.547498941421509
x3104c0s25b0n0: [2024-03-29 15:50:08,979] [INFO] [stage3.py:2251:step] Full outer step loop took 4.556459903717041
x3104c0s1b0n0: [2024-03-29 15:50:08,989] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4496.17
x3104c0s25b0n0: [2024-03-29 15:50:08,989] [INFO] [stage3.py:2277:step] End to end step took 4.5658721923828125
x3104c0s1b0n0: [2024-03-29 15:50:08,989] [INFO] [stage3.py:2277:step] End to end step took 4.56574559211731
x3104c0s25b0n0: [2024-03-29 15:50:08,989] [INFO] [stage3.py:2277:step] End to end step took 4.565990686416626
x3104c0s1b0n0: [2024-03-29 15:50:08,989] [INFO] [logging.py:96:log_dist] [Rank 0] step=9, skipped=0, lr=[3.814149619390238e-05], mom=[(0.9, 0.95)]
x3104c0s25b0n0: [2024-03-29 15:50:08,989] [INFO] [stage3.py:2277:step] End to end step took 4.566158771514893
x3104c0s1b0n0: [2024-03-29 15:50:08,989] [INFO] [stage3.py:2277:step] End to end step took 4.566159009933472
x3104c0s1b0n0: [2024-03-29 15:50:08,989] [INFO] [stage3.py:2277:step] End to end step took 4.566279411315918
x3104c0s1b0n0: [2024-03-29 15:50:08,989] [INFO] [stage3.py:2277:step] End to end step took 4.56632137298584
x3104c0s1b0n0: [2024-03-29 15:50:08,989] [INFO] [timer.py:260:stop] epoch=0/micro_step=9/global_step=9, RunningAvgSamplesPerSec=1.1319189524407565, CurrSamplesPerSec=1.1671561872029863, MemAllocated=10.26GB, MaxMemAllocated=12.98GB
x3104c0s1b0n0: [2024-03-29 15:50:08,990] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 5720.31 | bwd_microstep: 16756.18 | bwd_inner_microstep: 16671.18 | bwd_allreduce_microstep: 84.92 | step_microstep: 4590.46
x3104c0s1b0n0: [2024-03-29 15:50:08,990] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 5720.30 | bwd: 16756.17 | bwd_inner: 16671.18 | bwd_allreduce: 84.94 | step: 4590.47
x3104c0s25b0n0: [2024-03-29 15:50:08,990] [INFO] [stage3.py:2277:step] End to end step took 4.566859483718872
x3104c0s1b0n0: [2024-03-29 15:50:09,085] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:50:09,085] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.98 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:50:09,085] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.32 GB, percent = 78.4%
x3104c0s1b0n0: <TIMER:interval-time,27.646000146865845><TIMER:interval-time,27.646004915237427><TIMER:interval-time,27.646004915237427>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,27.646055221557617>
x3104c0s25b0n0: <TIMER:interval-time,27.646047592163086><TIMER:interval-time,27.64605164527893>
x3104c0s25b0n0: 
x3104c0s25b0n0: <TIMER:interval-time,27.646056175231934><TIMER:interval-time,27.646045923233032>
x3104c0s25b0n0: 
x3104c0s25b0n0:  elapsed_time 27.646052 | consumed samples:          288 | consumed tokens:       589824 | elapsed time per iteration (ms): 27646.1 | learning rate: 3.814E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.157 | TFLOPs: 80.09 |
x3104c0s1b0n0: In train_step in training.py!!!!!... True, False
x3104c0s1b0n0: [2024-03-29 15:50:09,223] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3104c0s1b0n0: [2024-03-29 15:50:09,223] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3104c0s1b0n0: [2024-03-29 15:50:09,223] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.33 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:50:15,594] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3104c0s1b0n0: [2024-03-29 15:50:15,595] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:50:15,595] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.32 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:50:15,673] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3104c0s1b0n0: [2024-03-29 15:50:15,674] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3104c0s1b0n0: [2024-03-29 15:50:15,674] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.32 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:50:32,565] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3104c0s1b0n0: [2024-03-29 15:50:32,565] [INFO] [utils.py:801:see_memory_usage] MA 12.98 GB         Max_MA 23.77 GB         CA 13.23 GB         Max_CA 33 GB 
x3104c0s1b0n0: [2024-03-29 15:50:32,565] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.32 GB, percent = 78.4%
x3104c0s1b0n0: [2024-03-29 15:50:32,637] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3104c0s1b0n0: [2024-03-29 15:50:32,638] [INFO] [utils.py:801:see_memory_usage] MA 12.98 GB         Max_MA 12.98 GB         CA 13.23 GB         Max_CA 13 GB 
x3104c0s1b0n0: [2024-03-29 15:50:32,638] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.32 GB, percent = 78.4%
x3104c0s25b0n0: [2024-03-29 15:50:37,075] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.413180828094482
x3104c0s1b0n0: [2024-03-29 15:50:37,074] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.411839008331299
x3104c0s1b0n0: [2024-03-29 15:50:37,084] [INFO] [stage3.py:2251:step] Full outer step loop took 4.422297716140747
x3104c0s1b0n0: [2024-03-29 15:50:37,098] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.4368157386779785
x3104c0s25b0n0: [2024-03-29 15:50:37,105] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.44305419921875
x3104c0s25b0n0: [2024-03-29 15:50:37,107] [INFO] [stage3.py:2251:step] Full outer step loop took 4.445568799972534
x3104c0s25b0n0: [2024-03-29 15:50:37,124] [INFO] [stage3.py:2251:step] Full outer step loop took 4.462826251983643
x3104c0s1b0n0: [2024-03-29 15:50:37,126] [INFO] [stage3.py:2251:step] Full outer step loop took 4.464057207107544
x3104c0s1b0n0: [2024-03-29 15:50:37,183] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.52096700668335
x3104c0s1b0n0: [2024-03-29 15:50:37,192] [INFO] [stage3.py:2251:step] Full outer step loop took 4.530459642410278
x3104c0s1b0n0: [2024-03-29 15:50:37,193] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.531428575515747
x3104c0s1b0n0: [2024-03-29 15:50:37,202] [INFO] [stage3.py:2251:step] Full outer step loop took 4.540428161621094
x3104c0s25b0n0: [2024-03-29 15:50:37,227] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.565368413925171
x3104c0s25b0n0: [2024-03-29 15:50:37,228] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.566671133041382
x3104c0s25b0n0: [2024-03-29 15:50:37,236] [INFO] [stage3.py:2251:step] Full outer step loop took 4.57437539100647
x3104c0s25b0n0: [2024-03-29 15:50:37,237] [INFO] [stage3.py:2251:step] Full outer step loop took 4.57571816444397
x3104c0s1b0n0: [2024-03-29 15:50:37,246] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4540.58
x3104c0s25b0n0: [2024-03-29 15:50:37,246] [INFO] [stage3.py:2277:step] End to end step took 4.585030555725098
x3104c0s25b0n0: [2024-03-29 15:50:37,246] [INFO] [stage3.py:2277:step] End to end step took 4.585073471069336
x3104c0s1b0n0: [2024-03-29 15:50:37,246] [INFO] [stage3.py:2277:step] End to end step took 4.585198163986206
x3104c0s1b0n0: [2024-03-29 15:50:37,247] [INFO] [stage3.py:2277:step] End to end step took 4.585216760635376
x3104c0s1b0n0: [2024-03-29 15:50:37,247] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[3e-05], mom=[(0.9, 0.95)]
x3104c0s25b0n0: [2024-03-29 15:50:37,247] [INFO] [stage3.py:2277:step] End to end step took 4.585517644882202
x3104c0s1b0n0: [2024-03-29 15:50:37,247] [INFO] [stage3.py:2277:step] End to end step took 4.585339069366455
x3104c0s1b0n0: [2024-03-29 15:50:37,247] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=1.1331619995525657, CurrSamplesPerSec=1.1419403663235184, MemAllocated=10.26GB, MaxMemAllocated=12.98GB
x3104c0s25b0n0: [2024-03-29 15:50:37,247] [INFO] [stage3.py:2277:step] End to end step took 4.585697412490845
x3104c0s1b0n0: [2024-03-29 15:50:37,247] [INFO] [stage3.py:2277:step] End to end step took 4.585801601409912
x3104c0s1b0n0: [2024-03-29 15:50:37,247] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6332.31 | bwd_microstep: 16729.29 | bwd_inner_microstep: 16644.48 | bwd_allreduce_microstep: 84.74 | step_microstep: 4609.40
x3104c0s1b0n0: [2024-03-29 15:50:37,247] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6332.28 | bwd: 16729.28 | bwd_inner: 16644.47 | bwd_allreduce: 84.75 | step: 4609.40
x3104c0s1b0n0: [2024-03-29 15:50:37,348] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3104c0s1b0n0: [2024-03-29 15:50:37,349] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.98 GB         CA 10.34 GB         Max_CA 14 GB 
x3104c0s1b0n0: [2024-03-29 15:50:37,349] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 394.32 GB, percent = 78.4%
x3104c0s1b0n0: <TIMER:interval-time,28.263269424438477><TIMER:interval-time,28.263269901275635><TIMER:interval-time,28.263280391693115>
x3104c0s1b0n0: 
x3104c0s1b0n0: 
x3104c0s1b0n0: <TIMER:interval-time,28.263296604156494>
x3104c0s25b0n0: <TIMER:interval-time,28.26330828666687><TIMER:interval-time,28.263306140899658><TIMER:interval-time,28.263309478759766><TIMER:interval-time,28.26331353187561>
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0: 
x3104c0s25b0n0:  elapsed_time 28.263308 | consumed samples:          320 | consumed tokens:       655360 | elapsed time per iteration (ms): 28263.3 | learning rate: 3.000E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.132 | TFLOPs: 78.34 |
x3104c0s25b0n0: <<<only_train:295.48023557662964>>>
x3104c0s1b0n0: <<<only_train:295.48020672798157>>>
x3104c0s25b0n0: <<<only_train:295.4803001880646>>>
x3104c0s25b0n0: <<<only_train:295.4801230430603>>>
x3104c0s25b0n0: <<<only_train:295.4802851676941>>>
x3104c0s1b0n0: <<<only_train:295.4802770614624>>>
x3104c0s1b0n0: <<<only_train:295.48010420799255>>>
x3104c0s1b0n0: <<<only_train:295.48008918762207>>>
x3104c0s1b0n0: [after training ends] datetime: 2024-03-29 15:50:37 
x3104c0s1b0n0: <<<full_time:295.48048734664917>>>
x3104c0s1b0n0: <<<full_time:295.48030066490173>>><<<full_time:295.4804844856262>>>
x3104c0s1b0n0: 
x3104c0s1b0n0: <<<full_time:295.4802770614624>>>
x3104c0s25b0n0: <<<full_time:295.4805235862732>>><<<full_time:295.4805727005005>>>
x3104c0s25b0n0: 
x3104c0s25b0n0: <<<full_time:295.4803931713104>>>
x3104c0s25b0n0: <<<full_time:295.4805476665497>>>
x3104c0s25b0n0: [2024-03-29 15:50:45,980] [INFO] [launch.py:348:main] Process 16376 exits successfully.
x3104c0s1b0n0: [2024-03-29 15:50:46,213] [INFO] [launch.py:348:main] Process 53498 exits successfully.
x3104c0s25b0n0: [2024-03-29 15:50:53,988] [INFO] [launch.py:348:main] Process 16378 exits successfully.
x3104c0s25b0n0: [2024-03-29 15:50:53,988] [INFO] [launch.py:348:main] Process 16377 exits successfully.
x3104c0s25b0n0: [2024-03-29 15:50:53,989] [INFO] [launch.py:348:main] Process 16375 exits successfully.
x3104c0s1b0n0: [2024-03-29 15:50:54,222] [INFO] [launch.py:348:main] Process 53499 exits successfully.
x3104c0s1b0n0: [2024-03-29 15:50:54,222] [INFO] [launch.py:348:main] Process 53497 exits successfully.
x3104c0s1b0n0: [2024-03-29 15:50:54,222] [INFO] [launch.py:348:main] Process 53500 exits successfully.

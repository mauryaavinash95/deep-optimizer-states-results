[2024-03-28 13:51:16,797] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-28 13:51:19,901] [INFO] [runner.py:463:main] Using IP address of 10.140.57.107 for node x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-28 13:51:19,903] [INFO] [runner.py:557:main] deepspeed_env file = ./.deepspeed_env
[2024-03-28 13:51:19,903] [INFO] [multinode_runner.py:80:get_cmd] Running on the following workers: x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov,x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-28 13:51:19,903] [INFO] [runner.py:568:main] cmd = pdsh -S -f 1024 -w x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov,x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov export PYTHONUSERBASE=/home/am6429/.local/polaris/conda/2023-10-04; export PYTHONPATH=/home/am6429/dl-io/Megatron-DeepSpeed; export PATH=/home/am6429/.conda/envs/dspeed_env/bin:/soft/datascience/conda/2023-10-04/mconda3/condabin:/soft/compilers/cudatoolkit/cuda-11.8.0/bin:/soft/buildtools/cmake/cmake-3.23.2/cmake-3.23.2-linux-x86_64/bin:/opt/cray/pe/gcc/11.2.0/bin:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/include:/opt/cray/pe/pals/1.2.11/bin:/opt/cray/pe/craype/2.7.20/bin:/opt/cray/libfabric/1.15.2.0/bin:/home/am6429/.conda/envs/dspeed_env/bin:/opt/cray/pe/perftools/23.03.0/bin:/opt/cray/pe/papi/7.0.0.1/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/am6429/.local/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/am6429/veloc-build/include:/home/am6429/veloc-build/bin:/soft/datascience/conda/2023-01-10/mconda3/include:/opt/cray/pe/bin:/soft/datascience/conda/2023-01-10/mconda3/include; export LD_LIBRARY_PATH=/usr/lib64/:/soft/compilers/cudatoolkit/cuda-11.8.0/lib64:/opt/cray/pe/gcc/11.2.0/snos/lib64:/soft/compilers/cudatoolkit/cuda-11.8.0/extras/CUPTI/lib64:/soft/libraries/trt/TensorRT-8.5.3.1.Linux.x86_64-gnu.cuda-11.8.cudnn8.6/lib:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/lib:/soft/libraries/cudnn/cudnn-11-linux-x64-v8.6.0.163/lib:/opt/cray/libfabric/1.15.2.0/lib64:/usr/lib64:/opt/cray/pe/papi/7.0.0.1/lib64:/home/am6429/veloc-build/lib:/home/am6429/veloc-build/lib64:/home/am6429/nvcomp/lib:/soft/datascience/conda/2023-01-10/mconda3/lib:/soft/datascience/conda/2023-01-10/mconda3/lib/; export http_proxy=http://proxy.alcf.anl.gov:3128; export https_proxy=http://proxy.alcf.anl.gov:3128; export CC=gcc; export CXX=g++; export IBV_FORK_SAFE=1; export CFLAGS=-I/soft/datascience/conda/2023-01-10/mconda3/include/; export LDFLAGS=-L/soft/datascience/conda/2023-01-10/mconda3/lib/; export CUDA_DEVICE_MAX_CONNECTIONS=1; export TORCHSNAPSHOT_PER_RANK_MEMORY_BUDGET_BYTES=34359738368; export _DEFAULT_MAX_PER_RANK_IO_CONCURRENCY=1; export _MAX_PER_RANK_IO_CONCURRENCY=1; export NSYS_REPORT_DIR=/home/am6429/dl-io/dl-io-outputs/act-output-30B//rep-30B-tp1-dp8-l60-h6656-a52-sl2048-gbs64-mbs4-ratio1-subg100000000-prefetch0-flush_async0-opt_gaps5-%n;  cd /home/am6429/dl-io/Megatron-DeepSpeed; /home/am6429/.conda/envs/dspeed_env/bin/python -u -m deepspeed.launcher.launch --world_info=eyJ4MzAwNmMwczE5YjFuMC5oc24uY20ucG9sYXJpcy5hbGNmLmFubC5nb3YiOiBbMCwgMSwgMiwgM10sICJ4MzAwNmMwczFiMG4wLmhzbi5jbS5wb2xhcmlzLmFsY2YuYW5sLmdvdiI6IFswLCAxLCAyLCAzXX0= --node_rank=%n --master_addr=10.140.57.107 --master_port=29700 /home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py --tensor-model-parallel-size 1 --num-layers 60 --hidden-size 6656 --num-attention-heads 52 --micro-batch-size 4 --global-batch-size 64 --seq-length 2048 --max-position-embeddings 2048 --train-iters 10 --save /local/scratch/llama2/zero3-tp1}_dp8 --data-path /home/am6429/dl-io/datasets/meg-gpt2_text_document --vocab-file /home/am6429/dl-io/datasets/gpt2-vocab.json --merge-file /home/am6429/dl-io/datasets/gpt2-merges.txt --data-impl mmap --tokenizer-type GPTSentencePieceTokenizer --tokenizer-model /home/am6429/dl-io/datasets/tokenizer.model --split 949,50,1 --distributed-backend nccl --lr 3e-4 --lr-decay-style cosine --min-lr 3e-5 --weight-decay 0.1 --clip-grad 1 --lr-warmup-iters 1 --optimizer adam --adam-beta1 0.9 --adam-beta2 0.95 --log-interval 1 --save-interval 1000 --eval-interval 1000 --eval-iters 0 --bf16 --no-query-key-layer-scaling --attention-dropout 0 --hidden-dropout 0 --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization rmsnorm --disable-bias-linear --num-key-value-heads 4 --deepspeed --exit-interval 20 --deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json --zero-stage=3 --no-pipeline-parallel --cpu-optimizer --checkpoint-activations --deepspeed-activation-checkpointing
x3006c0s1b0n0: [2024-03-28 13:51:21,910] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-28 13:51:21,923] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-28 13:51:23,790] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3006c0s19b1n0: [2024-03-28 13:51:23,790] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=0
x3006c0s19b1n0: [2024-03-28 13:51:23,790] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3006c0s19b1n0: [2024-03-28 13:51:23,790] [INFO] [launch.py:163:main] dist_world_size=8
x3006c0s19b1n0: [2024-03-28 13:51:23,790] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3006c0s19b1n0: [2024-03-28 13:51:23,791] [INFO] [launch.py:253:main] process 23253 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-28 13:51:23,792] [INFO] [launch.py:253:main] process 23254 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-28 13:51:23,792] [INFO] [launch.py:253:main] process 23255 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-28 13:51:23,793] [INFO] [launch.py:253:main] process 23256 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b0n0: [2024-03-28 13:51:24,273] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3006c0s1b0n0: [2024-03-28 13:51:24,273] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=1
x3006c0s1b0n0: [2024-03-28 13:51:24,273] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3006c0s1b0n0: [2024-03-28 13:51:24,273] [INFO] [launch.py:163:main] dist_world_size=8
x3006c0s1b0n0: [2024-03-28 13:51:24,273] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3006c0s1b0n0: [2024-03-28 13:51:24,273] [INFO] [launch.py:253:main] process 14851 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b0n0: [2024-03-28 13:51:24,274] [INFO] [launch.py:253:main] process 14852 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b0n0: [2024-03-28 13:51:24,275] [INFO] [launch.py:253:main] process 14853 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b0n0: [2024-03-28 13:51:24,275] [INFO] [launch.py:253:main] process 14854 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-28 13:51:25,588] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-28 13:51:25,590] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-28 13:51:25,608] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-28 13:51:25,615] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b0n0: [2024-03-28 13:51:26,075] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b0n0: [2024-03-28 13:51:26,090] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b0n0: [2024-03-28 13:51:26,107] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b0n0: [2024-03-28 13:51:26,109] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: [2024-03-28 13:51:28,557] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: INFO: overriding default arguments for tokenizer_type:GPTSentencePieceTokenizer                    with tokenizer_type:GPT2BPETokenizer
x3006c0s19b1n0: using world size: 8, data-parallel-size: 8, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
x3006c0s19b1n0: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
x3006c0s19b1n0: using torch.bfloat16 for parameters ...
x3006c0s19b1n0: ------------------------ arguments ------------------------
x3006c0s19b1n0:   accumulate_allreduce_grads_in_fp32 .............. True
x3006c0s19b1n0:   adam_beta1 ...................................... 0.9
x3006c0s19b1n0:   adam_beta2 ...................................... 0.95
x3006c0s19b1n0:   adam_eps ........................................ 1e-08
x3006c0s19b1n0:   add_bias_linear ................................. False
x3006c0s19b1n0:   add_position_embedding .......................... False
x3006c0s19b1n0:   adlr_autoresume ................................. False
x3006c0s19b1n0:   adlr_autoresume_interval ........................ 1000
x3006c0s19b1n0:   aml_data_download_path .......................... None
x3006c0s19b1n0:   apply_layernorm_1p .............................. False
x3006c0s19b1n0:   apply_query_key_layer_scaling ................... False
x3006c0s19b1n0:   apply_residual_connection_post_layernorm ........ False
x3006c0s19b1n0:   async_tensor_model_parallel_allreduce ........... False
x3006c0s19b1n0:   attention_dropout ............................... 0.0
x3006c0s19b1n0:   attention_softmax_in_fp32 ....................... False
x3006c0s19b1n0:   barrier_with_L1_time ............................ True
x3006c0s19b1n0:   bert_binary_head ................................ True
x3006c0s19b1n0:   bert_embedder_type .............................. megatron
x3006c0s19b1n0:   bert_load ....................................... None
x3006c0s19b1n0:   bf16 ............................................ True
x3006c0s19b1n0:   bias_dropout_fusion ............................. True
x3006c0s19b1n0:   bias_gelu_fusion ................................ False
x3006c0s19b1n0:   biencoder_projection_dim ........................ 0
x3006c0s19b1n0:   biencoder_shared_query_context_model ............ False
x3006c0s19b1n0:   block_data_path ................................. None
x3006c0s19b1n0:   checkpoint_activations .......................... True
x3006c0s19b1n0:   checkpoint_in_cpu ............................... False
x3006c0s19b1n0:   checkpoint_num_layers ........................... 1
x3006c0s19b1n0:   classes_fraction ................................ 1.0
x3006c0s19b1n0:   clip_grad ....................................... 1.0
x3006c0s19b1n0:   compression_training ............................ False
x3006c0s19b1n0:   consumed_train_samples .......................... 0
x3006c0s19b1n0:   consumed_train_tokens ........................... 0
x3006c0s19b1n0:   consumed_valid_samples .......................... 0
x3006c0s19b1n0:   contigious_checkpointing ........................ False
x3006c0s19b1n0:   cpu_optimizer ................................... True
x3006c0s19b1n0:   cpu_torch_adam .................................. False
x3006c0s19b1n0:   create_moe_param_group .......................... False
x3006c0s19b1n0:   curriculum_learning_legacy ...................... False
x3006c0s19b1n0:   data_cache_path ................................. None
x3006c0s19b1n0:   data_efficiency_curriculum_learning ............. False
x3006c0s19b1n0:   data_impl ....................................... mmap
x3006c0s19b1n0:   data_parallel_random_init ....................... False
x3006c0s19b1n0:   data_parallel_size .............................. 8
x3006c0s19b1n0:   data_path ....................................... ['/home/am6429/dl-io/datasets/meg-gpt2_text_document']
x3006c0s19b1n0:   data_per_class_fraction ......................... 1.0
x3006c0s19b1n0:   data_sharding ................................... True
x3006c0s19b1n0:   dataloader_type ................................. single
x3006c0s19b1n0:   DDP_impl ........................................ local
x3006c0s19b1n0:   decoder_num_layers .............................. None
x3006c0s19b1n0:   decoder_seq_length .............................. None
x3006c0s19b1n0:   deepscale ....................................... False
x3006c0s19b1n0:   deepscale_config ................................ None
x3006c0s19b1n0:   deepspeed ....................................... True
x3006c0s19b1n0:   deepspeed_activation_checkpointing .............. True
x3006c0s19b1n0:   deepspeed_config ................................ /home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json
x3006c0s19b1n0:   dino_bottleneck_size ............................ 256
x3006c0s19b1n0:   dino_freeze_last_layer .......................... 1
x3006c0s19b1n0:   dino_head_hidden_size ........................... 2048
x3006c0s19b1n0:   dino_local_crops_number ......................... 10
x3006c0s19b1n0:   dino_local_img_size ............................. 96
x3006c0s19b1n0:   dino_norm_last_layer ............................ False
x3006c0s19b1n0:   dino_teacher_temp ............................... 0.07
x3006c0s19b1n0:   dino_warmup_teacher_temp ........................ 0.04
x3006c0s19b1n0:   dino_warmup_teacher_temp_epochs ................. 30
x3006c0s19b1n0:   distribute_checkpointed_activations ............. False
x3006c0s19b1n0:   distribute_saved_activations .................... False
x3006c0s19b1n0:   distributed_backend ............................. nccl
x3006c0s19b1n0:   distributed_timeout_minutes ..................... 10
x3006c0s19b1n0:   ds_inference .................................... False
x3006c0s19b1n0:   ds_pipeline_enabled ............................. False
x3006c0s19b1n0:   ds_sequence_parallel_size ....................... 1
x3006c0s19b1n0:   embedding_path .................................. None
x3006c0s19b1n0:   embedding_weights_in_fp32 ....................... False
x3006c0s19b1n0:   empty_unused_memory_level ....................... 0
x3006c0s19b1n0:   enable_expert_tensor_parallelism ................ False
x3006c0s19b1n0:   encoder_num_layers .............................. 60
x3006c0s19b1n0:   encoder_seq_length .............................. 2048
x3006c0s19b1n0:   end_weight_decay ................................ 0.1
x3006c0s19b1n0:   eod_mask_loss ................................... False
x3006c0s19b1n0:   eval_interval ................................... 1000
x3006c0s19b1n0:   eval_iters ...................................... 0
x3006c0s19b1n0:   evidence_data_path .............................. None
x3006c0s19b1n0:   exit_duration_in_mins ........................... None
x3006c0s19b1n0:   exit_interval ................................... 20
x3006c0s19b1n0:   exit_on_missing_checkpoint ...................... False
x3006c0s19b1n0:   exit_signal_handler ............................. False
x3006c0s19b1n0:   expert_interval ................................. 2
x3006c0s19b1n0:   ffn_hidden_size ................................. 17728
x3006c0s19b1n0:   finetune ........................................ False
x3006c0s19b1n0:   force_ds_sequence_parallel ...................... False
x3006c0s19b1n0:   fp16 ............................................ False
x3006c0s19b1n0:   fp16_lm_cross_entropy ........................... False
x3006c0s19b1n0:   fp32_residual_connection ........................ False
x3006c0s19b1n0:   fp8_amax_compute_algo ........................... most_recent
x3006c0s19b1n0:   fp8_amax_history_len ............................ 1
x3006c0s19b1n0:   fp8_e4m3 ........................................ False
x3006c0s19b1n0:   fp8_hybrid ...................................... False
x3006c0s19b1n0:   fp8_interval .................................... 1
x3006c0s19b1n0:   fp8_margin ...................................... 0
x3006c0s19b1n0:   fp8_wgrad ....................................... True
x3006c0s19b1n0:   global_batch_size ............................... 64
x3006c0s19b1n0:   gradient_accumulation_fusion .................... True
x3006c0s19b1n0:   head_lr_mult .................................... 1.0
x3006c0s19b1n0:   hidden_dropout .................................. 0.0
x3006c0s19b1n0:   hidden_size ..................................... 6656
x3006c0s19b1n0:   hidden_size_teacher ............................. None
x3006c0s19b1n0:   hysteresis ...................................... 2
x3006c0s19b1n0:   ict_head_size ................................... None
x3006c0s19b1n0:   ict_load ........................................ None
x3006c0s19b1n0:   img_h ........................................... 224
x3006c0s19b1n0:   img_w ........................................... 224
x3006c0s19b1n0:   indexer_batch_size .............................. 128
x3006c0s19b1n0:   indexer_log_interval ............................ 1000
x3006c0s19b1n0:   inference ....................................... False
x3006c0s19b1n0:   inference_batch_times_seqlen_threshold .......... 512
x3006c0s19b1n0:   init_method_std ................................. 0.02
x3006c0s19b1n0:   init_method_xavier_uniform ...................... False
x3006c0s19b1n0:   initial_loss_scale .............................. 4294967296
x3006c0s19b1n0:   iter_per_epoch .................................. 1250
x3006c0s19b1n0:   kd .............................................. False
x3006c0s19b1n0:   kd_alpha_ce ..................................... 1
x3006c0s19b1n0:   kd_beta_ce ...................................... 1
x3006c0s19b1n0:   kd_temp ......................................... 1.0
x3006c0s19b1n0:   kv_channels ..................................... 128
x3006c0s19b1n0:   layernorm_epsilon ............................... 1e-05
x3006c0s19b1n0:   lazy_mpu_init ................................... None
x3006c0s19b1n0:   load ............................................ None
x3006c0s19b1n0:   load_teacher .................................... None
x3006c0s19b1n0:   local_rank ...................................... 0
x3006c0s19b1n0:   log_batch_size_to_tensorboard ................... False
x3006c0s19b1n0:   log_interval .................................... 1
x3006c0s19b1n0:   log_learning_rate_to_tensorboard ................ True
x3006c0s19b1n0:   log_loss_scale_to_tensorboard ................... True
x3006c0s19b1n0:   log_memory_to_tensorboard ....................... False
x3006c0s19b1n0:   log_num_zeros_in_grad ........................... False
x3006c0s19b1n0:   log_optimizer_states_to_tensorboard ............. False
x3006c0s19b1n0:   log_params_norm ................................. False
x3006c0s19b1n0:   log_timers_to_tensorboard ....................... False
x3006c0s19b1n0:   log_validation_ppl_to_tensorboard ............... False
x3006c0s19b1n0:   log_world_size_to_tensorboard ................... False
x3006c0s19b1n0:   loss_scale ...................................... None
x3006c0s19b1n0:   loss_scale_window ............................... 1000
x3006c0s19b1n0:   lr .............................................. 0.0003
x3006c0s19b1n0:   lr_decay_iters .................................. None
x3006c0s19b1n0:   lr_decay_samples ................................ None
x3006c0s19b1n0:   lr_decay_style .................................. cosine
x3006c0s19b1n0:   lr_decay_tokens ................................. None
x3006c0s19b1n0:   lr_warmup_fraction .............................. None
x3006c0s19b1n0:   lr_warmup_iters ................................. 1
x3006c0s19b1n0:   lr_warmup_samples ............................... 0
x3006c0s19b1n0:   lr_warmup_tokens ................................ None
x3006c0s19b1n0:   make_vocab_size_divisible_by .................... 128
x3006c0s19b1n0:   mask_factor ..................................... 1.0
x3006c0s19b1n0:   mask_prob ....................................... 0.15
x3006c0s19b1n0:   mask_type ....................................... random
x3006c0s19b1n0:   masked_softmax_fusion ........................... True
x3006c0s19b1n0:   max_position_embeddings ......................... 2048
x3006c0s19b1n0:   max_tokens_to_oom ............................... 12000
x3006c0s19b1n0:   memory_centric_tiled_linear ..................... False
x3006c0s19b1n0:   merge_file ...................................... /home/am6429/dl-io/datasets/gpt2-merges.txt
x3006c0s19b1n0:   micro_batch_size ................................ 4
x3006c0s19b1n0:   min_loss_scale .................................. 1.0
x3006c0s19b1n0:   min_lr .......................................... 3e-05
x3006c0s19b1n0:   mlp_type ........................................ standard
x3006c0s19b1n0:   mmap_warmup ..................................... False
x3006c0s19b1n0:   moe_eval_capacity_factor ........................ 1.0
x3006c0s19b1n0:   moe_expert_parallel_size ........................ 1
x3006c0s19b1n0:   moe_loss_coeff .................................. 0.1
x3006c0s19b1n0:   moe_min_capacity ................................ 4
x3006c0s19b1n0:   moe_token_dropping .............................. True
x3006c0s19b1n0:   moe_train_capacity_factor ....................... 1.0
x3006c0s19b1n0:   mos ............................................. False
x3006c0s19b1n0:   no_load_lr_state ................................ False
x3006c0s19b1n0:   no_load_optim ................................... None
x3006c0s19b1n0:   no_load_rng ..................................... None
x3006c0s19b1n0:   no_persist_layer_norm ........................... False
x3006c0s19b1n0:   no_pipeline_parallel ............................ True
x3006c0s19b1n0:   no_save_optim ................................... None
x3006c0s19b1n0:   no_save_rng ..................................... None
x3006c0s19b1n0:   normalization ................................... rmsnorm
x3006c0s19b1n0:   num_attention_heads ............................. 52
x3006c0s19b1n0:   num_attention_heads_teacher ..................... None
x3006c0s19b1n0:   num_channels .................................... 3
x3006c0s19b1n0:   num_classes ..................................... 1000
x3006c0s19b1n0:   num_experts ..................................... [1]
x3006c0s19b1n0:   num_experts_switch .............................. None
x3006c0s19b1n0:   num_experts_teacher ............................. [1]
x3006c0s19b1n0:   num_key_value_heads ............................. 4
x3006c0s19b1n0:   num_layers ...................................... 60
x3006c0s19b1n0:   num_layers_per_virtual_pipeline_stage ........... None
x3006c0s19b1n0:   num_layers_teacher .............................. None
x3006c0s19b1n0:   num_workers ..................................... 2
x3006c0s19b1n0:   onnx_safe ....................................... None
x3006c0s19b1n0:   openai_gelu ..................................... False
x3006c0s19b1n0:   optimizer ....................................... adam
x3006c0s19b1n0:   output_bert_embeddings .......................... False
x3006c0s19b1n0:   overlap_p2p_comm ................................ False
x3006c0s19b1n0:   override_opt_param_scheduler .................... False
x3006c0s19b1n0:   params_dtype .................................... torch.bfloat16
x3006c0s19b1n0:   partition_activations ........................... False
x3006c0s19b1n0:   patch_dim ....................................... 16
x3006c0s19b1n0:   perform_initialization .......................... True
x3006c0s19b1n0:   pipeline_model_parallel_size .................... 1
x3006c0s19b1n0:   pipeline_model_parallel_split_rank .............. None
x3006c0s19b1n0:   profile_backward ................................ False
x3006c0s19b1n0:   query_in_block_prob ............................. 0.1
x3006c0s19b1n0:   rampup_batch_size ............................... None
x3006c0s19b1n0:   random_ltd ...................................... False
x3006c0s19b1n0:   rank ............................................ 0
x3006c0s19b1n0:   recompute_granularity ........................... None
x3006c0s19b1n0:   recompute_method ................................ None
x3006c0s19b1n0:   recompute_num_layers ............................ 1
x3006c0s19b1n0:   remote_device ................................... none
x3006c0s19b1n0:   reset_attention_mask ............................ False
x3006c0s19b1n0:   reset_iteration ................................. False
x3006c0s19b1n0:   reset_position_ids .............................. False
x3006c0s19b1n0:   retriever_report_topk_accuracies ................ []
x3006c0s19b1n0:   retriever_score_scaling ......................... False
x3006c0s19b1n0:   retriever_seq_length ............................ 256
x3006c0s19b1n0:   retro_add_retriever ............................. False
x3006c0s19b1n0:   retro_cyclic_train_iters ........................ None
x3006c0s19b1n0:   retro_encoder_attention_dropout ................. 0.1
x3006c0s19b1n0:   retro_encoder_hidden_dropout .................... 0.1
x3006c0s19b1n0:   retro_encoder_layers ............................ 2
x3006c0s19b1n0:   retro_num_neighbors ............................. 2
x3006c0s19b1n0:   retro_num_retrieved_chunks ...................... 2
x3006c0s19b1n0:   retro_return_doc_ids ............................ False
x3006c0s19b1n0:   retro_workdir ................................... None
x3006c0s19b1n0:   return_data_index ............................... False
x3006c0s19b1n0:   rotary_percent .................................. 1.0
x3006c0s19b1n0:   sample_rate ..................................... 1.0
x3006c0s19b1n0:   save ............................................ /local/scratch/llama2/zero3-tp1}_dp8
x3006c0s19b1n0:   save_interval ................................... 1000
x3006c0s19b1n0:   scatter_gather_tensors_in_pipeline .............. True
x3006c0s19b1n0:   scattered_embeddings ............................ False
x3006c0s19b1n0:   seed ............................................ 1234
x3006c0s19b1n0:   seq_length ...................................... 2048
x3006c0s19b1n0:   sequence_parallel ............................... False
x3006c0s19b1n0:   sgd_momentum .................................... 0.9
x3006c0s19b1n0:   short_seq_prob .................................. 0.1
x3006c0s19b1n0:   skip_train ...................................... False
x3006c0s19b1n0:   split ........................................... 949,50,1
x3006c0s19b1n0:   split_transformers .............................. False
x3006c0s19b1n0:   squared_relu .................................... False
x3006c0s19b1n0:   standalone_embedding_stage ...................... False
x3006c0s19b1n0:   start_weight_decay .............................. 0.1
x3006c0s19b1n0:   swiglu .......................................... True
x3006c0s19b1n0:   swin_backbone_type .............................. tiny
x3006c0s19b1n0:   synchronize_each_layer .......................... False
x3006c0s19b1n0:   tensor_model_parallel_size ...................... 1
x3006c0s19b1n0:   tensorboard_dir ................................. None
x3006c0s19b1n0:   tensorboard_log_interval ........................ 1
x3006c0s19b1n0:   tensorboard_queue_size .......................... 1000
x3006c0s19b1n0:   test_data_path .................................. None
x3006c0s19b1n0:   tile_factor ..................................... 1
x3006c0s19b1n0:   timing_log_level ................................ 0
x3006c0s19b1n0:   timing_log_option ............................... minmax
x3006c0s19b1n0:   titles_data_path ................................ None
x3006c0s19b1n0:   tokenizer_model ................................. /home/am6429/dl-io/datasets/tokenizer.model
x3006c0s19b1n0:   tokenizer_type .................................. GPT2BPETokenizer
x3006c0s19b1n0:   topk ............................................ 1
x3006c0s19b1n0:   train_data_exact_num_epochs ..................... None
x3006c0s19b1n0:   train_data_path ................................. None
x3006c0s19b1n0:   train_desc_path ................................. None
x3006c0s19b1n0:   train_doc_idx_path .............................. None
x3006c0s19b1n0:   train_idx_path .................................. None
x3006c0s19b1n0:   train_iters ..................................... 10
x3006c0s19b1n0:   train_sample_idx_path ........................... None
x3006c0s19b1n0:   train_samples ................................... None
x3006c0s19b1n0:   train_shuffle_idx_path .......................... None
x3006c0s19b1n0:   train_tokens .................................... None
x3006c0s19b1n0:   transformer_impl ................................ local
x3006c0s19b1n0:   transformer_pipeline_model_parallel_size ........ 1
x3006c0s19b1n0:   untie_embeddings_and_output_weights ............. True
x3006c0s19b1n0:   use_checkpoint_args ............................. False
x3006c0s19b1n0:   use_checkpoint_opt_param_scheduler .............. False
x3006c0s19b1n0:   use_contiguous_buffers_in_local_ddp ............. True
x3006c0s19b1n0:   use_cpu_initialization .......................... None
x3006c0s19b1n0:   use_dataset_only ................................ False
x3006c0s19b1n0:   use_distributed_optimizer ....................... False
x3006c0s19b1n0:   use_flash_attn .................................. False
x3006c0s19b1n0:   use_flash_attn_triton ........................... False
x3006c0s19b1n0:   use_flash_attn_v1 ............................... False
x3006c0s19b1n0:   use_flash_attn_v2 ............................... False
x3006c0s19b1n0:   use_one_sent_docs ............................... False
x3006c0s19b1n0:   use_pin_memory .................................. False
x3006c0s19b1n0:   use_ring_exchange_p2p ........................... False
x3006c0s19b1n0:   use_rotary_position_embeddings .................. True
x3006c0s19b1n0:   use_tutel ....................................... False
x3006c0s19b1n0:   valid_data_path ................................. None
x3006c0s19b1n0:   variable_seq_lengths ............................ False
x3006c0s19b1n0:   virtual_pipeline_model_parallel_size ............ None
x3006c0s19b1n0:   vision_backbone_type ............................ vit
x3006c0s19b1n0:   vision_pretraining .............................. False
x3006c0s19b1n0:   vision_pretraining_type ......................... classify
x3006c0s19b1n0:   vocab_extra_ids ................................. 0
x3006c0s19b1n0:   vocab_file ...................................... /home/am6429/dl-io/datasets/gpt2-vocab.json
x3006c0s19b1n0:   vocab_size ...................................... None
x3006c0s19b1n0:   weight_decay .................................... 0.1
x3006c0s19b1n0:   weight_decay_incr_style ......................... constant
x3006c0s19b1n0:   world_size ...................................... 8
x3006c0s19b1n0:   zero_allgather_bucket_size ...................... 0.0
x3006c0s19b1n0:   zero_contigious_gradients ....................... False
x3006c0s19b1n0:   zero_reduce_bucket_size ......................... 0.0
x3006c0s19b1n0:   zero_reduce_scatter ............................. False
x3006c0s19b1n0:   zero_stage ...................................... 3
x3006c0s19b1n0: -------------------- end of arguments ---------------------
x3006c0s19b1n0: setting number of micro-batches to constant 2
x3006c0s19b1n0: > building GPT2BPETokenizer tokenizer ...
x3006c0s19b1n0: [2024-03-28 13:51:28,591] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
x3006c0s19b1n0: > initializing torch distributed ...
x3006c0s19b1n0: [2024-03-28 13:51:28,601] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: [2024-03-28 13:51:28,601] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
x3006c0s19b1n0: [2024-03-28 13:51:28,602] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b0n0:       meet the required dependencies to JIT install the op.
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: JIT compiled ops requires ninja
x3006c0s1b0n0: ninja .................. [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: op name ................ installed .. compatible
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b0n0:       meet the required dependencies to JIT install the op.
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: JIT compiled ops requires ninja
x3006c0s1b0n0: ninja .................. [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: op name ................ installed .. compatible
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b0n0:       meet the required dependencies to JIT install the op.
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: JIT compiled ops requires ninja
x3006c0s1b0n0: ninja .................. [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: op name ................ installed .. compatible
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b0n0:       meet the required dependencies to JIT install the op.
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: JIT compiled ops requires ninja
x3006c0s1b0n0: ninja .................. [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: op name ................ installed .. compatible
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed general environment info:
x3006c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b0n0: torch version .................... 2.0.1+cu118
x3006c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b0n0: torch cuda version ............... 11.8
x3006c0s1b0n0: torch hip version ................ None
x3006c0s1b0n0: nvcc version ..................... 11.8
x3006c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: DeepSpeed general environment info:
x3006c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b0n0: torch version .................... 2.0.1+cu118
x3006c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b0n0: torch cuda version ............... 11.8
x3006c0s1b0n0: torch hip version ................ None
x3006c0s1b0n0: nvcc version ..................... 11.8
x3006c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed general environment info:
x3006c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b0n0: torch version .................... 2.0.1+cu118
x3006c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b0n0: torch cuda version ............... 11.8
x3006c0s1b0n0: torch hip version ................ None
x3006c0s1b0n0: nvcc version ..................... 11.8
x3006c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed general environment info:
x3006c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b0n0: torch version .................... 2.0.1+cu118
x3006c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b0n0: torch cuda version ............... 11.8
x3006c0s1b0n0: torch hip version ................ None
x3006c0s1b0n0: nvcc version ..................... 11.8
x3006c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b0n0: [2024-03-28 13:51:29,051] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b0n0: [2024-03-28 13:51:29,054] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b0n0: [2024-03-28 13:51:29,067] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b0n0: [2024-03-28 13:51:29,100] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: > initialized tensor model parallel with size 1
x3006c0s19b1n0: > initialized pipeline model parallel with size 1
x3006c0s19b1n0: > setting random seeds to 1234 ...
x3006c0s19b1n0: [2024-03-28 13:51:30,046] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
x3006c0s19b1n0: > compiling dataset index builder ...
x3006c0s19b1n0: make: Entering directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3006c0s19b1n0: make: Nothing to be done for 'default'.
x3006c0s19b1n0: make: Leaving directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3006c0s19b1n0: >>> done with dataset index builder. Compilation time: 0.087 seconds
x3006c0s19b1n0: > compiling and loading fused kernels ...
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: >>> done with compiling and loading fused kernels. Compilation time: 3.466 seconds
x3006c0s19b1n0: initialize_megatron took 5.834956884384155
x3006c0s19b1n0: <<<<<<<<<<< 0
x3006c0s1b0n0: <<<<<<<<<<< 5
x3006c0s1b0n0: <<<<<<<<<<< 4
x3006c0s19b1n0: <<<<<<<<<<< 3
x3006c0s19b1n0: <<<<<<<<<<< 1
x3006c0s19b1n0: <<<<<<<<<<< 2
x3006c0s1b0n0: <<<<<<<<<<< 7
x3006c0s1b0n0: <<<<<<<<<<< 6
x3006c0s19b1n0: time to initialize megatron (seconds): 6.169
x3006c0s19b1n0: [after megatron is initialized] datetime: 2024-03-28 13:51:34 
x3006c0s19b1n0: get_accelerator and all_reduce  took 0.007801532745361328
x3006c0s19b1n0: building GPT model ...
x3006c0s19b1n0: [2024-03-28 13:51:34,468] [INFO] [utils.py:800:see_memory_usage] Before Building Model
x3006c0s19b1n0: [2024-03-28 13:51:34,468] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 4.62 GB         CA 0.0 GB         Max_CA 5 GB 
x3006c0s19b1n0: [2024-03-28 13:51:34,468] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 21.62 GB, percent = 4.3%
x3006c0s19b1n0: [2024-03-28 13:51:40,873] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 423, num_elems = 27.64B
x3006c0s19b1n0: [2024-03-28 13:51:40,942] [INFO] [utils.py:800:see_memory_usage] After Building Model
x3006c0s19b1n0: [2024-03-28 13:51:40,943] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 7.07 GB         CA 21.86 GB         Max_CA 38 GB 
x3006c0s19b1n0: [2024-03-28 13:51:40,943] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 22.07 GB, percent = 4.4%
x3006c0s19b1n0:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 27635239424
x3006c0s1b0n0: ninja: no work to do.
x3006c0s1b0n0: Time to load cpu_adam op: 2.4433178901672363 seconds
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: Time to load cpu_adam op: 2.442892074584961 seconds
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: Time to load cpu_adam op: 2.440006732940674 seconds
x3006c0s19b1n0: Time to load cpu_adam op: 2.479297161102295 seconds
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: Time to load cpu_adam op: 2.459303855895996 seconds
x3006c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s1b0n0: ninja: no work to do.
x3006c0s1b0n0: Time to load cpu_adam op: 2.4364542961120605 seconds
x3006c0s1b0n0: Time to load cpu_adam op: 2.47416090965271 seconds
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: > learning rate decay style: cosine
x3006c0s19b1n0: DeepSpeed is enabled.
x3006c0s19b1n0: [2024-03-28 13:51:45,343] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.3+8074cd62, git-hash=8074cd62, git-branch=hybrid_opt_offload
x3006c0s19b1n0: [2024-03-28 13:51:45,415] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After args sanity test
x3006c0s19b1n0: [2024-03-28 13:51:45,415] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:51:45,415] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.93 GB, percent = 5.5%
x3006c0s19b1n0: [2024-03-28 13:51:45,470] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure distributed model
x3006c0s19b1n0: [2024-03-28 13:51:45,471] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:51:45,471] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 28.18 GB, percent = 5.6%
x3006c0s1b0n0: ninja: no work to do.
x3006c0s1b0n0: Time to load cpu_adam op: 2.4693715572357178 seconds
x3006c0s19b1n0: [2024-03-28 13:51:45,531] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After configure distributed model
x3006c0s19b1n0: [2024-03-28 13:51:45,531] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:51:45,531] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 28.47 GB, percent = 5.7%
x3006c0s19b1n0: [2024-03-28 13:51:45,531] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
x3006c0s19b1n0: [2024-03-28 13:51:45,584] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After setting model parameters
x3006c0s19b1n0: [2024-03-28 13:51:45,584] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:51:45,584] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 28.73 GB, percent = 5.7%
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-28 13:51:45,638] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure optimizer
x3006c0s19b1n0: [2024-03-28 13:51:45,638] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:51:45,638] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 28.99 GB, percent = 5.8%
x3006c0s19b1n0: [2024-03-28 13:51:45,639] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
x3006c0s19b1n0: [2024-03-28 13:51:45,639] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
x3006c0s19b1n0: [2024-03-28 13:51:45,658] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
x3006c0s19b1n0: [2024-03-28 13:51:45,658] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
x3006c0s19b1n0: [2024-03-28 13:51:45,658] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
x3006c0s19b1n0: [2024-03-28 13:51:45,658] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
x3006c0s19b1n0: [2024-03-28 13:51:45,709] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
x3006c0s19b1n0: [2024-03-28 13:51:45,709] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:51:45,709] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.23 GB, percent = 5.8%
x3006c0s19b1n0: [2024-03-28 13:51:45,711] [INFO] [stage3.py:137:__init__] Reduce bucket size 500,000,000
x3006c0s19b1n0: [2024-03-28 13:51:45,711] [INFO] [stage3.py:138:__init__] Prefetch bucket size 50,000,000
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-28 13:51:45,764] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
x3006c0s19b1n0: [2024-03-28 13:51:45,764] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:51:45,764] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.34 GB, percent = 5.8%
x3006c0s19b1n0: Parameter Offload: Total persistent parameters: 805376 in 121 params
x3006c0s19b1n0: [2024-03-28 13:51:45,841] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
x3006c0s19b1n0: [2024-03-28 13:51:45,842] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:51:45,842] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.46 GB, percent = 5.9%
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-28 13:51:45,897] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
x3006c0s19b1n0: [2024-03-28 13:51:45,897] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:51:45,897] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.48 GB, percent = 5.9%
x3006c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-28 13:51:46,329] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-28 13:51:46,329] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-28 13:51:46,329] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-28 13:51:46,329] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b0n0: [2024-03-28 13:51:46,329] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b0n0: [2024-03-28 13:51:46,329] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b0n0: [2024-03-28 13:51:46,329] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b0n0: [2024-03-28 13:51:46,329] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b0n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b0n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b0n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b0n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,330] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,331] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,332] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,333] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,334] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b0n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b0n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b0n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b0n0: [2024-03-28 13:51:46,335] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-28 13:51:48,529] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 31
x3006c0s19b1n0: [2024-03-28 13:51:48,530] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.44 GB         CA 6.44 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:51:48,530] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 48.83 GB, percent = 9.7%
x3006c0s19b1n0: [2024-03-28 13:51:48,595] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
x3006c0s19b1n0: [2024-03-28 13:51:48,596] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-28 13:51:48,596] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 48.83 GB, percent = 9.7%
x3006c0s19b1n0: [2024-03-28 13:51:51,289] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
x3006c0s19b1n0: [2024-03-28 13:51:51,290] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-28 13:51:51,290] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 63.18 GB, percent = 12.6%
x3006c0s19b1n0: [2024-03-28 13:51:53,050] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
x3006c0s19b1n0: [2024-03-28 13:51:53,051] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-28 13:51:53,051] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 84.77 GB, percent = 16.8%
x3006c0s19b1n0: [2024-03-28 13:51:59,843] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | init_optimizer_state: 6771.57
x3006c0s19b1n0: [2024-03-28 13:51:59,915] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
x3006c0s19b1n0: [2024-03-28 13:51:59,915] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-28 13:51:59,916] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 144.45 GB, percent = 28.7%
x3006c0s19b1n0: [2024-03-28 13:51:59,949] [INFO] [stage3.py:520:_setup_for_real_optimizer] optimizer state initialized
x3006c0s19b1n0: [2024-03-28 13:52:08,095] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
x3006c0s19b1n0: [2024-03-28 13:52:08,095] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 8.61 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:52:08,095] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 177.15 GB, percent = 35.2%
x3006c0s19b1n0: [2024-03-28 13:52:08,095] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
x3006c0s19b1n0: [2024-03-28 13:52:08,164] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure LR scheduler
x3006c0s19b1n0: [2024-03-28 13:52:08,164] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:52:08,164] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 177.16 GB, percent = 35.2%
x3006c0s19b1n0: [2024-03-28 13:52:08,164] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
x3006c0s19b1n0: [2024-03-28 13:52:08,164] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7fdc7d414160>
x3006c0s19b1n0: [2024-03-28 13:52:08,165] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:52:08,230] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before rewriting optimizer step
x3006c0s19b1n0: [2024-03-28 13:52:08,230] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:52:08,230] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 177.16 GB, percent = 35.2%
x3006c0s19b1n0: [2024-03-28 13:52:08,295] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure checkpointing
x3006c0s19b1n0: [2024-03-28 13:52:08,296] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:52:08,296] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 177.15 GB, percent = 35.2%
x3006c0s19b1n0: [2024-03-28 13:52:08,296] [INFO] [config.py:998:print] DeepSpeedEngine configuration:
x3006c0s19b1n0: [2024-03-28 13:52:08,296] [INFO] [config.py:1002:print]   activation_checkpointing_config  {
x3006c0s19b1n0:     "partition_activations": false, 
x3006c0s19b1n0:     "contiguous_memory_optimization": false, 
x3006c0s19b1n0:     "cpu_checkpointing": false, 
x3006c0s19b1n0:     "number_checkpoints": null, 
x3006c0s19b1n0:     "synchronize_checkpoint_boundary": false, 
x3006c0s19b1n0:     "profile": false
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 13:52:08,296] [INFO] [config.py:1002:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
x3006c0s19b1n0: [2024-03-28 13:52:08,296] [INFO] [config.py:1002:print]   amp_enabled .................. False
x3006c0s19b1n0: [2024-03-28 13:52:08,296] [INFO] [config.py:1002:print]   amp_params ................... False
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   autotuning_config ............ {
x3006c0s19b1n0:     "enabled": false, 
x3006c0s19b1n0:     "start_step": null, 
x3006c0s19b1n0:     "end_step": null, 
x3006c0s19b1n0:     "metric_path": null, 
x3006c0s19b1n0:     "arg_mappings": null, 
x3006c0s19b1n0:     "metric": "throughput", 
x3006c0s19b1n0:     "model_info": null, 
x3006c0s19b1n0:     "results_dir": "autotuning_results", 
x3006c0s19b1n0:     "exps_dir": "autotuning_exps", 
x3006c0s19b1n0:     "overwrite": true, 
x3006c0s19b1n0:     "fast": true, 
x3006c0s19b1n0:     "start_profile_step": 3, 
x3006c0s19b1n0:     "end_profile_step": 5, 
x3006c0s19b1n0:     "tuner_type": "gridsearch", 
x3006c0s19b1n0:     "tuner_early_stopping": 5, 
x3006c0s19b1n0:     "tuner_num_trials": 50, 
x3006c0s19b1n0:     "model_info_path": null, 
x3006c0s19b1n0:     "mp_size": 1, 
x3006c0s19b1n0:     "max_train_batch_size": null, 
x3006c0s19b1n0:     "min_train_batch_size": 1, 
x3006c0s19b1n0:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
x3006c0s19b1n0:     "min_train_micro_batch_size_per_gpu": 1, 
x3006c0s19b1n0:     "num_tuning_micro_batch_sizes": 3
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   bfloat16_enabled ............. True
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   bfloat16_immediate_grad_update  False
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   checkpoint_parallel_write_pipeline  False
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   checkpoint_tag_validation_enabled  True
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   checkpoint_tag_validation_fail  False
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fdc7d414af0>
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   communication_data_type ...... None
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   curriculum_enabled_legacy .... False
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   curriculum_params_legacy ..... False
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   data_efficiency_enabled ...... False
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   dataloader_drop_last ......... False
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   datastates_config ............ {
x3006c0s19b1n0:     "enabled": null, 
x3006c0s19b1n0:     "config": {
x3006c0s19b1n0:     }
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   disable_allgather ............ False
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   dump_state ................... False
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   dynamic_loss_scale_args ...... None
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   eigenvalue_enabled ........... False
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   eigenvalue_gas_boundary_resolution  1
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   eigenvalue_layer_name ........ bert.encoder.layer
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   eigenvalue_layer_num ......... 0
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   eigenvalue_max_iter .......... 100
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   eigenvalue_stability ......... 1e-06
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   eigenvalue_tol ............... 0.01
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   eigenvalue_verbose ........... False
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   elasticity_enabled ........... False
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   flops_profiler_config ........ {
x3006c0s19b1n0:     "enabled": false, 
x3006c0s19b1n0:     "recompute_fwd_factor": 0.0, 
x3006c0s19b1n0:     "profile_step": 1, 
x3006c0s19b1n0:     "module_depth": -1, 
x3006c0s19b1n0:     "top_modules": 1, 
x3006c0s19b1n0:     "detailed": true, 
x3006c0s19b1n0:     "output_file": null
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   fp16_auto_cast ............... None
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   fp16_enabled ................. False
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   fp16_master_weights_and_gradients  False
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   global_rank .................. 0
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   grad_accum_dtype ............. bf16
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   gradient_accumulation_steps .. 2
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   gradient_clipping ............ 0.0
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   gradient_predivide_factor .... 1.0
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   graph_harvesting ............. False
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   initial_dynamic_scale ........ 1
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   load_universal_checkpoint .... False
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   loss_scale ................... 1.0
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   memory_breakdown ............. True
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   mics_hierarchial_params_gather  False
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   mics_shard_size .............. -1
x3006c0s19b1n0: [2024-03-28 13:52:08,297] [INFO] [config.py:1002:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
x3006c0s19b1n0: [2024-03-28 13:52:08,298] [INFO] [config.py:1002:print]   nebula_config ................ {
x3006c0s19b1n0:     "enabled": false, 
x3006c0s19b1n0:     "persistent_storage_path": null, 
x3006c0s19b1n0:     "persistent_time_interval": 100, 
x3006c0s19b1n0:     "num_of_version_in_retention": 2, 
x3006c0s19b1n0:     "enable_nebula_load": true, 
x3006c0s19b1n0:     "load_path": null
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 13:52:08,298] [INFO] [config.py:1002:print]   optimizer_legacy_fusion ...... False
x3006c0s19b1n0: [2024-03-28 13:52:08,298] [INFO] [config.py:1002:print]   optimizer_name ............... None
x3006c0s19b1n0: [2024-03-28 13:52:08,298] [INFO] [config.py:1002:print]   optimizer_params ............. None
x3006c0s19b1n0: [2024-03-28 13:52:08,298] [INFO] [config.py:1002:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
x3006c0s19b1n0: [2024-03-28 13:52:08,298] [INFO] [config.py:1002:print]   pld_enabled .................. False
x3006c0s19b1n0: [2024-03-28 13:52:08,298] [INFO] [config.py:1002:print]   pld_params ................... False
x3006c0s19b1n0: [2024-03-28 13:52:08,298] [INFO] [config.py:1002:print]   prescale_gradients ........... False
x3006c0s19b1n0: [2024-03-28 13:52:08,298] [INFO] [config.py:1002:print]   scheduler_name ............... None
x3006c0s19b1n0: [2024-03-28 13:52:08,298] [INFO] [config.py:1002:print]   scheduler_params ............. None
x3006c0s19b1n0: [2024-03-28 13:52:08,298] [INFO] [config.py:1002:print]   seq_parallel_communication_data_type  torch.float32
x3006c0s19b1n0: [2024-03-28 13:52:08,298] [INFO] [config.py:1002:print]   sparse_attention ............. None
x3006c0s19b1n0: [2024-03-28 13:52:08,298] [INFO] [config.py:1002:print]   sparse_gradients_enabled ..... False
x3006c0s19b1n0: [2024-03-28 13:52:08,298] [INFO] [config.py:1002:print]   steps_per_print .............. 1
x3006c0s19b1n0: [2024-03-28 13:52:08,298] [INFO] [config.py:1002:print]   train_batch_size ............. 64
x3006c0s19b1n0: [2024-03-28 13:52:08,298] [INFO] [config.py:1002:print]   train_micro_batch_size_per_gpu  4
x3006c0s19b1n0: [2024-03-28 13:52:08,298] [INFO] [config.py:1002:print]   use_data_before_expert_parallel_  False
x3006c0s19b1n0: [2024-03-28 13:52:08,298] [INFO] [config.py:1002:print]   use_node_local_storage ....... False
x3006c0s19b1n0: [2024-03-28 13:52:08,298] [INFO] [config.py:1002:print]   wall_clock_breakdown ......... True
x3006c0s19b1n0: [2024-03-28 13:52:08,298] [INFO] [config.py:1002:print]   weight_quantization_config ... None
x3006c0s19b1n0: [2024-03-28 13:52:08,298] [INFO] [config.py:1002:print]   world_size ................... 8
x3006c0s19b1n0: [2024-03-28 13:52:08,298] [INFO] [config.py:1002:print]   zero_allow_untested_optimizer  False
x3006c0s19b1n0: [2024-03-28 13:52:08,298] [INFO] [config.py:1002:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0, prefetch_optimizer=False, part_grads_async=False, prefetch_optimizer_gap=5) sub_group_size=100000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
x3006c0s19b1n0: [2024-03-28 13:52:08,298] [INFO] [config.py:1002:print]   zero_enabled ................. True
x3006c0s19b1n0: [2024-03-28 13:52:08,298] [INFO] [config.py:1002:print]   zero_force_ds_cpu_optimizer .. True
x3006c0s19b1n0: [2024-03-28 13:52:08,298] [INFO] [config.py:1002:print]   zero_optimization_stage ...... 3
x3006c0s19b1n0: [2024-03-28 13:52:08,298] [INFO] [config.py:988:print_user_config]   json = {
x3006c0s19b1n0:     "train_batch_size": 64, 
x3006c0s19b1n0:     "train_micro_batch_size_per_gpu": 4, 
x3006c0s19b1n0:     "steps_per_print": 1, 
x3006c0s19b1n0:     "zero_optimization": {
x3006c0s19b1n0:         "stage": 3, 
x3006c0s19b1n0:         "offload_optimizer": {
x3006c0s19b1n0:             "device": "cpu", 
x3006c0s19b1n0:             "ratio": 1, 
x3006c0s19b1n0:             "pin_memory": true, 
x3006c0s19b1n0:             "prefetch_optimizer": 0, 
x3006c0s19b1n0:             "part_grads_async": 0, 
x3006c0s19b1n0:             "prefetch_optimizer_gap": 5
x3006c0s19b1n0:         }, 
x3006c0s19b1n0:         "sub_group_size": 1.000000e+08
x3006c0s19b1n0:     }, 
x3006c0s19b1n0:     "bf16": {
x3006c0s19b1n0:         "enabled": true
x3006c0s19b1n0:     }, 
x3006c0s19b1n0:     "data_types": {
x3006c0s19b1n0:         "grad_accum_dtype": "bf16"
x3006c0s19b1n0:     }, 
x3006c0s19b1n0:     "wall_clock_breakdown": true, 
x3006c0s19b1n0:     "memory_breakdown": true, 
x3006c0s19b1n0:     "flops_profiler": {
x3006c0s19b1n0:         "enabled": false
x3006c0s19b1n0:     }
x3006c0s19b1n0: }
x3006c0s1b0n0: <TIMER:model-and-optimizer-setup,35.061413288116455>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,35.061522006988525>
x3006c0s1b0n0: <TIMER:model-and-optimizer-setup,35.06186318397522>
x3006c0s1b0n0: <TIMER:model-and-optimizer-setup,35.06243872642517>
x3006c0s1b0n0: <TIMER:model-and-optimizer-setup,35.06246209144592>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,35.062894344329834>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,35.063010931015015>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,35.063523292541504>
x3006c0s19b1n0: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-28 13:52:09 
x3006c0s19b1n0: > building train, validation, and test datasets ...
x3006c0s19b1n0:  > datasets target sizes (minimum size):
x3006c0s19b1n0:     train:      640
x3006c0s19b1n0:     validation: 0
x3006c0s19b1n0:     test:       0
x3006c0s19b1n0: > building train, validation, and test datasets for GPT ...
x3006c0s19b1n0: Single data path provided for train, valid & test
x3006c0s19b1n0:  > building dataset index ...
x3006c0s19b1n0:     reading sizes...
x3006c0s19b1n0:     reading pointers...
x3006c0s19b1n0:     reading document index...
x3006c0s19b1n0:     creating numpy buffer of mmap...
x3006c0s19b1n0:     creating memory view of numpy buffer...
x3006c0s19b1n0:  > finished creating indexed dataset in 0.002568 seconds
x3006c0s19b1n0:     number of documents: 79000
x3006c0s19b1n0:  > dataset split:
x3006c0s19b1n0:     train:
x3006c0s19b1n0:      document indices in [0, 74971) total of 74971 documents
x3006c0s19b1n0:     validation:
x3006c0s19b1n0:      document indices in [74971, 78921) total of 3950 documents
x3006c0s19b1n0:     test:
x3006c0s19b1n0:      document indices in [78921, 79000) total of 79 documents
x3006c0s19b1n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/d5d1daec41eb416469c3827ed48205ed_doc_idx.npy
x3006c0s19b1n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/d5d1daec41eb416469c3827ed48205ed_sample_idx.npy
x3006c0s19b1n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/d5d1daec41eb416469c3827ed48205ed_shuffle_idx.npy
x3006c0s19b1n0:     loaded indexed file in 0.002 seconds
x3006c0s19b1n0:     total number of samples: 108448
x3006c0s19b1n0:     total number of epochs: 1
x3006c0s19b1n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_doc_idx.npy
x3006c0s19b1n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_sample_idx.npy
x3006c0s19b1n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_shuffle_idx.npy
x3006c0s19b1n0:     loaded indexed file in 0.003 seconds
x3006c0s19b1n0:     total number of samples: 5792
x3006c0s19b1n0:     total number of epochs: 1
x3006c0s19b1n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_doc_idx.npy
x3006c0s19b1n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_sample_idx.npy
x3006c0s19b1n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_shuffle_idx.npy
x3006c0s19b1n0:     loaded indexed file in 0.004 seconds
x3006c0s19b1n0:     total number of samples: 185
x3006c0s19b1n0:     total number of epochs: 1
x3006c0s19b1n0: > finished creating GPT datasets ...
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5461399555206299>
x3006c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5500278472900391>
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5556924343109131>
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5560786724090576>
x3006c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5742263793945312>
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5792150497436523>
x3006c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.6885638236999512>
x3006c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.697838544845581>
x3006c0s19b1n0: [after dataloaders are built] datetime: 2024-03-28 13:52:10 
x3006c0s19b1n0: done with setup ...
x3006c0s19b1n0: training ...
x3006c0s1b0n0: (min, max) time across ranks (ms):
x3006c0s1b0n0:     model-and-optimizer-setup ......................: (35061.41, 35063.52)
x3006c0s1b0n0:     train/valid/test-data-iterators-setup ..........: (546.14, 697.84)
x3006c0s19b1n0: [before training begins] datetime: 2024-03-28 13:52:10 
x3006c0s19b1n0: [before the start of training step] datetime: 2024-03-28 13:52:10 
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:52:10,407] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:52:10,407] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 7.45 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:52:10,407] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 180.1 GB, percent = 35.8%
x3006c0s19b1n0: [2024-03-28 13:52:10,530] [INFO] [checkpointing.py:539:forward] Activation Checkpointing Information
x3006c0s19b1n0: [2024-03-28 13:52:10,530] [INFO] [checkpointing.py:540:forward] ----Partition Activations False, CPU CHECKPOINTING False
x3006c0s19b1n0: [2024-03-28 13:52:10,530] [INFO] [checkpointing.py:541:forward] ----contiguous Memory Checkpointing False with 60 total layers
x3006c0s19b1n0: [2024-03-28 13:52:10,530] [INFO] [checkpointing.py:543:forward] ----Synchronization False
x3006c0s19b1n0: [2024-03-28 13:52:10,530] [INFO] [checkpointing.py:544:forward] ----Profiling time in checkpointing False
x3006c0s19b1n0: [2024-03-28 13:52:20,097] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:52:20,097] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 19.76 GB         CA 22.95 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:52:20,097] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 180.25 GB, percent = 35.8%
x3006c0s19b1n0: [2024-03-28 13:52:20,277] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:52:20,278] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 16.84 GB         CA 16.91 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:52:20,278] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 180.26 GB, percent = 35.8%
x3006c0s19b1n0: [2024-03-28 13:52:45,056] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:52:45,057] [INFO] [utils.py:801:see_memory_usage] MA 10.25 GB         Max_MA 20.22 GB         CA 10.33 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:52:45,057] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 180.32 GB, percent = 35.8%
x3006c0s19b1n0: [2024-03-28 13:52:45,142] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:52:45,142] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:52:45,142] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 180.34 GB, percent = 35.8%
x3006c0s19b1n0: [2024-03-28 13:52:51,606] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:52:51,607] [INFO] [utils.py:801:see_memory_usage] MA 20.07 GB         Max_MA 22.37 GB         CA 25.45 GB         Max_CA 25 GB 
x3006c0s19b1n0: [2024-03-28 13:52:51,607] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 180.33 GB, percent = 35.8%
x3006c0s19b1n0: [2024-03-28 13:52:51,700] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:52:51,700] [INFO] [utils.py:801:see_memory_usage] MA 20.07 GB         Max_MA 20.07 GB         CA 20.68 GB         Max_CA 25 GB 
x3006c0s19b1n0: [2024-03-28 13:52:51,700] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 180.33 GB, percent = 35.8%
x3006c0s19b1n0: [2024-03-28 13:53:11,834] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:53:11,834] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.59 GB         CA 10.33 GB         Max_CA 28 GB 
x3006c0s19b1n0: [2024-03-28 13:53:11,834] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 183.8 GB, percent = 36.5%
x3006c0s19b1n0: [2024-03-28 13:53:11,915] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:53:11,916] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:53:11,916] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 183.85 GB, percent = 36.5%
x3006c0s19b1n0: [2024-03-28 13:53:21,099] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.108042478561401
x3006c0s19b1n0: [2024-03-28 13:53:21,099] [INFO] [stage3.py:2251:step] Full outer step loop took 9.10820984840393
x3006c0s19b1n0: [2024-03-28 13:53:21,306] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.315346717834473
x3006c0s19b1n0: [2024-03-28 13:53:21,306] [INFO] [stage3.py:2251:step] Full outer step loop took 9.315830707550049
x3006c0s1b0n0: [2024-03-28 13:53:21,381] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.389992952346802
x3006c0s1b0n0: [2024-03-28 13:53:21,382] [INFO] [stage3.py:2251:step] Full outer step loop took 9.391035556793213
x3006c0s1b0n0: [2024-03-28 13:53:21,433] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.442237854003906
x3006c0s1b0n0: [2024-03-28 13:53:21,433] [INFO] [stage3.py:2251:step] Full outer step loop took 9.442454814910889
x3006c0s1b0n0: [2024-03-28 13:53:21,528] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.537235260009766
x3006c0s1b0n0: [2024-03-28 13:53:21,528] [INFO] [stage3.py:2251:step] Full outer step loop took 9.537496566772461
x3006c0s1b0n0: [2024-03-28 13:53:21,540] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.534112215042114
x3006c0s1b0n0: [2024-03-28 13:53:21,540] [INFO] [stage3.py:2251:step] Full outer step loop took 9.534320592880249
x3006c0s19b1n0: [2024-03-28 13:53:21,549] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.558204889297485
x3006c0s19b1n0: [2024-03-28 13:53:21,549] [INFO] [stage3.py:2251:step] Full outer step loop took 9.558527946472168
x3006c0s19b1n0: [2024-03-28 13:53:21,560] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.569663524627686
x3006c0s19b1n0: [2024-03-28 13:53:21,560] [INFO] [stage3.py:2251:step] Full outer step loop took 9.56987452507019
x3006c0s1b0n0: [2024-03-28 13:53:21,572] [INFO] [stage3.py:2277:step] End to end step took 9.566171884536743
x3006c0s1b0n0: [2024-03-28 13:53:21,572] [INFO] [stage3.py:2277:step] End to end step took 9.581047058105469
x3006c0s19b1n0: [2024-03-28 13:53:21,572] [INFO] [stage3.py:2277:step] End to end step took 9.581141948699951
x3006c0s19b1n0: [2024-03-28 13:53:21,572] [INFO] [stage3.py:2277:step] End to end step took 9.581230878829956
x3006c0s19b1n0: [2024-03-28 13:53:21,572] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 9108.41
x3006c0s19b1n0: [2024-03-28 13:53:21,572] [WARNING] [stage3.py:2267:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3006c0s19b1n0: [2024-03-28 13:53:21,572] [INFO] [stage3.py:2277:step] End to end step took 9.58165693283081
x3006c0s19b1n0: [2024-03-28 13:53:21,572] [INFO] [stage3.py:2277:step] End to end step took 9.581773519515991
x3006c0s1b0n0: [2024-03-28 13:53:21,572] [INFO] [stage3.py:2277:step] End to end step took 9.58161211013794
x3006c0s1b0n0: [2024-03-28 13:53:21,572] [INFO] [stage3.py:2277:step] End to end step took 9.581668853759766
x3006c0s19b1n0: [2024-03-28 13:53:21,573] [INFO] [logging.py:96:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:53:21,573] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 16200.72 | bwd_microstep: 44631.76 | bwd_inner_microstep: 44455.67 | bwd_allreduce_microstep: 175.86 | step_microstep: 9656.74
x3006c0s19b1n0: [2024-03-28 13:53:21,573] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 16200.72 | bwd: 44631.77 | bwd_inner: 44455.70 | bwd_allreduce: 175.87 | step: 9656.74
x3006c0s19b1n0: [2024-03-28 13:53:21,677] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:53:21,678] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:53:21,678] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.47 GB, percent = 67.3%
x3006c0s19b1n0: <TIMER:interval-time,71.50487041473389><TIMER:interval-time,71.50487279891968><TIMER:interval-time,71.5048713684082>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,71.50482082366943><TIMER:interval-time,71.50491714477539><TIMER:interval-time,71.50491428375244><TIMER:interval-time,71.5049057006836>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s19b1n0: <TIMER:interval-time,71.50499653816223>
x3006c0s1b0n0:  elapsed_time 71.504821 | consumed samples:           64 | consumed tokens:       131072 | elapsed time per iteration (ms): 71504.8 | learning rate: 3.000E-04 | global batch size:    64 | lm loss: 1.215771E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.895 | TFLOPs: 61.93 |
x3006c0s19b1n0: [Rank 0] (after 1 iterations) memory (MB) | allocated: 9224.5439453125 | max allocated: 9224.544921875 | reserved: 9296.0 | max reserved: 9296.0
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:53:21,815] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:53:21,815] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 13:53:21,815] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.44 GB, percent = 67.3%
x3006c0s19b1n0: [2024-03-28 13:53:28,315] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:53:28,315] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:53:28,315] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.43 GB, percent = 67.3%
x3006c0s19b1n0: [2024-03-28 13:53:28,393] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:53:28,394] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:53:28,394] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.37 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:53:45,610] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:53:45,611] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 29 GB 
x3006c0s19b1n0: [2024-03-28 13:53:45,611] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.17 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:53:45,690] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:53:45,690] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:53:45,691] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.17 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:53:52,204] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:53:52,205] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:53:52,205] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.16 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:53:52,288] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:53:52,289] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:53:52,289] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.02 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:54:12,094] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:54:12,095] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:54:12,095] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.29 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:54:12,163] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:54:12,164] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:54:12,164] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.29 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:54:18,916] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.727998971939087
x3006c0s19b1n0: [2024-03-28 13:54:18,916] [INFO] [stage3.py:2251:step] Full outer step loop took 6.72825288772583
x3006c0s19b1n0: [2024-03-28 13:54:19,178] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.990414381027222
x3006c0s19b1n0: [2024-03-28 13:54:19,179] [INFO] [stage3.py:2251:step] Full outer step loop took 6.990718603134155
x3006c0s19b1n0: [2024-03-28 13:54:19,243] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.054683208465576
x3006c0s19b1n0: [2024-03-28 13:54:19,243] [INFO] [stage3.py:2251:step] Full outer step loop took 7.055004119873047
x3006c0s1b0n0: [2024-03-28 13:54:19,251] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.06248664855957
x3006c0s1b0n0: [2024-03-28 13:54:19,251] [INFO] [stage3.py:2251:step] Full outer step loop took 7.0626935958862305
x3006c0s19b1n0: [2024-03-28 13:54:19,257] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.068681716918945
x3006c0s19b1n0: [2024-03-28 13:54:19,257] [INFO] [stage3.py:2251:step] Full outer step loop took 7.068861484527588
x3006c0s1b0n0: [2024-03-28 13:54:19,321] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.132713794708252
x3006c0s1b0n0: [2024-03-28 13:54:19,321] [INFO] [stage3.py:2251:step] Full outer step loop took 7.133074760437012
x3006c0s1b0n0: [2024-03-28 13:54:19,332] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.120558738708496
x3006c0s1b0n0: [2024-03-28 13:54:19,332] [INFO] [stage3.py:2251:step] Full outer step loop took 7.120977878570557
x3006c0s1b0n0: [2024-03-28 13:54:19,355] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.167005777359009
x3006c0s1b0n0: [2024-03-28 13:54:19,355] [INFO] [stage3.py:2251:step] Full outer step loop took 7.167156219482422
x3006c0s1b0n0: [2024-03-28 13:54:19,366] [INFO] [stage3.py:2277:step] End to end step took 7.17770528793335
x3006c0s1b0n0: [2024-03-28 13:54:19,366] [INFO] [stage3.py:2277:step] End to end step took 7.1778857707977295
x3006c0s19b1n0: [2024-03-28 13:54:19,366] [INFO] [stage3.py:2277:step] End to end step took 7.177859544754028
x3006c0s1b0n0: [2024-03-28 13:54:19,366] [INFO] [stage3.py:2277:step] End to end step took 7.154813051223755
x3006c0s19b1n0: [2024-03-28 13:54:19,366] [INFO] [stage3.py:2277:step] End to end step took 7.177982330322266
x3006c0s19b1n0: [2024-03-28 13:54:19,366] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6728.50
x3006c0s19b1n0: [2024-03-28 13:54:19,366] [INFO] [stage3.py:2277:step] End to end step took 7.178198337554932
x3006c0s19b1n0: [2024-03-28 13:54:19,366] [INFO] [stage3.py:2277:step] End to end step took 7.178259372711182
x3006c0s1b0n0: [2024-03-28 13:54:19,366] [INFO] [stage3.py:2277:step] End to end step took 7.17823338508606
x3006c0s19b1n0: [2024-03-28 13:54:19,367] [INFO] [logging.py:96:log_dist] [Rank 0] step=2, skipped=0, lr=[0.0002918585038060976], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:54:19,367] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 12955.26 | bwd_microstep: 36689.60 | bwd_inner_microstep: 36507.62 | bwd_allreduce_microstep: 181.80 | step_microstep: 7202.67
x3006c0s19b1n0: [2024-03-28 13:54:19,367] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 12955.25 | bwd: 36689.60 | bwd_inner: 36507.63 | bwd_allreduce: 181.82 | step: 7202.67
x3006c0s19b1n0: [2024-03-28 13:54:19,492] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:54:19,493] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:54:19,493] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.24 GB, percent = 67.2%
x3006c0s19b1n0: <TIMER:interval-time,57.81471633911133><TIMER:interval-time,57.81472182273865><TIMER:interval-time,57.81472373008728>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,57.81473684310913><TIMER:interval-time,57.8147394657135><TIMER:interval-time,57.8147394657135>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,57.81474757194519>
x3006c0s19b1n0: <TIMER:interval-time,57.81481146812439>
x3006c0s1b0n0:  elapsed_time 57.814748 | consumed samples:          128 | consumed tokens:       262144 | elapsed time per iteration (ms): 57814.7 | learning rate: 2.919E-04 | global batch size:    64 | lm loss: 1.215431E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.107 | TFLOPs: 76.59 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:54:19,620] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:54:19,620] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 13:54:19,621] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.24 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:54:26,115] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:54:26,116] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:54:26,116] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.22 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:54:26,199] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:54:26,199] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:54:26,200] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.22 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:54:44,479] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:54:44,480] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 29 GB 
x3006c0s19b1n0: [2024-03-28 13:54:44,480] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.01 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:54:44,558] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:54:44,558] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:54:44,558] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.02 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:54:51,615] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:54:51,616] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:54:51,616] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.01 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:54:51,695] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:54:51,695] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:54:51,696] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.01 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:55:12,286] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:55:12,287] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:55:12,287] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.08 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:55:12,359] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:55:12,359] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:55:12,360] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.08 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:55:19,204] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.820835113525391
x3006c0s19b1n0: [2024-03-28 13:55:19,205] [INFO] [stage3.py:2251:step] Full outer step loop took 6.821812629699707
x3006c0s1b0n0: [2024-03-28 13:55:19,339] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.9555299282073975
x3006c0s1b0n0: [2024-03-28 13:55:19,340] [INFO] [stage3.py:2251:step] Full outer step loop took 6.9562599658966064
x3006c0s1b0n0: [2024-03-28 13:55:19,342] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.9589855670928955
x3006c0s1b0n0: [2024-03-28 13:55:19,343] [INFO] [stage3.py:2251:step] Full outer step loop took 6.959392309188843
x3006c0s1b0n0: [2024-03-28 13:55:19,348] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.964268445968628
x3006c0s1b0n0: [2024-03-28 13:55:19,348] [INFO] [stage3.py:2251:step] Full outer step loop took 6.964433908462524
x3006c0s19b1n0: [2024-03-28 13:55:19,389] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.006007671356201
x3006c0s19b1n0: [2024-03-28 13:55:19,389] [INFO] [stage3.py:2251:step] Full outer step loop took 7.006204843521118
x3006c0s19b1n0: [2024-03-28 13:55:19,401] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.017555236816406
x3006c0s19b1n0: [2024-03-28 13:55:19,401] [INFO] [stage3.py:2251:step] Full outer step loop took 7.017730951309204
x3006c0s19b1n0: [2024-03-28 13:55:19,415] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.031399250030518
x3006c0s19b1n0: [2024-03-28 13:55:19,415] [INFO] [stage3.py:2251:step] Full outer step loop took 7.031564712524414
x3006c0s1b0n0: [2024-03-28 13:55:19,421] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.037962436676025
x3006c0s1b0n0: [2024-03-28 13:55:19,421] [INFO] [stage3.py:2251:step] Full outer step loop took 7.038112640380859
x3006c0s19b1n0: [2024-03-28 13:55:19,432] [INFO] [stage3.py:2277:step] End to end step took 7.048938035964966
x3006c0s1b0n0: [2024-03-28 13:55:19,432] [INFO] [stage3.py:2277:step] End to end step took 7.048877954483032
x3006c0s19b1n0: [2024-03-28 13:55:19,432] [INFO] [stage3.py:2277:step] End to end step took 7.048976898193359
x3006c0s1b0n0: [2024-03-28 13:55:19,432] [INFO] [stage3.py:2277:step] End to end step took 7.048961639404297
x3006c0s19b1n0: [2024-03-28 13:55:19,432] [INFO] [stage3.py:2277:step] End to end step took 7.049110174179077
x3006c0s1b0n0: [2024-03-28 13:55:19,432] [INFO] [stage3.py:2277:step] End to end step took 7.049154996871948
x3006c0s19b1n0: [2024-03-28 13:55:19,432] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6822.55
x3006c0s19b1n0: [2024-03-28 13:55:19,433] [INFO] [stage3.py:2277:step] End to end step took 7.049337387084961
x3006c0s1b0n0: [2024-03-28 13:55:19,433] [INFO] [stage3.py:2277:step] End to end step took 7.04944109916687
x3006c0s19b1n0: [2024-03-28 13:55:19,433] [INFO] [logging.py:96:log_dist] [Rank 0] step=3, skipped=0, lr=[0.00026841599982106197], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:55:19,433] [INFO] [timer.py:260:stop] epoch=0/micro_step=3/global_step=3, RunningAvgSamplesPerSec=1.8352394946724304, CurrSamplesPerSec=1.8352394946724304, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 13:55:19,434] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 13485.11 | bwd_microstep: 38544.36 | bwd_inner_microstep: 38353.43 | bwd_allreduce_microstep: 190.73 | step_microstep: 7073.79
x3006c0s19b1n0: [2024-03-28 13:55:19,434] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 13485.10 | bwd: 38544.36 | bwd_inner: 38353.44 | bwd_allreduce: 190.75 | step: 7073.80
x3006c0s19b1n0: [2024-03-28 13:55:19,551] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:55:19,552] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:55:19,552] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.12 GB, percent = 67.2%
x3006c0s19b1n0: <TIMER:interval-time,60.05864596366882><TIMER:interval-time,60.05866551399231>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,60.05867075920105><TIMER:interval-time,60.05867075920105>
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,60.058650970458984><TIMER:interval-time,60.05865001678467><TIMER:interval-time,60.05865168571472>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,60.05865812301636>
x3006c0s1b0n0:  elapsed_time 60.058658 | consumed samples:          192 | consumed tokens:       393216 | elapsed time per iteration (ms): 60058.7 | learning rate: 2.684E-04 | global batch size:    64 | lm loss: 3.510575E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.066 | TFLOPs: 73.73 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:55:19,683] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:55:19,683] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 13:55:19,683] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.87 GB, percent = 67.1%
x3006c0s19b1n0: [2024-03-28 13:55:26,632] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:55:26,633] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:55:26,633] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.85 GB, percent = 67.1%
x3006c0s19b1n0: [2024-03-28 13:55:26,715] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:55:26,716] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:55:26,716] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.85 GB, percent = 67.1%
x3006c0s19b1n0: [2024-03-28 13:55:44,746] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:55:44,747] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 29 GB 
x3006c0s19b1n0: [2024-03-28 13:55:44,747] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.7 GB, percent = 67.1%
x3006c0s19b1n0: [2024-03-28 13:55:44,829] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:55:44,829] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:55:44,830] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.7 GB, percent = 67.1%
x3006c0s19b1n0: [2024-03-28 13:55:51,586] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:55:51,587] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:55:51,587] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.69 GB, percent = 67.1%
x3006c0s19b1n0: [2024-03-28 13:55:51,686] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:55:51,687] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:55:51,687] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.69 GB, percent = 67.1%
x3006c0s19b1n0: [2024-03-28 13:56:12,559] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:56:12,560] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:56:12,560] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.53 GB, percent = 67.3%
x3006c0s19b1n0: [2024-03-28 13:56:12,630] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:56:12,631] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:56:12,631] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.53 GB, percent = 67.3%
x3006c0s1b0n0: [2024-03-28 13:56:19,459] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.803818941116333
x3006c0s1b0n0: [2024-03-28 13:56:19,461] [INFO] [stage3.py:2251:step] Full outer step loop took 6.8057262897491455
x3006c0s19b1n0: [2024-03-28 13:56:19,497] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.842300176620483
x3006c0s19b1n0: [2024-03-28 13:56:19,500] [INFO] [stage3.py:2251:step] Full outer step loop took 6.844493865966797
x3006c0s19b1n0: [2024-03-28 13:56:19,670] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.985124588012695
x3006c0s19b1n0: [2024-03-28 13:56:19,671] [INFO] [stage3.py:2251:step] Full outer step loop took 6.985870838165283
x3006c0s1b0n0: [2024-03-28 13:56:19,685] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.030418634414673
x3006c0s1b0n0: [2024-03-28 13:56:19,686] [INFO] [stage3.py:2251:step] Full outer step loop took 7.030714273452759
x3006c0s19b1n0: [2024-03-28 13:56:19,747] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.091650724411011
x3006c0s19b1n0: [2024-03-28 13:56:19,747] [INFO] [stage3.py:2251:step] Full outer step loop took 7.091815710067749
x3006c0s1b0n0: [2024-03-28 13:56:19,774] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.119187116622925
x3006c0s1b0n0: [2024-03-28 13:56:19,774] [INFO] [stage3.py:2251:step] Full outer step loop took 7.1193687915802
x3006c0s1b0n0: [2024-03-28 13:56:19,782] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.127125263214111
x3006c0s1b0n0: [2024-03-28 13:56:19,782] [INFO] [stage3.py:2251:step] Full outer step loop took 7.127278089523315
x3006c0s19b1n0: [2024-03-28 13:56:19,789] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.134336709976196
x3006c0s19b1n0: [2024-03-28 13:56:19,789] [INFO] [stage3.py:2251:step] Full outer step loop took 7.134488344192505
x3006c0s19b1n0: [2024-03-28 13:56:19,801] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 7134.65
x3006c0s1b0n0: [2024-03-28 13:56:19,801] [INFO] [stage3.py:2277:step] End to end step took 7.146103620529175
x3006c0s19b1n0: [2024-03-28 13:56:19,801] [INFO] [stage3.py:2277:step] End to end step took 7.146330118179321
x3006c0s19b1n0: [2024-03-28 13:56:19,801] [INFO] [stage3.py:2277:step] End to end step took 7.146359205245972
x3006c0s1b0n0: [2024-03-28 13:56:19,801] [INFO] [stage3.py:2277:step] End to end step took 7.146219968795776
x3006c0s19b1n0: [2024-03-28 13:56:19,802] [INFO] [logging.py:96:log_dist] [Rank 0] step=4, skipped=0, lr=[0.00023249999999999996], mom=[(0.9, 0.95)]
x3006c0s1b0n0: [2024-03-28 13:56:19,802] [INFO] [stage3.py:2277:step] End to end step took 7.146646976470947
x3006c0s1b0n0: [2024-03-28 13:56:19,802] [INFO] [stage3.py:2277:step] End to end step took 7.146745443344116
x3006c0s19b1n0: [2024-03-28 13:56:19,802] [INFO] [stage3.py:2277:step] End to end step took 7.11688232421875
x3006c0s19b1n0: [2024-03-28 13:56:19,802] [INFO] [stage3.py:2277:step] End to end step took 7.14689040184021
x3006c0s19b1n0: [2024-03-28 13:56:19,802] [INFO] [timer.py:260:stop] epoch=0/micro_step=4/global_step=4, RunningAvgSamplesPerSec=1.8326613228136992, CurrSamplesPerSec=1.8300903845018581, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 13:56:19,802] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 13632.33 | bwd_microstep: 38559.50 | bwd_inner_microstep: 38359.59 | bwd_allreduce_microstep: 199.71 | step_microstep: 7170.81
x3006c0s19b1n0: [2024-03-28 13:56:19,802] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 13632.32 | bwd: 38559.50 | bwd_inner: 38359.61 | bwd_allreduce: 199.72 | step: 7170.81
x3006c0s19b1n0: [2024-03-28 13:56:19,919] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:56:19,920] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:56:19,920] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.39 GB, percent = 67.2%
x3006c0s19b1n0: <TIMER:interval-time,60.36766791343689><TIMER:interval-time,60.36766862869263><TIMER:interval-time,60.36767029762268>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,60.36766958236694><TIMER:interval-time,60.36766958236694><TIMER:interval-time,60.367671251297><TIMER:interval-time,60.36767292022705>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s19b1n0: <TIMER:interval-time,60.36776041984558>
x3006c0s1b0n0:  elapsed_time 60.367671 | consumed samples:          256 | consumed tokens:       524288 | elapsed time per iteration (ms): 60367.7 | learning rate: 2.325E-04 | global batch size:    64 | lm loss: 2.233065E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.060 | TFLOPs: 73.35 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:56:20,061] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:56:20,062] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 13:56:20,062] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.24 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:56:27,241] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:56:27,242] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:56:27,242] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.22 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:56:27,323] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:56:27,324] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:56:27,324] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.22 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:56:45,345] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:56:45,346] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 29 GB 
x3006c0s19b1n0: [2024-03-28 13:56:45,346] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.1 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:56:45,441] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:56:45,441] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:56:45,441] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.1 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:56:52,117] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:56:52,117] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:56:52,118] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.09 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:56:52,207] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:56:52,208] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:56:52,208] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.09 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:57:12,868] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:57:12,868] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:57:12,869] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.09 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:57:12,937] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:57:12,938] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:57:12,938] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.09 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:57:19,663] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.70160436630249
x3006c0s19b1n0: [2024-03-28 13:57:19,663] [INFO] [stage3.py:2251:step] Full outer step loop took 6.701892852783203
x3006c0s19b1n0: [2024-03-28 13:57:19,832] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.870874881744385
x3006c0s19b1n0: [2024-03-28 13:57:19,833] [INFO] [stage3.py:2251:step] Full outer step loop took 6.871201038360596
x3006c0s19b1n0: [2024-03-28 13:57:19,854] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.89294695854187
x3006c0s19b1n0: [2024-03-28 13:57:19,854] [INFO] [stage3.py:2251:step] Full outer step loop took 6.8931145668029785
x3006c0s1b0n0: [2024-03-28 13:57:19,978] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.0170228481292725
x3006c0s1b0n0: [2024-03-28 13:57:19,978] [INFO] [stage3.py:2251:step] Full outer step loop took 7.017253160476685
x3006c0s19b1n0: [2024-03-28 13:57:19,996] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.034652948379517
x3006c0s19b1n0: [2024-03-28 13:57:19,996] [INFO] [stage3.py:2251:step] Full outer step loop took 7.03480339050293
x3006c0s1b0n0: [2024-03-28 13:57:20,022] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.060859203338623
x3006c0s1b0n0: [2024-03-28 13:57:20,022] [INFO] [stage3.py:2251:step] Full outer step loop took 7.061047315597534
x3006c0s1b0n0: [2024-03-28 13:57:20,098] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.136353969573975
x3006c0s1b0n0: [2024-03-28 13:57:20,098] [INFO] [stage3.py:2251:step] Full outer step loop took 7.136544942855835
x3006c0s1b0n0: [2024-03-28 13:57:20,242] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.2807557582855225
x3006c0s1b0n0: [2024-03-28 13:57:20,242] [INFO] [stage3.py:2251:step] Full outer step loop took 7.28090763092041
x3006c0s1b0n0: [2024-03-28 13:57:20,252] [INFO] [stage3.py:2277:step] End to end step took 7.291247129440308
x3006c0s19b1n0: [2024-03-28 13:57:20,252] [INFO] [stage3.py:2277:step] End to end step took 7.291205406188965
x3006c0s19b1n0: [2024-03-28 13:57:20,253] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6702.17
x3006c0s1b0n0: [2024-03-28 13:57:20,253] [INFO] [stage3.py:2277:step] End to end step took 7.291667222976685
x3006c0s19b1n0: [2024-03-28 13:57:20,253] [INFO] [stage3.py:2277:step] End to end step took 7.2917563915252686
x3006c0s19b1n0: [2024-03-28 13:57:20,253] [INFO] [stage3.py:2277:step] End to end step took 7.291622638702393
x3006c0s19b1n0: [2024-03-28 13:57:20,253] [INFO] [stage3.py:2277:step] End to end step took 7.291577100753784
x3006c0s1b0n0: [2024-03-28 13:57:20,253] [INFO] [stage3.py:2277:step] End to end step took 7.29141640663147
x3006c0s1b0n0: [2024-03-28 13:57:20,253] [INFO] [stage3.py:2277:step] End to end step took 7.291807174682617
x3006c0s19b1n0: [2024-03-28 13:57:20,253] [INFO] [logging.py:96:log_dist] [Rank 0] step=5, skipped=0, lr=[0.0001884425039850356], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:57:20,254] [INFO] [timer.py:260:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=1.8346127394934093, CurrSamplesPerSec=1.8385280666635098, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 13:57:20,254] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 13773.25 | bwd_microstep: 38336.57 | bwd_inner_microstep: 38132.10 | bwd_allreduce_microstep: 204.28 | step_microstep: 7315.96
x3006c0s19b1n0: [2024-03-28 13:57:20,254] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 13773.23 | bwd: 38336.57 | bwd_inner: 38132.11 | bwd_allreduce: 204.30 | step: 7315.96
x3006c0s19b1n0: [2024-03-28 13:57:20,365] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:57:20,365] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:57:20,365] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.13 GB, percent = 67.2%
x3006c0s19b1n0: <TIMER:interval-time,60.445276975631714><TIMER:interval-time,60.44527721405029><TIMER:interval-time,60.44527721405029>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,60.445268869400024>
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,60.44526171684265><TIMER:interval-time,60.445261001586914>
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,60.44526696205139>
x3006c0s1b0n0: <TIMER:interval-time,60.44536280632019>
x3006c0s1b0n0:  elapsed_time 60.445262 | consumed samples:          320 | consumed tokens:       655360 | elapsed time per iteration (ms): 60445.3 | learning rate: 1.884E-04 | global batch size:    64 | lm loss: 1.478109E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.059 | TFLOPs: 73.26 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:57:20,500] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:57:20,500] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 13:57:20,500] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.13 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:57:27,376] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:57:27,377] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:57:27,377] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.11 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:57:27,468] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:57:27,469] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:57:27,469] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.11 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:57:45,630] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:57:45,630] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 29 GB 
x3006c0s19b1n0: [2024-03-28 13:57:45,631] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.95 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:57:45,714] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:57:45,714] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:57:45,715] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.95 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:57:51,989] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:57:51,989] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:57:51,989] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.94 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:57:52,068] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:57:52,068] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:57:52,068] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.94 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:58:12,560] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:58:12,560] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:58:12,560] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.38 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:58:12,627] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:58:12,628] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:58:12,628] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.38 GB, percent = 67.2%
x3006c0s1b0n0: [2024-03-28 13:58:19,580] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.927154779434204
x3006c0s1b0n0: [2024-03-28 13:58:19,581] [INFO] [stage3.py:2251:step] Full outer step loop took 6.927553415298462
x3006c0s19b1n0: [2024-03-28 13:58:19,634] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.980297088623047
x3006c0s19b1n0: [2024-03-28 13:58:19,634] [INFO] [stage3.py:2251:step] Full outer step loop took 6.980546236038208
x3006c0s1b0n0: [2024-03-28 13:58:19,644] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.990414142608643
x3006c0s1b0n0: [2024-03-28 13:58:19,644] [INFO] [stage3.py:2251:step] Full outer step loop took 6.990752696990967
x3006c0s19b1n0: [2024-03-28 13:58:19,651] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.998139142990112
x3006c0s19b1n0: [2024-03-28 13:58:19,652] [INFO] [stage3.py:2251:step] Full outer step loop took 6.998322248458862
x3006c0s19b1n0: [2024-03-28 13:58:19,683] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.011178016662598
x3006c0s19b1n0: [2024-03-28 13:58:19,683] [INFO] [stage3.py:2251:step] Full outer step loop took 7.011362075805664
x3006c0s19b1n0: [2024-03-28 13:58:19,781] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.128021240234375
x3006c0s19b1n0: [2024-03-28 13:58:19,781] [INFO] [stage3.py:2251:step] Full outer step loop took 7.12817120552063
x3006c0s1b0n0: [2024-03-28 13:58:19,795] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.141522645950317
x3006c0s1b0n0: [2024-03-28 13:58:19,795] [INFO] [stage3.py:2251:step] Full outer step loop took 7.141707420349121
x3006c0s1b0n0: [2024-03-28 13:58:19,858] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.204945802688599
x3006c0s1b0n0: [2024-03-28 13:58:19,858] [INFO] [stage3.py:2251:step] Full outer step loop took 7.205099821090698
x3006c0s19b1n0: [2024-03-28 13:58:19,869] [INFO] [stage3.py:2277:step] End to end step took 7.2157301902771
x3006c0s1b0n0: [2024-03-28 13:58:19,869] [INFO] [stage3.py:2277:step] End to end step took 7.215606689453125
x3006c0s19b1n0: [2024-03-28 13:58:19,869] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6980.79
x3006c0s1b0n0: [2024-03-28 13:58:19,869] [INFO] [stage3.py:2277:step] End to end step took 7.2160210609436035
x3006c0s1b0n0: [2024-03-28 13:58:19,869] [INFO] [stage3.py:2277:step] End to end step took 7.216098785400391
x3006c0s1b0n0: [2024-03-28 13:58:19,869] [INFO] [stage3.py:2277:step] End to end step took 7.2161431312561035
x3006c0s19b1n0: [2024-03-28 13:58:19,869] [INFO] [stage3.py:2277:step] End to end step took 7.216164827346802
x3006c0s19b1n0: [2024-03-28 13:58:19,870] [INFO] [stage3.py:2277:step] End to end step took 7.216418504714966
x3006c0s19b1n0: [2024-03-28 13:58:19,870] [INFO] [stage3.py:2277:step] End to end step took 7.197807312011719
x3006c0s19b1n0: [2024-03-28 13:58:19,870] [INFO] [logging.py:96:log_dist] [Rank 0] step=6, skipped=0, lr=[0.0001415574960149644], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:58:19,870] [INFO] [timer.py:260:stop] epoch=0/micro_step=6/global_step=6, RunningAvgSamplesPerSec=1.8442770817641114, CurrSamplesPerSec=1.8738908340426221, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 13:58:19,871] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 13075.93 | bwd_microstep: 38303.16 | bwd_inner_microstep: 38115.17 | bwd_allreduce_microstep: 187.77 | step_microstep: 7242.68
x3006c0s19b1n0: [2024-03-28 13:58:19,871] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 13075.92 | bwd: 38303.16 | bwd_inner: 38115.19 | bwd_allreduce: 187.79 | step: 7242.68
x3006c0s19b1n0: [2024-03-28 13:58:19,985] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:58:19,985] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:58:19,985] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.43 GB, percent = 67.3%
x3006c0s19b1n0: <TIMER:interval-time,59.619425773620605><TIMER:interval-time,59.61942911148071>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,59.61943340301514><TIMER:interval-time,59.61943340301514>
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,59.6194269657135><TIMER:interval-time,59.619439125061035>
x3006c0s1b0n0: <TIMER:interval-time,59.61944079399109>
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,59.619544506073>
x3006c0s1b0n0:  elapsed_time 59.619439 | consumed samples:          384 | consumed tokens:       786432 | elapsed time per iteration (ms): 59619.4 | learning rate: 1.416E-04 | global batch size:    64 | lm loss: 1.503855E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.073 | TFLOPs: 74.27 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:58:20,136] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:58:20,137] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 13:58:20,137] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.44 GB, percent = 67.3%
x3006c0s19b1n0: [2024-03-28 13:58:26,930] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:58:26,931] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:58:26,931] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.42 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:58:27,007] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:58:27,008] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:58:27,008] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.42 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:58:45,086] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:58:45,086] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 29 GB 
x3006c0s19b1n0: [2024-03-28 13:58:45,086] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.11 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:58:45,171] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:58:45,172] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:58:45,172] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.11 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:58:51,885] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:58:51,886] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:58:51,886] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.1 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:58:51,964] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:58:51,965] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:58:51,965] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.1 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:59:12,677] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:59:12,678] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:59:12,678] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.5 GB, percent = 67.3%
x3006c0s19b1n0: [2024-03-28 13:59:12,750] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:59:12,751] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:59:12,751] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.5 GB, percent = 67.3%
x3006c0s19b1n0: [2024-03-28 13:59:19,646] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.871769905090332
x3006c0s19b1n0: [2024-03-28 13:59:19,647] [INFO] [stage3.py:2251:step] Full outer step loop took 6.872290134429932
x3006c0s1b0n0: [2024-03-28 13:59:19,758] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.983836889266968
x3006c0s1b0n0: [2024-03-28 13:59:19,764] [INFO] [stage3.py:2251:step] Full outer step loop took 6.98957896232605
x3006c0s1b0n0: [2024-03-28 13:59:19,791] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.0167999267578125
x3006c0s1b0n0: [2024-03-28 13:59:19,791] [INFO] [stage3.py:2251:step] Full outer step loop took 7.017085790634155
x3006c0s19b1n0: [2024-03-28 13:59:19,795] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.020896673202515
x3006c0s19b1n0: [2024-03-28 13:59:19,796] [INFO] [stage3.py:2251:step] Full outer step loop took 7.021220445632935
x3006c0s1b0n0: [2024-03-28 13:59:19,826] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.0499982833862305
x3006c0s1b0n0: [2024-03-28 13:59:19,826] [INFO] [stage3.py:2251:step] Full outer step loop took 7.050201892852783
x3006c0s1b0n0: [2024-03-28 13:59:19,828] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.05347204208374
x3006c0s1b0n0: [2024-03-28 13:59:19,828] [INFO] [stage3.py:2251:step] Full outer step loop took 7.053633451461792
x3006c0s19b1n0: [2024-03-28 13:59:19,903] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.12867283821106
x3006c0s19b1n0: [2024-03-28 13:59:19,903] [INFO] [stage3.py:2251:step] Full outer step loop took 7.128850936889648
x3006c0s19b1n0: [2024-03-28 13:59:19,905] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.130015134811401
x3006c0s19b1n0: [2024-03-28 13:59:19,905] [INFO] [stage3.py:2251:step] Full outer step loop took 7.130170583724976
x3006c0s19b1n0: [2024-03-28 13:59:19,915] [INFO] [stage3.py:2277:step] End to end step took 7.1408851146698
x3006c0s19b1n0: [2024-03-28 13:59:19,915] [INFO] [stage3.py:2277:step] End to end step took 7.140969753265381
x3006c0s1b0n0: [2024-03-28 13:59:19,915] [INFO] [stage3.py:2277:step] End to end step took 7.1397106647491455
x3006c0s1b0n0: [2024-03-28 13:59:19,915] [INFO] [stage3.py:2277:step] End to end step took 7.140951871871948
x3006c0s1b0n0: [2024-03-28 13:59:19,916] [INFO] [stage3.py:2277:step] End to end step took 7.1411073207855225
x3006c0s19b1n0: [2024-03-28 13:59:19,915] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6872.58
x3006c0s1b0n0: [2024-03-28 13:59:19,916] [INFO] [stage3.py:2277:step] End to end step took 7.141167163848877
x3006c0s19b1n0: [2024-03-28 13:59:19,916] [INFO] [stage3.py:2277:step] End to end step took 7.141398191452026
x3006c0s19b1n0: [2024-03-28 13:59:19,916] [INFO] [stage3.py:2277:step] End to end step took 7.141380310058594
x3006c0s19b1n0: [2024-03-28 13:59:19,916] [INFO] [logging.py:96:log_dist] [Rank 0] step=7, skipped=0, lr=[9.750000000000001e-05], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:59:19,916] [INFO] [timer.py:260:stop] epoch=0/micro_step=7/global_step=7, RunningAvgSamplesPerSec=1.843847958828505, CurrSamplesPerSec=1.8421334621796337, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 13:59:19,917] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 13436.14 | bwd_microstep: 38429.45 | bwd_inner_microstep: 38232.55 | bwd_allreduce_microstep: 196.70 | step_microstep: 7165.76
x3006c0s19b1n0: [2024-03-28 13:59:19,917] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 13436.13 | bwd: 38429.45 | bwd_inner: 38232.57 | bwd_allreduce: 196.70 | step: 7165.77
x3006c0s19b1n0: [2024-03-28 13:59:20,025] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:59:20,025] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:59:20,025] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.43 GB, percent = 67.3%
x3006c0s19b1n0: <TIMER:interval-time,60.03989100456238><TIMER:interval-time,60.03989315032959><TIMER:interval-time,60.039896965026855>
x3006c0s19b1n0: <TIMER:interval-time,60.03989577293396>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,60.03997588157654><TIMER:interval-time,60.03997874259949><TIMER:interval-time,60.03996777534485>
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,60.03997039794922>
x3006c0s1b0n0: 
x3006c0s1b0n0:  elapsed_time 60.039968 | consumed samples:          448 | consumed tokens:       917504 | elapsed time per iteration (ms): 60040.0 | learning rate: 9.750E-05 | global batch size:    64 | lm loss: 1.281675E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.066 | TFLOPs: 73.75 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:59:20,112] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:59:20,112] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 13:59:20,112] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.43 GB, percent = 67.3%
x3006c0s19b1n0: [2024-03-28 13:59:26,943] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:59:26,944] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:59:26,944] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.42 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:59:27,017] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:59:27,018] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:59:27,018] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.42 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:59:45,157] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:59:45,158] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 29 GB 
x3006c0s19b1n0: [2024-03-28 13:59:45,158] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.3 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:59:45,236] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:59:45,237] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:59:45,237] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.3 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:59:51,987] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:59:51,988] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:59:51,988] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.29 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:59:52,081] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:59:52,082] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:59:52,082] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.29 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 14:00:12,978] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 14:00:12,978] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 14:00:12,979] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.82 GB, percent = 67.3%
x3006c0s19b1n0: [2024-03-28 14:00:13,046] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 14:00:13,047] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:00:13,047] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.82 GB, percent = 67.3%
x3006c0s19b1n0: [2024-03-28 14:00:19,985] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.901071548461914
x3006c0s19b1n0: [2024-03-28 14:00:19,985] [INFO] [stage3.py:2251:step] Full outer step loop took 6.90127158164978
x3006c0s19b1n0: [2024-03-28 14:00:20,121] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.051316499710083
x3006c0s19b1n0: [2024-03-28 14:00:20,122] [INFO] [stage3.py:2251:step] Full outer step loop took 7.0515031814575195
x3006c0s19b1n0: [2024-03-28 14:00:20,124] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.053700923919678
x3006c0s19b1n0: [2024-03-28 14:00:20,124] [INFO] [stage3.py:2251:step] Full outer step loop took 7.0539069175720215
x3006c0s1b0n0: [2024-03-28 14:00:20,174] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.103834629058838
x3006c0s1b0n0: [2024-03-28 14:00:20,174] [INFO] [stage3.py:2251:step] Full outer step loop took 7.104246377944946
x3006c0s19b1n0: [2024-03-28 14:00:20,195] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.124778509140015
x3006c0s19b1n0: [2024-03-28 14:00:20,195] [INFO] [stage3.py:2251:step] Full outer step loop took 7.124929666519165
x3006c0s1b0n0: [2024-03-28 14:00:20,218] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.147608041763306
x3006c0s1b0n0: [2024-03-28 14:00:20,221] [INFO] [stage3.py:2251:step] Full outer step loop took 7.15077543258667
x3006c0s1b0n0: [2024-03-28 14:00:20,353] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.282541513442993
x3006c0s1b0n0: [2024-03-28 14:00:20,353] [INFO] [stage3.py:2251:step] Full outer step loop took 7.282759189605713
x3006c0s1b0n0: [2024-03-28 14:00:20,383] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.313256740570068
x3006c0s1b0n0: [2024-03-28 14:00:20,384] [INFO] [stage3.py:2251:step] Full outer step loop took 7.313412427902222
x3006c0s19b1n0: [2024-03-28 14:00:20,394] [INFO] [stage3.py:2277:step] End to end step took 7.324059963226318
x3006c0s1b0n0: [2024-03-28 14:00:20,394] [INFO] [stage3.py:2277:step] End to end step took 7.324082136154175
x3006c0s19b1n0: [2024-03-28 14:00:20,394] [INFO] [stage3.py:2277:step] End to end step took 7.32419490814209
x3006c0s1b0n0: [2024-03-28 14:00:20,394] [INFO] [stage3.py:2277:step] End to end step took 7.32414174079895
x3006c0s19b1n0: [2024-03-28 14:00:20,394] [INFO] [stage3.py:2277:step] End to end step took 7.324322462081909
x3006c0s1b0n0: [2024-03-28 14:00:20,394] [INFO] [stage3.py:2277:step] End to end step took 7.32431697845459
x3006c0s19b1n0: [2024-03-28 14:00:20,394] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6901.60
x3006c0s19b1n0: [2024-03-28 14:00:20,395] [INFO] [stage3.py:2277:step] End to end step took 7.311283588409424
x3006c0s1b0n0: [2024-03-28 14:00:20,395] [INFO] [stage3.py:2277:step] End to end step took 7.324632883071899
x3006c0s19b1n0: [2024-03-28 14:00:20,395] [INFO] [logging.py:96:log_dist] [Rank 0] step=8, skipped=0, lr=[6.158400017893797e-05], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 14:00:20,395] [INFO] [timer.py:260:stop] epoch=0/micro_step=8/global_step=8, RunningAvgSamplesPerSec=1.839904846256649, CurrSamplesPerSec=1.8204395802599107, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 14:00:20,396] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 13523.94 | bwd_microstep: 38708.88 | bwd_inner_microstep: 38519.78 | bwd_allreduce_microstep: 188.92 | step_microstep: 7348.44
x3006c0s19b1n0: [2024-03-28 14:00:20,396] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 13523.93 | bwd: 38708.88 | bwd_inner: 38519.79 | bwd_allreduce: 188.94 | step: 7348.42
x3006c0s19b1n0: [2024-03-28 14:00:20,514] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 14:00:20,515] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:00:20,515] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.81 GB, percent = 67.3%
x3006c0s19b1n0: <TIMER:interval-time,60.48910999298096><TIMER:interval-time,60.48911213874817><TIMER:interval-time,60.48910927772522><TIMER:interval-time,60.48910641670227>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,60.4891791343689><TIMER:interval-time,60.489176750183105><TIMER:interval-time,60.4891791343689><TIMER:interval-time,60.48918151855469>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0:  elapsed_time 60.489182 | consumed samples:          512 | consumed tokens:      1048576 | elapsed time per iteration (ms): 60489.2 | learning rate: 6.158E-05 | global batch size:    64 | lm loss: 1.183349E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.058 | TFLOPs: 73.21 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 14:00:20,664] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 14:00:20,664] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 14:00:20,665] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.36 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 14:00:27,131] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 14:00:27,132] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:00:27,132] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.34 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 14:00:27,214] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 14:00:27,215] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:00:27,215] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.23 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 14:00:44,614] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 14:00:44,614] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 29 GB 
x3006c0s19b1n0: [2024-03-28 14:00:44,615] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.18 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 14:00:44,705] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 14:00:44,705] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:00:44,705] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.18 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 14:00:51,120] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 14:00:51,121] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:00:51,121] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.17 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 14:00:51,202] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 14:00:51,203] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:00:51,203] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.17 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 14:01:11,144] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 14:01:11,145] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 14:01:11,145] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.81 GB, percent = 67.3%
x3006c0s19b1n0: [2024-03-28 14:01:11,218] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 14:01:11,219] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:01:11,219] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.81 GB, percent = 67.3%
x3006c0s1b0n0: [2024-03-28 14:01:18,160] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.885432004928589
x3006c0s1b0n0: [2024-03-28 14:01:18,160] [INFO] [stage3.py:2251:step] Full outer step loop took 6.885644197463989
x3006c0s19b1n0: [2024-03-28 14:01:18,201] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.9584550857543945
x3006c0s19b1n0: [2024-03-28 14:01:18,202] [INFO] [stage3.py:2251:step] Full outer step loop took 6.958685874938965
x3006c0s1b0n0: [2024-03-28 14:01:18,250] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.007580995559692
x3006c0s1b0n0: [2024-03-28 14:01:18,251] [INFO] [stage3.py:2251:step] Full outer step loop took 7.007778882980347
x3006c0s19b1n0: [2024-03-28 14:01:18,250] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.007359266281128
x3006c0s19b1n0: [2024-03-28 14:01:18,250] [INFO] [stage3.py:2251:step] Full outer step loop took 7.0076210498809814
x3006c0s1b0n0: [2024-03-28 14:01:18,280] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.036689519882202
x3006c0s1b0n0: [2024-03-28 14:01:18,280] [INFO] [stage3.py:2251:step] Full outer step loop took 7.036892652511597
x3006c0s1b0n0: [2024-03-28 14:01:18,324] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.081305742263794
x3006c0s1b0n0: [2024-03-28 14:01:18,324] [INFO] [stage3.py:2251:step] Full outer step loop took 7.08147668838501
x3006c0s19b1n0: [2024-03-28 14:01:18,328] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.0851171016693115
x3006c0s19b1n0: [2024-03-28 14:01:18,328] [INFO] [stage3.py:2251:step] Full outer step loop took 7.085312843322754
x3006c0s19b1n0: [2024-03-28 14:01:18,335] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.0924341678619385
x3006c0s19b1n0: [2024-03-28 14:01:18,335] [INFO] [stage3.py:2251:step] Full outer step loop took 7.092593431472778
x3006c0s19b1n0: [2024-03-28 14:01:18,346] [INFO] [stage3.py:2277:step] End to end step took 7.103402376174927
x3006c0s1b0n0: [2024-03-28 14:01:18,346] [INFO] [stage3.py:2277:step] End to end step took 7.103452682495117
x3006c0s19b1n0: [2024-03-28 14:01:18,346] [INFO] [stage3.py:2277:step] End to end step took 7.103531360626221
x3006c0s1b0n0: [2024-03-28 14:01:18,346] [INFO] [stage3.py:2277:step] End to end step took 7.103600263595581
x3006c0s19b1n0: [2024-03-28 14:01:18,346] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6958.92
x3006c0s19b1n0: [2024-03-28 14:01:18,347] [INFO] [stage3.py:2277:step] End to end step took 7.104005336761475
x3006c0s1b0n0: [2024-03-28 14:01:18,347] [INFO] [stage3.py:2277:step] End to end step took 7.103853464126587
x3006c0s1b0n0: [2024-03-28 14:01:18,347] [INFO] [stage3.py:2277:step] End to end step took 7.072612524032593
x3006c0s19b1n0: [2024-03-28 14:01:18,347] [INFO] [stage3.py:2277:step] End to end step took 7.103943586349487
x3006c0s19b1n0: [2024-03-28 14:01:18,347] [INFO] [logging.py:96:log_dist] [Rank 0] step=9, skipped=0, lr=[3.814149619390238e-05], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 14:01:18,347] [INFO] [timer.py:260:stop] epoch=0/micro_step=9/global_step=9, RunningAvgSamplesPerSec=1.848590236950086, CurrSamplesPerSec=1.9024747736214076, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 14:01:18,348] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 12796.26 | bwd_microstep: 37002.25 | bwd_inner_microstep: 36826.34 | bwd_allreduce_microstep: 174.99 | step_microstep: 7128.48
x3006c0s19b1n0: [2024-03-28 14:01:18,348] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 12796.25 | bwd: 37002.51 | bwd_inner: 36826.36 | bwd_allreduce: 175.38 | step: 7128.48
x3006c0s19b1n0: [2024-03-28 14:01:18,465] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 14:01:18,466] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:01:18,466] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.85 GB, percent = 67.3%
x3006c0s19b1n0: <TIMER:interval-time,57.95085644721985><TIMER:interval-time,57.95085692405701><TIMER:interval-time,57.95085692405701><TIMER:interval-time,57.950860261917114>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,57.950907707214355><TIMER:interval-time,57.95090866088867><TIMER:interval-time,57.95091104507446><TIMER:interval-time,57.95091152191162>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0:  elapsed_time 57.950912 | consumed samples:          576 | consumed tokens:      1179648 | elapsed time per iteration (ms): 57950.9 | learning rate: 3.814E-05 | global batch size:    64 | lm loss: 1.147080E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.104 | TFLOPs: 76.41 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 14:01:18,602] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 14:01:18,603] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 14:01:18,603] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.86 GB, percent = 67.3%
x3006c0s19b1n0: [2024-03-28 14:01:25,120] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 14:01:25,121] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:01:25,121] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.84 GB, percent = 67.3%
x3006c0s19b1n0: [2024-03-28 14:01:25,203] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 14:01:25,204] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:01:25,204] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.84 GB, percent = 67.3%
x3006c0s19b1n0: [2024-03-28 14:01:42,747] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 14:01:42,748] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 29 GB 
x3006c0s19b1n0: [2024-03-28 14:01:42,748] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.4 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 14:01:42,841] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 14:01:42,841] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:01:42,841] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.4 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 14:01:49,373] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 14:01:49,373] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:01:49,373] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.39 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 14:01:49,455] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 14:01:49,456] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:01:49,456] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.39 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 14:02:10,490] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 14:02:10,491] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 14:02:10,491] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.49 GB, percent = 67.3%
x3006c0s19b1n0: [2024-03-28 14:02:10,563] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 14:02:10,564] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:02:10,564] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.48 GB, percent = 67.3%
x3006c0s19b1n0: [2024-03-28 14:02:17,317] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.729053258895874
x3006c0s19b1n0: [2024-03-28 14:02:17,318] [INFO] [stage3.py:2251:step] Full outer step loop took 6.729325771331787
x3006c0s1b0n0: [2024-03-28 14:02:17,362] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.7542641162872314
x3006c0s1b0n0: [2024-03-28 14:02:17,363] [INFO] [stage3.py:2251:step] Full outer step loop took 6.755123138427734
x3006c0s19b1n0: [2024-03-28 14:02:17,464] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.874775171279907
x3006c0s19b1n0: [2024-03-28 14:02:17,468] [INFO] [stage3.py:2251:step] Full outer step loop took 6.879995584487915
x3006c0s1b0n0: [2024-03-28 14:02:17,544] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.955664396286011
x3006c0s1b0n0: [2024-03-28 14:02:17,544] [INFO] [stage3.py:2251:step] Full outer step loop took 6.955872535705566
x3006c0s19b1n0: [2024-03-28 14:02:17,559] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.969283103942871
x3006c0s19b1n0: [2024-03-28 14:02:17,559] [INFO] [stage3.py:2251:step] Full outer step loop took 6.969596862792969
x3006c0s19b1n0: [2024-03-28 14:02:17,608] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.019562244415283
x3006c0s19b1n0: [2024-03-28 14:02:17,608] [INFO] [stage3.py:2251:step] Full outer step loop took 7.019705295562744
x3006c0s1b0n0: [2024-03-28 14:02:17,668] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.0792012214660645
x3006c0s1b0n0: [2024-03-28 14:02:17,668] [INFO] [stage3.py:2251:step] Full outer step loop took 7.079357385635376
x3006c0s1b0n0: [2024-03-28 14:02:17,679] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.09101128578186
x3006c0s1b0n0: [2024-03-28 14:02:17,680] [INFO] [stage3.py:2251:step] Full outer step loop took 7.091161489486694
x3006c0s19b1n0: [2024-03-28 14:02:17,691] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 7019.87
x3006c0s1b0n0: [2024-03-28 14:02:17,691] [INFO] [stage3.py:2277:step] End to end step took 7.102730751037598
x3006c0s1b0n0: [2024-03-28 14:02:17,691] [INFO] [stage3.py:2277:step] End to end step took 7.102550506591797
x3006c0s19b1n0: [2024-03-28 14:02:17,691] [INFO] [stage3.py:2277:step] End to end step took 7.10291314125061
x3006c0s19b1n0: [2024-03-28 14:02:17,691] [INFO] [stage3.py:2277:step] End to end step took 7.1019909381866455
x3006c0s19b1n0: [2024-03-28 14:02:17,691] [INFO] [stage3.py:2277:step] End to end step took 7.103137731552124
x3006c0s19b1n0: [2024-03-28 14:02:17,692] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[3e-05], mom=[(0.9, 0.95)]
x3006c0s1b0n0: [2024-03-28 14:02:17,692] [INFO] [stage3.py:2277:step] End to end step took 7.103079557418823
x3006c0s19b1n0: [2024-03-28 14:02:17,692] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=1.8470692446967913, CurrSamplesPerSec=1.836491978778211, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s1b0n0: [2024-03-28 14:02:17,692] [INFO] [stage3.py:2277:step] End to end step took 7.084186553955078
x3006c0s19b1n0: [2024-03-28 14:02:17,692] [INFO] [stage3.py:2277:step] End to end step took 7.103546619415283
x3006c0s19b1n0: [2024-03-28 14:02:17,692] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 12963.35 | bwd_microstep: 38174.54 | bwd_inner_microstep: 37995.17 | bwd_allreduce_microstep: 179.16 | step_microstep: 7127.70
x3006c0s19b1n0: [2024-03-28 14:02:17,692] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 12963.34 | bwd: 38174.54 | bwd_inner: 37995.18 | bwd_allreduce: 179.20 | step: 7127.70
x3006c0s19b1n0: [2024-03-28 14:02:17,800] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 14:02:17,801] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:02:17,801] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.46 GB, percent = 67.3%
x3006c0s19b1n0: <TIMER:interval-time,59.33503174781799><TIMER:interval-time,59.33503460884094><TIMER:interval-time,59.33503603935242>
x3006c0s19b1n0: <TIMER:interval-time,59.335036277770996>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,59.33506202697754><TIMER:interval-time,59.33506488800049><TIMER:interval-time,59.33506369590759>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,59.33515977859497>
x3006c0s1b0n0:  elapsed_time 59.335062 | consumed samples:          640 | consumed tokens:      1310720 | elapsed time per iteration (ms): 59335.1 | learning rate: 3.000E-05 | global batch size:    64 | lm loss: 1.112424E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.079 | TFLOPs: 74.63 |
x3006c0s1b0n0: <<<only_train:607.6359539031982>>>
x3006c0s19b1n0: <<<only_train:607.6362988948822>>>
x3006c0s19b1n0: <<<only_train:607.6363070011139>>>
x3006c0s1b0n0: <<<only_train:607.6363551616669>>>
x3006c0s19b1n0: <<<only_train:607.6361837387085>>>
x3006c0s19b1n0: <<<only_train:607.6363711357117>>>
x3006c0s1b0n0: <<<only_train:607.6363635063171>>>
x3006c0s1b0n0: <<<only_train:607.6363899707794>>>
x3006c0s19b1n0: [after training ends] datetime: 2024-03-28 14:02:17 
x3006c0s19b1n0: <<<full_time:607.6366419792175>>><<<full_time:607.6364879608154>>>
x3006c0s19b1n0: 
x3006c0s19b1n0: <<<full_time:607.6366419792175>>><<<full_time:607.636620759964>>>
x3006c0s19b1n0: 
x3006c0s1b0n0: <<<full_time:607.6366350650787>>><<<full_time:607.6363384723663>>><<<full_time:607.6366798877716>>>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: <<<full_time:607.6366398334503>>>
x3006c0s19b1n0: [2024-03-28 14:02:21,615] [INFO] [launch.py:348:main] Process 23255 exits successfully.
x3006c0s1b0n0: [2024-03-28 14:02:22,037] [INFO] [launch.py:348:main] Process 14853 exits successfully.
x3006c0s19b1n0: [2024-03-28 14:02:24,618] [INFO] [launch.py:348:main] Process 23254 exits successfully.
x3006c0s19b1n0: [2024-03-28 14:02:24,618] [INFO] [launch.py:348:main] Process 23253 exits successfully.
x3006c0s19b1n0: [2024-03-28 14:02:24,618] [INFO] [launch.py:348:main] Process 23256 exits successfully.
x3006c0s1b0n0: [2024-03-28 14:02:25,041] [INFO] [launch.py:348:main] Process 14851 exits successfully.
x3006c0s1b0n0: [2024-03-28 14:02:25,041] [INFO] [launch.py:348:main] Process 14854 exits successfully.
x3006c0s1b0n0: [2024-03-28 14:02:25,041] [INFO] [launch.py:348:main] Process 14852 exits successfully.

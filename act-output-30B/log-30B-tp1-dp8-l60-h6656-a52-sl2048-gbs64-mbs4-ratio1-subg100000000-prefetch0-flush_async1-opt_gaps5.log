[2024-03-28 14:02:30,575] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-28 14:02:33,669] [INFO] [runner.py:463:main] Using IP address of 10.140.57.107 for node x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-28 14:02:33,671] [INFO] [runner.py:557:main] deepspeed_env file = ./.deepspeed_env
[2024-03-28 14:02:33,672] [INFO] [multinode_runner.py:80:get_cmd] Running on the following workers: x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov,x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-28 14:02:33,672] [INFO] [runner.py:568:main] cmd = pdsh -S -f 1024 -w x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov,x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov export PYTHONUSERBASE=/home/am6429/.local/polaris/conda/2023-10-04; export PYTHONPATH=/home/am6429/dl-io/Megatron-DeepSpeed; export PATH=/home/am6429/.conda/envs/dspeed_env/bin:/soft/datascience/conda/2023-10-04/mconda3/condabin:/soft/compilers/cudatoolkit/cuda-11.8.0/bin:/soft/buildtools/cmake/cmake-3.23.2/cmake-3.23.2-linux-x86_64/bin:/opt/cray/pe/gcc/11.2.0/bin:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/include:/opt/cray/pe/pals/1.2.11/bin:/opt/cray/pe/craype/2.7.20/bin:/opt/cray/libfabric/1.15.2.0/bin:/home/am6429/.conda/envs/dspeed_env/bin:/opt/cray/pe/perftools/23.03.0/bin:/opt/cray/pe/papi/7.0.0.1/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/am6429/.local/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/am6429/veloc-build/include:/home/am6429/veloc-build/bin:/soft/datascience/conda/2023-01-10/mconda3/include:/opt/cray/pe/bin:/soft/datascience/conda/2023-01-10/mconda3/include; export LD_LIBRARY_PATH=/usr/lib64/:/soft/compilers/cudatoolkit/cuda-11.8.0/lib64:/opt/cray/pe/gcc/11.2.0/snos/lib64:/soft/compilers/cudatoolkit/cuda-11.8.0/extras/CUPTI/lib64:/soft/libraries/trt/TensorRT-8.5.3.1.Linux.x86_64-gnu.cuda-11.8.cudnn8.6/lib:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/lib:/soft/libraries/cudnn/cudnn-11-linux-x64-v8.6.0.163/lib:/opt/cray/libfabric/1.15.2.0/lib64:/usr/lib64:/opt/cray/pe/papi/7.0.0.1/lib64:/home/am6429/veloc-build/lib:/home/am6429/veloc-build/lib64:/home/am6429/nvcomp/lib:/soft/datascience/conda/2023-01-10/mconda3/lib:/soft/datascience/conda/2023-01-10/mconda3/lib/; export http_proxy=http://proxy.alcf.anl.gov:3128; export https_proxy=http://proxy.alcf.anl.gov:3128; export CC=gcc; export CXX=g++; export IBV_FORK_SAFE=1; export CFLAGS=-I/soft/datascience/conda/2023-01-10/mconda3/include/; export LDFLAGS=-L/soft/datascience/conda/2023-01-10/mconda3/lib/; export CUDA_DEVICE_MAX_CONNECTIONS=1; export TORCHSNAPSHOT_PER_RANK_MEMORY_BUDGET_BYTES=34359738368; export _DEFAULT_MAX_PER_RANK_IO_CONCURRENCY=1; export _MAX_PER_RANK_IO_CONCURRENCY=1; export NSYS_REPORT_DIR=/home/am6429/dl-io/dl-io-outputs/act-output-30B//rep-30B-tp1-dp8-l60-h6656-a52-sl2048-gbs64-mbs4-ratio1-subg100000000-prefetch0-flush_async1-opt_gaps5-%n;  cd /home/am6429/dl-io/Megatron-DeepSpeed; /home/am6429/.conda/envs/dspeed_env/bin/python -u -m deepspeed.launcher.launch --world_info=eyJ4MzAwNmMwczE5YjFuMC5oc24uY20ucG9sYXJpcy5hbGNmLmFubC5nb3YiOiBbMCwgMSwgMiwgM10sICJ4MzAwNmMwczFiMG4wLmhzbi5jbS5wb2xhcmlzLmFsY2YuYW5sLmdvdiI6IFswLCAxLCAyLCAzXX0= --node_rank=%n --master_addr=10.140.57.107 --master_port=29700 /home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py --tensor-model-parallel-size 1 --num-layers 60 --hidden-size 6656 --num-attention-heads 52 --micro-batch-size 4 --global-batch-size 64 --seq-length 2048 --max-position-embeddings 2048 --train-iters 10 --save /local/scratch/llama2/zero3-tp1}_dp8 --data-path /home/am6429/dl-io/datasets/meg-gpt2_text_document --vocab-file /home/am6429/dl-io/datasets/gpt2-vocab.json --merge-file /home/am6429/dl-io/datasets/gpt2-merges.txt --data-impl mmap --tokenizer-type GPTSentencePieceTokenizer --tokenizer-model /home/am6429/dl-io/datasets/tokenizer.model --split 949,50,1 --distributed-backend nccl --lr 3e-4 --lr-decay-style cosine --min-lr 3e-5 --weight-decay 0.1 --clip-grad 1 --lr-warmup-iters 1 --optimizer adam --adam-beta1 0.9 --adam-beta2 0.95 --log-interval 1 --save-interval 1000 --eval-interval 1000 --eval-iters 0 --bf16 --no-query-key-layer-scaling --attention-dropout 0 --hidden-dropout 0 --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization rmsnorm --disable-bias-linear --num-key-value-heads 4 --deepspeed --exit-interval 20 --deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json --zero-stage=3 --no-pipeline-parallel --cpu-optimizer --checkpoint-activations --deepspeed-activation-checkpointing
x3006c0s1b0n0: [2024-03-28 14:02:35,695] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-28 14:02:35,712] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-28 14:02:37,533] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3006c0s19b1n0: [2024-03-28 14:02:37,533] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=0
x3006c0s19b1n0: [2024-03-28 14:02:37,533] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3006c0s19b1n0: [2024-03-28 14:02:37,533] [INFO] [launch.py:163:main] dist_world_size=8
x3006c0s19b1n0: [2024-03-28 14:02:37,533] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3006c0s19b1n0: [2024-03-28 14:02:37,534] [INFO] [launch.py:253:main] process 27034 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-28 14:02:37,535] [INFO] [launch.py:253:main] process 27035 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-28 14:02:37,535] [INFO] [launch.py:253:main] process 27036 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-28 14:02:37,536] [INFO] [launch.py:253:main] process 27037 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b0n0: [2024-03-28 14:02:37,992] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3006c0s1b0n0: [2024-03-28 14:02:37,992] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=1
x3006c0s1b0n0: [2024-03-28 14:02:37,992] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3006c0s1b0n0: [2024-03-28 14:02:37,992] [INFO] [launch.py:163:main] dist_world_size=8
x3006c0s1b0n0: [2024-03-28 14:02:37,992] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3006c0s1b0n0: [2024-03-28 14:02:37,993] [INFO] [launch.py:253:main] process 18085 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b0n0: [2024-03-28 14:02:37,993] [INFO] [launch.py:253:main] process 18086 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b0n0: [2024-03-28 14:02:37,994] [INFO] [launch.py:253:main] process 18087 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b0n0: [2024-03-28 14:02:37,995] [INFO] [launch.py:253:main] process 18088 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-28 14:02:39,295] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-28 14:02:39,326] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-28 14:02:39,329] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-28 14:02:39,332] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b0n0: [2024-03-28 14:02:39,826] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b0n0: [2024-03-28 14:02:39,830] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b0n0: [2024-03-28 14:02:39,837] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b0n0: [2024-03-28 14:02:39,839] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [2024-03-28 14:02:42,193] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: INFO: overriding default arguments for tokenizer_type:GPTSentencePieceTokenizer                    with tokenizer_type:GPT2BPETokenizer
x3006c0s19b1n0: using world size: 8, data-parallel-size: 8, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
x3006c0s19b1n0: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
x3006c0s19b1n0: using torch.bfloat16 for parameters ...
x3006c0s19b1n0: ------------------------ arguments ------------------------
x3006c0s19b1n0:   accumulate_allreduce_grads_in_fp32 .............. True
x3006c0s19b1n0:   adam_beta1 ...................................... 0.9
x3006c0s19b1n0:   adam_beta2 ...................................... 0.95
x3006c0s19b1n0:   adam_eps ........................................ 1e-08
x3006c0s19b1n0:   add_bias_linear ................................. False
x3006c0s19b1n0:   add_position_embedding .......................... False
x3006c0s19b1n0:   adlr_autoresume ................................. False
x3006c0s19b1n0:   adlr_autoresume_interval ........................ 1000
x3006c0s19b1n0:   aml_data_download_path .......................... None
x3006c0s19b1n0:   apply_layernorm_1p .............................. False
x3006c0s19b1n0:   apply_query_key_layer_scaling ................... False
x3006c0s19b1n0:   apply_residual_connection_post_layernorm ........ False
x3006c0s19b1n0:   async_tensor_model_parallel_allreduce ........... False
x3006c0s19b1n0:   attention_dropout ............................... 0.0
x3006c0s19b1n0:   attention_softmax_in_fp32 ....................... False
x3006c0s19b1n0:   barrier_with_L1_time ............................ True
x3006c0s19b1n0:   bert_binary_head ................................ True
x3006c0s19b1n0:   bert_embedder_type .............................. megatron
x3006c0s19b1n0:   bert_load ....................................... None
x3006c0s19b1n0:   bf16 ............................................ True
x3006c0s19b1n0:   bias_dropout_fusion ............................. True
x3006c0s19b1n0:   bias_gelu_fusion ................................ False
x3006c0s19b1n0:   biencoder_projection_dim ........................ 0
x3006c0s19b1n0:   biencoder_shared_query_context_model ............ False
x3006c0s19b1n0:   block_data_path ................................. None
x3006c0s19b1n0:   checkpoint_activations .......................... True
x3006c0s19b1n0:   checkpoint_in_cpu ............................... False
x3006c0s19b1n0:   checkpoint_num_layers ........................... 1
x3006c0s19b1n0:   classes_fraction ................................ 1.0
x3006c0s19b1n0:   clip_grad ....................................... 1.0
x3006c0s19b1n0:   compression_training ............................ False
x3006c0s19b1n0:   consumed_train_samples .......................... 0
x3006c0s19b1n0:   consumed_train_tokens ........................... 0
x3006c0s19b1n0:   consumed_valid_samples .......................... 0
x3006c0s19b1n0:   contigious_checkpointing ........................ False
x3006c0s19b1n0:   cpu_optimizer ................................... True
x3006c0s19b1n0:   cpu_torch_adam .................................. False
x3006c0s19b1n0:   create_moe_param_group .......................... False
x3006c0s19b1n0:   curriculum_learning_legacy ...................... False
x3006c0s19b1n0:   data_cache_path ................................. None
x3006c0s19b1n0:   data_efficiency_curriculum_learning ............. False
x3006c0s19b1n0:   data_impl ....................................... mmap
x3006c0s19b1n0:   data_parallel_random_init ....................... False
x3006c0s19b1n0:   data_parallel_size .............................. 8
x3006c0s19b1n0:   data_path ....................................... ['/home/am6429/dl-io/datasets/meg-gpt2_text_document']
x3006c0s19b1n0:   data_per_class_fraction ......................... 1.0
x3006c0s19b1n0:   data_sharding ................................... True
x3006c0s19b1n0:   dataloader_type ................................. single
x3006c0s19b1n0:   DDP_impl ........................................ local
x3006c0s19b1n0:   decoder_num_layers .............................. None
x3006c0s19b1n0:   decoder_seq_length .............................. None
x3006c0s19b1n0:   deepscale ....................................... False
x3006c0s19b1n0:   deepscale_config ................................ None
x3006c0s19b1n0:   deepspeed ....................................... True
x3006c0s19b1n0:   deepspeed_activation_checkpointing .............. True
x3006c0s19b1n0:   deepspeed_config ................................ /home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json
x3006c0s19b1n0:   dino_bottleneck_size ............................ 256
x3006c0s19b1n0:   dino_freeze_last_layer .......................... 1
x3006c0s19b1n0:   dino_head_hidden_size ........................... 2048
x3006c0s19b1n0:   dino_local_crops_number ......................... 10
x3006c0s19b1n0:   dino_local_img_size ............................. 96
x3006c0s19b1n0:   dino_norm_last_layer ............................ False
x3006c0s19b1n0:   dino_teacher_temp ............................... 0.07
x3006c0s19b1n0:   dino_warmup_teacher_temp ........................ 0.04
x3006c0s19b1n0:   dino_warmup_teacher_temp_epochs ................. 30
x3006c0s19b1n0:   distribute_checkpointed_activations ............. False
x3006c0s19b1n0:   distribute_saved_activations .................... False
x3006c0s19b1n0:   distributed_backend ............................. nccl
x3006c0s19b1n0:   distributed_timeout_minutes ..................... 10
x3006c0s19b1n0:   ds_inference .................................... False
x3006c0s19b1n0:   ds_pipeline_enabled ............................. False
x3006c0s19b1n0:   ds_sequence_parallel_size ....................... 1
x3006c0s19b1n0:   embedding_path .................................. None
x3006c0s19b1n0:   embedding_weights_in_fp32 ....................... False
x3006c0s19b1n0:   empty_unused_memory_level ....................... 0
x3006c0s19b1n0:   enable_expert_tensor_parallelism ................ False
x3006c0s19b1n0:   encoder_num_layers .............................. 60
x3006c0s19b1n0:   encoder_seq_length .............................. 2048
x3006c0s19b1n0:   end_weight_decay ................................ 0.1
x3006c0s19b1n0:   eod_mask_loss ................................... False
x3006c0s19b1n0:   eval_interval ................................... 1000
x3006c0s19b1n0:   eval_iters ...................................... 0
x3006c0s19b1n0:   evidence_data_path .............................. None
x3006c0s19b1n0:   exit_duration_in_mins ........................... None
x3006c0s19b1n0:   exit_interval ................................... 20
x3006c0s19b1n0:   exit_on_missing_checkpoint ...................... False
x3006c0s19b1n0:   exit_signal_handler ............................. False
x3006c0s19b1n0:   expert_interval ................................. 2
x3006c0s19b1n0:   ffn_hidden_size ................................. 17728
x3006c0s19b1n0:   finetune ........................................ False
x3006c0s19b1n0:   force_ds_sequence_parallel ...................... False
x3006c0s19b1n0:   fp16 ............................................ False
x3006c0s19b1n0:   fp16_lm_cross_entropy ........................... False
x3006c0s19b1n0:   fp32_residual_connection ........................ False
x3006c0s19b1n0:   fp8_amax_compute_algo ........................... most_recent
x3006c0s19b1n0:   fp8_amax_history_len ............................ 1
x3006c0s19b1n0:   fp8_e4m3 ........................................ False
x3006c0s19b1n0:   fp8_hybrid ...................................... False
x3006c0s19b1n0:   fp8_interval .................................... 1
x3006c0s19b1n0:   fp8_margin ...................................... 0
x3006c0s19b1n0:   fp8_wgrad ....................................... True
x3006c0s19b1n0:   global_batch_size ............................... 64
x3006c0s19b1n0:   gradient_accumulation_fusion .................... True
x3006c0s19b1n0:   head_lr_mult .................................... 1.0
x3006c0s19b1n0:   hidden_dropout .................................. 0.0
x3006c0s19b1n0:   hidden_size ..................................... 6656
x3006c0s19b1n0:   hidden_size_teacher ............................. None
x3006c0s19b1n0:   hysteresis ...................................... 2
x3006c0s19b1n0:   ict_head_size ................................... None
x3006c0s19b1n0:   ict_load ........................................ None
x3006c0s19b1n0:   img_h ........................................... 224
x3006c0s19b1n0:   img_w ........................................... 224
x3006c0s19b1n0:   indexer_batch_size .............................. 128
x3006c0s19b1n0:   indexer_log_interval ............................ 1000
x3006c0s19b1n0:   inference ....................................... False
x3006c0s19b1n0:   inference_batch_times_seqlen_threshold .......... 512
x3006c0s19b1n0:   init_method_std ................................. 0.02
x3006c0s19b1n0:   init_method_xavier_uniform ...................... False
x3006c0s19b1n0:   initial_loss_scale .............................. 4294967296
x3006c0s19b1n0:   iter_per_epoch .................................. 1250
x3006c0s19b1n0:   kd .............................................. False
x3006c0s19b1n0:   kd_alpha_ce ..................................... 1
x3006c0s19b1n0:   kd_beta_ce ...................................... 1
x3006c0s19b1n0:   kd_temp ......................................... 1.0
x3006c0s19b1n0:   kv_channels ..................................... 128
x3006c0s19b1n0:   layernorm_epsilon ............................... 1e-05
x3006c0s19b1n0:   lazy_mpu_init ................................... None
x3006c0s19b1n0:   load ............................................ None
x3006c0s19b1n0:   load_teacher .................................... None
x3006c0s19b1n0:   local_rank ...................................... 0
x3006c0s19b1n0:   log_batch_size_to_tensorboard ................... False
x3006c0s19b1n0:   log_interval .................................... 1
x3006c0s19b1n0:   log_learning_rate_to_tensorboard ................ True
x3006c0s19b1n0:   log_loss_scale_to_tensorboard ................... True
x3006c0s19b1n0:   log_memory_to_tensorboard ....................... False
x3006c0s19b1n0:   log_num_zeros_in_grad ........................... False
x3006c0s19b1n0:   log_optimizer_states_to_tensorboard ............. False
x3006c0s19b1n0:   log_params_norm ................................. False
x3006c0s19b1n0:   log_timers_to_tensorboard ....................... False
x3006c0s19b1n0:   log_validation_ppl_to_tensorboard ............... False
x3006c0s19b1n0:   log_world_size_to_tensorboard ................... False
x3006c0s19b1n0:   loss_scale ...................................... None
x3006c0s19b1n0:   loss_scale_window ............................... 1000
x3006c0s19b1n0:   lr .............................................. 0.0003
x3006c0s19b1n0:   lr_decay_iters .................................. None
x3006c0s19b1n0:   lr_decay_samples ................................ None
x3006c0s19b1n0:   lr_decay_style .................................. cosine
x3006c0s19b1n0:   lr_decay_tokens ................................. None
x3006c0s19b1n0:   lr_warmup_fraction .............................. None
x3006c0s19b1n0:   lr_warmup_iters ................................. 1
x3006c0s19b1n0:   lr_warmup_samples ............................... 0
x3006c0s19b1n0:   lr_warmup_tokens ................................ None
x3006c0s19b1n0:   make_vocab_size_divisible_by .................... 128
x3006c0s19b1n0:   mask_factor ..................................... 1.0
x3006c0s19b1n0:   mask_prob ....................................... 0.15
x3006c0s19b1n0:   mask_type ....................................... random
x3006c0s19b1n0:   masked_softmax_fusion ........................... True
x3006c0s19b1n0:   max_position_embeddings ......................... 2048
x3006c0s19b1n0:   max_tokens_to_oom ............................... 12000
x3006c0s19b1n0:   memory_centric_tiled_linear ..................... False
x3006c0s19b1n0:   merge_file ...................................... /home/am6429/dl-io/datasets/gpt2-merges.txt
x3006c0s19b1n0:   micro_batch_size ................................ 4
x3006c0s19b1n0:   min_loss_scale .................................. 1.0
x3006c0s19b1n0:   min_lr .......................................... 3e-05
x3006c0s19b1n0:   mlp_type ........................................ standard
x3006c0s19b1n0:   mmap_warmup ..................................... False
x3006c0s19b1n0:   moe_eval_capacity_factor ........................ 1.0
x3006c0s19b1n0:   moe_expert_parallel_size ........................ 1
x3006c0s19b1n0:   moe_loss_coeff .................................. 0.1
x3006c0s19b1n0:   moe_min_capacity ................................ 4
x3006c0s19b1n0:   moe_token_dropping .............................. True
x3006c0s19b1n0:   moe_train_capacity_factor ....................... 1.0
x3006c0s19b1n0:   mos ............................................. False
x3006c0s19b1n0:   no_load_lr_state ................................ False
x3006c0s19b1n0:   no_load_optim ................................... None
x3006c0s19b1n0:   no_load_rng ..................................... None
x3006c0s19b1n0:   no_persist_layer_norm ........................... False
x3006c0s19b1n0:   no_pipeline_parallel ............................ True
x3006c0s19b1n0:   no_save_optim ................................... None
x3006c0s19b1n0:   no_save_rng ..................................... None
x3006c0s19b1n0:   normalization ................................... rmsnorm
x3006c0s19b1n0:   num_attention_heads ............................. 52
x3006c0s19b1n0:   num_attention_heads_teacher ..................... None
x3006c0s19b1n0:   num_channels .................................... 3
x3006c0s19b1n0:   num_classes ..................................... 1000
x3006c0s19b1n0:   num_experts ..................................... [1]
x3006c0s19b1n0:   num_experts_switch .............................. None
x3006c0s19b1n0:   num_experts_teacher ............................. [1]
x3006c0s19b1n0:   num_key_value_heads ............................. 4
x3006c0s19b1n0:   num_layers ...................................... 60
x3006c0s19b1n0:   num_layers_per_virtual_pipeline_stage ........... None
x3006c0s19b1n0:   num_layers_teacher .............................. None
x3006c0s19b1n0:   num_workers ..................................... 2
x3006c0s19b1n0:   onnx_safe ....................................... None
x3006c0s19b1n0:   openai_gelu ..................................... False
x3006c0s19b1n0:   optimizer ....................................... adam
x3006c0s19b1n0:   output_bert_embeddings .......................... False
x3006c0s19b1n0:   overlap_p2p_comm ................................ False
x3006c0s19b1n0:   override_opt_param_scheduler .................... False
x3006c0s19b1n0:   params_dtype .................................... torch.bfloat16
x3006c0s19b1n0:   partition_activations ........................... False
x3006c0s19b1n0:   patch_dim ....................................... 16
x3006c0s19b1n0:   perform_initialization .......................... True
x3006c0s19b1n0:   pipeline_model_parallel_size .................... 1
x3006c0s19b1n0:   pipeline_model_parallel_split_rank .............. None
x3006c0s19b1n0:   profile_backward ................................ False
x3006c0s19b1n0:   query_in_block_prob ............................. 0.1
x3006c0s19b1n0:   rampup_batch_size ............................... None
x3006c0s19b1n0:   random_ltd ...................................... False
x3006c0s19b1n0:   rank ............................................ 0
x3006c0s19b1n0:   recompute_granularity ........................... None
x3006c0s19b1n0:   recompute_method ................................ None
x3006c0s19b1n0:   recompute_num_layers ............................ 1
x3006c0s19b1n0:   remote_device ................................... none
x3006c0s19b1n0:   reset_attention_mask ............................ False
x3006c0s19b1n0:   reset_iteration ................................. False
x3006c0s19b1n0:   reset_position_ids .............................. False
x3006c0s19b1n0:   retriever_report_topk_accuracies ................ []
x3006c0s19b1n0:   retriever_score_scaling ......................... False
x3006c0s19b1n0:   retriever_seq_length ............................ 256
x3006c0s19b1n0:   retro_add_retriever ............................. False
x3006c0s19b1n0:   retro_cyclic_train_iters ........................ None
x3006c0s19b1n0:   retro_encoder_attention_dropout ................. 0.1
x3006c0s19b1n0:   retro_encoder_hidden_dropout .................... 0.1
x3006c0s19b1n0:   retro_encoder_layers ............................ 2
x3006c0s19b1n0:   retro_num_neighbors ............................. 2
x3006c0s19b1n0:   retro_num_retrieved_chunks ...................... 2
x3006c0s19b1n0:   retro_return_doc_ids ............................ False
x3006c0s19b1n0:   retro_workdir ................................... None
x3006c0s19b1n0:   return_data_index ............................... False
x3006c0s19b1n0:   rotary_percent .................................. 1.0
x3006c0s19b1n0:   sample_rate ..................................... 1.0
x3006c0s19b1n0:   save ............................................ /local/scratch/llama2/zero3-tp1}_dp8
x3006c0s19b1n0:   save_interval ................................... 1000
x3006c0s19b1n0:   scatter_gather_tensors_in_pipeline .............. True
x3006c0s19b1n0:   scattered_embeddings ............................ False
x3006c0s19b1n0:   seed ............................................ 1234
x3006c0s19b1n0:   seq_length ...................................... 2048
x3006c0s19b1n0:   sequence_parallel ............................... False
x3006c0s19b1n0:   sgd_momentum .................................... 0.9
x3006c0s19b1n0:   short_seq_prob .................................. 0.1
x3006c0s19b1n0:   skip_train ...................................... False
x3006c0s19b1n0:   split ........................................... 949,50,1
x3006c0s19b1n0:   split_transformers .............................. False
x3006c0s19b1n0:   squared_relu .................................... False
x3006c0s19b1n0:   standalone_embedding_stage ...................... False
x3006c0s19b1n0:   start_weight_decay .............................. 0.1
x3006c0s19b1n0:   swiglu .......................................... True
x3006c0s19b1n0:   swin_backbone_type .............................. tiny
x3006c0s19b1n0:   synchronize_each_layer .......................... False
x3006c0s19b1n0:   tensor_model_parallel_size ...................... 1
x3006c0s19b1n0:   tensorboard_dir ................................. None
x3006c0s19b1n0:   tensorboard_log_interval ........................ 1
x3006c0s19b1n0:   tensorboard_queue_size .......................... 1000
x3006c0s19b1n0:   test_data_path .................................. None
x3006c0s19b1n0:   tile_factor ..................................... 1
x3006c0s19b1n0:   timing_log_level ................................ 0
x3006c0s19b1n0:   timing_log_option ............................... minmax
x3006c0s19b1n0:   titles_data_path ................................ None
x3006c0s19b1n0:   tokenizer_model ................................. /home/am6429/dl-io/datasets/tokenizer.model
x3006c0s19b1n0:   tokenizer_type .................................. GPT2BPETokenizer
x3006c0s19b1n0:   topk ............................................ 1
x3006c0s19b1n0:   train_data_exact_num_epochs ..................... None
x3006c0s19b1n0:   train_data_path ................................. None
x3006c0s19b1n0:   train_desc_path ................................. None
x3006c0s19b1n0:   train_doc_idx_path .............................. None
x3006c0s19b1n0:   train_idx_path .................................. None
x3006c0s19b1n0:   train_iters ..................................... 10
x3006c0s19b1n0:   train_sample_idx_path ........................... None
x3006c0s19b1n0:   train_samples ................................... None
x3006c0s19b1n0:   train_shuffle_idx_path .......................... None
x3006c0s19b1n0:   train_tokens .................................... None
x3006c0s19b1n0:   transformer_impl ................................ local
x3006c0s19b1n0:   transformer_pipeline_model_parallel_size ........ 1
x3006c0s19b1n0:   untie_embeddings_and_output_weights ............. True
x3006c0s19b1n0:   use_checkpoint_args ............................. False
x3006c0s19b1n0:   use_checkpoint_opt_param_scheduler .............. False
x3006c0s19b1n0:   use_contiguous_buffers_in_local_ddp ............. True
x3006c0s19b1n0:   use_cpu_initialization .......................... None
x3006c0s19b1n0:   use_dataset_only ................................ False
x3006c0s19b1n0:   use_distributed_optimizer ....................... False
x3006c0s19b1n0:   use_flash_attn .................................. False
x3006c0s19b1n0:   use_flash_attn_triton ........................... False
x3006c0s19b1n0:   use_flash_attn_v1 ............................... False
x3006c0s19b1n0:   use_flash_attn_v2 ............................... False
x3006c0s19b1n0:   use_one_sent_docs ............................... False
x3006c0s19b1n0:   use_pin_memory .................................. False
x3006c0s19b1n0:   use_ring_exchange_p2p ........................... False
x3006c0s19b1n0:   use_rotary_position_embeddings .................. True
x3006c0s19b1n0:   use_tutel ....................................... False
x3006c0s19b1n0:   valid_data_path ................................. None
x3006c0s19b1n0:   variable_seq_lengths ............................ False
x3006c0s19b1n0:   virtual_pipeline_model_parallel_size ............ None
x3006c0s19b1n0:   vision_backbone_type ............................ vit
x3006c0s19b1n0:   vision_pretraining .............................. False
x3006c0s19b1n0:   vision_pretraining_type ......................... classify
x3006c0s19b1n0:   vocab_extra_ids ................................. 0
x3006c0s19b1n0:   vocab_file ...................................... /home/am6429/dl-io/datasets/gpt2-vocab.json
x3006c0s19b1n0:   vocab_size ...................................... None
x3006c0s19b1n0:   weight_decay .................................... 0.1
x3006c0s19b1n0:   weight_decay_incr_style ......................... constant
x3006c0s19b1n0:   world_size ...................................... 8
x3006c0s19b1n0:   zero_allgather_bucket_size ...................... 0.0
x3006c0s19b1n0:   zero_contigious_gradients ....................... False
x3006c0s19b1n0:   zero_reduce_bucket_size ......................... 0.0
x3006c0s19b1n0:   zero_reduce_scatter ............................. False
x3006c0s19b1n0:   zero_stage ...................................... 3
x3006c0s19b1n0: -------------------- end of arguments ---------------------
x3006c0s19b1n0: setting number of micro-batches to constant 2
x3006c0s19b1n0: > building GPT2BPETokenizer tokenizer ...
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
x3006c0s19b1n0: > initializing torch distributed ...
x3006c0s19b1n0: [2024-03-28 14:02:42,295] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: [2024-03-28 14:02:42,296] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
x3006c0s19b1n0: [2024-03-28 14:02:42,313] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: [2024-03-28 14:02:42,326] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b0n0:       meet the required dependencies to JIT install the op.
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: JIT compiled ops requires ninja
x3006c0s1b0n0: ninja .................. [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: op name ................ installed .. compatible
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b0n0:       meet the required dependencies to JIT install the op.
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: JIT compiled ops requires ninja
x3006c0s1b0n0: ninja .................. [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: op name ................ installed .. compatible
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b0n0:       meet the required dependencies to JIT install the op.
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: JIT compiled ops requires ninja
x3006c0s1b0n0: ninja .................. [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: op name ................ installed .. compatible
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b0n0:       meet the required dependencies to JIT install the op.
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: JIT compiled ops requires ninja
x3006c0s1b0n0: ninja .................. [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: op name ................ installed .. compatible
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: DeepSpeed general environment info:
x3006c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b0n0: torch version .................... 2.0.1+cu118
x3006c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b0n0: torch cuda version ............... 11.8
x3006c0s1b0n0: torch hip version ................ None
x3006c0s1b0n0: nvcc version ..................... 11.8
x3006c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: DeepSpeed general environment info:
x3006c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b0n0: torch version .................... 2.0.1+cu118
x3006c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b0n0: torch cuda version ............... 11.8
x3006c0s1b0n0: torch hip version ................ None
x3006c0s1b0n0: nvcc version ..................... 11.8
x3006c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed general environment info:
x3006c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b0n0: torch version .................... 2.0.1+cu118
x3006c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b0n0: torch cuda version ............... 11.8
x3006c0s1b0n0: torch hip version ................ None
x3006c0s1b0n0: nvcc version ..................... 11.8
x3006c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b0n0: DeepSpeed general environment info:
x3006c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b0n0: torch version .................... 2.0.1+cu118
x3006c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b0n0: torch cuda version ............... 11.8
x3006c0s1b0n0: torch hip version ................ None
x3006c0s1b0n0: nvcc version ..................... 11.8
x3006c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b0n0: [2024-03-28 14:02:42,866] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b0n0: [2024-03-28 14:02:42,892] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b0n0: [2024-03-28 14:02:42,902] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b0n0: [2024-03-28 14:02:42,903] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: > initialized tensor model parallel with size 1
x3006c0s19b1n0: > initialized pipeline model parallel with size 1
x3006c0s19b1n0: > setting random seeds to 1234 ...
x3006c0s19b1n0: [2024-03-28 14:02:43,651] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
x3006c0s19b1n0: > compiling dataset index builder ...
x3006c0s19b1n0: make: Entering directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3006c0s19b1n0: make: Nothing to be done for 'default'.
x3006c0s19b1n0: make: Leaving directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3006c0s19b1n0: >>> done with dataset index builder. Compilation time: 0.083 seconds
x3006c0s19b1n0: > compiling and loading fused kernels ...
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: >>> done with compiling and loading fused kernels. Compilation time: 3.578 seconds
x3006c0s1b0n0: <<<<<<<<<<< 4
x3006c0s1b0n0: <<<<<<<<<<< 6
x3006c0s19b1n0: <<<<<<<<<<< 1
x3006c0s1b0n0: <<<<<<<<<<< 5
x3006c0s1b0n0: <<<<<<<<<<< 7
x3006c0s19b1n0: <<<<<<<<<<< 3
x3006c0s19b1n0: <<<<<<<<<<< 2
x3006c0s19b1n0: initialize_megatron took 5.860059499740601
x3006c0s19b1n0: <<<<<<<<<<< 0
x3006c0s19b1n0: time to initialize megatron (seconds): 6.243
x3006c0s19b1n0: [after megatron is initialized] datetime: 2024-03-28 14:02:48 
x3006c0s19b1n0: get_accelerator and all_reduce  took 0.0007295608520507812
x3006c0s19b1n0: building GPT model ...
x3006c0s19b1n0: [2024-03-28 14:02:48,184] [INFO] [utils.py:800:see_memory_usage] Before Building Model
x3006c0s19b1n0: [2024-03-28 14:02:48,185] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 4.62 GB         CA 0.0 GB         Max_CA 5 GB 
x3006c0s19b1n0: [2024-03-28 14:02:48,185] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 21.64 GB, percent = 4.3%
x3006c0s19b1n0: [2024-03-28 14:02:54,444] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 423, num_elems = 27.64B
x3006c0s19b1n0: [2024-03-28 14:02:54,515] [INFO] [utils.py:800:see_memory_usage] After Building Model
x3006c0s19b1n0: [2024-03-28 14:02:54,515] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 7.07 GB         CA 21.89 GB         Max_CA 38 GB 
x3006c0s19b1n0: [2024-03-28 14:02:54,516] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 22.06 GB, percent = 4.4%
x3006c0s19b1n0:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 27635239424
x3006c0s1b0n0: ninja: no work to do.
x3006c0s1b0n0: Time to load cpu_adam op: 2.4411745071411133 seconds
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: Time to load cpu_adam op: 2.463533639907837 seconds
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: Time to load cpu_adam op: 2.444836139678955 seconds
x3006c0s19b1n0: Time to load cpu_adam op: 2.546257495880127 seconds
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: Time to load cpu_adam op: 2.574481725692749 seconds
x3006c0s1b0n0: ninja: no work to do.
x3006c0s1b0n0: Time to load cpu_adam op: 2.4393365383148193 seconds
x3006c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s1b0n0: Time to load cpu_adam op: 2.438943386077881 seconds
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: > learning rate decay style: cosine
x3006c0s19b1n0: DeepSpeed is enabled.
x3006c0s19b1n0: [2024-03-28 14:02:58,985] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.3+8074cd62, git-hash=8074cd62, git-branch=hybrid_opt_offload
x3006c0s19b1n0: [2024-03-28 14:02:59,058] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After args sanity test
x3006c0s19b1n0: [2024-03-28 14:02:59,058] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.89 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 14:02:59,059] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 28.25 GB, percent = 5.6%
x3006c0s1b0n0: ninja: no work to do.
x3006c0s1b0n0: Time to load cpu_adam op: 2.52630352973938 seconds
x3006c0s19b1n0: [2024-03-28 14:02:59,117] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure distributed model
x3006c0s19b1n0: [2024-03-28 14:02:59,117] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.89 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 14:02:59,117] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 28.54 GB, percent = 5.7%
x3006c0s19b1n0: [2024-03-28 14:02:59,181] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After configure distributed model
x3006c0s19b1n0: [2024-03-28 14:02:59,182] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.89 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 14:02:59,182] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 28.85 GB, percent = 5.7%
x3006c0s19b1n0: [2024-03-28 14:02:59,182] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-28 14:02:59,237] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After setting model parameters
x3006c0s19b1n0: [2024-03-28 14:02:59,237] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.89 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 14:02:59,237] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.05 GB, percent = 5.8%
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-28 14:02:59,293] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure optimizer
x3006c0s19b1n0: [2024-03-28 14:02:59,293] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.89 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 14:02:59,293] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.23 GB, percent = 5.8%
x3006c0s19b1n0: [2024-03-28 14:02:59,294] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
x3006c0s19b1n0: [2024-03-28 14:02:59,294] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
x3006c0s19b1n0: [2024-03-28 14:02:59,311] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
x3006c0s19b1n0: [2024-03-28 14:02:59,312] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
x3006c0s19b1n0: [2024-03-28 14:02:59,312] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
x3006c0s19b1n0: [2024-03-28 14:02:59,312] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
x3006c0s19b1n0: [2024-03-28 14:02:59,364] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
x3006c0s19b1n0: [2024-03-28 14:02:59,365] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.89 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 14:02:59,365] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.34 GB, percent = 5.8%
x3006c0s19b1n0: [2024-03-28 14:02:59,367] [INFO] [stage3.py:137:__init__] Reduce bucket size 500,000,000
x3006c0s19b1n0: [2024-03-28 14:02:59,367] [INFO] [stage3.py:138:__init__] Prefetch bucket size 50,000,000
x3006c0s19b1n0: [2024-03-28 14:02:59,421] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
x3006c0s19b1n0: [2024-03-28 14:02:59,422] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.89 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 14:02:59,422] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.43 GB, percent = 5.8%
x3006c0s19b1n0: Parameter Offload: Total persistent parameters: 805376 in 121 params
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-28 14:02:59,500] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
x3006c0s19b1n0: [2024-03-28 14:02:59,501] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.89 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 14:02:59,501] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.48 GB, percent = 5.9%
x3006c0s19b1n0: [2024-03-28 14:02:59,557] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
x3006c0s19b1n0: [2024-03-28 14:02:59,558] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.89 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 14:02:59,558] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.48 GB, percent = 5.9%
x3006c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-28 14:02:59,933] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-28 14:02:59,933] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-28 14:02:59,933] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-28 14:02:59,933] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b0n0: [2024-03-28 14:02:59,933] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b0n0: [2024-03-28 14:02:59,933] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b0n0: [2024-03-28 14:02:59,933] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b0n0: [2024-03-28 14:02:59,933] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-28 14:02:59,933] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-28 14:02:59,933] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-28 14:02:59,933] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-28 14:02:59,933] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b0n0: [2024-03-28 14:02:59,933] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b0n0: [2024-03-28 14:02:59,933] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b0n0: [2024-03-28 14:02:59,933] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b0n0: [2024-03-28 14:02:59,933] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-28 14:02:59,933] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,933] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,933] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,933] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,933] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,933] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,933] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,933] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b0n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b0n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b0n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b0n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b0n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-28 14:02:59,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-28 14:03:01,985] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 31
x3006c0s19b1n0: [2024-03-28 14:03:01,986] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.44 GB         CA 6.44 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 14:03:01,986] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 48.83 GB, percent = 9.7%
x3006c0s19b1n0: [2024-03-28 14:03:02,052] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
x3006c0s19b1n0: [2024-03-28 14:03:02,052] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-28 14:03:02,053] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 48.83 GB, percent = 9.7%
x3006c0s19b1n0: [2024-03-28 14:03:04,776] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
x3006c0s19b1n0: [2024-03-28 14:03:04,776] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-28 14:03:04,777] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 61.65 GB, percent = 12.3%
x3006c0s19b1n0: [2024-03-28 14:03:07,165] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
x3006c0s19b1n0: [2024-03-28 14:03:07,165] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-28 14:03:07,165] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 84.27 GB, percent = 16.7%
x3006c0s19b1n0: [2024-03-28 14:03:13,703] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | init_optimizer_state: 6525.10
x3006c0s19b1n0: [2024-03-28 14:03:13,800] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
x3006c0s19b1n0: [2024-03-28 14:03:13,801] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-28 14:03:13,801] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 144.46 GB, percent = 28.7%
x3006c0s19b1n0: [2024-03-28 14:03:14,000] [INFO] [stage3.py:520:_setup_for_real_optimizer] optimizer state initialized
x3006c0s19b1n0: [2024-03-28 14:03:22,427] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
x3006c0s19b1n0: [2024-03-28 14:03:22,427] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 8.61 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 14:03:22,427] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 177.15 GB, percent = 35.2%
x3006c0s19b1n0: [2024-03-28 14:03:22,428] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
x3006c0s19b1n0: [2024-03-28 14:03:22,494] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure LR scheduler
x3006c0s19b1n0: [2024-03-28 14:03:22,495] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 14:03:22,495] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 177.16 GB, percent = 35.2%
x3006c0s19b1n0: [2024-03-28 14:03:22,495] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
x3006c0s19b1n0: [2024-03-28 14:03:22,495] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7f9c25f401f0>
x3006c0s19b1n0: [2024-03-28 14:03:22,495] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 14:03:22,559] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before rewriting optimizer step
x3006c0s19b1n0: [2024-03-28 14:03:22,560] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 14:03:22,560] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 177.17 GB, percent = 35.2%
x3006c0s19b1n0: [2024-03-28 14:03:22,626] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure checkpointing
x3006c0s19b1n0: [2024-03-28 14:03:22,626] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 14:03:22,626] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 177.15 GB, percent = 35.2%
x3006c0s19b1n0: [2024-03-28 14:03:22,626] [INFO] [config.py:998:print] DeepSpeedEngine configuration:
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   activation_checkpointing_config  {
x3006c0s19b1n0:     "partition_activations": false, 
x3006c0s19b1n0:     "contiguous_memory_optimization": false, 
x3006c0s19b1n0:     "cpu_checkpointing": false, 
x3006c0s19b1n0:     "number_checkpoints": null, 
x3006c0s19b1n0:     "synchronize_checkpoint_boundary": false, 
x3006c0s19b1n0:     "profile": false
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   amp_enabled .................. False
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   amp_params ................... False
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   autotuning_config ............ {
x3006c0s19b1n0:     "enabled": false, 
x3006c0s19b1n0:     "start_step": null, 
x3006c0s19b1n0:     "end_step": null, 
x3006c0s19b1n0:     "metric_path": null, 
x3006c0s19b1n0:     "arg_mappings": null, 
x3006c0s19b1n0:     "metric": "throughput", 
x3006c0s19b1n0:     "model_info": null, 
x3006c0s19b1n0:     "results_dir": "autotuning_results", 
x3006c0s19b1n0:     "exps_dir": "autotuning_exps", 
x3006c0s19b1n0:     "overwrite": true, 
x3006c0s19b1n0:     "fast": true, 
x3006c0s19b1n0:     "start_profile_step": 3, 
x3006c0s19b1n0:     "end_profile_step": 5, 
x3006c0s19b1n0:     "tuner_type": "gridsearch", 
x3006c0s19b1n0:     "tuner_early_stopping": 5, 
x3006c0s19b1n0:     "tuner_num_trials": 50, 
x3006c0s19b1n0:     "model_info_path": null, 
x3006c0s19b1n0:     "mp_size": 1, 
x3006c0s19b1n0:     "max_train_batch_size": null, 
x3006c0s19b1n0:     "min_train_batch_size": 1, 
x3006c0s19b1n0:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
x3006c0s19b1n0:     "min_train_micro_batch_size_per_gpu": 1, 
x3006c0s19b1n0:     "num_tuning_micro_batch_sizes": 3
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   bfloat16_enabled ............. True
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   bfloat16_immediate_grad_update  False
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   checkpoint_parallel_write_pipeline  False
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   checkpoint_tag_validation_enabled  True
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   checkpoint_tag_validation_fail  False
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9c25f40b50>
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   communication_data_type ...... None
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   curriculum_enabled_legacy .... False
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   curriculum_params_legacy ..... False
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   data_efficiency_enabled ...... False
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   dataloader_drop_last ......... False
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   datastates_config ............ {
x3006c0s19b1n0:     "enabled": null, 
x3006c0s19b1n0:     "config": {
x3006c0s19b1n0:     }
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   disable_allgather ............ False
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   dump_state ................... False
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   dynamic_loss_scale_args ...... None
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   eigenvalue_enabled ........... False
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   eigenvalue_gas_boundary_resolution  1
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   eigenvalue_layer_name ........ bert.encoder.layer
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   eigenvalue_layer_num ......... 0
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   eigenvalue_max_iter .......... 100
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   eigenvalue_stability ......... 1e-06
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   eigenvalue_tol ............... 0.01
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   eigenvalue_verbose ........... False
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   elasticity_enabled ........... False
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   flops_profiler_config ........ {
x3006c0s19b1n0:     "enabled": false, 
x3006c0s19b1n0:     "recompute_fwd_factor": 0.0, 
x3006c0s19b1n0:     "profile_step": 1, 
x3006c0s19b1n0:     "module_depth": -1, 
x3006c0s19b1n0:     "top_modules": 1, 
x3006c0s19b1n0:     "detailed": true, 
x3006c0s19b1n0:     "output_file": null
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   fp16_auto_cast ............... None
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   fp16_enabled ................. False
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   fp16_master_weights_and_gradients  False
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   global_rank .................. 0
x3006c0s19b1n0: [2024-03-28 14:03:22,627] [INFO] [config.py:1002:print]   grad_accum_dtype ............. bf16
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   gradient_accumulation_steps .. 2
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   gradient_clipping ............ 0.0
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   gradient_predivide_factor .... 1.0
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   graph_harvesting ............. False
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   initial_dynamic_scale ........ 1
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   load_universal_checkpoint .... False
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   loss_scale ................... 1.0
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   memory_breakdown ............. True
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   mics_hierarchial_params_gather  False
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   mics_shard_size .............. -1
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   nebula_config ................ {
x3006c0s19b1n0:     "enabled": false, 
x3006c0s19b1n0:     "persistent_storage_path": null, 
x3006c0s19b1n0:     "persistent_time_interval": 100, 
x3006c0s19b1n0:     "num_of_version_in_retention": 2, 
x3006c0s19b1n0:     "enable_nebula_load": true, 
x3006c0s19b1n0:     "load_path": null
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   optimizer_legacy_fusion ...... False
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   optimizer_name ............... None
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   optimizer_params ............. None
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   pld_enabled .................. False
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   pld_params ................... False
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   prescale_gradients ........... False
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   scheduler_name ............... None
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   scheduler_params ............. None
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   seq_parallel_communication_data_type  torch.float32
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   sparse_attention ............. None
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   sparse_gradients_enabled ..... False
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   steps_per_print .............. 1
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   train_batch_size ............. 64
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   train_micro_batch_size_per_gpu  4
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   use_data_before_expert_parallel_  False
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   use_node_local_storage ....... False
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   wall_clock_breakdown ......... True
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   weight_quantization_config ... None
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   world_size ................... 8
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   zero_allow_untested_optimizer  False
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0, prefetch_optimizer=False, part_grads_async=True, prefetch_optimizer_gap=5) sub_group_size=100000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   zero_enabled ................. True
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   zero_force_ds_cpu_optimizer .. True
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:1002:print]   zero_optimization_stage ...... 3
x3006c0s19b1n0: [2024-03-28 14:03:22,628] [INFO] [config.py:988:print_user_config]   json = {
x3006c0s19b1n0:     "train_batch_size": 64, 
x3006c0s19b1n0:     "train_micro_batch_size_per_gpu": 4, 
x3006c0s19b1n0:     "steps_per_print": 1, 
x3006c0s19b1n0:     "zero_optimization": {
x3006c0s19b1n0:         "stage": 3, 
x3006c0s19b1n0:         "offload_optimizer": {
x3006c0s19b1n0:             "device": "cpu", 
x3006c0s19b1n0:             "ratio": 1, 
x3006c0s19b1n0:             "pin_memory": true, 
x3006c0s19b1n0:             "prefetch_optimizer": 0, 
x3006c0s19b1n0:             "part_grads_async": 1, 
x3006c0s19b1n0:             "prefetch_optimizer_gap": 5
x3006c0s19b1n0:         }, 
x3006c0s19b1n0:         "sub_group_size": 1.000000e+08
x3006c0s19b1n0:     }, 
x3006c0s19b1n0:     "bf16": {
x3006c0s19b1n0:         "enabled": true
x3006c0s19b1n0:     }, 
x3006c0s19b1n0:     "data_types": {
x3006c0s19b1n0:         "grad_accum_dtype": "bf16"
x3006c0s19b1n0:     }, 
x3006c0s19b1n0:     "wall_clock_breakdown": true, 
x3006c0s19b1n0:     "memory_breakdown": true, 
x3006c0s19b1n0:     "flops_profiler": {
x3006c0s19b1n0:         "enabled": false
x3006c0s19b1n0:     }
x3006c0s19b1n0: }
x3006c0s1b0n0: <TIMER:model-and-optimizer-setup,35.761146068573>
x3006c0s1b0n0: <TIMER:model-and-optimizer-setup,35.76168990135193>
x3006c0s1b0n0: <TIMER:model-and-optimizer-setup,35.762335777282715>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,35.76301169395447>
x3006c0s1b0n0: <TIMER:model-and-optimizer-setup,35.76300525665283>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,35.764039278030396>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,35.76463580131531>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,35.7649621963501>
x3006c0s19b1n0: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-28 14:03:23 
x3006c0s19b1n0: > building train, validation, and test datasets ...
x3006c0s19b1n0:  > datasets target sizes (minimum size):
x3006c0s19b1n0:     train:      640
x3006c0s19b1n0:     validation: 0
x3006c0s19b1n0:     test:       0
x3006c0s19b1n0: > building train, validation, and test datasets for GPT ...
x3006c0s19b1n0: Single data path provided for train, valid & test
x3006c0s19b1n0:  > building dataset index ...
x3006c0s19b1n0:     reading sizes...
x3006c0s19b1n0:     reading pointers...
x3006c0s19b1n0:     reading document index...
x3006c0s19b1n0:     creating numpy buffer of mmap...
x3006c0s19b1n0:     creating memory view of numpy buffer...
x3006c0s19b1n0:  > finished creating indexed dataset in 0.002651 seconds
x3006c0s19b1n0:     number of documents: 79000
x3006c0s19b1n0:  > dataset split:
x3006c0s19b1n0:     train:
x3006c0s19b1n0:      document indices in [0, 74971) total of 74971 documents
x3006c0s19b1n0:     validation:
x3006c0s19b1n0:      document indices in [74971, 78921) total of 3950 documents
x3006c0s19b1n0:     test:
x3006c0s19b1n0:      document indices in [78921, 79000) total of 79 documents
x3006c0s19b1n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/d5d1daec41eb416469c3827ed48205ed_doc_idx.npy
x3006c0s19b1n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/d5d1daec41eb416469c3827ed48205ed_sample_idx.npy
x3006c0s19b1n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/d5d1daec41eb416469c3827ed48205ed_shuffle_idx.npy
x3006c0s19b1n0:     loaded indexed file in 0.002 seconds
x3006c0s19b1n0:     total number of samples: 108448
x3006c0s19b1n0:     total number of epochs: 1
x3006c0s19b1n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_doc_idx.npy
x3006c0s19b1n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_sample_idx.npy
x3006c0s19b1n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_shuffle_idx.npy
x3006c0s19b1n0:     loaded indexed file in 0.004 seconds
x3006c0s19b1n0:     total number of samples: 5792
x3006c0s19b1n0:     total number of epochs: 1
x3006c0s19b1n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_doc_idx.npy
x3006c0s19b1n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_sample_idx.npy
x3006c0s19b1n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_shuffle_idx.npy
x3006c0s19b1n0:     loaded indexed file in 0.003 seconds
x3006c0s19b1n0:     total number of samples: 185
x3006c0s19b1n0:     total number of epochs: 1
x3006c0s19b1n0: > finished creating GPT datasets ...
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5528907775878906>
x3006c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5630745887756348>
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.564537525177002>
x3006c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5661718845367432>
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5728483200073242>
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.6068742275238037>
x3006c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.6692094802856445>
x3006c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.6722688674926758>
x3006c0s19b1n0: [after dataloaders are built] datetime: 2024-03-28 14:03:24 
x3006c0s19b1n0: done with setup ...
x3006c0s19b1n0: training ...
x3006c0s1b0n0: (min, max) time across ranks (ms):
x3006c0s1b0n0:     model-and-optimizer-setup ......................: (35761.15, 35764.96)
x3006c0s1b0n0:     train/valid/test-data-iterators-setup ..........: (552.89, 672.27)
x3006c0s19b1n0: [before training begins] datetime: 2024-03-28 14:03:24 
x3006c0s19b1n0: [before the start of training step] datetime: 2024-03-28 14:03:24 
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 14:03:24,737] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 14:03:24,738] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 7.45 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 14:03:24,738] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 179.94 GB, percent = 35.8%
x3006c0s19b1n0: [2024-03-28 14:03:24,869] [INFO] [checkpointing.py:539:forward] Activation Checkpointing Information
x3006c0s19b1n0: [2024-03-28 14:03:24,869] [INFO] [checkpointing.py:540:forward] ----Partition Activations False, CPU CHECKPOINTING False
x3006c0s19b1n0: [2024-03-28 14:03:24,869] [INFO] [checkpointing.py:541:forward] ----contiguous Memory Checkpointing False with 60 total layers
x3006c0s19b1n0: [2024-03-28 14:03:24,869] [INFO] [checkpointing.py:543:forward] ----Synchronization False
x3006c0s19b1n0: [2024-03-28 14:03:24,869] [INFO] [checkpointing.py:544:forward] ----Profiling time in checkpointing False
x3006c0s19b1n0: [2024-03-28 14:03:33,559] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 14:03:33,560] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 19.76 GB         CA 22.95 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 14:03:33,560] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 180.13 GB, percent = 35.8%
x3006c0s19b1n0: [2024-03-28 14:03:33,752] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 14:03:33,753] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 16.84 GB         CA 16.91 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 14:03:33,753] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 180.14 GB, percent = 35.8%
x3006c0s19b1n0: [2024-03-28 14:03:58,192] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 14:03:58,192] [INFO] [utils.py:801:see_memory_usage] MA 10.25 GB         Max_MA 20.22 GB         CA 10.33 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:03:58,193] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 180.22 GB, percent = 35.8%
x3006c0s19b1n0: [2024-03-28 14:03:58,274] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 14:03:58,274] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:03:58,274] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 180.24 GB, percent = 35.8%
x3006c0s19b1n0: [2024-03-28 14:04:04,940] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 14:04:04,941] [INFO] [utils.py:801:see_memory_usage] MA 20.07 GB         Max_MA 22.37 GB         CA 25.45 GB         Max_CA 25 GB 
x3006c0s19b1n0: [2024-03-28 14:04:04,941] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 180.24 GB, percent = 35.8%
x3006c0s19b1n0: [2024-03-28 14:04:05,019] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 14:04:05,019] [INFO] [utils.py:801:see_memory_usage] MA 20.07 GB         Max_MA 20.07 GB         CA 20.68 GB         Max_CA 25 GB 
x3006c0s19b1n0: [2024-03-28 14:04:05,020] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 180.24 GB, percent = 35.8%
x3006c0s19b1n0: [2024-03-28 14:04:23,655] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 14:04:23,656] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.59 GB         CA 10.33 GB         Max_CA 29 GB 
x3006c0s19b1n0: [2024-03-28 14:04:23,656] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 180.25 GB, percent = 35.8%
x3006c0s19b1n0: [2024-03-28 14:04:23,726] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 14:04:23,727] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:04:23,727] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 180.33 GB, percent = 35.8%
x3006c0s19b1n0: [2024-03-28 14:04:33,375] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.381853580474854
x3006c0s19b1n0: [2024-03-28 14:04:33,376] [INFO] [stage3.py:2251:step] Full outer step loop took 9.383442640304565
x3006c0s19b1n0: [2024-03-28 14:04:33,516] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.522986888885498
x3006c0s19b1n0: [2024-03-28 14:04:33,516] [INFO] [stage3.py:2251:step] Full outer step loop took 9.523503065109253
x3006c0s19b1n0: [2024-03-28 14:04:33,523] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.530134916305542
x3006c0s19b1n0: [2024-03-28 14:04:33,523] [INFO] [stage3.py:2251:step] Full outer step loop took 9.53034257888794
x3006c0s19b1n0: [2024-03-28 14:04:33,529] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.535688877105713
x3006c0s19b1n0: [2024-03-28 14:04:33,529] [INFO] [stage3.py:2251:step] Full outer step loop took 9.53584337234497
x3006c0s1b0n0: [2024-03-28 14:04:33,557] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.563522815704346
x3006c0s1b0n0: [2024-03-28 14:04:33,557] [INFO] [stage3.py:2251:step] Full outer step loop took 9.564262866973877
x3006c0s1b0n0: [2024-03-28 14:04:33,803] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.809118270874023
x3006c0s1b0n0: [2024-03-28 14:04:33,804] [INFO] [stage3.py:2251:step] Full outer step loop took 9.811171770095825
x3006c0s1b0n0: [2024-03-28 14:04:33,810] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.817394495010376
x3006c0s1b0n0: [2024-03-28 14:04:33,811] [INFO] [stage3.py:2251:step] Full outer step loop took 9.817835330963135
x3006c0s1b0n0: [2024-03-28 14:04:33,879] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.885865449905396
x3006c0s1b0n0: [2024-03-28 14:04:33,879] [INFO] [stage3.py:2251:step] Full outer step loop took 9.886064052581787
x3006c0s19b1n0: [2024-03-28 14:04:33,891] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 9536.02
x3006c0s19b1n0: [2024-03-28 14:04:33,891] [WARNING] [stage3.py:2267:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3006c0s19b1n0: [2024-03-28 14:04:33,891] [INFO] [stage3.py:2277:step] End to end step took 9.89814567565918
x3006c0s19b1n0: [2024-03-28 14:04:33,891] [INFO] [logging.py:96:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 14:04:33,891] [INFO] [stage3.py:2277:step] End to end step took 9.89834189414978
x3006c0s1b0n0: [2024-03-28 14:04:33,891] [INFO] [stage3.py:2277:step] End to end step took 9.898391008377075
x3006c0s1b0n0: [2024-03-28 14:04:33,891] [INFO] [stage3.py:2277:step] End to end step took 9.898355722427368
x3006c0s1b0n0: [2024-03-28 14:04:33,891] [INFO] [stage3.py:2277:step] End to end step took 9.89844298362732
x3006c0s19b1n0: [2024-03-28 14:04:33,891] [INFO] [stage3.py:2277:step] End to end step took 9.898379564285278
x3006c0s1b0n0: [2024-03-28 14:04:33,891] [INFO] [stage3.py:2277:step] End to end step took 9.898574113845825
x3006c0s19b1n0: [2024-03-28 14:04:33,892] [INFO] [stage3.py:2277:step] End to end step took 9.898559093475342
x3006c0s19b1n0: [2024-03-28 14:04:33,892] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 15547.40 | bwd_microstep: 42752.79 | bwd_inner_microstep: 42597.36 | bwd_allreduce_microstep: 155.26 | step_microstep: 10164.72
x3006c0s19b1n0: [2024-03-28 14:04:33,892] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 15547.40 | bwd: 42752.78 | bwd_inner: 42597.37 | bwd_allreduce: 155.27 | step: 10164.72
x3006c0s19b1n0: [2024-03-28 14:04:33,997] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 14:04:33,997] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:04:33,997] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.1 GB, percent = 66.6%
x3006c0s19b1n0: <TIMER:interval-time,69.43439817428589><TIMER:interval-time,69.43422198295593>
x3006c0s19b1n0: <TIMER:interval-time,69.43439936637878>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,69.43440389633179>
x3006c0s1b0n0: <TIMER:interval-time,69.43442273139954><TIMER:interval-time,69.43441772460938><TIMER:interval-time,69.43437576293945>
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,69.43441963195801>
x3006c0s1b0n0: 
x3006c0s19b1n0: [Rank 0] (after 1 iterations) memory (MB) | allocated: 9224.5439453125 | max allocated: 9224.544921875 | reserved: 9296.0 | max reserved: 9296.0
x3006c0s1b0n0:  elapsed_time 69.434376 | consumed samples:           64 | consumed tokens:       131072 | elapsed time per iteration (ms): 69434.4 | learning rate: 3.000E-04 | global batch size:    64 | lm loss: 1.215771E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.922 | TFLOPs: 63.78 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 14:04:34,141] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 14:04:34,141] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 14:04:34,142] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.1 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:04:40,432] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 14:04:40,433] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:04:40,433] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.1 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:04:40,529] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 14:04:40,530] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:04:40,530] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.1 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:04:58,089] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 14:04:58,090] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 29 GB 
x3006c0s19b1n0: [2024-03-28 14:04:58,090] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.09 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:04:58,180] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 14:04:58,181] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:04:58,181] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.11 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:05:04,388] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 14:05:04,389] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:05:04,389] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.1 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:05:04,495] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 14:05:04,495] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:05:04,496] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.1 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:05:22,861] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 14:05:22,862] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-28 14:05:22,862] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.09 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:05:22,938] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 14:05:22,938] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:05:22,939] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.09 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:05:30,043] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.080934524536133
x3006c0s19b1n0: [2024-03-28 14:05:30,044] [INFO] [stage3.py:2251:step] Full outer step loop took 7.081639766693115
x3006c0s19b1n0: [2024-03-28 14:05:30,110] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.14809775352478
x3006c0s19b1n0: [2024-03-28 14:05:30,110] [INFO] [stage3.py:2251:step] Full outer step loop took 7.148355960845947
x3006c0s19b1n0: [2024-03-28 14:05:30,179] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.217399835586548
x3006c0s19b1n0: [2024-03-28 14:05:30,180] [INFO] [stage3.py:2251:step] Full outer step loop took 7.2175822257995605
x3006c0s19b1n0: [2024-03-28 14:05:30,207] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.231890916824341
x3006c0s19b1n0: [2024-03-28 14:05:30,207] [INFO] [stage3.py:2251:step] Full outer step loop took 7.232046127319336
x3006c0s1b0n0: [2024-03-28 14:05:30,386] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.424259424209595
x3006c0s1b0n0: [2024-03-28 14:05:30,387] [INFO] [stage3.py:2251:step] Full outer step loop took 7.424459218978882
x3006c0s1b0n0: [2024-03-28 14:05:30,441] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.4780433177948
x3006c0s1b0n0: [2024-03-28 14:05:30,445] [INFO] [stage3.py:2251:step] Full outer step loop took 7.482715129852295
x3006c0s1b0n0: [2024-03-28 14:05:30,506] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.5441203117370605
x3006c0s1b0n0: [2024-03-28 14:05:30,507] [INFO] [stage3.py:2251:step] Full outer step loop took 7.544283390045166
x3006c0s1b0n0: [2024-03-28 14:05:30,516] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.553727626800537
x3006c0s1b0n0: [2024-03-28 14:05:30,516] [INFO] [stage3.py:2251:step] Full outer step loop took 7.55388331413269
x3006c0s1b0n0: [2024-03-28 14:05:30,527] [INFO] [stage3.py:2277:step] End to end step took 7.564544677734375
x3006c0s1b0n0: [2024-03-28 14:05:30,527] [INFO] [stage3.py:2277:step] End to end step took 7.564411878585815
x3006c0s19b1n0: [2024-03-28 14:05:30,527] [INFO] [stage3.py:2277:step] End to end step took 7.5517778396606445
x3006c0s19b1n0: [2024-03-28 14:05:30,527] [INFO] [stage3.py:2277:step] End to end step took 7.564885854721069
x3006c0s19b1n0: [2024-03-28 14:05:30,527] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 7082.47
x3006c0s1b0n0: [2024-03-28 14:05:30,527] [INFO] [stage3.py:2277:step] End to end step took 7.5650293827056885
x3006c0s19b1n0: [2024-03-28 14:05:30,527] [INFO] [stage3.py:2277:step] End to end step took 7.565269947052002
x3006c0s1b0n0: [2024-03-28 14:05:30,527] [INFO] [stage3.py:2277:step] End to end step took 7.565173864364624
x3006c0s19b1n0: [2024-03-28 14:05:30,527] [INFO] [stage3.py:2277:step] End to end step took 7.565247058868408
x3006c0s19b1n0: [2024-03-28 14:05:30,528] [INFO] [logging.py:96:log_dist] [Rank 0] step=2, skipped=0, lr=[0.0002918585038060976], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 14:05:30,528] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 12411.20 | bwd_microstep: 35567.46 | bwd_inner_microstep: 35413.04 | bwd_allreduce_microstep: 154.27 | step_microstep: 7589.25
x3006c0s19b1n0: [2024-03-28 14:05:30,528] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 12411.19 | bwd: 35567.47 | bwd_inner: 35413.03 | bwd_allreduce: 154.30 | step: 7589.26
x3006c0s19b1n0: [2024-03-28 14:05:30,641] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 14:05:30,642] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:05:30,642] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.16 GB, percent = 66.6%
x3006c0s19b1n0: <TIMER:interval-time,56.64426589012146>
x3006c0s19b1n0: <TIMER:interval-time,56.644272565841675><TIMER:interval-time,56.644272565841675>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,56.64427423477173>
x3006c0s1b0n0: <TIMER:interval-time,56.644299268722534><TIMER:interval-time,56.644299268722534><TIMER:interval-time,56.64430212974548><TIMER:interval-time,56.6443030834198>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0:  elapsed_time 56.644302 | consumed samples:          128 | consumed tokens:       262144 | elapsed time per iteration (ms): 56644.3 | learning rate: 2.919E-04 | global batch size:    64 | lm loss: 1.215431E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.130 | TFLOPs: 78.18 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 14:05:30,788] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 14:05:30,789] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 14:05:30,789] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.12 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:05:37,513] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 14:05:37,514] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:05:37,514] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.1 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:05:37,604] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 14:05:37,605] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:05:37,605] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.1 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:05:55,320] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 14:05:55,320] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 29 GB 
x3006c0s19b1n0: [2024-03-28 14:05:55,321] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.1 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:05:55,423] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 14:05:55,423] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:05:55,423] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.11 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:06:02,124] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 14:06:02,125] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:06:02,125] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.1 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:06:02,230] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 14:06:02,230] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:06:02,230] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.1 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:06:20,876] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 14:06:20,877] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-28 14:06:20,877] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.09 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:06:20,954] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 14:06:20,955] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:06:20,955] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.09 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:06:27,944] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.9652183055877686
x3006c0s19b1n0: [2024-03-28 14:06:27,944] [INFO] [stage3.py:2251:step] Full outer step loop took 6.965432643890381
x3006c0s1b0n0: [2024-03-28 14:06:28,036] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.028841018676758
x3006c0s1b0n0: [2024-03-28 14:06:28,038] [INFO] [stage3.py:2251:step] Full outer step loop took 7.031087160110474
x3006c0s19b1n0: [2024-03-28 14:06:28,089] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.109917879104614
x3006c0s19b1n0: [2024-03-28 14:06:28,089] [INFO] [stage3.py:2251:step] Full outer step loop took 7.110182046890259
x3006c0s19b1n0: [2024-03-28 14:06:28,123] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.144293546676636
x3006c0s19b1n0: [2024-03-28 14:06:28,123] [INFO] [stage3.py:2251:step] Full outer step loop took 7.144528388977051
x3006c0s19b1n0: [2024-03-28 14:06:28,196] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.217150449752808
x3006c0s19b1n0: [2024-03-28 14:06:28,196] [INFO] [stage3.py:2251:step] Full outer step loop took 7.217301368713379
x3006c0s1b0n0: [2024-03-28 14:06:28,419] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.440685272216797
x3006c0s1b0n0: [2024-03-28 14:06:28,420] [INFO] [stage3.py:2251:step] Full outer step loop took 7.440916299819946
x3006c0s1b0n0: [2024-03-28 14:06:28,528] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.549312591552734
x3006c0s1b0n0: [2024-03-28 14:06:28,528] [INFO] [stage3.py:2251:step] Full outer step loop took 7.549602508544922
x3006c0s1b0n0: [2024-03-28 14:06:28,540] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.560849189758301
x3006c0s1b0n0: [2024-03-28 14:06:28,540] [INFO] [stage3.py:2251:step] Full outer step loop took 7.5609965324401855
x3006c0s1b0n0: [2024-03-28 14:06:28,550] [INFO] [stage3.py:2277:step] End to end step took 7.571384429931641
x3006c0s1b0n0: [2024-03-28 14:06:28,550] [INFO] [stage3.py:2277:step] End to end step took 7.57146692276001
x3006c0s1b0n0: [2024-03-28 14:06:28,551] [INFO] [stage3.py:2277:step] End to end step took 7.5435686111450195
x3006c0s19b1n0: [2024-03-28 14:06:28,551] [INFO] [stage3.py:2277:step] End to end step took 7.571889162063599
x3006c0s19b1n0: [2024-03-28 14:06:28,551] [INFO] [stage3.py:2277:step] End to end step took 7.57192063331604
x3006c0s19b1n0: [2024-03-28 14:06:28,550] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6965.63
x3006c0s1b0n0: [2024-03-28 14:06:28,551] [INFO] [stage3.py:2277:step] End to end step took 7.571943759918213
x3006c0s19b1n0: [2024-03-28 14:06:28,551] [INFO] [stage3.py:2277:step] End to end step took 7.572023153305054
x3006c0s19b1n0: [2024-03-28 14:06:28,551] [INFO] [stage3.py:2277:step] End to end step took 7.572164058685303
x3006c0s19b1n0: [2024-03-28 14:06:28,551] [INFO] [logging.py:96:log_dist] [Rank 0] step=3, skipped=0, lr=[0.00026841599982106197], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 14:06:28,552] [INFO] [timer.py:260:stop] epoch=0/micro_step=3/global_step=3, RunningAvgSamplesPerSec=1.9320015631023775, CurrSamplesPerSec=1.9320015631023775, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 14:06:28,552] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 13342.63 | bwd_microstep: 36008.51 | bwd_inner_microstep: 35850.01 | bwd_allreduce_microstep: 158.33 | step_microstep: 7596.76
x3006c0s19b1n0: [2024-03-28 14:06:28,553] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 13342.62 | bwd: 36008.50 | bwd_inner: 35849.99 | bwd_allreduce: 158.36 | step: 7596.76
x3006c0s19b1n0: [2024-03-28 14:06:28,674] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 14:06:28,675] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:06:28,675] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.09 GB, percent = 66.6%
x3006c0s19b1n0: <TIMER:interval-time,58.033217430114746><TIMER:interval-time,58.0332145690918><TIMER:interval-time,58.033217430114746>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,58.033217668533325>
x3006c0s1b0n0: <TIMER:interval-time,58.03327035903931><TIMER:interval-time,58.03327012062073>
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,58.03339433670044><TIMER:interval-time,58.033395528793335>
x3006c0s1b0n0: 
x3006c0s1b0n0:  elapsed_time 58.033394 | consumed samples:          192 | consumed tokens:       393216 | elapsed time per iteration (ms): 58033.4 | learning rate: 2.684E-04 | global batch size:    64 | lm loss: 3.510575E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.103 | TFLOPs: 76.30 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 14:06:28,809] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 14:06:28,810] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 14:06:28,810] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.13 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:06:35,076] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 14:06:35,077] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:06:35,077] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.11 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:06:35,159] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 14:06:35,160] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:06:35,160] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.11 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:06:53,011] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 14:06:53,012] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 29 GB 
x3006c0s19b1n0: [2024-03-28 14:06:53,012] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.11 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:06:53,108] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 14:06:53,109] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:06:53,109] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.14 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:07:00,054] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 14:07:00,054] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:07:00,054] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.13 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:07:00,136] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 14:07:00,137] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:07:00,137] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.13 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:07:19,242] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 14:07:19,243] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-28 14:07:19,243] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.12 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:07:19,318] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 14:07:19,318] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:07:19,319] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.12 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:07:26,249] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.906680345535278
x3006c0s19b1n0: [2024-03-28 14:07:26,249] [INFO] [stage3.py:2251:step] Full outer step loop took 6.907017230987549
x3006c0s19b1n0: [2024-03-28 14:07:26,467] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.125110626220703
x3006c0s19b1n0: [2024-03-28 14:07:26,468] [INFO] [stage3.py:2251:step] Full outer step loop took 7.125307083129883
x3006c0s1b0n0: [2024-03-28 14:07:26,497] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.154736757278442
x3006c0s1b0n0: [2024-03-28 14:07:26,497] [INFO] [stage3.py:2251:step] Full outer step loop took 7.155009984970093
x3006c0s19b1n0: [2024-03-28 14:07:26,588] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.246061563491821
x3006c0s19b1n0: [2024-03-28 14:07:26,589] [INFO] [stage3.py:2251:step] Full outer step loop took 7.246295213699341
x3006c0s1b0n0: [2024-03-28 14:07:26,653] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.309805631637573
x3006c0s1b0n0: [2024-03-28 14:07:26,654] [INFO] [stage3.py:2251:step] Full outer step loop took 7.311537504196167
x3006c0s19b1n0: [2024-03-28 14:07:26,679] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.323101758956909
x3006c0s19b1n0: [2024-03-28 14:07:26,679] [INFO] [stage3.py:2251:step] Full outer step loop took 7.323265314102173
x3006c0s1b0n0: [2024-03-28 14:07:26,715] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.372708082199097
x3006c0s1b0n0: [2024-03-28 14:07:26,715] [INFO] [stage3.py:2251:step] Full outer step loop took 7.372925281524658
x3006c0s1b0n0: [2024-03-28 14:07:26,779] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.436459302902222
x3006c0s1b0n0: [2024-03-28 14:07:26,779] [INFO] [stage3.py:2251:step] Full outer step loop took 7.436607599258423
x3006c0s1b0n0: [2024-03-28 14:07:26,789] [INFO] [stage3.py:2277:step] End to end step took 7.446716785430908
x3006c0s19b1n0: [2024-03-28 14:07:26,789] [INFO] [stage3.py:2277:step] End to end step took 7.433691740036011
x3006c0s1b0n0: [2024-03-28 14:07:26,789] [INFO] [stage3.py:2277:step] End to end step took 7.447031259536743
x3006c0s19b1n0: [2024-03-28 14:07:26,789] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6907.28
x3006c0s1b0n0: [2024-03-28 14:07:26,790] [INFO] [stage3.py:2277:step] End to end step took 7.44725227355957
x3006c0s1b0n0: [2024-03-28 14:07:26,790] [INFO] [stage3.py:2277:step] End to end step took 7.447082281112671
x3006c0s19b1n0: [2024-03-28 14:07:26,790] [INFO] [stage3.py:2277:step] End to end step took 7.447323799133301
x3006c0s19b1n0: [2024-03-28 14:07:26,790] [INFO] [stage3.py:2277:step] End to end step took 7.447354793548584
x3006c0s19b1n0: [2024-03-28 14:07:26,790] [INFO] [stage3.py:2277:step] End to end step took 7.447489976882935
x3006c0s19b1n0: [2024-03-28 14:07:26,790] [INFO] [logging.py:96:log_dist] [Rank 0] step=4, skipped=0, lr=[0.00023249999999999996], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 14:07:26,791] [INFO] [timer.py:260:stop] epoch=0/micro_step=4/global_step=4, RunningAvgSamplesPerSec=1.9159770390489674, CurrSamplesPerSec=1.9002161514764357, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 14:07:26,791] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 13131.48 | bwd_microstep: 36603.58 | bwd_inner_microstep: 36446.36 | bwd_allreduce_microstep: 157.09 | step_microstep: 7471.99
x3006c0s19b1n0: [2024-03-28 14:07:26,791] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 13131.47 | bwd: 36603.58 | bwd_inner: 36446.34 | bwd_allreduce: 157.12 | step: 7471.99
x3006c0s19b1n0: [2024-03-28 14:07:26,913] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 14:07:26,914] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:07:26,914] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.18 GB, percent = 66.6%
x3006c0s1b0n0: <TIMER:interval-time,58.23855662345886><TIMER:interval-time,58.238558530807495>
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,58.23856329917908>
x3006c0s1b0n0: <TIMER:interval-time,58.23867750167847>
x3006c0s19b1n0: <TIMER:interval-time,58.2385139465332><TIMER:interval-time,58.2385139465332><TIMER:interval-time,58.23851537704468><TIMER:interval-time,58.23850893974304>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b0n0:  elapsed_time 58.238559 | consumed samples:          256 | consumed tokens:       524288 | elapsed time per iteration (ms): 58238.6 | learning rate: 2.325E-04 | global batch size:    64 | lm loss: 2.233065E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.099 | TFLOPs: 76.04 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 14:07:27,052] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 14:07:27,052] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 14:07:27,052] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.15 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:07:34,093] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 14:07:34,094] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:07:34,094] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.13 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:07:34,176] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 14:07:34,176] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:07:34,176] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.13 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:07:53,058] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 14:07:53,059] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 29 GB 
x3006c0s19b1n0: [2024-03-28 14:07:53,059] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.12 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:07:53,146] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 14:07:53,146] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:07:53,146] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.14 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:07:59,922] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 14:07:59,923] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:07:59,923] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.12 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:08:00,002] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 14:08:00,002] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:08:00,003] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.12 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:08:18,581] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 14:08:18,582] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-28 14:08:18,582] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.11 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:08:18,652] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 14:08:18,653] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:08:18,653] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.11 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:08:25,420] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.74383020401001
x3006c0s19b1n0: [2024-03-28 14:08:25,422] [INFO] [stage3.py:2251:step] Full outer step loop took 6.745141983032227
x3006c0s19b1n0: [2024-03-28 14:08:25,894] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.217183351516724
x3006c0s19b1n0: [2024-03-28 14:08:25,894] [INFO] [stage3.py:2251:step] Full outer step loop took 7.2173545360565186
x3006c0s19b1n0: [2024-03-28 14:08:25,903] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.226632118225098
x3006c0s19b1n0: [2024-03-28 14:08:25,903] [INFO] [stage3.py:2251:step] Full outer step loop took 7.22686243057251
x3006c0s19b1n0: [2024-03-28 14:08:25,909] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.233017206192017
x3006c0s19b1n0: [2024-03-28 14:08:25,910] [INFO] [stage3.py:2251:step] Full outer step loop took 7.2331531047821045
x3006c0s1b0n0: [2024-03-28 14:08:26,032] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.3555519580841064
x3006c0s1b0n0: [2024-03-28 14:08:26,032] [INFO] [stage3.py:2251:step] Full outer step loop took 7.355920791625977
x3006c0s1b0n0: [2024-03-28 14:08:26,274] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.597085475921631
x3006c0s1b0n0: [2024-03-28 14:08:26,278] [INFO] [stage3.py:2251:step] Full outer step loop took 7.601365089416504
x3006c0s1b0n0: [2024-03-28 14:08:26,314] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.637760400772095
x3006c0s1b0n0: [2024-03-28 14:08:26,314] [INFO] [stage3.py:2251:step] Full outer step loop took 7.6379289627075195
x3006c0s1b0n0: [2024-03-28 14:08:26,319] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.642763137817383
x3006c0s1b0n0: [2024-03-28 14:08:26,320] [INFO] [stage3.py:2251:step] Full outer step loop took 7.642944812774658
x3006c0s1b0n0: [2024-03-28 14:08:26,331] [INFO] [stage3.py:2277:step] End to end step took 7.654634714126587
x3006c0s1b0n0: [2024-03-28 14:08:26,331] [INFO] [stage3.py:2277:step] End to end step took 7.654797554016113
x3006c0s19b1n0: [2024-03-28 14:08:26,331] [INFO] [stage3.py:2277:step] End to end step took 7.654863595962524
x3006c0s19b1n0: [2024-03-28 14:08:26,331] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6746.32
x3006c0s19b1n0: [2024-03-28 14:08:26,332] [INFO] [stage3.py:2277:step] End to end step took 7.655293226242065
x3006c0s1b0n0: [2024-03-28 14:08:26,332] [INFO] [stage3.py:2277:step] End to end step took 7.655321836471558
x3006c0s1b0n0: [2024-03-28 14:08:26,332] [INFO] [stage3.py:2277:step] End to end step took 7.6552956104278564
x3006c0s19b1n0: [2024-03-28 14:08:26,332] [INFO] [stage3.py:2277:step] End to end step took 7.6554272174835205
x3006c0s19b1n0: [2024-03-28 14:08:26,332] [INFO] [stage3.py:2277:step] End to end step took 7.655410528182983
x3006c0s19b1n0: [2024-03-28 14:08:26,332] [INFO] [logging.py:96:log_dist] [Rank 0] step=5, skipped=0, lr=[0.0001884425039850356], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 14:08:26,332] [INFO] [timer.py:260:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=1.920184299610494, CurrSamplesPerSec=1.9286544971518025, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 14:08:26,333] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 13743.69 | bwd_microstep: 37115.21 | bwd_inner_microstep: 36960.36 | bwd_allreduce_microstep: 154.72 | step_microstep: 7679.57
x3006c0s19b1n0: [2024-03-28 14:08:26,333] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 13743.68 | bwd: 37115.21 | bwd_inner: 36960.35 | bwd_allreduce: 154.74 | step: 7679.57
x3006c0s19b1n0: [2024-03-28 14:08:26,446] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 14:08:26,447] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:08:26,447] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.16 GB, percent = 66.6%
x3006c0s19b1n0: <TIMER:interval-time,59.53214931488037><TIMER:interval-time,59.532151222229004><TIMER:interval-time,59.532153367996216><TIMER:interval-time,59.532153367996216>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,59.53216910362244><TIMER:interval-time,59.53217053413391><TIMER:interval-time,59.53216528892517><TIMER:interval-time,59.53217434883118>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0:  elapsed_time 59.532174 | consumed samples:          320 | consumed tokens:       655360 | elapsed time per iteration (ms): 59532.2 | learning rate: 1.884E-04 | global batch size:    64 | lm loss: 1.478109E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.075 | TFLOPs: 74.38 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 14:08:26,560] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 14:08:26,561] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 14:08:26,561] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.14 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:08:33,575] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 14:08:33,575] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:08:33,576] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.11 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:08:33,684] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 14:08:33,685] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:08:33,685] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.11 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:08:52,254] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 14:08:52,254] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 29 GB 
x3006c0s19b1n0: [2024-03-28 14:08:52,255] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.11 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:08:52,348] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 14:08:52,349] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:08:52,349] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.12 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:08:59,141] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 14:08:59,141] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:08:59,142] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.11 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:08:59,230] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 14:08:59,231] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:08:59,231] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.11 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:09:18,347] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 14:09:18,348] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-28 14:09:18,348] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.11 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:09:18,424] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 14:09:18,424] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:09:18,424] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.06 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:09:25,426] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.977864980697632
x3006c0s19b1n0: [2024-03-28 14:09:25,427] [INFO] [stage3.py:2251:step] Full outer step loop took 6.9792091846466064
x3006c0s19b1n0: [2024-03-28 14:09:25,487] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.039496898651123
x3006c0s19b1n0: [2024-03-28 14:09:25,487] [INFO] [stage3.py:2251:step] Full outer step loop took 7.039696216583252
x3006c0s1b0n0: [2024-03-28 14:09:25,656] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.189976930618286
x3006c0s1b0n0: [2024-03-28 14:09:25,656] [INFO] [stage3.py:2251:step] Full outer step loop took 7.190265655517578
x3006c0s19b1n0: [2024-03-28 14:09:25,692] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.242239952087402
x3006c0s19b1n0: [2024-03-28 14:09:25,692] [INFO] [stage3.py:2251:step] Full outer step loop took 7.242563486099243
x3006c0s19b1n0: [2024-03-28 14:09:25,703] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.25543475151062
x3006c0s19b1n0: [2024-03-28 14:09:25,703] [INFO] [stage3.py:2251:step] Full outer step loop took 7.255584955215454
x3006c0s1b0n0: [2024-03-28 14:09:25,822] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.374661922454834
x3006c0s1b0n0: [2024-03-28 14:09:25,823] [INFO] [stage3.py:2251:step] Full outer step loop took 7.374916076660156
x3006c0s1b0n0: [2024-03-28 14:09:25,835] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.387609243392944
x3006c0s1b0n0: [2024-03-28 14:09:25,836] [INFO] [stage3.py:2251:step] Full outer step loop took 7.387776851654053
x3006c0s1b0n0: [2024-03-28 14:09:25,842] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.394013404846191
x3006c0s1b0n0: [2024-03-28 14:09:25,842] [INFO] [stage3.py:2251:step] Full outer step loop took 7.394166469573975
x3006c0s1b0n0: [2024-03-28 14:09:25,853] [INFO] [stage3.py:2277:step] End to end step took 7.404714107513428
x3006c0s1b0n0: [2024-03-28 14:09:25,853] [INFO] [stage3.py:2277:step] End to end step took 7.404732704162598
x3006c0s19b1n0: [2024-03-28 14:09:25,853] [INFO] [stage3.py:2277:step] End to end step took 7.402668237686157
x3006c0s19b1n0: [2024-03-28 14:09:25,853] [INFO] [stage3.py:2277:step] End to end step took 7.404770374298096
x3006c0s1b0n0: [2024-03-28 14:09:25,853] [INFO] [stage3.py:2277:step] End to end step took 7.387425422668457
x3006c0s19b1n0: [2024-03-28 14:09:25,853] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6980.23
x3006c0s1b0n0: [2024-03-28 14:09:25,853] [INFO] [stage3.py:2277:step] End to end step took 7.4053099155426025
x3006c0s19b1n0: [2024-03-28 14:09:25,853] [INFO] [stage3.py:2277:step] End to end step took 7.405272483825684
x3006c0s19b1n0: [2024-03-28 14:09:25,853] [INFO] [stage3.py:2277:step] End to end step took 7.405437469482422
x3006c0s19b1n0: [2024-03-28 14:09:25,854] [INFO] [logging.py:96:log_dist] [Rank 0] step=6, skipped=0, lr=[0.0001415574960149644], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 14:09:25,854] [INFO] [timer.py:260:stop] epoch=0/micro_step=6/global_step=6, RunningAvgSamplesPerSec=1.9176917846899677, CurrSamplesPerSec=1.9102529145370943, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 14:09:25,854] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 13713.33 | bwd_microstep: 37336.96 | bwd_inner_microstep: 37171.62 | bwd_allreduce_microstep: 165.18 | step_microstep: 7429.70
x3006c0s19b1n0: [2024-03-28 14:09:25,854] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 13713.32 | bwd: 37336.95 | bwd_inner: 37171.61 | bwd_allreduce: 165.21 | step: 7429.70
x3006c0s19b1n0: [2024-03-28 14:09:25,978] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 14:09:25,979] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:09:25,979] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.12 GB, percent = 66.6%
x3006c0s19b1n0: <TIMER:interval-time,59.53212928771973><TIMER:interval-time,59.53212928771973><TIMER:interval-time,59.53212809562683>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,59.53215742111206><TIMER:interval-time,59.532155990600586><TIMER:interval-time,59.53216075897217>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,59.532163858413696>
x3006c0s19b1n0: <TIMER:interval-time,59.53224730491638>
x3006c0s1b0n0:  elapsed_time 59.532164 | consumed samples:          384 | consumed tokens:       786432 | elapsed time per iteration (ms): 59532.2 | learning rate: 1.416E-04 | global batch size:    64 | lm loss: 1.503855E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.075 | TFLOPs: 74.38 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 14:09:26,111] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 14:09:26,111] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 14:09:26,112] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.13 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:09:33,014] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 14:09:33,015] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:09:33,015] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.11 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:09:33,103] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 14:09:33,103] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:09:33,103] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.11 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:09:50,087] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 14:09:50,087] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 29 GB 
x3006c0s19b1n0: [2024-03-28 14:09:50,087] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.12 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:09:50,179] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 14:09:50,180] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:09:50,180] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.14 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:09:56,560] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 14:09:56,561] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:09:56,561] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.12 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:09:56,649] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 14:09:56,650] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:09:56,650] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.12 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:10:15,323] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 14:10:15,324] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-28 14:10:15,324] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.12 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:10:15,400] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 14:10:15,400] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:10:15,400] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.12 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:10:22,360] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.920485019683838
x3006c0s19b1n0: [2024-03-28 14:10:22,363] [INFO] [stage3.py:2251:step] Full outer step loop took 6.923473834991455
x3006c0s19b1n0: [2024-03-28 14:10:22,501] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.077386379241943
x3006c0s19b1n0: [2024-03-28 14:10:22,502] [INFO] [stage3.py:2251:step] Full outer step loop took 7.077624082565308
x3006c0s19b1n0: [2024-03-28 14:10:22,596] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.171761751174927
x3006c0s19b1n0: [2024-03-28 14:10:22,596] [INFO] [stage3.py:2251:step] Full outer step loop took 7.1719231605529785
x3006c0s19b1n0: [2024-03-28 14:10:22,603] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.179183006286621
x3006c0s19b1n0: [2024-03-28 14:10:22,603] [INFO] [stage3.py:2251:step] Full outer step loop took 7.179338455200195
x3006c0s1b0n0: [2024-03-28 14:10:22,695] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.256098985671997
x3006c0s1b0n0: [2024-03-28 14:10:22,695] [INFO] [stage3.py:2251:step] Full outer step loop took 7.256273984909058
x3006c0s1b0n0: [2024-03-28 14:10:22,875] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.450674533843994
x3006c0s1b0n0: [2024-03-28 14:10:22,875] [INFO] [stage3.py:2251:step] Full outer step loop took 7.451005458831787
x3006c0s1b0n0: [2024-03-28 14:10:22,921] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.497215270996094
x3006c0s1b0n0: [2024-03-28 14:10:22,921] [INFO] [stage3.py:2251:step] Full outer step loop took 7.497407674789429
x3006c0s1b0n0: [2024-03-28 14:10:22,950] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.526331186294556
x3006c0s1b0n0: [2024-03-28 14:10:22,950] [INFO] [stage3.py:2251:step] Full outer step loop took 7.526485204696655
x3006c0s1b0n0: [2024-03-28 14:10:22,961] [INFO] [stage3.py:2277:step] End to end step took 7.53682804107666
x3006c0s1b0n0: [2024-03-28 14:10:22,961] [INFO] [stage3.py:2277:step] End to end step took 7.536872863769531
x3006c0s19b1n0: [2024-03-28 14:10:22,961] [INFO] [stage3.py:2277:step] End to end step took 7.5368711948394775
x3006c0s19b1n0: [2024-03-28 14:10:22,961] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6926.03
x3006c0s19b1n0: [2024-03-28 14:10:22,961] [INFO] [stage3.py:2277:step] End to end step took 7.52236008644104
x3006c0s1b0n0: [2024-03-28 14:10:22,961] [INFO] [stage3.py:2277:step] End to end step took 7.537229061126709
x3006c0s1b0n0: [2024-03-28 14:10:22,961] [INFO] [stage3.py:2277:step] End to end step took 7.522375583648682
x3006c0s19b1n0: [2024-03-28 14:10:22,961] [INFO] [stage3.py:2277:step] End to end step took 7.537315130233765
x3006c0s19b1n0: [2024-03-28 14:10:22,961] [INFO] [stage3.py:2277:step] End to end step took 7.537445068359375
x3006c0s19b1n0: [2024-03-28 14:10:22,962] [INFO] [logging.py:96:log_dist] [Rank 0] step=7, skipped=0, lr=[9.750000000000001e-05], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 14:10:22,962] [INFO] [timer.py:260:stop] epoch=0/micro_step=7/global_step=7, RunningAvgSamplesPerSec=1.9245309747068544, CurrSamplesPerSec=1.9523826155175268, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 14:10:22,962] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 13200.28 | bwd_microstep: 35314.93 | bwd_inner_microstep: 35155.41 | bwd_allreduce_microstep: 159.37 | step_microstep: 7561.49
x3006c0s19b1n0: [2024-03-28 14:10:22,962] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 13200.27 | bwd: 35314.92 | bwd_inner: 35155.40 | bwd_allreduce: 159.40 | step: 7561.49
x3006c0s19b1n0: [2024-03-28 14:10:23,074] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 14:10:23,075] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:10:23,075] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.16 GB, percent = 66.6%
x3006c0s1b0n0: <TIMER:interval-time,57.09530997276306><TIMER:interval-time,57.09530997276306>
x3006c0s1b0n0: <TIMER:interval-time,57.095314502716064>
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,57.095399141311646>
x3006c0s19b1n0: <TIMER:interval-time,57.09529495239258><TIMER:interval-time,57.09529995918274><TIMER:interval-time,57.09528183937073><TIMER:interval-time,57.09529900550842>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b0n0:  elapsed_time 57.095315 | consumed samples:          448 | consumed tokens:       917504 | elapsed time per iteration (ms): 57095.3 | learning rate: 9.750E-05 | global batch size:    64 | lm loss: 1.281675E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.121 | TFLOPs: 77.56 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 14:10:23,210] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 14:10:23,211] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 14:10:23,211] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.17 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:10:30,277] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 14:10:30,277] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:10:30,278] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.15 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:10:30,368] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 14:10:30,369] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:10:30,369] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.15 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:10:48,067] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 14:10:48,067] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 29 GB 
x3006c0s19b1n0: [2024-03-28 14:10:48,068] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.15 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:10:48,151] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 14:10:48,151] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:10:48,152] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.15 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:10:55,254] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 14:10:55,254] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:10:55,254] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.14 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:10:55,335] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 14:10:55,336] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:10:55,336] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.14 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:11:13,317] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 14:11:13,318] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-28 14:11:13,318] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.13 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:11:13,389] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 14:11:13,390] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:11:13,390] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.13 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:11:20,068] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.654515981674194
x3006c0s19b1n0: [2024-03-28 14:11:20,068] [INFO] [stage3.py:2251:step] Full outer step loop took 6.655236482620239
x3006c0s1b0n0: [2024-03-28 14:11:20,441] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.027029037475586
x3006c0s1b0n0: [2024-03-28 14:11:20,441] [INFO] [stage3.py:2251:step] Full outer step loop took 7.02723240852356
x3006c0s19b1n0: [2024-03-28 14:11:20,583] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.1698384284973145
x3006c0s19b1n0: [2024-03-28 14:11:20,583] [INFO] [stage3.py:2251:step] Full outer step loop took 7.17016339302063
x3006c0s1b0n0: [2024-03-28 14:11:20,656] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.242644309997559
x3006c0s1b0n0: [2024-03-28 14:11:20,659] [INFO] [stage3.py:2251:step] Full outer step loop took 7.2455925941467285
x3006c0s19b1n0: [2024-03-28 14:11:20,686] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.273231029510498
x3006c0s19b1n0: [2024-03-28 14:11:20,687] [INFO] [stage3.py:2251:step] Full outer step loop took 7.2734222412109375
x3006c0s19b1n0: [2024-03-28 14:11:20,698] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.284548759460449
x3006c0s19b1n0: [2024-03-28 14:11:20,698] [INFO] [stage3.py:2251:step] Full outer step loop took 7.284704923629761
x3006c0s1b0n0: [2024-03-28 14:11:20,903] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.489608287811279
x3006c0s1b0n0: [2024-03-28 14:11:20,903] [INFO] [stage3.py:2251:step] Full outer step loop took 7.48976993560791
x3006c0s1b0n0: [2024-03-28 14:11:20,910] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.496887445449829
x3006c0s1b0n0: [2024-03-28 14:11:20,910] [INFO] [stage3.py:2251:step] Full outer step loop took 7.4970386028289795
x3006c0s1b0n0: [2024-03-28 14:11:20,922] [INFO] [stage3.py:2277:step] End to end step took 7.508257150650024
x3006c0s1b0n0: [2024-03-28 14:11:20,922] [INFO] [stage3.py:2277:step] End to end step took 7.508373022079468
x3006c0s19b1n0: [2024-03-28 14:11:20,922] [INFO] [stage3.py:2277:step] End to end step took 7.508437395095825
x3006c0s19b1n0: [2024-03-28 14:11:20,922] [INFO] [stage3.py:2277:step] End to end step took 7.508465528488159
x3006c0s1b0n0: [2024-03-28 14:11:20,922] [INFO] [stage3.py:2277:step] End to end step took 7.508720874786377
x3006c0s19b1n0: [2024-03-28 14:11:20,922] [INFO] [stage3.py:2277:step] End to end step took 7.508796691894531
x3006c0s19b1n0: [2024-03-28 14:11:20,922] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6655.66
x3006c0s1b0n0: [2024-03-28 14:11:20,922] [INFO] [stage3.py:2277:step] End to end step took 7.508503198623657
x3006c0s19b1n0: [2024-03-28 14:11:20,922] [INFO] [stage3.py:2277:step] End to end step took 7.509139060974121
x3006c0s19b1n0: [2024-03-28 14:11:20,923] [INFO] [logging.py:96:log_dist] [Rank 0] step=8, skipped=0, lr=[6.158400017893797e-05], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 14:11:20,923] [INFO] [timer.py:260:stop] epoch=0/micro_step=8/global_step=8, RunningAvgSamplesPerSec=1.9292186717659188, CurrSamplesPerSec=1.9530039235206316, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 14:11:20,923] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 14101.36 | bwd_microstep: 35337.03 | bwd_inner_microstep: 35174.07 | bwd_allreduce_microstep: 162.81 | step_microstep: 7533.35
x3006c0s19b1n0: [2024-03-28 14:11:20,923] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 14101.35 | bwd: 35337.02 | bwd_inner: 35174.05 | bwd_allreduce: 162.84 | step: 7533.35
x3006c0s19b1n0: [2024-03-28 14:11:21,038] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 14:11:21,038] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:11:21,038] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.14 GB, percent = 66.6%
x3006c0s1b0n0: <TIMER:interval-time,57.963287115097046><TIMER:interval-time,57.963287591934204><TIMER:interval-time,57.9632887840271><TIMER:interval-time,57.96328330039978>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s19b1n0: <TIMER:interval-time,57.9632465839386><TIMER:interval-time,57.963244915008545>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,57.96329045295715>
x3006c0s19b1n0: <TIMER:interval-time,57.96337103843689>
x3006c0s1b0n0:  elapsed_time 57.963289 | consumed samples:          512 | consumed tokens:      1048576 | elapsed time per iteration (ms): 57963.3 | learning rate: 6.158E-05 | global batch size:    64 | lm loss: 1.183349E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.104 | TFLOPs: 76.40 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 14:11:21,145] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 14:11:21,145] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 14:11:21,145] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.14 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:11:28,576] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 14:11:28,576] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:11:28,577] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.12 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:11:28,683] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 14:11:28,684] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:11:28,684] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.12 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:11:47,138] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 14:11:47,139] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 29 GB 
x3006c0s19b1n0: [2024-03-28 14:11:47,139] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.13 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:11:47,224] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 14:11:47,225] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:11:47,225] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.13 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:11:53,966] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 14:11:53,967] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:11:53,967] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.12 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:11:54,054] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 14:11:54,055] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:11:54,055] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.12 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:12:13,459] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 14:12:13,460] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-28 14:12:13,460] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.11 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:12:13,535] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 14:12:13,536] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:12:13,536] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.11 GB, percent = 66.6%
x3006c0s1b0n0: [2024-03-28 14:12:20,705] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.144958257675171
x3006c0s1b0n0: [2024-03-28 14:12:20,705] [INFO] [stage3.py:2251:step] Full outer step loop took 7.145343542098999
x3006c0s19b1n0: [2024-03-28 14:12:20,730] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.170266389846802
x3006c0s19b1n0: [2024-03-28 14:12:20,730] [INFO] [stage3.py:2251:step] Full outer step loop took 7.170459508895874
x3006c0s19b1n0: [2024-03-28 14:12:20,734] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.173770427703857
x3006c0s19b1n0: [2024-03-28 14:12:20,734] [INFO] [stage3.py:2251:step] Full outer step loop took 7.1740944385528564
x3006c0s19b1n0: [2024-03-28 14:12:20,746] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.186228275299072
x3006c0s19b1n0: [2024-03-28 14:12:20,746] [INFO] [stage3.py:2251:step] Full outer step loop took 7.186428785324097
x3006c0s19b1n0: [2024-03-28 14:12:20,812] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.251775503158569
x3006c0s19b1n0: [2024-03-28 14:12:20,812] [INFO] [stage3.py:2251:step] Full outer step loop took 7.251932859420776
x3006c0s1b0n0: [2024-03-28 14:12:20,989] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.4288976192474365
x3006c0s1b0n0: [2024-03-28 14:12:20,989] [INFO] [stage3.py:2251:step] Full outer step loop took 7.429177522659302
x3006c0s1b0n0: [2024-03-28 14:12:20,994] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.43413233757019
x3006c0s1b0n0: [2024-03-28 14:12:20,994] [INFO] [stage3.py:2251:step] Full outer step loop took 7.4343438148498535
x3006c0s1b0n0: [2024-03-28 14:12:21,090] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.517341375350952
x3006c0s1b0n0: [2024-03-28 14:12:21,091] [INFO] [stage3.py:2251:step] Full outer step loop took 7.517501354217529
x3006c0s1b0n0: [2024-03-28 14:12:21,102] [INFO] [stage3.py:2277:step] End to end step took 7.52876091003418
x3006c0s19b1n0: [2024-03-28 14:12:21,102] [INFO] [stage3.py:2277:step] End to end step took 7.542170286178589
x3006c0s1b0n0: [2024-03-28 14:12:21,102] [INFO] [stage3.py:2277:step] End to end step took 7.542204856872559
x3006c0s19b1n0: [2024-03-28 14:12:21,102] [INFO] [stage3.py:2277:step] End to end step took 7.542339086532593
x3006c0s19b1n0: [2024-03-28 14:12:21,102] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 7174.53
x3006c0s1b0n0: [2024-03-28 14:12:21,102] [INFO] [stage3.py:2277:step] End to end step took 7.542495965957642
x3006c0s1b0n0: [2024-03-28 14:12:21,102] [INFO] [stage3.py:2277:step] End to end step took 7.542581796646118
x3006c0s19b1n0: [2024-03-28 14:12:21,102] [INFO] [stage3.py:2277:step] End to end step took 7.542535066604614
x3006c0s19b1n0: [2024-03-28 14:12:21,103] [INFO] [stage3.py:2277:step] End to end step took 7.542528867721558
x3006c0s19b1n0: [2024-03-28 14:12:21,103] [INFO] [logging.py:96:log_dist] [Rank 0] step=9, skipped=0, lr=[3.814149619390238e-05], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 14:12:21,103] [INFO] [timer.py:260:stop] epoch=0/micro_step=9/global_step=9, RunningAvgSamplesPerSec=1.9234007260206254, CurrSamplesPerSec=1.8892168525894244, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 14:12:21,103] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 14089.13 | bwd_microstep: 37509.71 | bwd_inner_microstep: 37346.46 | bwd_allreduce_microstep: 163.10 | step_microstep: 7567.14
x3006c0s19b1n0: [2024-03-28 14:12:21,104] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 14089.12 | bwd: 37509.71 | bwd_inner: 37346.45 | bwd_allreduce: 163.13 | step: 7567.14
x3006c0s19b1n0: [2024-03-28 14:12:21,218] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 14:12:21,218] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:12:21,218] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.16 GB, percent = 66.6%
x3006c0s19b1n0: <TIMER:interval-time,60.179359674453735><TIMER:interval-time,60.179367780685425>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,60.17937397956848>
x3006c0s1b0n0: <TIMER:interval-time,60.17934989929199><TIMER:interval-time,60.179354190826416><TIMER:interval-time,60.17935585975647>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,60.17946743965149>
x3006c0s19b1n0: <TIMER:interval-time,60.179452896118164>
x3006c0s1b0n0:  elapsed_time 60.179356 | consumed samples:          576 | consumed tokens:      1179648 | elapsed time per iteration (ms): 60179.4 | learning rate: 3.814E-05 | global batch size:    64 | lm loss: 1.147080E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.063 | TFLOPs: 73.58 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 14:12:21,353] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 14:12:21,354] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 14:12:21,354] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.13 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:12:28,182] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 14:12:28,182] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:12:28,182] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.12 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:12:28,272] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 14:12:28,273] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:12:28,273] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.12 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:12:46,504] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 14:12:46,505] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 29 GB 
x3006c0s19b1n0: [2024-03-28 14:12:46,505] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.15 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:12:46,586] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 14:12:46,586] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:12:46,586] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.15 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:12:53,984] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 14:12:53,984] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:12:53,985] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.14 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:12:54,077] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 14:12:54,078] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 14:12:54,078] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.14 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:13:14,644] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 14:13:14,645] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-28 14:13:14,645] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.13 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:13:14,714] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 14:13:14,715] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:13:14,715] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.13 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 14:13:21,381] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.642656326293945
x3006c0s19b1n0: [2024-03-28 14:13:21,381] [INFO] [stage3.py:2251:step] Full outer step loop took 6.6429595947265625
x3006c0s19b1n0: [2024-03-28 14:13:21,773] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.034573316574097
x3006c0s19b1n0: [2024-03-28 14:13:21,774] [INFO] [stage3.py:2251:step] Full outer step loop took 7.035810947418213
x3006c0s19b1n0: [2024-03-28 14:13:21,874] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.136012315750122
x3006c0s19b1n0: [2024-03-28 14:13:21,875] [INFO] [stage3.py:2251:step] Full outer step loop took 7.136315584182739
x3006c0s19b1n0: [2024-03-28 14:13:21,926] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.187635898590088
x3006c0s19b1n0: [2024-03-28 14:13:21,926] [INFO] [stage3.py:2251:step] Full outer step loop took 7.187792778015137
x3006c0s1b0n0: [2024-03-28 14:13:22,193] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.454105615615845
x3006c0s1b0n0: [2024-03-28 14:13:22,194] [INFO] [stage3.py:2251:step] Full outer step loop took 7.4543211460113525
x3006c0s1b0n0: [2024-03-28 14:13:22,207] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.468353509902954
x3006c0s1b0n0: [2024-03-28 14:13:22,207] [INFO] [stage3.py:2251:step] Full outer step loop took 7.468791723251343
x3006c0s1b0n0: [2024-03-28 14:13:22,223] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.485178470611572
x3006c0s1b0n0: [2024-03-28 14:13:22,223] [INFO] [stage3.py:2251:step] Full outer step loop took 7.485339641571045
x3006c0s1b0n0: [2024-03-28 14:13:22,296] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.549679756164551
x3006c0s1b0n0: [2024-03-28 14:13:22,296] [INFO] [stage3.py:2251:step] Full outer step loop took 7.549839973449707
x3006c0s1b0n0: [2024-03-28 14:13:22,306] [INFO] [stage3.py:2277:step] End to end step took 7.560340642929077
x3006c0s19b1n0: [2024-03-28 14:13:22,307] [INFO] [stage3.py:2277:step] End to end step took 7.568435907363892
x3006c0s1b0n0: [2024-03-28 14:13:22,307] [INFO] [stage3.py:2277:step] End to end step took 7.568532943725586
x3006c0s1b0n0: [2024-03-28 14:13:22,307] [INFO] [stage3.py:2277:step] End to end step took 7.5685718059539795
x3006c0s19b1n0: [2024-03-28 14:13:22,307] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6643.19
x3006c0s19b1n0: [2024-03-28 14:13:22,307] [INFO] [stage3.py:2277:step] End to end step took 7.568785905838013
x3006c0s1b0n0: [2024-03-28 14:13:22,307] [INFO] [stage3.py:2277:step] End to end step took 7.567629337310791
x3006c0s19b1n0: [2024-03-28 14:13:22,307] [INFO] [stage3.py:2277:step] End to end step took 7.568718910217285
x3006c0s19b1n0: [2024-03-28 14:13:22,307] [INFO] [stage3.py:2277:step] End to end step took 7.568981170654297
x3006c0s19b1n0: [2024-03-28 14:13:22,307] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[3e-05], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 14:13:22,308] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=1.9058979984272775, CurrSamplesPerSec=1.7917640562161272, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 14:13:22,308] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 14157.62 | bwd_microstep: 38462.69 | bwd_inner_microstep: 38291.10 | bwd_allreduce_microstep: 171.43 | step_microstep: 7593.01
x3006c0s19b1n0: [2024-03-28 14:13:22,308] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 14157.61 | bwd: 38462.70 | bwd_inner: 38291.09 | bwd_allreduce: 171.47 | step: 7593.01
x3006c0s19b1n0: [2024-03-28 14:13:22,427] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 14:13:22,428] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 14:13:22,428] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.18 GB, percent = 66.6%
x3006c0s1b0n0: <TIMER:interval-time,61.20951819419861><TIMER:interval-time,61.20952272415161><TIMER:interval-time,61.20952272415161><TIMER:interval-time,61.2095251083374>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s19b1n0: <TIMER:interval-time,61.209492683410645><TIMER:interval-time,61.209490060806274><TIMER:interval-time,61.20949649810791><TIMER:interval-time,61.20949602127075>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b0n0:  elapsed_time 61.209525 | consumed samples:          640 | consumed tokens:      1310720 | elapsed time per iteration (ms): 61209.5 | learning rate: 3.000E-05 | global batch size:    64 | lm loss: 1.112424E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.046 | TFLOPs: 72.35 |
x3006c0s1b0n0: <<<only_train:597.8747670650482>>>
x3006c0s1b0n0: <<<only_train:597.8738753795624>>>
x3006c0s19b1n0: <<<only_train:597.8746960163116>>>
x3006c0s19b1n0: <<<only_train:597.874694108963>>>
x3006c0s1b0n0: <<<only_train:597.8745272159576>>>
x3006c0s19b1n0: <<<only_train:597.8746988773346>>><<<only_train:597.8747124671936>>>
x3006c0s1b0n0: <<<only_train:597.8743443489075>>>
x3006c0s19b1n0: 
x3006c0s19b1n0: [after training ends] datetime: 2024-03-28 14:13:22 
x3006c0s19b1n0: <<<full_time:597.8756222724915>>><<<full_time:597.8756551742554>>><<<full_time:597.8756356239319>>><<<full_time:597.875646352768>>>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b0n0: <<<full_time:597.8758211135864>>>
x3006c0s1b0n0: <<<full_time:597.8754487037659>>><<<full_time:597.8752229213715>>>
x3006c0s1b0n0: <<<full_time:597.8748826980591>>>
x3006c0s1b0n0: 
x3006c0s19b1n0: [2024-03-28 14:13:26,282] [INFO] [launch.py:348:main] Process 27034 exits successfully.
x3006c0s1b0n0: [2024-03-28 14:13:26,753] [INFO] [launch.py:348:main] Process 18087 exits successfully.
x3006c0s19b1n0: [2024-03-28 14:13:29,285] [INFO] [launch.py:348:main] Process 27036 exits successfully.
x3006c0s19b1n0: [2024-03-28 14:13:29,285] [INFO] [launch.py:348:main] Process 27037 exits successfully.
x3006c0s19b1n0: [2024-03-28 14:13:29,285] [INFO] [launch.py:348:main] Process 27035 exits successfully.
x3006c0s1b0n0: [2024-03-28 14:13:29,757] [INFO] [launch.py:348:main] Process 18085 exits successfully.
x3006c0s1b0n0: [2024-03-28 14:13:29,757] [INFO] [launch.py:348:main] Process 18088 exits successfully.
x3006c0s1b0n0: [2024-03-28 14:13:29,757] [INFO] [launch.py:348:main] Process 18086 exits successfully.

[2024-03-28 13:19:19,969] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-28 13:19:23,090] [INFO] [runner.py:463:main] Using IP address of 10.140.57.107 for node x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-28 13:19:23,092] [INFO] [runner.py:557:main] deepspeed_env file = ./.deepspeed_env
[2024-03-28 13:19:23,093] [INFO] [multinode_runner.py:80:get_cmd] Running on the following workers: x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov,x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-28 13:19:23,093] [INFO] [runner.py:568:main] cmd = pdsh -S -f 1024 -w x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov,x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov export PYTHONUSERBASE=/home/am6429/.local/polaris/conda/2023-10-04; export PYTHONPATH=/home/am6429/dl-io/Megatron-DeepSpeed; export PATH=/home/am6429/.conda/envs/dspeed_env/bin:/soft/datascience/conda/2023-10-04/mconda3/condabin:/soft/compilers/cudatoolkit/cuda-11.8.0/bin:/soft/buildtools/cmake/cmake-3.23.2/cmake-3.23.2-linux-x86_64/bin:/opt/cray/pe/gcc/11.2.0/bin:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/include:/opt/cray/pe/pals/1.2.11/bin:/opt/cray/pe/craype/2.7.20/bin:/opt/cray/libfabric/1.15.2.0/bin:/home/am6429/.conda/envs/dspeed_env/bin:/opt/cray/pe/perftools/23.03.0/bin:/opt/cray/pe/papi/7.0.0.1/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/am6429/.local/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/am6429/veloc-build/include:/home/am6429/veloc-build/bin:/soft/datascience/conda/2023-01-10/mconda3/include:/opt/cray/pe/bin:/soft/datascience/conda/2023-01-10/mconda3/include; export LD_LIBRARY_PATH=/usr/lib64/:/soft/compilers/cudatoolkit/cuda-11.8.0/lib64:/opt/cray/pe/gcc/11.2.0/snos/lib64:/soft/compilers/cudatoolkit/cuda-11.8.0/extras/CUPTI/lib64:/soft/libraries/trt/TensorRT-8.5.3.1.Linux.x86_64-gnu.cuda-11.8.cudnn8.6/lib:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/lib:/soft/libraries/cudnn/cudnn-11-linux-x64-v8.6.0.163/lib:/opt/cray/libfabric/1.15.2.0/lib64:/usr/lib64:/opt/cray/pe/papi/7.0.0.1/lib64:/home/am6429/veloc-build/lib:/home/am6429/veloc-build/lib64:/home/am6429/nvcomp/lib:/soft/datascience/conda/2023-01-10/mconda3/lib:/soft/datascience/conda/2023-01-10/mconda3/lib/; export http_proxy=http://proxy.alcf.anl.gov:3128; export https_proxy=http://proxy.alcf.anl.gov:3128; export CC=gcc; export CXX=g++; export IBV_FORK_SAFE=1; export CFLAGS=-I/soft/datascience/conda/2023-01-10/mconda3/include/; export LDFLAGS=-L/soft/datascience/conda/2023-01-10/mconda3/lib/; export CUDA_DEVICE_MAX_CONNECTIONS=1; export TORCHSNAPSHOT_PER_RANK_MEMORY_BUDGET_BYTES=34359738368; export _DEFAULT_MAX_PER_RANK_IO_CONCURRENCY=1; export _MAX_PER_RANK_IO_CONCURRENCY=1; export NSYS_REPORT_DIR=/home/am6429/dl-io/dl-io-outputs/act-output-30B//rep-30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch1-flush_async1-opt_gaps5-%n;  cd /home/am6429/dl-io/Megatron-DeepSpeed; /home/am6429/.conda/envs/dspeed_env/bin/python -u -m deepspeed.launcher.launch --world_info=eyJ4MzAwNmMwczE5YjFuMC5oc24uY20ucG9sYXJpcy5hbGNmLmFubC5nb3YiOiBbMCwgMSwgMiwgM10sICJ4MzAwNmMwczFiMG4wLmhzbi5jbS5wb2xhcmlzLmFsY2YuYW5sLmdvdiI6IFswLCAxLCAyLCAzXX0= --node_rank=%n --master_addr=10.140.57.107 --master_port=29700 /home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py --tensor-model-parallel-size 1 --num-layers 60 --hidden-size 6656 --num-attention-heads 52 --micro-batch-size 4 --global-batch-size 32 --seq-length 2048 --max-position-embeddings 2048 --train-iters 10 --save /local/scratch/llama2/zero3-tp1}_dp8 --data-path /home/am6429/dl-io/datasets/meg-gpt2_text_document --vocab-file /home/am6429/dl-io/datasets/gpt2-vocab.json --merge-file /home/am6429/dl-io/datasets/gpt2-merges.txt --data-impl mmap --tokenizer-type GPTSentencePieceTokenizer --tokenizer-model /home/am6429/dl-io/datasets/tokenizer.model --split 949,50,1 --distributed-backend nccl --lr 3e-4 --lr-decay-style cosine --min-lr 3e-5 --weight-decay 0.1 --clip-grad 1 --lr-warmup-iters 1 --optimizer adam --adam-beta1 0.9 --adam-beta2 0.95 --log-interval 1 --save-interval 1000 --eval-interval 1000 --eval-iters 0 --bf16 --no-query-key-layer-scaling --attention-dropout 0 --hidden-dropout 0 --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization rmsnorm --disable-bias-linear --num-key-value-heads 4 --deepspeed --exit-interval 20 --deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json --zero-stage=3 --no-pipeline-parallel --cpu-optimizer --checkpoint-activations --deepspeed-activation-checkpointing
x3006c0s19b1n0: [2024-03-28 13:19:25,122] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b0n0: [2024-03-28 13:19:25,123] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-28 13:19:26,953] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3006c0s19b1n0: [2024-03-28 13:19:26,953] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=0
x3006c0s19b1n0: [2024-03-28 13:19:26,953] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3006c0s19b1n0: [2024-03-28 13:19:26,953] [INFO] [launch.py:163:main] dist_world_size=8
x3006c0s19b1n0: [2024-03-28 13:19:26,953] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3006c0s19b1n0: [2024-03-28 13:19:26,954] [INFO] [launch.py:253:main] process 9024 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-28 13:19:26,955] [INFO] [launch.py:253:main] process 9025 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-28 13:19:26,955] [INFO] [launch.py:253:main] process 9026 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-28 13:19:26,956] [INFO] [launch.py:253:main] process 9027 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b0n0: [2024-03-28 13:19:27,421] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3006c0s1b0n0: [2024-03-28 13:19:27,421] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=1
x3006c0s1b0n0: [2024-03-28 13:19:27,421] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3006c0s1b0n0: [2024-03-28 13:19:27,421] [INFO] [launch.py:163:main] dist_world_size=8
x3006c0s1b0n0: [2024-03-28 13:19:27,421] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3006c0s1b0n0: [2024-03-28 13:19:27,422] [INFO] [launch.py:253:main] process 2648 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b0n0: [2024-03-28 13:19:27,422] [INFO] [launch.py:253:main] process 2649 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b0n0: [2024-03-28 13:19:27,423] [INFO] [launch.py:253:main] process 2650 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b0n0: [2024-03-28 13:19:27,423] [INFO] [launch.py:253:main] process 2651 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-28 13:19:28,708] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-28 13:19:28,763] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-28 13:19:28,769] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-28 13:19:28,776] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b0n0: [2024-03-28 13:19:29,228] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b0n0: [2024-03-28 13:19:29,238] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b0n0: [2024-03-28 13:19:29,253] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b0n0: [2024-03-28 13:19:29,259] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: [2024-03-28 13:19:31,561] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: INFO: overriding default arguments for tokenizer_type:GPTSentencePieceTokenizer                    with tokenizer_type:GPT2BPETokenizer
x3006c0s19b1n0: using world size: 8, data-parallel-size: 8, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
x3006c0s19b1n0: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
x3006c0s19b1n0: using torch.bfloat16 for parameters ...
x3006c0s19b1n0: ------------------------ arguments ------------------------
x3006c0s19b1n0:   accumulate_allreduce_grads_in_fp32 .............. True
x3006c0s19b1n0:   adam_beta1 ...................................... 0.9
x3006c0s19b1n0:   adam_beta2 ...................................... 0.95
x3006c0s19b1n0:   adam_eps ........................................ 1e-08
x3006c0s19b1n0:   add_bias_linear ................................. False
x3006c0s19b1n0:   add_position_embedding .......................... False
x3006c0s19b1n0:   adlr_autoresume ................................. False
x3006c0s19b1n0:   adlr_autoresume_interval ........................ 1000
x3006c0s19b1n0:   aml_data_download_path .......................... None
x3006c0s19b1n0:   apply_layernorm_1p .............................. False
x3006c0s19b1n0:   apply_query_key_layer_scaling ................... False
x3006c0s19b1n0:   apply_residual_connection_post_layernorm ........ False
x3006c0s19b1n0:   async_tensor_model_parallel_allreduce ........... False
x3006c0s19b1n0:   attention_dropout ............................... 0.0
x3006c0s19b1n0:   attention_softmax_in_fp32 ....................... False
x3006c0s19b1n0:   barrier_with_L1_time ............................ True
x3006c0s19b1n0:   bert_binary_head ................................ True
x3006c0s19b1n0:   bert_embedder_type .............................. megatron
x3006c0s19b1n0:   bert_load ....................................... None
x3006c0s19b1n0:   bf16 ............................................ True
x3006c0s19b1n0:   bias_dropout_fusion ............................. True
x3006c0s19b1n0:   bias_gelu_fusion ................................ False
x3006c0s19b1n0:   biencoder_projection_dim ........................ 0
x3006c0s19b1n0:   biencoder_shared_query_context_model ............ False
x3006c0s19b1n0:   block_data_path ................................. None
x3006c0s19b1n0:   checkpoint_activations .......................... True
x3006c0s19b1n0:   checkpoint_in_cpu ............................... False
x3006c0s19b1n0:   checkpoint_num_layers ........................... 1
x3006c0s19b1n0:   classes_fraction ................................ 1.0
x3006c0s19b1n0:   clip_grad ....................................... 1.0
x3006c0s19b1n0:   compression_training ............................ False
x3006c0s19b1n0:   consumed_train_samples .......................... 0
x3006c0s19b1n0:   consumed_train_tokens ........................... 0
x3006c0s19b1n0:   consumed_valid_samples .......................... 0
x3006c0s19b1n0:   contigious_checkpointing ........................ False
x3006c0s19b1n0:   cpu_optimizer ................................... True
x3006c0s19b1n0:   cpu_torch_adam .................................. False
x3006c0s19b1n0:   create_moe_param_group .......................... False
x3006c0s19b1n0:   curriculum_learning_legacy ...................... False
x3006c0s19b1n0:   data_cache_path ................................. None
x3006c0s19b1n0:   data_efficiency_curriculum_learning ............. False
x3006c0s19b1n0:   data_impl ....................................... mmap
x3006c0s19b1n0:   data_parallel_random_init ....................... False
x3006c0s19b1n0:   data_parallel_size .............................. 8
x3006c0s19b1n0:   data_path ....................................... ['/home/am6429/dl-io/datasets/meg-gpt2_text_document']
x3006c0s19b1n0:   data_per_class_fraction ......................... 1.0
x3006c0s19b1n0:   data_sharding ................................... True
x3006c0s19b1n0:   dataloader_type ................................. single
x3006c0s19b1n0:   DDP_impl ........................................ local
x3006c0s19b1n0:   decoder_num_layers .............................. None
x3006c0s19b1n0:   decoder_seq_length .............................. None
x3006c0s19b1n0:   deepscale ....................................... False
x3006c0s19b1n0:   deepscale_config ................................ None
x3006c0s19b1n0:   deepspeed ....................................... True
x3006c0s19b1n0:   deepspeed_activation_checkpointing .............. True
x3006c0s19b1n0:   deepspeed_config ................................ /home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json
x3006c0s19b1n0:   dino_bottleneck_size ............................ 256
x3006c0s19b1n0:   dino_freeze_last_layer .......................... 1
x3006c0s19b1n0:   dino_head_hidden_size ........................... 2048
x3006c0s19b1n0:   dino_local_crops_number ......................... 10
x3006c0s19b1n0:   dino_local_img_size ............................. 96
x3006c0s19b1n0:   dino_norm_last_layer ............................ False
x3006c0s19b1n0:   dino_teacher_temp ............................... 0.07
x3006c0s19b1n0:   dino_warmup_teacher_temp ........................ 0.04
x3006c0s19b1n0:   dino_warmup_teacher_temp_epochs ................. 30
x3006c0s19b1n0:   distribute_checkpointed_activations ............. False
x3006c0s19b1n0:   distribute_saved_activations .................... False
x3006c0s19b1n0:   distributed_backend ............................. nccl
x3006c0s19b1n0:   distributed_timeout_minutes ..................... 10
x3006c0s19b1n0:   ds_inference .................................... False
x3006c0s19b1n0:   ds_pipeline_enabled ............................. False
x3006c0s19b1n0:   ds_sequence_parallel_size ....................... 1
x3006c0s19b1n0:   embedding_path .................................. None
x3006c0s19b1n0:   embedding_weights_in_fp32 ....................... False
x3006c0s19b1n0:   empty_unused_memory_level ....................... 0
x3006c0s19b1n0:   enable_expert_tensor_parallelism ................ False
x3006c0s19b1n0:   encoder_num_layers .............................. 60
x3006c0s19b1n0:   encoder_seq_length .............................. 2048
x3006c0s19b1n0:   end_weight_decay ................................ 0.1
x3006c0s19b1n0:   eod_mask_loss ................................... False
x3006c0s19b1n0:   eval_interval ................................... 1000
x3006c0s19b1n0:   eval_iters ...................................... 0
x3006c0s19b1n0:   evidence_data_path .............................. None
x3006c0s19b1n0:   exit_duration_in_mins ........................... None
x3006c0s19b1n0:   exit_interval ................................... 20
x3006c0s19b1n0:   exit_on_missing_checkpoint ...................... False
x3006c0s19b1n0:   exit_signal_handler ............................. False
x3006c0s19b1n0:   expert_interval ................................. 2
x3006c0s19b1n0:   ffn_hidden_size ................................. 17728
x3006c0s19b1n0:   finetune ........................................ False
x3006c0s19b1n0:   force_ds_sequence_parallel ...................... False
x3006c0s19b1n0:   fp16 ............................................ False
x3006c0s19b1n0:   fp16_lm_cross_entropy ........................... False
x3006c0s19b1n0:   fp32_residual_connection ........................ False
x3006c0s19b1n0:   fp8_amax_compute_algo ........................... most_recent
x3006c0s19b1n0:   fp8_amax_history_len ............................ 1
x3006c0s19b1n0:   fp8_e4m3 ........................................ False
x3006c0s19b1n0:   fp8_hybrid ...................................... False
x3006c0s19b1n0:   fp8_interval .................................... 1
x3006c0s19b1n0:   fp8_margin ...................................... 0
x3006c0s19b1n0:   fp8_wgrad ....................................... True
x3006c0s19b1n0:   global_batch_size ............................... 32
x3006c0s19b1n0:   gradient_accumulation_fusion .................... True
x3006c0s19b1n0:   head_lr_mult .................................... 1.0
x3006c0s19b1n0:   hidden_dropout .................................. 0.0
x3006c0s19b1n0:   hidden_size ..................................... 6656
x3006c0s19b1n0:   hidden_size_teacher ............................. None
x3006c0s19b1n0:   hysteresis ...................................... 2
x3006c0s19b1n0:   ict_head_size ................................... None
x3006c0s19b1n0:   ict_load ........................................ None
x3006c0s19b1n0:   img_h ........................................... 224
x3006c0s19b1n0:   img_w ........................................... 224
x3006c0s19b1n0:   indexer_batch_size .............................. 128
x3006c0s19b1n0:   indexer_log_interval ............................ 1000
x3006c0s19b1n0:   inference ....................................... False
x3006c0s19b1n0:   inference_batch_times_seqlen_threshold .......... 512
x3006c0s19b1n0:   init_method_std ................................. 0.02
x3006c0s19b1n0:   init_method_xavier_uniform ...................... False
x3006c0s19b1n0:   initial_loss_scale .............................. 4294967296
x3006c0s19b1n0:   iter_per_epoch .................................. 1250
x3006c0s19b1n0:   kd .............................................. False
x3006c0s19b1n0:   kd_alpha_ce ..................................... 1
x3006c0s19b1n0:   kd_beta_ce ...................................... 1
x3006c0s19b1n0:   kd_temp ......................................... 1.0
x3006c0s19b1n0:   kv_channels ..................................... 128
x3006c0s19b1n0:   layernorm_epsilon ............................... 1e-05
x3006c0s19b1n0:   lazy_mpu_init ................................... None
x3006c0s19b1n0:   load ............................................ None
x3006c0s19b1n0:   load_teacher .................................... None
x3006c0s19b1n0:   local_rank ...................................... 0
x3006c0s19b1n0:   log_batch_size_to_tensorboard ................... False
x3006c0s19b1n0:   log_interval .................................... 1
x3006c0s19b1n0:   log_learning_rate_to_tensorboard ................ True
x3006c0s19b1n0:   log_loss_scale_to_tensorboard ................... True
x3006c0s19b1n0:   log_memory_to_tensorboard ....................... False
x3006c0s19b1n0:   log_num_zeros_in_grad ........................... False
x3006c0s19b1n0:   log_optimizer_states_to_tensorboard ............. False
x3006c0s19b1n0:   log_params_norm ................................. False
x3006c0s19b1n0:   log_timers_to_tensorboard ....................... False
x3006c0s19b1n0:   log_validation_ppl_to_tensorboard ............... False
x3006c0s19b1n0:   log_world_size_to_tensorboard ................... False
x3006c0s19b1n0:   loss_scale ...................................... None
x3006c0s19b1n0:   loss_scale_window ............................... 1000
x3006c0s19b1n0:   lr .............................................. 0.0003
x3006c0s19b1n0:   lr_decay_iters .................................. None
x3006c0s19b1n0:   lr_decay_samples ................................ None
x3006c0s19b1n0:   lr_decay_style .................................. cosine
x3006c0s19b1n0:   lr_decay_tokens ................................. None
x3006c0s19b1n0:   lr_warmup_fraction .............................. None
x3006c0s19b1n0:   lr_warmup_iters ................................. 1
x3006c0s19b1n0:   lr_warmup_samples ............................... 0
x3006c0s19b1n0:   lr_warmup_tokens ................................ None
x3006c0s19b1n0:   make_vocab_size_divisible_by .................... 128
x3006c0s19b1n0:   mask_factor ..................................... 1.0
x3006c0s19b1n0:   mask_prob ....................................... 0.15
x3006c0s19b1n0:   mask_type ....................................... random
x3006c0s19b1n0:   masked_softmax_fusion ........................... True
x3006c0s19b1n0:   max_position_embeddings ......................... 2048
x3006c0s19b1n0:   max_tokens_to_oom ............................... 12000
x3006c0s19b1n0:   memory_centric_tiled_linear ..................... False
x3006c0s19b1n0:   merge_file ...................................... /home/am6429/dl-io/datasets/gpt2-merges.txt
x3006c0s19b1n0:   micro_batch_size ................................ 4
x3006c0s19b1n0:   min_loss_scale .................................. 1.0
x3006c0s19b1n0:   min_lr .......................................... 3e-05
x3006c0s19b1n0:   mlp_type ........................................ standard
x3006c0s19b1n0:   mmap_warmup ..................................... False
x3006c0s19b1n0:   moe_eval_capacity_factor ........................ 1.0
x3006c0s19b1n0:   moe_expert_parallel_size ........................ 1
x3006c0s19b1n0:   moe_loss_coeff .................................. 0.1
x3006c0s19b1n0:   moe_min_capacity ................................ 4
x3006c0s19b1n0:   moe_token_dropping .............................. True
x3006c0s19b1n0:   moe_train_capacity_factor ....................... 1.0
x3006c0s19b1n0:   mos ............................................. False
x3006c0s19b1n0:   no_load_lr_state ................................ False
x3006c0s19b1n0:   no_load_optim ................................... None
x3006c0s19b1n0:   no_load_rng ..................................... None
x3006c0s19b1n0:   no_persist_layer_norm ........................... False
x3006c0s19b1n0:   no_pipeline_parallel ............................ True
x3006c0s19b1n0:   no_save_optim ................................... None
x3006c0s19b1n0:   no_save_rng ..................................... None
x3006c0s19b1n0:   normalization ................................... rmsnorm
x3006c0s19b1n0:   num_attention_heads ............................. 52
x3006c0s19b1n0:   num_attention_heads_teacher ..................... None
x3006c0s19b1n0:   num_channels .................................... 3
x3006c0s19b1n0:   num_classes ..................................... 1000
x3006c0s19b1n0:   num_experts ..................................... [1]
x3006c0s19b1n0:   num_experts_switch .............................. None
x3006c0s19b1n0:   num_experts_teacher ............................. [1]
x3006c0s19b1n0:   num_key_value_heads ............................. 4
x3006c0s19b1n0:   num_layers ...................................... 60
x3006c0s19b1n0:   num_layers_per_virtual_pipeline_stage ........... None
x3006c0s19b1n0:   num_layers_teacher .............................. None
x3006c0s19b1n0:   num_workers ..................................... 2
x3006c0s19b1n0:   onnx_safe ....................................... None
x3006c0s19b1n0:   openai_gelu ..................................... False
x3006c0s19b1n0:   optimizer ....................................... adam
x3006c0s19b1n0:   output_bert_embeddings .......................... False
x3006c0s19b1n0:   overlap_p2p_comm ................................ False
x3006c0s19b1n0:   override_opt_param_scheduler .................... False
x3006c0s19b1n0:   params_dtype .................................... torch.bfloat16
x3006c0s19b1n0:   partition_activations ........................... False
x3006c0s19b1n0:   patch_dim ....................................... 16
x3006c0s19b1n0:   perform_initialization .......................... True
x3006c0s19b1n0:   pipeline_model_parallel_size .................... 1
x3006c0s19b1n0:   pipeline_model_parallel_split_rank .............. None
x3006c0s19b1n0:   profile_backward ................................ False
x3006c0s19b1n0:   query_in_block_prob ............................. 0.1
x3006c0s19b1n0:   rampup_batch_size ............................... None
x3006c0s19b1n0:   random_ltd ...................................... False
x3006c0s19b1n0:   rank ............................................ 0
x3006c0s19b1n0:   recompute_granularity ........................... None
x3006c0s19b1n0:   recompute_method ................................ None
x3006c0s19b1n0:   recompute_num_layers ............................ 1
x3006c0s19b1n0:   remote_device ................................... none
x3006c0s19b1n0:   reset_attention_mask ............................ False
x3006c0s19b1n0:   reset_iteration ................................. False
x3006c0s19b1n0:   reset_position_ids .............................. False
x3006c0s19b1n0:   retriever_report_topk_accuracies ................ []
x3006c0s19b1n0:   retriever_score_scaling ......................... False
x3006c0s19b1n0:   retriever_seq_length ............................ 256
x3006c0s19b1n0:   retro_add_retriever ............................. False
x3006c0s19b1n0:   retro_cyclic_train_iters ........................ None
x3006c0s19b1n0:   retro_encoder_attention_dropout ................. 0.1
x3006c0s19b1n0:   retro_encoder_hidden_dropout .................... 0.1
x3006c0s19b1n0:   retro_encoder_layers ............................ 2
x3006c0s19b1n0:   retro_num_neighbors ............................. 2
x3006c0s19b1n0:   retro_num_retrieved_chunks ...................... 2
x3006c0s19b1n0:   retro_return_doc_ids ............................ False
x3006c0s19b1n0:   retro_workdir ................................... None
x3006c0s19b1n0:   return_data_index ............................... False
x3006c0s19b1n0:   rotary_percent .................................. 1.0
x3006c0s19b1n0:   sample_rate ..................................... 1.0
x3006c0s19b1n0:   save ............................................ /local/scratch/llama2/zero3-tp1}_dp8
x3006c0s19b1n0:   save_interval ................................... 1000
x3006c0s19b1n0:   scatter_gather_tensors_in_pipeline .............. True
x3006c0s19b1n0:   scattered_embeddings ............................ False
x3006c0s19b1n0:   seed ............................................ 1234
x3006c0s19b1n0:   seq_length ...................................... 2048
x3006c0s19b1n0:   sequence_parallel ............................... False
x3006c0s19b1n0:   sgd_momentum .................................... 0.9
x3006c0s19b1n0:   short_seq_prob .................................. 0.1
x3006c0s19b1n0:   skip_train ...................................... False
x3006c0s19b1n0:   split ........................................... 949,50,1
x3006c0s19b1n0:   split_transformers .............................. False
x3006c0s19b1n0:   squared_relu .................................... False
x3006c0s19b1n0:   standalone_embedding_stage ...................... False
x3006c0s19b1n0:   start_weight_decay .............................. 0.1
x3006c0s19b1n0:   swiglu .......................................... True
x3006c0s19b1n0:   swin_backbone_type .............................. tiny
x3006c0s19b1n0:   synchronize_each_layer .......................... False
x3006c0s19b1n0:   tensor_model_parallel_size ...................... 1
x3006c0s19b1n0:   tensorboard_dir ................................. None
x3006c0s19b1n0:   tensorboard_log_interval ........................ 1
x3006c0s19b1n0:   tensorboard_queue_size .......................... 1000
x3006c0s19b1n0:   test_data_path .................................. None
x3006c0s19b1n0:   tile_factor ..................................... 1
x3006c0s19b1n0:   timing_log_level ................................ 0
x3006c0s19b1n0:   timing_log_option ............................... minmax
x3006c0s19b1n0:   titles_data_path ................................ None
x3006c0s19b1n0:   tokenizer_model ................................. /home/am6429/dl-io/datasets/tokenizer.model
x3006c0s19b1n0:   tokenizer_type .................................. GPT2BPETokenizer
x3006c0s19b1n0:   topk ............................................ 1
x3006c0s19b1n0:   train_data_exact_num_epochs ..................... None
x3006c0s19b1n0:   train_data_path ................................. None
x3006c0s19b1n0:   train_desc_path ................................. None
x3006c0s19b1n0:   train_doc_idx_path .............................. None
x3006c0s19b1n0:   train_idx_path .................................. None
x3006c0s19b1n0:   train_iters ..................................... 10
x3006c0s19b1n0:   train_sample_idx_path ........................... None
x3006c0s19b1n0:   train_samples ................................... None
x3006c0s19b1n0:   train_shuffle_idx_path .......................... None
x3006c0s19b1n0:   train_tokens .................................... None
x3006c0s19b1n0:   transformer_impl ................................ local
x3006c0s19b1n0:   transformer_pipeline_model_parallel_size ........ 1
x3006c0s19b1n0:   untie_embeddings_and_output_weights ............. True
x3006c0s19b1n0:   use_checkpoint_args ............................. False
x3006c0s19b1n0:   use_checkpoint_opt_param_scheduler .............. False
x3006c0s19b1n0:   use_contiguous_buffers_in_local_ddp ............. True
x3006c0s19b1n0:   use_cpu_initialization .......................... None
x3006c0s19b1n0:   use_dataset_only ................................ False
x3006c0s19b1n0:   use_distributed_optimizer ....................... False
x3006c0s19b1n0:   use_flash_attn .................................. False
x3006c0s19b1n0:   use_flash_attn_triton ........................... False
x3006c0s19b1n0:   use_flash_attn_v1 ............................... False
x3006c0s19b1n0:   use_flash_attn_v2 ............................... False
x3006c0s19b1n0:   use_one_sent_docs ............................... False
x3006c0s19b1n0:   use_pin_memory .................................. False
x3006c0s19b1n0:   use_ring_exchange_p2p ........................... False
x3006c0s19b1n0:   use_rotary_position_embeddings .................. True
x3006c0s19b1n0:   use_tutel ....................................... False
x3006c0s19b1n0:   valid_data_path ................................. None
x3006c0s19b1n0:   variable_seq_lengths ............................ False
x3006c0s19b1n0:   virtual_pipeline_model_parallel_size ............ None
x3006c0s19b1n0:   vision_backbone_type ............................ vit
x3006c0s19b1n0:   vision_pretraining .............................. False
x3006c0s19b1n0:   vision_pretraining_type ......................... classify
x3006c0s19b1n0:   vocab_extra_ids ................................. 0
x3006c0s19b1n0:   vocab_file ...................................... /home/am6429/dl-io/datasets/gpt2-vocab.json
x3006c0s19b1n0:   vocab_size ...................................... None
x3006c0s19b1n0:   weight_decay .................................... 0.1
x3006c0s19b1n0:   weight_decay_incr_style ......................... constant
x3006c0s19b1n0:   world_size ...................................... 8
x3006c0s19b1n0:   zero_allgather_bucket_size ...................... 0.0
x3006c0s19b1n0:   zero_contigious_gradients ....................... False
x3006c0s19b1n0:   zero_reduce_bucket_size ......................... 0.0
x3006c0s19b1n0:   zero_reduce_scatter ............................. False
x3006c0s19b1n0:   zero_stage ...................................... 3
x3006c0s19b1n0: -------------------- end of arguments ---------------------
x3006c0s19b1n0: setting number of micro-batches to constant 1
x3006c0s19b1n0: > building GPT2BPETokenizer tokenizer ...
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
x3006c0s19b1n0: > initializing torch distributed ...
x3006c0s19b1n0: [2024-03-28 13:19:31,757] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: [2024-03-28 13:19:31,757] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
x3006c0s19b1n0: [2024-03-28 13:19:31,768] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: [2024-03-28 13:19:31,770] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b0n0:       meet the required dependencies to JIT install the op.
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: JIT compiled ops requires ninja
x3006c0s1b0n0: ninja .................. [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: op name ................ installed .. compatible
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b0n0:       meet the required dependencies to JIT install the op.
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: JIT compiled ops requires ninja
x3006c0s1b0n0: ninja .................. [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: op name ................ installed .. compatible
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b0n0:       meet the required dependencies to JIT install the op.
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: JIT compiled ops requires ninja
x3006c0s1b0n0: ninja .................. [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: op name ................ installed .. compatible
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b0n0:       meet the required dependencies to JIT install the op.
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: JIT compiled ops requires ninja
x3006c0s1b0n0: ninja .................. [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: op name ................ installed .. compatible
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed general environment info:
x3006c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b0n0: torch version .................... 2.0.1+cu118
x3006c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b0n0: torch cuda version ............... 11.8
x3006c0s1b0n0: torch hip version ................ None
x3006c0s1b0n0: nvcc version ..................... 11.8
x3006c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [2024-03-28 13:19:32,117] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: DeepSpeed general environment info:
x3006c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b0n0: torch version .................... 2.0.1+cu118
x3006c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b0n0: torch cuda version ............... 11.8
x3006c0s1b0n0: torch hip version ................ None
x3006c0s1b0n0: nvcc version ..................... 11.8
x3006c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: DeepSpeed general environment info:
x3006c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b0n0: torch version .................... 2.0.1+cu118
x3006c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b0n0: torch cuda version ............... 11.8
x3006c0s1b0n0: torch hip version ................ None
x3006c0s1b0n0: nvcc version ..................... 11.8
x3006c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed general environment info:
x3006c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b0n0: torch version .................... 2.0.1+cu118
x3006c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b0n0: torch cuda version ............... 11.8
x3006c0s1b0n0: torch hip version ................ None
x3006c0s1b0n0: nvcc version ..................... 11.8
x3006c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b0n0: [2024-03-28 13:19:32,240] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b0n0: [2024-03-28 13:19:32,249] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b0n0: [2024-03-28 13:19:32,255] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: > initialized tensor model parallel with size 1
x3006c0s19b1n0: > initialized pipeline model parallel with size 1
x3006c0s19b1n0: > setting random seeds to 1234 ...
x3006c0s19b1n0: [2024-03-28 13:19:33,018] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
x3006c0s19b1n0: > compiling dataset index builder ...
x3006c0s19b1n0: make: Entering directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3006c0s19b1n0: make: Nothing to be done for 'default'.
x3006c0s19b1n0: make: Leaving directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3006c0s19b1n0: >>> done with dataset index builder. Compilation time: 0.095 seconds
x3006c0s19b1n0: > compiling and loading fused kernels ...
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: >>> done with compiling and loading fused kernels. Compilation time: 5.035 seconds
x3006c0s19b1n0: <<<<<<<<<<< 2
x3006c0s19b1n0: <<<<<<<<<<< 3
x3006c0s19b1n0: <<<<<<<<<<< 1
x3006c0s19b1n0: initialize_megatron took 8.092983961105347
x3006c0s19b1n0: <<<<<<<<<<< 0
x3006c0s1b0n0: <<<<<<<<<<< 6
x3006c0s1b0n0: <<<<<<<<<<< 7
x3006c0s1b0n0: <<<<<<<<<<< 5
x3006c0s1b0n0: <<<<<<<<<<< 4
x3006c0s19b1n0: time to initialize megatron (seconds): 8.609
x3006c0s19b1n0: [after megatron is initialized] datetime: 2024-03-28 13:19:39 
x3006c0s19b1n0: get_accelerator and all_reduce  took 0.04273104667663574
x3006c0s19b1n0: building GPT model ...
x3006c0s19b1n0: [2024-03-28 13:19:39,911] [INFO] [utils.py:800:see_memory_usage] Before Building Model
x3006c0s19b1n0: [2024-03-28 13:19:39,913] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 4.62 GB         CA 0.0 GB         Max_CA 5 GB 
x3006c0s19b1n0: [2024-03-28 13:19:39,913] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 21.6 GB, percent = 4.3%
x3006c0s19b1n0: [2024-03-28 13:19:46,001] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 423, num_elems = 27.64B
x3006c0s19b1n0: [2024-03-28 13:19:46,063] [INFO] [utils.py:800:see_memory_usage] After Building Model
x3006c0s19b1n0: [2024-03-28 13:19:46,063] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 7.07 GB         CA 21.45 GB         Max_CA 38 GB 
x3006c0s19b1n0: [2024-03-28 13:19:46,063] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 22.03 GB, percent = 4.4%
x3006c0s19b1n0:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 27635239424
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: Time to load cpu_adam op: 2.475811243057251 seconds
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: Time to load cpu_adam op: 2.4774279594421387 seconds
x3006c0s19b1n0: Time to load cpu_adam op: 2.4405834674835205 seconds
x3006c0s1b0n0: ninja: no work to do.
x3006c0s19b1n0: Time to load cpu_adam op: 2.4619646072387695 seconds
x3006c0s1b0n0: Time to load cpu_adam op: 2.4711544513702393 seconds
x3006c0s1b0n0: ninja: no work to do.
x3006c0s1b0n0: Time to load cpu_adam op: 2.4374663829803467 seconds
x3006c0s1b0n0: Time to load cpu_adam op: 2.447726011276245 seconds
x3006c0s1b0n0: ninja: no work to do.
x3006c0s1b0n0: Time to load cpu_adam op: 2.502695322036743 seconds
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: > learning rate decay style: cosine
x3006c0s19b1n0: DeepSpeed is enabled.
x3006c0s19b1n0: [2024-03-28 13:19:51,403] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.3+8074cd62, git-hash=8074cd62, git-branch=hybrid_opt_offload
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-28 13:19:51,469] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After args sanity test
x3006c0s19b1n0: [2024-03-28 13:19:51,470] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 21.45 GB         Max_CA 21 GB 
x3006c0s19b1n0: [2024-03-28 13:19:51,470] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.5 GB, percent = 5.9%
x3006c0s19b1n0: [2024-03-28 13:19:51,521] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure distributed model
x3006c0s19b1n0: [2024-03-28 13:19:51,521] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 21.45 GB         Max_CA 21 GB 
x3006c0s19b1n0: [2024-03-28 13:19:51,521] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.51 GB, percent = 5.9%
x3006c0s19b1n0: [2024-03-28 13:19:51,578] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After configure distributed model
x3006c0s19b1n0: [2024-03-28 13:19:51,578] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 21.45 GB         Max_CA 21 GB 
x3006c0s19b1n0: [2024-03-28 13:19:51,578] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.51 GB, percent = 5.9%
x3006c0s19b1n0: [2024-03-28 13:19:51,578] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
x3006c0s19b1n0: [2024-03-28 13:19:51,626] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After setting model parameters
x3006c0s19b1n0: [2024-03-28 13:19:51,626] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 21.45 GB         Max_CA 21 GB 
x3006c0s19b1n0: [2024-03-28 13:19:51,627] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.51 GB, percent = 5.9%
x3006c0s19b1n0: [2024-03-28 13:19:51,675] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure optimizer
x3006c0s19b1n0: [2024-03-28 13:19:51,675] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 21.45 GB         Max_CA 21 GB 
x3006c0s19b1n0: [2024-03-28 13:19:51,675] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.51 GB, percent = 5.9%
x3006c0s19b1n0: [2024-03-28 13:19:51,676] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
x3006c0s19b1n0: [2024-03-28 13:19:51,676] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
x3006c0s19b1n0: [2024-03-28 13:19:51,696] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
x3006c0s19b1n0: [2024-03-28 13:19:51,696] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
x3006c0s19b1n0: [2024-03-28 13:19:51,696] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
x3006c0s19b1n0: [2024-03-28 13:19:51,696] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
x3006c0s19b1n0: [2024-03-28 13:19:51,744] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
x3006c0s19b1n0: [2024-03-28 13:19:51,744] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 21.45 GB         Max_CA 21 GB 
x3006c0s19b1n0: [2024-03-28 13:19:51,744] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.51 GB, percent = 5.9%
x3006c0s19b1n0: [2024-03-28 13:19:51,746] [INFO] [stage3.py:137:__init__] Reduce bucket size 500,000,000
x3006c0s19b1n0: [2024-03-28 13:19:51,746] [INFO] [stage3.py:138:__init__] Prefetch bucket size 50,000,000
x3006c0s19b1n0: [2024-03-28 13:19:51,795] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
x3006c0s19b1n0: [2024-03-28 13:19:51,795] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 21.45 GB         Max_CA 21 GB 
x3006c0s19b1n0: [2024-03-28 13:19:51,795] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.51 GB, percent = 5.9%
x3006c0s19b1n0: Parameter Offload: Total persistent parameters: 805376 in 121 params
x3006c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-28 13:19:51,868] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
x3006c0s19b1n0: [2024-03-28 13:19:51,868] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 21.45 GB         Max_CA 21 GB 
x3006c0s19b1n0: [2024-03-28 13:19:51,869] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.51 GB, percent = 5.9%
x3006c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-28 13:19:51,919] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
x3006c0s19b1n0: [2024-03-28 13:19:51,920] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 21.45 GB         Max_CA 21 GB 
x3006c0s19b1n0: [2024-03-28 13:19:51,920] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.51 GB, percent = 5.9%
x3006c0s19b1n0: [2024-03-28 13:19:51,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-28 13:19:51,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-28 13:19:51,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-28 13:19:51,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b0n0: [2024-03-28 13:19:51,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b0n0: [2024-03-28 13:19:51,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b0n0: [2024-03-28 13:19:51,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b0n0: [2024-03-28 13:19:51,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-28 13:19:51,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-28 13:19:51,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-28 13:19:51,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-28 13:19:51,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b0n0: [2024-03-28 13:19:51,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b0n0: [2024-03-28 13:19:51,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b0n0: [2024-03-28 13:19:51,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b0n0: [2024-03-28 13:19:51,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-28 13:19:51,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,934] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,935] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,936] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,937] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,938] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,940] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:19:51,939] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:19:51,940] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b0n0: [2024-03-28 13:19:51,940] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b0n0: [2024-03-28 13:19:51,940] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b0n0: [2024-03-28 13:19:51,940] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-28 13:19:51,940] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-28 13:19:51,940] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-28 13:19:51,940] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-28 13:19:51,940] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-28 13:19:55,364] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 31
x3006c0s19b1n0: [2024-03-28 13:19:55,364] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.45 GB         CA 6.44 GB         Max_CA 21 GB 
x3006c0s19b1n0: [2024-03-28 13:19:55,364] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 35.89 GB, percent = 7.1%
x3006c0s19b1n0: [2024-03-28 13:19:55,478] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
x3006c0s19b1n0: [2024-03-28 13:19:55,479] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-28 13:19:55,479] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 36.73 GB, percent = 7.3%
x3006c0s19b1n0: [2024-03-28 13:20:14,652] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
x3006c0s19b1n0: [2024-03-28 13:20:14,652] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-28 13:20:14,653] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 219.58 GB, percent = 43.6%
x3006c0s19b1n0: [2024-03-28 13:20:14,777] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
x3006c0s19b1n0: [2024-03-28 13:20:14,778] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-28 13:20:14,778] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 222.67 GB, percent = 44.2%
x3006c0s19b1n0: [2024-03-28 13:20:20,979] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | init_optimizer_state: 6187.19
x3006c0s19b1n0: [2024-03-28 13:20:21,047] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
x3006c0s19b1n0: [2024-03-28 13:20:21,048] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-28 13:20:21,048] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 282.93 GB, percent = 56.2%
x3006c0s19b1n0: [2024-03-28 13:20:21,623] [INFO] [stage3.py:520:_setup_for_real_optimizer] optimizer state initialized
x3006c0s19b1n0: [2024-03-28 13:20:30,099] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
x3006c0s19b1n0: [2024-03-28 13:20:30,100] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 8.61 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:20:30,100] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 315.61 GB, percent = 62.7%
x3006c0s19b1n0: [2024-03-28 13:20:30,100] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
x3006c0s19b1n0: [2024-03-28 13:20:30,167] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure LR scheduler
x3006c0s19b1n0: [2024-03-28 13:20:30,167] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:20:30,167] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 315.61 GB, percent = 62.7%
x3006c0s19b1n0: [2024-03-28 13:20:30,167] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
x3006c0s19b1n0: [2024-03-28 13:20:30,167] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7f8d3c3941f0>
x3006c0s19b1n0: [2024-03-28 13:20:30,167] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:20:30,231] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before rewriting optimizer step
x3006c0s19b1n0: [2024-03-28 13:20:30,231] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:20:30,231] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 315.61 GB, percent = 62.7%
x3006c0s19b1n0: [2024-03-28 13:20:30,296] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure checkpointing
x3006c0s19b1n0: [2024-03-28 13:20:30,296] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:20:30,296] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 315.61 GB, percent = 62.7%
x3006c0s19b1n0: [2024-03-28 13:20:30,296] [INFO] [config.py:998:print] DeepSpeedEngine configuration:
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   activation_checkpointing_config  {
x3006c0s19b1n0:     "partition_activations": false, 
x3006c0s19b1n0:     "contiguous_memory_optimization": false, 
x3006c0s19b1n0:     "cpu_checkpointing": false, 
x3006c0s19b1n0:     "number_checkpoints": null, 
x3006c0s19b1n0:     "synchronize_checkpoint_boundary": false, 
x3006c0s19b1n0:     "profile": false
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   amp_enabled .................. False
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   amp_params ................... False
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   autotuning_config ............ {
x3006c0s19b1n0:     "enabled": false, 
x3006c0s19b1n0:     "start_step": null, 
x3006c0s19b1n0:     "end_step": null, 
x3006c0s19b1n0:     "metric_path": null, 
x3006c0s19b1n0:     "arg_mappings": null, 
x3006c0s19b1n0:     "metric": "throughput", 
x3006c0s19b1n0:     "model_info": null, 
x3006c0s19b1n0:     "results_dir": "autotuning_results", 
x3006c0s19b1n0:     "exps_dir": "autotuning_exps", 
x3006c0s19b1n0:     "overwrite": true, 
x3006c0s19b1n0:     "fast": true, 
x3006c0s19b1n0:     "start_profile_step": 3, 
x3006c0s19b1n0:     "end_profile_step": 5, 
x3006c0s19b1n0:     "tuner_type": "gridsearch", 
x3006c0s19b1n0:     "tuner_early_stopping": 5, 
x3006c0s19b1n0:     "tuner_num_trials": 50, 
x3006c0s19b1n0:     "model_info_path": null, 
x3006c0s19b1n0:     "mp_size": 1, 
x3006c0s19b1n0:     "max_train_batch_size": null, 
x3006c0s19b1n0:     "min_train_batch_size": 1, 
x3006c0s19b1n0:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
x3006c0s19b1n0:     "min_train_micro_batch_size_per_gpu": 1, 
x3006c0s19b1n0:     "num_tuning_micro_batch_sizes": 3
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   bfloat16_enabled ............. True
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   bfloat16_immediate_grad_update  False
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   checkpoint_parallel_write_pipeline  False
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   checkpoint_tag_validation_enabled  True
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   checkpoint_tag_validation_fail  False
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8d3c394b50>
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   communication_data_type ...... None
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   curriculum_enabled_legacy .... False
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   curriculum_params_legacy ..... False
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   data_efficiency_enabled ...... False
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   dataloader_drop_last ......... False
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   datastates_config ............ {
x3006c0s19b1n0:     "enabled": null, 
x3006c0s19b1n0:     "config": {
x3006c0s19b1n0:     }
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   disable_allgather ............ False
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   dump_state ................... False
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   dynamic_loss_scale_args ...... None
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   eigenvalue_enabled ........... False
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   eigenvalue_gas_boundary_resolution  1
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   eigenvalue_layer_name ........ bert.encoder.layer
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   eigenvalue_layer_num ......... 0
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   eigenvalue_max_iter .......... 100
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   eigenvalue_stability ......... 1e-06
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   eigenvalue_tol ............... 0.01
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   eigenvalue_verbose ........... False
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   elasticity_enabled ........... False
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   flops_profiler_config ........ {
x3006c0s19b1n0:     "enabled": false, 
x3006c0s19b1n0:     "recompute_fwd_factor": 0.0, 
x3006c0s19b1n0:     "profile_step": 1, 
x3006c0s19b1n0:     "module_depth": -1, 
x3006c0s19b1n0:     "top_modules": 1, 
x3006c0s19b1n0:     "detailed": true, 
x3006c0s19b1n0:     "output_file": null
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   fp16_auto_cast ............... None
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   fp16_enabled ................. False
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   fp16_master_weights_and_gradients  False
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   global_rank .................. 0
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   grad_accum_dtype ............. bf16
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   gradient_accumulation_steps .. 1
x3006c0s19b1n0: [2024-03-28 13:20:30,297] [INFO] [config.py:1002:print]   gradient_clipping ............ 0.0
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   gradient_predivide_factor .... 1.0
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   graph_harvesting ............. False
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   initial_dynamic_scale ........ 1
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   load_universal_checkpoint .... False
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   loss_scale ................... 1.0
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   memory_breakdown ............. True
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   mics_hierarchial_params_gather  False
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   mics_shard_size .............. -1
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   nebula_config ................ {
x3006c0s19b1n0:     "enabled": false, 
x3006c0s19b1n0:     "persistent_storage_path": null, 
x3006c0s19b1n0:     "persistent_time_interval": 100, 
x3006c0s19b1n0:     "num_of_version_in_retention": 2, 
x3006c0s19b1n0:     "enable_nebula_load": true, 
x3006c0s19b1n0:     "load_path": null
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   optimizer_legacy_fusion ...... False
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   optimizer_name ............... None
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   optimizer_params ............. None
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   pld_enabled .................. False
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   pld_params ................... False
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   prescale_gradients ........... False
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   scheduler_name ............... None
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   scheduler_params ............. None
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   seq_parallel_communication_data_type  torch.float32
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   sparse_attention ............. None
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   sparse_gradients_enabled ..... False
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   steps_per_print .............. 1
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   train_batch_size ............. 32
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   train_micro_batch_size_per_gpu  4
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   use_data_before_expert_parallel_  False
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   use_node_local_storage ....... False
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   wall_clock_breakdown ......... True
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   weight_quantization_config ... None
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   world_size ................... 8
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   zero_allow_untested_optimizer  False
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0, prefetch_optimizer=True, part_grads_async=True, prefetch_optimizer_gap=5) sub_group_size=100000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   zero_enabled ................. True
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   zero_force_ds_cpu_optimizer .. True
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:1002:print]   zero_optimization_stage ...... 3
x3006c0s19b1n0: [2024-03-28 13:20:30,298] [INFO] [config.py:988:print_user_config]   json = {
x3006c0s19b1n0:     "train_batch_size": 32, 
x3006c0s19b1n0:     "train_micro_batch_size_per_gpu": 4, 
x3006c0s19b1n0:     "steps_per_print": 1, 
x3006c0s19b1n0:     "zero_optimization": {
x3006c0s19b1n0:         "stage": 3, 
x3006c0s19b1n0:         "offload_optimizer": {
x3006c0s19b1n0:             "device": "cpu", 
x3006c0s19b1n0:             "ratio": 1, 
x3006c0s19b1n0:             "pin_memory": true, 
x3006c0s19b1n0:             "prefetch_optimizer": 1, 
x3006c0s19b1n0:             "part_grads_async": 1, 
x3006c0s19b1n0:             "prefetch_optimizer_gap": 5
x3006c0s19b1n0:         }, 
x3006c0s19b1n0:         "sub_group_size": 1.000000e+08
x3006c0s19b1n0:     }, 
x3006c0s19b1n0:     "bf16": {
x3006c0s19b1n0:         "enabled": true
x3006c0s19b1n0:     }, 
x3006c0s19b1n0:     "data_types": {
x3006c0s19b1n0:         "grad_accum_dtype": "bf16"
x3006c0s19b1n0:     }, 
x3006c0s19b1n0:     "wall_clock_breakdown": true, 
x3006c0s19b1n0:     "memory_breakdown": true, 
x3006c0s19b1n0:     "flops_profiler": {
x3006c0s19b1n0:         "enabled": false
x3006c0s19b1n0:     }
x3006c0s19b1n0: }
x3006c0s1b0n0: <TIMER:model-and-optimizer-setup,51.89843153953552>
x3006c0s1b0n0: <TIMER:model-and-optimizer-setup,51.900569438934326>
x3006c0s1b0n0: <TIMER:model-and-optimizer-setup,51.90064549446106>
x3006c0s1b0n0: <TIMER:model-and-optimizer-setup,51.900686264038086>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,51.90105652809143>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,51.90112853050232>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,51.902750730514526>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,51.90284323692322>
x3006c0s19b1n0: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-28 13:20:31 
x3006c0s19b1n0: > building train, validation, and test datasets ...
x3006c0s19b1n0:  > datasets target sizes (minimum size):
x3006c0s19b1n0:     train:      320
x3006c0s19b1n0:     validation: 0
x3006c0s19b1n0:     test:       0
x3006c0s19b1n0: > building train, validation, and test datasets for GPT ...
x3006c0s19b1n0: Single data path provided for train, valid & test
x3006c0s19b1n0:  > building dataset index ...
x3006c0s19b1n0:     reading sizes...
x3006c0s19b1n0:     reading pointers...
x3006c0s19b1n0:     reading document index...
x3006c0s19b1n0:     creating numpy buffer of mmap...
x3006c0s19b1n0:     creating memory view of numpy buffer...
x3006c0s19b1n0:  > finished creating indexed dataset in 0.005914 seconds
x3006c0s19b1n0:     number of documents: 79000
x3006c0s19b1n0:  > dataset split:
x3006c0s19b1n0:     train:
x3006c0s19b1n0:      document indices in [0, 74971) total of 74971 documents
x3006c0s19b1n0:     validation:
x3006c0s19b1n0:      document indices in [74971, 78921) total of 3950 documents
x3006c0s19b1n0:     test:
x3006c0s19b1n0:      document indices in [78921, 79000) total of 79 documents
x3006c0s19b1n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_doc_idx.npy
x3006c0s19b1n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_sample_idx.npy
x3006c0s19b1n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_shuffle_idx.npy
x3006c0s19b1n0:     loaded indexed file in 0.010 seconds
x3006c0s19b1n0:     total number of samples: 108448
x3006c0s19b1n0:     total number of epochs: 1
x3006c0s19b1n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_doc_idx.npy
x3006c0s19b1n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_sample_idx.npy
x3006c0s19b1n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_shuffle_idx.npy
x3006c0s19b1n0:     loaded indexed file in 0.008 seconds
x3006c0s19b1n0:     total number of samples: 5792
x3006c0s19b1n0:     total number of epochs: 1
x3006c0s19b1n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_doc_idx.npy
x3006c0s19b1n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_sample_idx.npy
x3006c0s19b1n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_shuffle_idx.npy
x3006c0s19b1n0:     loaded indexed file in 0.003 seconds
x3006c0s19b1n0:     total number of samples: 185
x3006c0s19b1n0:     total number of epochs: 1
x3006c0s19b1n0: > finished creating GPT datasets ...
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5496721267700195>
x3006c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5645124912261963>
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5674715042114258>
x3006c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5760195255279541>
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5779769420623779>
x3006c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5967397689819336>
x3006c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.6019856929779053>
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.6023523807525635>
x3006c0s19b1n0: [after dataloaders are built] datetime: 2024-03-28 13:20:32 
x3006c0s19b1n0: done with setup ...
x3006c0s1b0n0: (min, max) time across ranks (ms):
x3006c0s1b0n0:     model-and-optimizer-setup ......................: (51898.43, 51902.84)
x3006c0s1b0n0:     train/valid/test-data-iterators-setup ..........: (549.67, 602.35)
x3006c0s19b1n0: training ...
x3006c0s19b1n0: [before training begins] datetime: 2024-03-28 13:20:32 
x3006c0s19b1n0: [before the start of training step] datetime: 2024-03-28 13:20:32 
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:20:32,607] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:20:32,607] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 7.45 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:20:32,607] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 317.93 GB, percent = 63.2%
x3006c0s19b1n0: [2024-03-28 13:20:32,757] [INFO] [checkpointing.py:539:forward] Activation Checkpointing Information
x3006c0s19b1n0: [2024-03-28 13:20:32,757] [INFO] [checkpointing.py:540:forward] ----Partition Activations False, CPU CHECKPOINTING False
x3006c0s19b1n0: [2024-03-28 13:20:32,757] [INFO] [checkpointing.py:541:forward] ----contiguous Memory Checkpointing False with 60 total layers
x3006c0s19b1n0: [2024-03-28 13:20:32,757] [INFO] [checkpointing.py:543:forward] ----Synchronization False
x3006c0s19b1n0: [2024-03-28 13:20:32,757] [INFO] [checkpointing.py:544:forward] ----Profiling time in checkpointing False
x3006c0s19b1n0: [2024-03-28 13:20:42,002] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:20:42,002] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 19.76 GB         CA 22.95 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:20:42,003] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 318.12 GB, percent = 63.2%
x3006c0s19b1n0: [2024-03-28 13:20:42,192] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:20:42,192] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 16.84 GB         CA 16.91 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:20:42,193] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 318.12 GB, percent = 63.2%
x3006c0s19b1n0: [2024-03-28 13:21:10,823] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:21:10,823] [INFO] [utils.py:801:see_memory_usage] MA 11.51 GB         Max_MA 20.43 GB         CA 11.75 GB         Max_CA 28 GB 
x3006c0s19b1n0: [2024-03-28 13:21:10,824] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 318.17 GB, percent = 63.2%
x3006c0s19b1n0: [2024-03-28 13:21:10,891] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:21:10,892] [INFO] [utils.py:801:see_memory_usage] MA 11.51 GB         Max_MA 11.51 GB         CA 11.75 GB         Max_CA 12 GB 
x3006c0s19b1n0: [2024-03-28 13:21:10,892] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 318.18 GB, percent = 63.2%
x3006c0s19b1n0: [2024-03-28 13:21:17,430] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.35814094543457
x3006c0s19b1n0: [2024-03-28 13:21:17,447] [INFO] [stage3.py:2251:step] Full outer step loop took 6.376006126403809
x3006c0s19b1n0: [2024-03-28 13:21:17,463] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.391565322875977
x3006c0s19b1n0: [2024-03-28 13:21:17,472] [INFO] [stage3.py:2251:step] Full outer step loop took 6.40062141418457
x3006c0s1b0n0: [2024-03-28 13:21:17,519] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.444593906402588
x3006c0s1b0n0: [2024-03-28 13:21:17,534] [INFO] [stage3.py:2251:step] Full outer step loop took 6.459704399108887
x3006c0s19b1n0: [2024-03-28 13:21:17,570] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.497953653335571
x3006c0s19b1n0: [2024-03-28 13:21:17,590] [INFO] [stage3.py:2251:step] Full outer step loop took 6.518052816390991
x3006c0s19b1n0: [2024-03-28 13:21:17,590] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.518601417541504
x3006c0s19b1n0: [2024-03-28 13:21:17,599] [INFO] [stage3.py:2251:step] Full outer step loop took 6.527664661407471
x3006c0s1b0n0: [2024-03-28 13:21:17,607] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.532232761383057
x3006c0s1b0n0: [2024-03-28 13:21:17,610] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.535207509994507
x3006c0s1b0n0: [2024-03-28 13:21:17,625] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.55121374130249
x3006c0s1b0n0: [2024-03-28 13:21:17,626] [INFO] [stage3.py:2251:step] Full outer step loop took 6.551722526550293
x3006c0s1b0n0: [2024-03-28 13:21:17,627] [INFO] [stage3.py:2251:step] Full outer step loop took 6.553246021270752
x3006c0s1b0n0: [2024-03-28 13:21:17,634] [INFO] [stage3.py:2251:step] Full outer step loop took 6.5602593421936035
x3006c0s19b1n0: [2024-03-28 13:21:17,644] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6518.32
x3006c0s19b1n0: [2024-03-28 13:21:17,644] [INFO] [stage3.py:2277:step] End to end step took 6.573016881942749
x3006c0s19b1n0: [2024-03-28 13:21:17,644] [WARNING] [stage3.py:2267:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3006c0s19b1n0: [2024-03-28 13:21:17,645] [INFO] [stage3.py:2277:step] End to end step took 6.573086500167847
x3006c0s19b1n0: [2024-03-28 13:21:17,645] [INFO] [logging.py:96:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
x3006c0s1b0n0: [2024-03-28 13:21:17,645] [INFO] [stage3.py:2277:step] End to end step took 6.570863246917725
x3006c0s1b0n0: [2024-03-28 13:21:17,645] [INFO] [stage3.py:2277:step] End to end step took 6.570881366729736
x3006c0s1b0n0: [2024-03-28 13:21:17,645] [INFO] [stage3.py:2277:step] End to end step took 6.5708746910095215
x3006c0s19b1n0: [2024-03-28 13:21:17,645] [INFO] [stage3.py:2277:step] End to end step took 6.573517799377441
x3006c0s19b1n0: [2024-03-28 13:21:17,645] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 9491.70 | bwd_microstep: 28485.01 | bwd_inner_microstep: 28385.81 | bwd_allreduce_microstep: 99.10 | step_microstep: 6753.19
x3006c0s1b0n0: [2024-03-28 13:21:17,645] [INFO] [stage3.py:2277:step] End to end step took 6.57091498374939
x3006c0s19b1n0: [2024-03-28 13:21:17,645] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 9491.70 | bwd: 28485.00 | bwd_inner: 28385.81 | bwd_allreduce: 99.10 | step: 6753.19
x3006c0s19b1n0: [2024-03-28 13:21:17,645] [INFO] [stage3.py:2277:step] End to end step took 6.573882102966309
x3006c0s19b1n0: [2024-03-28 13:21:17,737] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:21:17,738] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.56 GB         CA 10.34 GB         Max_CA 14 GB 
x3006c0s19b1n0: [2024-03-28 13:21:17,738] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.28 GB, percent = 79.7%
x3006c0s19b1n0: <TIMER:interval-time,45.352829694747925><TIMER:interval-time,45.35283303260803><TIMER:interval-time,45.35284757614136>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,45.35283374786377>
x3006c0s1b0n0: <TIMER:interval-time,45.352818727493286><TIMER:interval-time,45.35281443595886><TIMER:interval-time,45.35281777381897>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,45.35283589363098>
x3006c0s1b0n0:  elapsed_time 45.352814 | consumed samples:           32 | consumed tokens:        65536 | elapsed time per iteration (ms): 45352.8 | learning rate: 3.000E-04 | global batch size:    32 | lm loss: 1.215015E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.706 | TFLOPs: 48.82 |
x3006c0s19b1n0: [Rank 0] (after 1 iterations) memory (MB) | allocated: 10510.36328125 | max allocated: 10510.36376953125 | reserved: 10586.0 | max reserved: 10586.0
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:21:17,877] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:21:17,877] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:21:17,877] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.38 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:21:24,871] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:21:24,871] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:21:24,871] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.39 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:21:24,947] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:21:24,947] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:21:24,947] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.39 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:21:44,144] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:21:44,145] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 23.87 GB         CA 13.01 GB         Max_CA 32 GB 
x3006c0s19b1n0: [2024-03-28 13:21:44,145] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.4 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:21:44,212] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:21:44,213] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 12.77 GB         CA 13.01 GB         Max_CA 13 GB 
x3006c0s19b1n0: [2024-03-28 13:21:44,213] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.4 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:21:48,818] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.581685781478882
x3006c0s19b1n0: [2024-03-28 13:21:48,845] [INFO] [stage3.py:2251:step] Full outer step loop took 4.608210325241089
x3006c0s19b1n0: [2024-03-28 13:21:48,864] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.627103567123413
x3006c0s19b1n0: [2024-03-28 13:21:48,895] [INFO] [stage3.py:2251:step] Full outer step loop took 4.658628702163696
x3006c0s19b1n0: [2024-03-28 13:21:48,911] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.675067663192749
x3006c0s19b1n0: [2024-03-28 13:21:48,912] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.675245761871338
x3006c0s19b1n0: [2024-03-28 13:21:48,920] [INFO] [stage3.py:2251:step] Full outer step loop took 4.684097766876221
x3006c0s19b1n0: [2024-03-28 13:21:48,921] [INFO] [stage3.py:2251:step] Full outer step loop took 4.684270620346069
x3006c0s1b0n0: [2024-03-28 13:21:48,921] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.68390679359436
x3006c0s1b0n0: [2024-03-28 13:21:48,954] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.717460632324219
x3006c0s1b0n0: [2024-03-28 13:21:48,954] [INFO] [stage3.py:2251:step] Full outer step loop took 4.717832088470459
x3006c0s1b0n0: [2024-03-28 13:21:48,956] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.719355583190918
x3006c0s1b0n0: [2024-03-28 13:21:48,962] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.726168155670166
x3006c0s1b0n0: [2024-03-28 13:21:48,965] [INFO] [stage3.py:2251:step] Full outer step loop took 4.728292226791382
x3006c0s1b0n0: [2024-03-28 13:21:48,965] [INFO] [stage3.py:2251:step] Full outer step loop took 4.72918701171875
x3006c0s1b0n0: [2024-03-28 13:21:48,972] [INFO] [stage3.py:2251:step] Full outer step loop took 4.735301733016968
x3006c0s19b1n0: [2024-03-28 13:21:48,982] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4684.47
x3006c0s19b1n0: [2024-03-28 13:21:48,982] [INFO] [stage3.py:2277:step] End to end step took 4.745729684829712
x3006c0s1b0n0: [2024-03-28 13:21:48,982] [INFO] [stage3.py:2277:step] End to end step took 4.7457194328308105
x3006c0s1b0n0: [2024-03-28 13:21:48,982] [INFO] [stage3.py:2277:step] End to end step took 4.74570631980896
x3006c0s1b0n0: [2024-03-28 13:21:48,982] [INFO] [stage3.py:2277:step] End to end step took 4.745774984359741
x3006c0s19b1n0: [2024-03-28 13:21:48,982] [INFO] [stage3.py:2277:step] End to end step took 4.745595932006836
x3006c0s19b1n0: [2024-03-28 13:21:48,982] [INFO] [stage3.py:2277:step] End to end step took 4.745846509933472
x3006c0s1b0n0: [2024-03-28 13:21:48,982] [INFO] [stage3.py:2277:step] End to end step took 4.745901823043823
x3006c0s19b1n0: [2024-03-28 13:21:48,982] [INFO] [logging.py:96:log_dist] [Rank 0] step=2, skipped=0, lr=[0.0002918585038060976], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:21:48,982] [INFO] [stage3.py:2277:step] End to end step took 4.746244430541992
x3006c0s19b1n0: [2024-03-28 13:21:48,983] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6959.90 | bwd_microstep: 19038.65 | bwd_inner_microstep: 18946.14 | bwd_allreduce_microstep: 92.44 | step_microstep: 4769.68
x3006c0s19b1n0: [2024-03-28 13:21:48,983] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6959.88 | bwd: 19038.65 | bwd_inner: 18946.14 | bwd_allreduce: 92.45 | step: 4769.68
x3006c0s19b1n0: [2024-03-28 13:21:49,078] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:21:49,078] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.77 GB         CA 10.34 GB         Max_CA 14 GB 
x3006c0s19b1n0: [2024-03-28 13:21:49,078] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.39 GB, percent = 79.8%
x3006c0s19b1n0: <TIMER:interval-time,31.340346813201904><TIMER:interval-time,31.340339422225952><TIMER:interval-time,31.340357780456543>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,31.340362548828125>
x3006c0s1b0n0: <TIMER:interval-time,31.34038805961609><TIMER:interval-time,31.340389490127563><TIMER:interval-time,31.34039282798767>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,31.340514659881592>
x3006c0s1b0n0:  elapsed_time 31.340393 | consumed samples:           64 | consumed tokens:       131072 | elapsed time per iteration (ms): 31340.4 | learning rate: 2.919E-04 | global batch size:    32 | lm loss: 1.082593E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.021 | TFLOPs: 70.65 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:21:49,213] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:21:49,213] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:21:49,213] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.42 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:21:55,952] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:21:55,953] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:21:55,953] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.41 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:21:56,052] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:21:56,053] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:21:56,053] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.41 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:22:14,567] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:22:14,568] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 23.87 GB         CA 13.01 GB         Max_CA 32 GB 
x3006c0s19b1n0: [2024-03-28 13:22:14,568] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.38 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:22:14,645] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:22:14,646] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 12.77 GB         CA 13.01 GB         Max_CA 13 GB 
x3006c0s19b1n0: [2024-03-28 13:22:14,646] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.38 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:22:19,249] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.58007025718689
x3006c0s19b1n0: [2024-03-28 13:22:19,283] [INFO] [stage3.py:2251:step] Full outer step loop took 4.613666296005249
x3006c0s19b1n0: [2024-03-28 13:22:19,296] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.626499891281128
x3006c0s19b1n0: [2024-03-28 13:22:19,307] [INFO] [stage3.py:2251:step] Full outer step loop took 4.638368368148804
x3006c0s1b0n0: [2024-03-28 13:22:19,355] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.686489105224609
x3006c0s1b0n0: [2024-03-28 13:22:19,360] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.691071510314941
x3006c0s19b1n0: [2024-03-28 13:22:19,371] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.701611518859863
x3006c0s19b1n0: [2024-03-28 13:22:19,372] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.703271150588989
x3006c0s19b1n0: [2024-03-28 13:22:19,380] [INFO] [stage3.py:2251:step] Full outer step loop took 4.710783004760742
x3006c0s19b1n0: [2024-03-28 13:22:19,381] [INFO] [stage3.py:2251:step] Full outer step loop took 4.712362766265869
x3006c0s1b0n0: [2024-03-28 13:22:19,382] [INFO] [stage3.py:2251:step] Full outer step loop took 4.713149785995483
x3006c0s1b0n0: [2024-03-28 13:22:19,387] [INFO] [stage3.py:2251:step] Full outer step loop took 4.717585325241089
x3006c0s1b0n0: [2024-03-28 13:22:19,423] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.753853797912598
x3006c0s1b0n0: [2024-03-28 13:22:19,433] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.764258861541748
x3006c0s1b0n0: [2024-03-28 13:22:19,434] [INFO] [stage3.py:2251:step] Full outer step loop took 4.764862537384033
x3006c0s1b0n0: [2024-03-28 13:22:19,442] [INFO] [stage3.py:2251:step] Full outer step loop took 4.773270606994629
x3006c0s19b1n0: [2024-03-28 13:22:19,453] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4712.55
x3006c0s1b0n0: [2024-03-28 13:22:19,454] [INFO] [stage3.py:2277:step] End to end step took 4.7846808433532715
x3006c0s19b1n0: [2024-03-28 13:22:19,454] [INFO] [stage3.py:2277:step] End to end step took 4.7847418785095215
x3006c0s19b1n0: [2024-03-28 13:22:19,454] [INFO] [stage3.py:2277:step] End to end step took 4.784639835357666
x3006c0s1b0n0: [2024-03-28 13:22:19,454] [INFO] [stage3.py:2277:step] End to end step took 4.784714460372925
x3006c0s19b1n0: [2024-03-28 13:22:19,454] [INFO] [stage3.py:2277:step] End to end step took 4.785022735595703
x3006c0s19b1n0: [2024-03-28 13:22:19,454] [INFO] [logging.py:96:log_dist] [Rank 0] step=3, skipped=0, lr=[0.00026841599982106197], mom=[(0.9, 0.95)]
x3006c0s1b0n0: [2024-03-28 13:22:19,454] [INFO] [stage3.py:2277:step] End to end step took 4.785059690475464
x3006c0s1b0n0: [2024-03-28 13:22:19,454] [INFO] [stage3.py:2277:step] End to end step took 4.785251140594482
x3006c0s19b1n0: [2024-03-28 13:22:19,454] [INFO] [stage3.py:2277:step] End to end step took 4.785290479660034
x3006c0s19b1n0: [2024-03-28 13:22:19,454] [INFO] [timer.py:260:stop] epoch=0/micro_step=3/global_step=3, RunningAvgSamplesPerSec=1.0582082246231073, CurrSamplesPerSec=1.0582082246231073, MemAllocated=10.26GB, MaxMemAllocated=12.77GB
x3006c0s19b1n0: [2024-03-28 13:22:19,454] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6694.44 | bwd_microstep: 18341.92 | bwd_inner_microstep: 18258.53 | bwd_allreduce_microstep: 83.31 | step_microstep: 4808.54
x3006c0s19b1n0: [2024-03-28 13:22:19,455] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6694.43 | bwd: 18341.92 | bwd_inner: 18258.53 | bwd_allreduce: 83.33 | step: 4808.54
x3006c0s19b1n0: [2024-03-28 13:22:19,549] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:22:19,549] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.77 GB         CA 10.34 GB         Max_CA 14 GB 
x3006c0s19b1n0: [2024-03-28 13:22:19,549] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.38 GB, percent = 79.8%
x3006c0s19b1n0: <TIMER:interval-time,30.47043466567993><TIMER:interval-time,30.47044086456299>
x3006c0s19b1n0: <TIMER:interval-time,30.470443964004517>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,30.47045397758484>
x3006c0s1b0n0: <TIMER:interval-time,30.4704167842865><TIMER:interval-time,30.47042179107666>
x3006c0s1b0n0: <TIMER:interval-time,30.470405340194702>
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,30.470550060272217>
x3006c0s1b0n0:  elapsed_time 30.470550 | consumed samples:           96 | consumed tokens:       196608 | elapsed time per iteration (ms): 30470.6 | learning rate: 2.684E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.050 | TFLOPs: 72.66 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:22:19,688] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:22:19,689] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:22:19,689] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.42 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:22:26,403] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:22:26,404] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:22:26,404] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.42 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:22:26,478] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:22:26,479] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:22:26,479] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.42 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:22:44,906] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:22:44,907] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 23.87 GB         CA 13.01 GB         Max_CA 32 GB 
x3006c0s19b1n0: [2024-03-28 13:22:44,907] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.43 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:22:44,976] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:22:44,976] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 12.77 GB         CA 13.01 GB         Max_CA 13 GB 
x3006c0s19b1n0: [2024-03-28 13:22:44,976] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.42 GB, percent = 79.8%
x3006c0s1b0n0: [2024-03-28 13:22:49,595] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.594242334365845
x3006c0s19b1n0: [2024-03-28 13:22:49,597] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.596282005310059
x3006c0s19b1n0: [2024-03-28 13:22:49,623] [INFO] [stage3.py:2251:step] Full outer step loop took 4.622831344604492
x3006c0s19b1n0: [2024-03-28 13:22:49,637] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.636580228805542
x3006c0s1b0n0: [2024-03-28 13:22:49,637] [INFO] [stage3.py:2251:step] Full outer step loop took 4.6361541748046875
x3006c0s19b1n0: [2024-03-28 13:22:49,662] [INFO] [stage3.py:2251:step] Full outer step loop took 4.661404848098755
x3006c0s19b1n0: [2024-03-28 13:22:49,663] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.66297173500061
x3006c0s19b1n0: [2024-03-28 13:22:49,670] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.669346570968628
x3006c0s19b1n0: [2024-03-28 13:22:49,673] [INFO] [stage3.py:2251:step] Full outer step loop took 4.673197031021118
x3006c0s19b1n0: [2024-03-28 13:22:49,679] [INFO] [stage3.py:2251:step] Full outer step loop took 4.678382158279419
x3006c0s1b0n0: [2024-03-28 13:22:49,678] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.67806339263916
x3006c0s1b0n0: [2024-03-28 13:22:49,689] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.688777208328247
x3006c0s1b0n0: [2024-03-28 13:22:49,697] [INFO] [stage3.py:2251:step] Full outer step loop took 4.697088241577148
x3006c0s1b0n0: [2024-03-28 13:22:49,698] [INFO] [stage3.py:2251:step] Full outer step loop took 4.697819471359253
x3006c0s1b0n0: [2024-03-28 13:22:49,699] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.698450565338135
x3006c0s1b0n0: [2024-03-28 13:22:49,708] [INFO] [stage3.py:2251:step] Full outer step loop took 4.707439661026001
x3006c0s1b0n0: [2024-03-28 13:22:49,718] [INFO] [stage3.py:2277:step] End to end step took 4.717526197433472
x3006c0s1b0n0: [2024-03-28 13:22:49,718] [INFO] [stage3.py:2277:step] End to end step took 4.717532634735107
x3006c0s1b0n0: [2024-03-28 13:22:49,718] [INFO] [stage3.py:2277:step] End to end step took 4.717609405517578
x3006c0s19b1n0: [2024-03-28 13:22:49,718] [INFO] [stage3.py:2277:step] End to end step took 4.7176291942596436
x3006c0s19b1n0: [2024-03-28 13:22:49,718] [INFO] [stage3.py:2277:step] End to end step took 4.717617988586426
x3006c0s19b1n0: [2024-03-28 13:22:49,718] [INFO] [stage3.py:2277:step] End to end step took 4.717592239379883
x3006c0s1b0n0: [2024-03-28 13:22:49,718] [INFO] [stage3.py:2277:step] End to end step took 4.7176737785339355
x3006c0s19b1n0: [2024-03-28 13:22:49,718] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4623.89
x3006c0s19b1n0: [2024-03-28 13:22:49,718] [INFO] [stage3.py:2277:step] End to end step took 4.718080282211304
x3006c0s19b1n0: [2024-03-28 13:22:49,719] [INFO] [logging.py:96:log_dist] [Rank 0] step=4, skipped=0, lr=[0.00023249999999999996], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:22:49,719] [INFO] [timer.py:260:stop] epoch=0/micro_step=4/global_step=4, RunningAvgSamplesPerSec=1.0619154794409007, CurrSamplesPerSec=1.06564880106883, MemAllocated=10.26GB, MaxMemAllocated=12.77GB
x3006c0s19b1n0: [2024-03-28 13:22:49,719] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6681.98 | bwd_microstep: 18263.34 | bwd_inner_microstep: 18179.97 | bwd_allreduce_microstep: 83.28 | step_microstep: 4742.60
x3006c0s19b1n0: [2024-03-28 13:22:49,719] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6681.96 | bwd: 18263.34 | bwd_inner: 18179.96 | bwd_allreduce: 83.31 | step: 4742.60
x3006c0s19b1n0: [2024-03-28 13:22:49,819] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:22:49,819] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.77 GB         CA 10.34 GB         Max_CA 14 GB 
x3006c0s19b1n0: [2024-03-28 13:22:49,820] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.42 GB, percent = 79.8%
x3006c0s19b1n0: <TIMER:interval-time,30.26982021331787><TIMER:interval-time,30.26982831954956>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,30.269831657409668>
x3006c0s19b1n0: <TIMER:interval-time,30.269834756851196>
x3006c0s1b0n0: <TIMER:interval-time,30.269886255264282><TIMER:interval-time,30.269886255264282><TIMER:interval-time,30.269868850708008>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,30.269999265670776>
x3006c0s1b0n0:  elapsed_time 30.269869 | consumed samples:          128 | consumed tokens:       262144 | elapsed time per iteration (ms): 30269.9 | learning rate: 2.325E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.057 | TFLOPs: 73.15 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:22:49,951] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:22:49,952] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:22:49,952] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.42 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:22:55,982] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:22:55,983] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:22:55,983] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.41 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:22:56,067] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:22:56,068] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:22:56,068] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.41 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:23:13,123] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:23:13,124] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 23.87 GB         CA 13.01 GB         Max_CA 32 GB 
x3006c0s19b1n0: [2024-03-28 13:23:13,124] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.41 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:23:13,198] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:23:13,199] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 12.77 GB         CA 13.01 GB         Max_CA 13 GB 
x3006c0s19b1n0: [2024-03-28 13:23:13,199] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.41 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:23:17,851] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.628841876983643
x3006c0s19b1n0: [2024-03-28 13:23:17,861] [INFO] [stage3.py:2251:step] Full outer step loop took 4.638225078582764
x3006c0s1b0n0: [2024-03-28 13:23:17,871] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.648289203643799
x3006c0s1b0n0: [2024-03-28 13:23:17,877] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.654070615768433
x3006c0s1b0n0: [2024-03-28 13:23:17,884] [INFO] [stage3.py:2251:step] Full outer step loop took 4.661305665969849
x3006c0s1b0n0: [2024-03-28 13:23:17,887] [INFO] [stage3.py:2251:step] Full outer step loop took 4.664527416229248
x3006c0s19b1n0: [2024-03-28 13:23:17,911] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.688443660736084
x3006c0s19b1n0: [2024-03-28 13:23:17,936] [INFO] [stage3.py:2251:step] Full outer step loop took 4.713435888290405
x3006c0s19b1n0: [2024-03-28 13:23:17,990] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.767669916152954
x3006c0s19b1n0: [2024-03-28 13:23:17,991] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.768828392028809
x3006c0s19b1n0: [2024-03-28 13:23:17,999] [INFO] [stage3.py:2251:step] Full outer step loop took 4.776793956756592
x3006c0s19b1n0: [2024-03-28 13:23:18,000] [INFO] [stage3.py:2251:step] Full outer step loop took 4.777850151062012
x3006c0s1b0n0: [2024-03-28 13:23:18,037] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.814294815063477
x3006c0s1b0n0: [2024-03-28 13:23:18,042] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.818882703781128
x3006c0s1b0n0: [2024-03-28 13:23:18,047] [INFO] [stage3.py:2251:step] Full outer step loop took 4.82409930229187
x3006c0s1b0n0: [2024-03-28 13:23:18,051] [INFO] [stage3.py:2251:step] Full outer step loop took 4.827946901321411
x3006c0s19b1n0: [2024-03-28 13:23:18,061] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4778.03
x3006c0s19b1n0: [2024-03-28 13:23:18,061] [INFO] [stage3.py:2277:step] End to end step took 4.838355779647827
x3006c0s1b0n0: [2024-03-28 13:23:18,061] [INFO] [stage3.py:2277:step] End to end step took 4.8382933139801025
x3006c0s1b0n0: [2024-03-28 13:23:18,061] [INFO] [stage3.py:2277:step] End to end step took 4.838292598724365
x3006c0s19b1n0: [2024-03-28 13:23:18,061] [INFO] [stage3.py:2277:step] End to end step took 4.838256120681763
x3006c0s19b1n0: [2024-03-28 13:23:18,061] [INFO] [logging.py:96:log_dist] [Rank 0] step=5, skipped=0, lr=[0.0001884425039850356], mom=[(0.9, 0.95)]
x3006c0s1b0n0: [2024-03-28 13:23:18,061] [INFO] [stage3.py:2277:step] End to end step took 4.8387291431427
x3006c0s19b1n0: [2024-03-28 13:23:18,061] [INFO] [stage3.py:2277:step] End to end step took 4.83884072303772
x3006c0s1b0n0: [2024-03-28 13:23:18,061] [INFO] [stage3.py:2277:step] End to end step took 4.838892221450806
x3006c0s19b1n0: [2024-03-28 13:23:18,061] [INFO] [stage3.py:2277:step] End to end step took 4.838919162750244
x3006c0s19b1n0: [2024-03-28 13:23:18,061] [INFO] [timer.py:260:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=1.0862605217318884, CurrSamplesPerSec=1.1384602760506697, MemAllocated=10.26GB, MaxMemAllocated=12.77GB
x3006c0s19b1n0: [2024-03-28 13:23:18,062] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 5995.04 | bwd_microstep: 16888.97 | bwd_inner_microstep: 16812.74 | bwd_allreduce_microstep: 76.16 | step_microstep: 4862.48
x3006c0s19b1n0: [2024-03-28 13:23:18,062] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 5995.02 | bwd: 16888.96 | bwd_inner: 16812.73 | bwd_allreduce: 76.17 | step: 4862.48
x3006c0s19b1n0: [2024-03-28 13:23:18,153] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:23:18,154] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.77 GB         CA 10.34 GB         Max_CA 14 GB 
x3006c0s19b1n0: [2024-03-28 13:23:18,154] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.41 GB, percent = 79.8%
x3006c0s19b1n0: <TIMER:interval-time,28.333951234817505>
x3006c0s19b1n0: <TIMER:interval-time,28.333946704864502><TIMER:interval-time,28.33395743370056><TIMER:interval-time,28.3339581489563>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,28.334003925323486><TIMER:interval-time,28.334003925323486><TIMER:interval-time,28.334007024765015>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,28.334137201309204>
x3006c0s1b0n0:  elapsed_time 28.334137 | consumed samples:          160 | consumed tokens:       327680 | elapsed time per iteration (ms): 28334.1 | learning rate: 1.884E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.129 | TFLOPs: 78.14 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:23:18,281] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:23:18,281] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:23:18,281] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.43 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:23:25,781] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:23:25,782] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:23:25,782] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.43 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:23:25,858] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:23:25,858] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:23:25,858] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.43 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:23:44,056] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:23:44,056] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 23.87 GB         CA 13.01 GB         Max_CA 32 GB 
x3006c0s19b1n0: [2024-03-28 13:23:44,057] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.43 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:23:44,123] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:23:44,124] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 12.77 GB         CA 13.01 GB         Max_CA 13 GB 
x3006c0s19b1n0: [2024-03-28 13:23:44,124] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.43 GB, percent = 79.8%
x3006c0s1b0n0: [2024-03-28 13:23:48,739] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.591391086578369
x3006c0s1b0n0: [2024-03-28 13:23:48,749] [INFO] [stage3.py:2251:step] Full outer step loop took 4.601992130279541
x3006c0s1b0n0: [2024-03-28 13:23:48,830] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.682998895645142
x3006c0s1b0n0: [2024-03-28 13:23:48,843] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.695636749267578
x3006c0s1b0n0: [2024-03-28 13:23:48,856] [INFO] [stage3.py:2251:step] Full outer step loop took 4.708482027053833
x3006c0s19b1n0: [2024-03-28 13:23:48,864] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.716482877731323
x3006c0s1b0n0: [2024-03-28 13:23:48,879] [INFO] [stage3.py:2251:step] Full outer step loop took 4.731550931930542
x3006c0s1b0n0: [2024-03-28 13:23:48,884] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.736571311950684
x3006c0s1b0n0: [2024-03-28 13:23:48,893] [INFO] [stage3.py:2251:step] Full outer step loop took 4.745596885681152
x3006c0s19b1n0: [2024-03-28 13:23:48,921] [INFO] [stage3.py:2251:step] Full outer step loop took 4.773770093917847
x3006c0s19b1n0: [2024-03-28 13:23:48,934] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.786485433578491
x3006c0s19b1n0: [2024-03-28 13:23:48,946] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.798686265945435
x3006c0s19b1n0: [2024-03-28 13:23:48,947] [INFO] [stage3.py:2251:step] Full outer step loop took 4.799739837646484
x3006c0s19b1n0: [2024-03-28 13:23:48,970] [INFO] [stage3.py:2251:step] Full outer step loop took 4.822228908538818
x3006c0s19b1n0: [2024-03-28 13:23:48,971] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.8231120109558105
x3006c0s19b1n0: [2024-03-28 13:23:48,980] [INFO] [stage3.py:2251:step] Full outer step loop took 4.832117080688477
x3006c0s19b1n0: [2024-03-28 13:23:48,990] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4832.31
x3006c0s1b0n0: [2024-03-28 13:23:48,990] [INFO] [stage3.py:2277:step] End to end step took 4.842564582824707
x3006c0s1b0n0: [2024-03-28 13:23:48,990] [INFO] [stage3.py:2277:step] End to end step took 4.842538833618164
x3006c0s19b1n0: [2024-03-28 13:23:48,990] [INFO] [stage3.py:2277:step] End to end step took 4.842525482177734
x3006c0s1b0n0: [2024-03-28 13:23:48,990] [INFO] [stage3.py:2277:step] End to end step took 4.842672348022461
x3006c0s19b1n0: [2024-03-28 13:23:48,990] [INFO] [stage3.py:2277:step] End to end step took 4.842552423477173
x3006c0s19b1n0: [2024-03-28 13:23:48,990] [INFO] [stage3.py:2277:step] End to end step took 4.842737436294556
x3006c0s19b1n0: [2024-03-28 13:23:48,990] [INFO] [logging.py:96:log_dist] [Rank 0] step=6, skipped=0, lr=[0.0001415574960149644], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:23:48,990] [INFO] [stage3.py:2277:step] End to end step took 4.843029022216797
x3006c0s1b0n0: [2024-03-28 13:23:48,990] [INFO] [stage3.py:2277:step] End to end step took 4.843123912811279
x3006c0s19b1n0: [2024-03-28 13:23:48,991] [INFO] [timer.py:260:stop] epoch=0/micro_step=6/global_step=6, RunningAvgSamplesPerSec=1.0748656368682399, CurrSamplesPerSec=1.0420716088296653, MemAllocated=10.26GB, MaxMemAllocated=12.77GB
x3006c0s19b1n0: [2024-03-28 13:23:48,991] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 7469.05 | bwd_microstep: 18039.74 | bwd_inner_microstep: 17938.11 | bwd_allreduce_microstep: 101.56 | step_microstep: 4866.65
x3006c0s19b1n0: [2024-03-28 13:23:48,991] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 7469.04 | bwd: 18039.73 | bwd_inner: 17938.10 | bwd_allreduce: 101.57 | step: 4866.64
x3006c0s19b1n0: [2024-03-28 13:23:49,084] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:23:49,085] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.77 GB         CA 10.34 GB         Max_CA 14 GB 
x3006c0s19b1n0: [2024-03-28 13:23:49,085] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.43 GB, percent = 79.8%
x3006c0s19b1n0: <TIMER:interval-time,30.93065071105957>
x3006c0s19b1n0: <TIMER:interval-time,30.930655241012573><TIMER:interval-time,30.9306583404541>
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,30.93066167831421><TIMER:interval-time,30.93065881729126><TIMER:interval-time,30.930647373199463><TIMER:interval-time,30.930666208267212>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s19b1n0: <TIMER:interval-time,30.930758237838745>
x3006c0s1b0n0:  elapsed_time 30.930647 | consumed samples:          192 | consumed tokens:       393216 | elapsed time per iteration (ms): 30930.6 | learning rate: 1.416E-04 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.035 | TFLOPs: 71.58 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:23:49,213] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:23:49,214] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:23:49,214] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.44 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:23:56,029] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:23:56,030] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:23:56,030] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.43 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:23:56,113] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:23:56,113] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:23:56,114] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.43 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:24:14,593] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:24:14,593] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 23.87 GB         CA 13.01 GB         Max_CA 32 GB 
x3006c0s19b1n0: [2024-03-28 13:24:14,593] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.43 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:24:14,667] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:24:14,668] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 12.77 GB         CA 13.01 GB         Max_CA 13 GB 
x3006c0s19b1n0: [2024-03-28 13:24:14,668] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.43 GB, percent = 79.8%
x3006c0s1b0n0: [2024-03-28 13:24:19,163] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.4712958335876465
x3006c0s1b0n0: [2024-03-28 13:24:19,172] [INFO] [stage3.py:2251:step] Full outer step loop took 4.480375528335571
x3006c0s1b0n0: [2024-03-28 13:24:19,270] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.578120231628418
x3006c0s1b0n0: [2024-03-28 13:24:19,280] [INFO] [stage3.py:2251:step] Full outer step loop took 4.588125467300415
x3006c0s1b0n0: [2024-03-28 13:24:19,296] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.603712797164917
x3006c0s1b0n0: [2024-03-28 13:24:19,308] [INFO] [stage3.py:2251:step] Full outer step loop took 4.616295099258423
x3006c0s1b0n0: [2024-03-28 13:24:19,309] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.617048501968384
x3006c0s1b0n0: [2024-03-28 13:24:19,319] [INFO] [stage3.py:2251:step] Full outer step loop took 4.6267852783203125
x3006c0s19b1n0: [2024-03-28 13:24:19,320] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.628264665603638
x3006c0s19b1n0: [2024-03-28 13:24:19,342] [INFO] [stage3.py:2251:step] Full outer step loop took 4.650434970855713
x3006c0s19b1n0: [2024-03-28 13:24:19,433] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.741293907165527
x3006c0s19b1n0: [2024-03-28 13:24:19,439] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.747163772583008
x3006c0s19b1n0: [2024-03-28 13:24:19,439] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.747101306915283
x3006c0s19b1n0: [2024-03-28 13:24:19,443] [INFO] [stage3.py:2251:step] Full outer step loop took 4.750905513763428
x3006c0s19b1n0: [2024-03-28 13:24:19,448] [INFO] [stage3.py:2251:step] Full outer step loop took 4.756168365478516
x3006c0s19b1n0: [2024-03-28 13:24:19,448] [INFO] [stage3.py:2251:step] Full outer step loop took 4.756108283996582
x3006c0s19b1n0: [2024-03-28 13:24:19,458] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4756.26
x3006c0s19b1n0: [2024-03-28 13:24:19,458] [INFO] [stage3.py:2277:step] End to end step took 4.766777276992798
x3006c0s19b1n0: [2024-03-28 13:24:19,459] [INFO] [stage3.py:2277:step] End to end step took 4.766626834869385
x3006c0s1b0n0: [2024-03-28 13:24:19,459] [INFO] [stage3.py:2277:step] End to end step took 4.766809940338135
x3006c0s1b0n0: [2024-03-28 13:24:19,459] [INFO] [stage3.py:2277:step] End to end step took 4.766816854476929
x3006c0s19b1n0: [2024-03-28 13:24:19,459] [INFO] [stage3.py:2277:step] End to end step took 4.766980409622192
x3006c0s19b1n0: [2024-03-28 13:24:19,459] [INFO] [logging.py:96:log_dist] [Rank 0] step=7, skipped=0, lr=[9.750000000000001e-05], mom=[(0.9, 0.95)]
x3006c0s1b0n0: [2024-03-28 13:24:19,459] [INFO] [stage3.py:2277:step] End to end step took 4.767157793045044
x3006c0s19b1n0: [2024-03-28 13:24:19,459] [INFO] [timer.py:260:stop] epoch=0/micro_step=7/global_step=7, RunningAvgSamplesPerSec=1.071466666537235, CurrSamplesPerSec=1.0580830664765932, MemAllocated=10.26GB, MaxMemAllocated=12.77GB
x3006c0s1b0n0: [2024-03-28 13:24:19,459] [INFO] [stage3.py:2277:step] End to end step took 4.767322063446045
x3006c0s19b1n0: [2024-03-28 13:24:19,459] [INFO] [stage3.py:2277:step] End to end step took 4.767388343811035
x3006c0s19b1n0: [2024-03-28 13:24:19,459] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6768.72 | bwd_microstep: 18311.18 | bwd_inner_microstep: 18227.10 | bwd_allreduce_microstep: 84.00 | step_microstep: 4791.24
x3006c0s19b1n0: [2024-03-28 13:24:19,460] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6768.70 | bwd: 18311.17 | bwd_inner: 18227.10 | bwd_allreduce: 84.02 | step: 4791.24
x3006c0s19b1n0: [2024-03-28 13:24:19,556] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:24:19,556] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.77 GB         CA 10.34 GB         Max_CA 14 GB 
x3006c0s19b1n0: [2024-03-28 13:24:19,556] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.43 GB, percent = 79.8%
x3006c0s19b1n0: <TIMER:interval-time,30.47090768814087><TIMER:interval-time,30.470954179763794>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,30.47095251083374>
x3006c0s1b0n0: <TIMER:interval-time,30.471036911010742><TIMER:interval-time,30.47103762626648>
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,30.471046924591064>
x3006c0s19b1n0: <TIMER:interval-time,30.47106623649597>
x3006c0s1b0n0: <TIMER:interval-time,30.47114396095276>
x3006c0s1b0n0:  elapsed_time 30.471144 | consumed samples:          224 | consumed tokens:       458752 | elapsed time per iteration (ms): 30471.1 | learning rate: 9.750E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.050 | TFLOPs: 72.66 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:24:19,698] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:24:19,699] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:24:19,699] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.44 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:24:26,512] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:24:26,512] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:24:26,512] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.43 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:24:26,597] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:24:26,598] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:24:26,598] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.43 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:24:45,142] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:24:45,143] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 23.87 GB         CA 13.01 GB         Max_CA 32 GB 
x3006c0s19b1n0: [2024-03-28 13:24:45,143] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.43 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:24:45,217] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:24:45,218] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 12.77 GB         CA 13.01 GB         Max_CA 13 GB 
x3006c0s19b1n0: [2024-03-28 13:24:45,218] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.43 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:24:49,811] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.568397521972656
x3006c0s1b0n0: [2024-03-28 13:24:49,820] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.5783281326293945
x3006c0s1b0n0: [2024-03-28 13:24:49,832] [INFO] [stage3.py:2251:step] Full outer step loop took 4.589792728424072
x3006c0s19b1n0: [2024-03-28 13:24:49,836] [INFO] [stage3.py:2251:step] Full outer step loop took 4.594591379165649
x3006c0s19b1n0: [2024-03-28 13:24:49,917] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.675360918045044
x3006c0s19b1n0: [2024-03-28 13:24:49,918] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.676661729812622
x3006c0s19b1n0: [2024-03-28 13:24:49,919] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.676823854446411
x3006c0s19b1n0: [2024-03-28 13:24:49,927] [INFO] [stage3.py:2251:step] Full outer step loop took 4.684993028640747
x3006c0s19b1n0: [2024-03-28 13:24:49,928] [INFO] [stage3.py:2251:step] Full outer step loop took 4.686356782913208
x3006c0s19b1n0: [2024-03-28 13:24:49,928] [INFO] [stage3.py:2251:step] Full outer step loop took 4.686457872390747
x3006c0s1b0n0: [2024-03-28 13:24:49,943] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.70147705078125
x3006c0s1b0n0: [2024-03-28 13:24:49,944] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.702594995498657
x3006c0s1b0n0: [2024-03-28 13:24:49,950] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.70835542678833
x3006c0s1b0n0: [2024-03-28 13:24:49,953] [INFO] [stage3.py:2251:step] Full outer step loop took 4.711369037628174
x3006c0s1b0n0: [2024-03-28 13:24:49,954] [INFO] [stage3.py:2251:step] Full outer step loop took 4.7120561599731445
x3006c0s1b0n0: [2024-03-28 13:24:49,959] [INFO] [stage3.py:2251:step] Full outer step loop took 4.717431306838989
x3006c0s19b1n0: [2024-03-28 13:24:49,970] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4686.67
x3006c0s1b0n0: [2024-03-28 13:24:49,970] [INFO] [stage3.py:2277:step] End to end step took 4.728323936462402
x3006c0s1b0n0: [2024-03-28 13:24:49,970] [INFO] [stage3.py:2277:step] End to end step took 4.728236675262451
x3006c0s1b0n0: [2024-03-28 13:24:49,970] [INFO] [stage3.py:2277:step] End to end step took 4.728374719619751
x3006c0s19b1n0: [2024-03-28 13:24:49,970] [INFO] [stage3.py:2277:step] End to end step took 4.7284016609191895
x3006c0s19b1n0: [2024-03-28 13:24:49,970] [INFO] [stage3.py:2277:step] End to end step took 4.728517770767212
x3006c0s19b1n0: [2024-03-28 13:24:49,970] [INFO] [stage3.py:2277:step] End to end step took 4.728555917739868
x3006c0s19b1n0: [2024-03-28 13:24:49,971] [INFO] [logging.py:96:log_dist] [Rank 0] step=8, skipped=0, lr=[6.158400017893797e-05], mom=[(0.9, 0.95)]
x3006c0s1b0n0: [2024-03-28 13:24:49,971] [INFO] [stage3.py:2277:step] End to end step took 4.72871732711792
x3006c0s19b1n0: [2024-03-28 13:24:49,971] [INFO] [stage3.py:2277:step] End to end step took 4.728986978530884
x3006c0s19b1n0: [2024-03-28 13:24:49,971] [INFO] [timer.py:260:stop] epoch=0/micro_step=8/global_step=8, RunningAvgSamplesPerSec=1.0690554072367038, CurrSamplesPerSec=1.0571600903795024, MemAllocated=10.26GB, MaxMemAllocated=12.77GB
x3006c0s19b1n0: [2024-03-28 13:24:49,971] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6769.71 | bwd_microstep: 18376.57 | bwd_inner_microstep: 18290.26 | bwd_allreduce_microstep: 86.23 | step_microstep: 4752.67
x3006c0s19b1n0: [2024-03-28 13:24:49,971] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6769.69 | bwd: 18376.56 | bwd_inner: 18290.26 | bwd_allreduce: 86.25 | step: 4752.67
x3006c0s19b1n0: [2024-03-28 13:24:50,065] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:24:50,066] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.77 GB         CA 10.34 GB         Max_CA 14 GB 
x3006c0s19b1n0: [2024-03-28 13:24:50,066] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.43 GB, percent = 79.8%
x3006c0s19b1n0: <TIMER:interval-time,30.50936007499695><TIMER:interval-time,30.509358406066895>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,30.509387493133545>
x3006c0s1b0n0: <TIMER:interval-time,30.509334087371826><TIMER:interval-time,30.509317636489868><TIMER:interval-time,30.509336471557617>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,30.50945520401001>
x3006c0s19b1n0: <TIMER:interval-time,30.50948929786682>
x3006c0s1b0n0:  elapsed_time 30.509318 | consumed samples:          256 | consumed tokens:       524288 | elapsed time per iteration (ms): 30509.3 | learning rate: 6.158E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.049 | TFLOPs: 72.57 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:24:50,191] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:24:50,192] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:24:50,192] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.44 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:24:56,866] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:24:56,867] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:24:56,867] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.43 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:24:56,946] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:24:56,947] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:24:56,947] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.43 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:25:16,117] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:25:16,117] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 23.87 GB         CA 13.01 GB         Max_CA 32 GB 
x3006c0s19b1n0: [2024-03-28 13:25:16,118] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.43 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:25:16,185] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:25:16,186] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 12.77 GB         CA 13.01 GB         Max_CA 13 GB 
x3006c0s19b1n0: [2024-03-28 13:25:16,186] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.43 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:25:20,751] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.541689872741699
x3006c0s19b1n0: [2024-03-28 13:25:20,775] [INFO] [stage3.py:2251:step] Full outer step loop took 4.565184593200684
x3006c0s19b1n0: [2024-03-28 13:25:20,796] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.58630633354187
x3006c0s1b0n0: [2024-03-28 13:25:20,806] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.596717834472656
x3006c0s19b1n0: [2024-03-28 13:25:20,830] [INFO] [stage3.py:2251:step] Full outer step loop took 4.62043833732605
x3006c0s1b0n0: [2024-03-28 13:25:20,830] [INFO] [stage3.py:2251:step] Full outer step loop took 4.620652198791504
x3006c0s19b1n0: [2024-03-28 13:25:20,847] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.636910438537598
x3006c0s19b1n0: [2024-03-28 13:25:20,862] [INFO] [stage3.py:2251:step] Full outer step loop took 4.651963233947754
x3006c0s19b1n0: [2024-03-28 13:25:20,869] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.658979892730713
x3006c0s19b1n0: [2024-03-28 13:25:20,878] [INFO] [stage3.py:2251:step] Full outer step loop took 4.6680076122283936
x3006c0s1b0n0: [2024-03-28 13:25:20,926] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.715752840042114
x3006c0s1b0n0: [2024-03-28 13:25:20,930] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.720856666564941
x3006c0s1b0n0: [2024-03-28 13:25:20,941] [INFO] [stage3.py:2251:step] Full outer step loop took 4.7312681674957275
x3006c0s1b0n0: [2024-03-28 13:25:20,947] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.737933158874512
x3006c0s1b0n0: [2024-03-28 13:25:20,949] [INFO] [stage3.py:2251:step] Full outer step loop took 4.739609479904175
x3006c0s1b0n0: [2024-03-28 13:25:20,956] [INFO] [stage3.py:2251:step] Full outer step loop took 4.746987342834473
x3006c0s1b0n0: [2024-03-28 13:25:20,967] [INFO] [stage3.py:2277:step] End to end step took 4.757271766662598
x3006c0s1b0n0: [2024-03-28 13:25:20,967] [INFO] [stage3.py:2277:step] End to end step took 4.757360219955444
x3006c0s1b0n0: [2024-03-28 13:25:20,967] [INFO] [stage3.py:2277:step] End to end step took 4.757489204406738
x3006c0s19b1n0: [2024-03-28 13:25:20,967] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4652.78
x3006c0s19b1n0: [2024-03-28 13:25:20,967] [INFO] [stage3.py:2277:step] End to end step took 4.757349491119385
x3006c0s19b1n0: [2024-03-28 13:25:20,967] [INFO] [stage3.py:2277:step] End to end step took 4.757549047470093
x3006c0s19b1n0: [2024-03-28 13:25:20,967] [INFO] [stage3.py:2277:step] End to end step took 4.757718324661255
x3006c0s19b1n0: [2024-03-28 13:25:20,967] [INFO] [logging.py:96:log_dist] [Rank 0] step=9, skipped=0, lr=[3.814149619390238e-05], mom=[(0.9, 0.95)]
x3006c0s1b0n0: [2024-03-28 13:25:20,967] [INFO] [stage3.py:2277:step] End to end step took 4.757954120635986
x3006c0s19b1n0: [2024-03-28 13:25:20,967] [INFO] [stage3.py:2277:step] End to end step took 4.75800633430481
x3006c0s19b1n0: [2024-03-28 13:25:20,968] [INFO] [timer.py:260:stop] epoch=0/micro_step=9/global_step=9, RunningAvgSamplesPerSec=1.0647795139862342, CurrSamplesPerSec=1.03982561561186, MemAllocated=10.26GB, MaxMemAllocated=12.77GB
x3006c0s19b1n0: [2024-03-28 13:25:20,968] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6638.47 | bwd_microstep: 19009.74 | bwd_inner_microstep: 18920.93 | bwd_allreduce_microstep: 88.75 | step_microstep: 4781.80
x3006c0s19b1n0: [2024-03-28 13:25:20,968] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6638.45 | bwd: 19009.74 | bwd_inner: 18920.92 | bwd_allreduce: 88.76 | step: 4781.80
x3006c0s19b1n0: [2024-03-28 13:25:21,063] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:25:21,064] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.77 GB         CA 10.34 GB         Max_CA 14 GB 
x3006c0s19b1n0: [2024-03-28 13:25:21,064] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.43 GB, percent = 79.8%
x3006c0s19b1n0: <TIMER:interval-time,30.997212648391724><TIMER:interval-time,30.99721097946167>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,30.997198820114136>
x3006c0s19b1n0: <TIMER:interval-time,30.997228860855103>
x3006c0s1b0n0: <TIMER:interval-time,30.99724817276001><TIMER:interval-time,30.9972505569458><TIMER:interval-time,30.9972505569458><TIMER:interval-time,30.997251749038696>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0:  elapsed_time 30.997251 | consumed samples:          288 | consumed tokens:       589824 | elapsed time per iteration (ms): 30997.3 | learning rate: 3.814E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.032 | TFLOPs: 71.43 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:25:21,195] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:25:21,196] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:25:21,196] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.44 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:25:27,975] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:25:27,976] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:25:27,976] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.43 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:25:28,076] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:25:28,077] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:25:28,077] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.43 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:25:47,054] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:25:47,054] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 23.87 GB         CA 13.01 GB         Max_CA 32 GB 
x3006c0s19b1n0: [2024-03-28 13:25:47,055] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.39 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:25:47,130] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:25:47,131] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 12.77 GB         CA 13.01 GB         Max_CA 13 GB 
x3006c0s19b1n0: [2024-03-28 13:25:47,131] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.39 GB, percent = 79.8%
x3006c0s1b0n0: [2024-03-28 13:25:51,688] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.5334999561309814
x3006c0s1b0n0: [2024-03-28 13:25:51,697] [INFO] [stage3.py:2251:step] Full outer step loop took 4.542855739593506
x3006c0s19b1n0: [2024-03-28 13:25:51,796] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.6413254737854
x3006c0s19b1n0: [2024-03-28 13:25:51,802] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.64742374420166
x3006c0s19b1n0: [2024-03-28 13:25:51,809] [INFO] [stage3.py:2251:step] Full outer step loop took 4.654985427856445
x3006c0s19b1n0: [2024-03-28 13:25:51,822] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.667669057846069
x3006c0s19b1n0: [2024-03-28 13:25:51,833] [INFO] [stage3.py:2251:step] Full outer step loop took 4.678615570068359
x3006c0s19b1n0: [2024-03-28 13:25:51,834] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.6799094676971436
x3006c0s19b1n0: [2024-03-28 13:25:51,835] [INFO] [stage3.py:2251:step] Full outer step loop took 4.680531024932861
x3006c0s19b1n0: [2024-03-28 13:25:51,843] [INFO] [stage3.py:2251:step] Full outer step loop took 4.6889214515686035
x3006c0s1b0n0: [2024-03-28 13:25:51,857] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.702457904815674
x3006c0s1b0n0: [2024-03-28 13:25:51,870] [INFO] [stage3.py:2251:step] Full outer step loop took 4.715455770492554
x3006c0s1b0n0: [2024-03-28 13:25:51,872] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.71758508682251
x3006c0s1b0n0: [2024-03-28 13:25:51,872] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.717737436294556
x3006c0s1b0n0: [2024-03-28 13:25:51,881] [INFO] [stage3.py:2251:step] Full outer step loop took 4.726686239242554
x3006c0s1b0n0: [2024-03-28 13:25:51,882] [INFO] [stage3.py:2251:step] Full outer step loop took 4.727283477783203
x3006c0s19b1n0: [2024-03-28 13:25:51,892] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4689.10
x3006c0s1b0n0: [2024-03-28 13:25:51,892] [INFO] [stage3.py:2277:step] End to end step took 4.7380640506744385
x3006c0s1b0n0: [2024-03-28 13:25:51,892] [INFO] [stage3.py:2277:step] End to end step took 4.738047361373901
x3006c0s19b1n0: [2024-03-28 13:25:51,893] [INFO] [stage3.py:2277:step] End to end step took 4.738262176513672
x3006c0s19b1n0: [2024-03-28 13:25:51,893] [INFO] [stage3.py:2277:step] End to end step took 4.7382121086120605
x3006c0s19b1n0: [2024-03-28 13:25:51,893] [INFO] [stage3.py:2277:step] End to end step took 4.738370180130005
x3006c0s1b0n0: [2024-03-28 13:25:51,893] [INFO] [stage3.py:2277:step] End to end step took 4.73834753036499
x3006c0s19b1n0: [2024-03-28 13:25:51,893] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[3e-05], mom=[(0.9, 0.95)]
x3006c0s1b0n0: [2024-03-28 13:25:51,893] [INFO] [stage3.py:2277:step] End to end step took 4.7388060092926025
x3006c0s19b1n0: [2024-03-28 13:25:51,893] [INFO] [stage3.py:2277:step] End to end step took 4.738827228546143
x3006c0s19b1n0: [2024-03-28 13:25:51,893] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=1.0619412377464967, CurrSamplesPerSec=1.0424892230599878, MemAllocated=10.26GB, MaxMemAllocated=12.77GB
x3006c0s19b1n0: [2024-03-28 13:25:51,893] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6738.94 | bwd_microstep: 18806.91 | bwd_inner_microstep: 18718.71 | bwd_allreduce_microstep: 88.13 | step_microstep: 4762.21
x3006c0s19b1n0: [2024-03-28 13:25:51,894] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6738.93 | bwd: 18806.91 | bwd_inner: 18718.70 | bwd_allreduce: 88.14 | step: 4762.21
x3006c0s19b1n0: [2024-03-28 13:25:51,990] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:25:51,991] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.77 GB         CA 10.34 GB         Max_CA 14 GB 
x3006c0s19b1n0: [2024-03-28 13:25:51,991] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.39 GB, percent = 79.8%
x3006c0s19b1n0: <TIMER:interval-time,30.92725682258606>
x3006c0s19b1n0: <TIMER:interval-time,30.927263498306274><TIMER:interval-time,30.92726230621338><TIMER:interval-time,30.92726230621338>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,30.92725682258606><TIMER:interval-time,30.927258491516113><TIMER:interval-time,30.927260398864746><TIMER:interval-time,30.92726182937622>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0:  elapsed_time 30.927260 | consumed samples:          320 | consumed tokens:       655360 | elapsed time per iteration (ms): 30927.3 | learning rate: 3.000E-05 | global batch size:    32 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.035 | TFLOPs: 71.59 |
x3006c0s1b0n0: <<<only_train:319.6156311035156>>>
x3006c0s1b0n0: <<<only_train:319.61565256118774>>>
x3006c0s19b1n0: <<<only_train:319.61389541625977>>>
x3006c0s19b1n0: <<<only_train:319.6139266490936>>>
x3006c0s19b1n0: <<<only_train:319.6139464378357>>>
x3006c0s1b0n0: <<<only_train:319.61570024490356>>>
x3006c0s1b0n0: <<<only_train:319.61574697494507>>>
x3006c0s19b1n0: <<<only_train:319.61395287513733>>>
x3006c0s19b1n0: [after training ends] datetime: 2024-03-28 13:25:51 
x3006c0s19b1n0: <<<full_time:319.61418318748474>>><<<full_time:319.614173412323>>>
x3006c0s19b1n0: 
x3006c0s19b1n0: <<<full_time:319.61419439315796>>>
x3006c0s19b1n0: <<<full_time:319.61418104171753>>>
x3006c0s1b0n0: <<<full_time:319.61593437194824>>>
x3006c0s1b0n0: <<<full_time:319.6159770488739>>><<<full_time:319.6159338951111>>><<<full_time:319.61593794822693>>>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: [2024-03-28 13:26:00,945] [INFO] [launch.py:348:main] Process 2651 exits successfully.
x3006c0s19b1n0: [2024-03-28 13:26:01,447] [INFO] [launch.py:348:main] Process 9027 exits successfully.
x3006c0s1b0n0: [2024-03-28 13:26:06,952] [INFO] [launch.py:348:main] Process 2650 exits successfully.
x3006c0s1b0n0: [2024-03-28 13:26:08,954] [INFO] [launch.py:348:main] Process 2649 exits successfully.
x3006c0s1b0n0: [2024-03-28 13:26:08,954] [INFO] [launch.py:348:main] Process 2648 exits successfully.
x3006c0s19b1n0: [2024-03-28 13:26:09,456] [INFO] [launch.py:348:main] Process 9026 exits successfully.
x3006c0s19b1n0: [2024-03-28 13:26:09,456] [INFO] [launch.py:348:main] Process 9024 exits successfully.
x3006c0s19b1n0: [2024-03-28 13:26:09,456] [INFO] [launch.py:348:main] Process 9025 exits successfully.

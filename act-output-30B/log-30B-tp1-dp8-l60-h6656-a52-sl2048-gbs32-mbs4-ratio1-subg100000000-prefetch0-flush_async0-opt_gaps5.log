[2024-03-28 13:26:14,617] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-28 13:26:17,726] [INFO] [runner.py:463:main] Using IP address of 10.140.57.107 for node x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-28 13:26:17,729] [INFO] [runner.py:557:main] deepspeed_env file = ./.deepspeed_env
[2024-03-28 13:26:17,729] [INFO] [multinode_runner.py:80:get_cmd] Running on the following workers: x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov,x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-28 13:26:17,729] [INFO] [runner.py:568:main] cmd = pdsh -S -f 1024 -w x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov,x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov export PYTHONUSERBASE=/home/am6429/.local/polaris/conda/2023-10-04; export PYTHONPATH=/home/am6429/dl-io/Megatron-DeepSpeed; export PATH=/home/am6429/.conda/envs/dspeed_env/bin:/soft/datascience/conda/2023-10-04/mconda3/condabin:/soft/compilers/cudatoolkit/cuda-11.8.0/bin:/soft/buildtools/cmake/cmake-3.23.2/cmake-3.23.2-linux-x86_64/bin:/opt/cray/pe/gcc/11.2.0/bin:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/include:/opt/cray/pe/pals/1.2.11/bin:/opt/cray/pe/craype/2.7.20/bin:/opt/cray/libfabric/1.15.2.0/bin:/home/am6429/.conda/envs/dspeed_env/bin:/opt/cray/pe/perftools/23.03.0/bin:/opt/cray/pe/papi/7.0.0.1/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/am6429/.local/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/am6429/veloc-build/include:/home/am6429/veloc-build/bin:/soft/datascience/conda/2023-01-10/mconda3/include:/opt/cray/pe/bin:/soft/datascience/conda/2023-01-10/mconda3/include; export LD_LIBRARY_PATH=/usr/lib64/:/soft/compilers/cudatoolkit/cuda-11.8.0/lib64:/opt/cray/pe/gcc/11.2.0/snos/lib64:/soft/compilers/cudatoolkit/cuda-11.8.0/extras/CUPTI/lib64:/soft/libraries/trt/TensorRT-8.5.3.1.Linux.x86_64-gnu.cuda-11.8.cudnn8.6/lib:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/lib:/soft/libraries/cudnn/cudnn-11-linux-x64-v8.6.0.163/lib:/opt/cray/libfabric/1.15.2.0/lib64:/usr/lib64:/opt/cray/pe/papi/7.0.0.1/lib64:/home/am6429/veloc-build/lib:/home/am6429/veloc-build/lib64:/home/am6429/nvcomp/lib:/soft/datascience/conda/2023-01-10/mconda3/lib:/soft/datascience/conda/2023-01-10/mconda3/lib/; export http_proxy=http://proxy.alcf.anl.gov:3128; export https_proxy=http://proxy.alcf.anl.gov:3128; export CC=gcc; export CXX=g++; export IBV_FORK_SAFE=1; export CFLAGS=-I/soft/datascience/conda/2023-01-10/mconda3/include/; export LDFLAGS=-L/soft/datascience/conda/2023-01-10/mconda3/lib/; export CUDA_DEVICE_MAX_CONNECTIONS=1; export TORCHSNAPSHOT_PER_RANK_MEMORY_BUDGET_BYTES=34359738368; export _DEFAULT_MAX_PER_RANK_IO_CONCURRENCY=1; export _MAX_PER_RANK_IO_CONCURRENCY=1; export NSYS_REPORT_DIR=/home/am6429/dl-io/dl-io-outputs/act-output-30B//rep-30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch0-flush_async0-opt_gaps5-%n;  cd /home/am6429/dl-io/Megatron-DeepSpeed; /home/am6429/.conda/envs/dspeed_env/bin/python -u -m deepspeed.launcher.launch --world_info=eyJ4MzAwNmMwczE5YjFuMC5oc24uY20ucG9sYXJpcy5hbGNmLmFubC5nb3YiOiBbMCwgMSwgMiwgM10sICJ4MzAwNmMwczFiMG4wLmhzbi5jbS5wb2xhcmlzLmFsY2YuYW5sLmdvdiI6IFswLCAxLCAyLCAzXX0= --node_rank=%n --master_addr=10.140.57.107 --master_port=29700 /home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py --tensor-model-parallel-size 1 --num-layers 60 --hidden-size 6656 --num-attention-heads 52 --micro-batch-size 4 --global-batch-size 32 --seq-length 2048 --max-position-embeddings 2048 --train-iters 10 --save /local/scratch/llama2/zero3-tp1}_dp8 --data-path /home/am6429/dl-io/datasets/meg-gpt2_text_document --vocab-file /home/am6429/dl-io/datasets/gpt2-vocab.json --merge-file /home/am6429/dl-io/datasets/gpt2-merges.txt --data-impl mmap --tokenizer-type GPTSentencePieceTokenizer --tokenizer-model /home/am6429/dl-io/datasets/tokenizer.model --split 949,50,1 --distributed-backend nccl --lr 3e-4 --lr-decay-style cosine --min-lr 3e-5 --weight-decay 0.1 --clip-grad 1 --lr-warmup-iters 1 --optimizer adam --adam-beta1 0.9 --adam-beta2 0.95 --log-interval 1 --save-interval 1000 --eval-interval 1000 --eval-iters 0 --bf16 --no-query-key-layer-scaling --attention-dropout 0 --hidden-dropout 0 --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization rmsnorm --disable-bias-linear --num-key-value-heads 4 --deepspeed --exit-interval 20 --deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json --zero-stage=3 --no-pipeline-parallel --cpu-optimizer --checkpoint-activations --deepspeed-activation-checkpointing
x3006c0s1b0n0: [2024-03-28 13:26:19,750] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-28 13:26:19,760] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-28 13:26:21,618] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3006c0s19b1n0: [2024-03-28 13:26:21,618] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=0
x3006c0s19b1n0: [2024-03-28 13:26:21,618] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3006c0s19b1n0: [2024-03-28 13:26:21,618] [INFO] [launch.py:163:main] dist_world_size=8
x3006c0s19b1n0: [2024-03-28 13:26:21,618] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3006c0s19b1n0: [2024-03-28 13:26:21,619] [INFO] [launch.py:253:main] process 12547 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-28 13:26:21,619] [INFO] [launch.py:253:main] process 12548 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-28 13:26:21,620] [INFO] [launch.py:253:main] process 12549 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-28 13:26:21,621] [INFO] [launch.py:253:main] process 12550 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b0n0: [2024-03-28 13:26:22,051] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3006c0s1b0n0: [2024-03-28 13:26:22,051] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=1
x3006c0s1b0n0: [2024-03-28 13:26:22,051] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3006c0s1b0n0: [2024-03-28 13:26:22,051] [INFO] [launch.py:163:main] dist_world_size=8
x3006c0s1b0n0: [2024-03-28 13:26:22,051] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3006c0s1b0n0: [2024-03-28 13:26:22,052] [INFO] [launch.py:253:main] process 5619 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b0n0: [2024-03-28 13:26:22,053] [INFO] [launch.py:253:main] process 5620 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b0n0: [2024-03-28 13:26:22,053] [INFO] [launch.py:253:main] process 5621 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b0n0: [2024-03-28 13:26:22,054] [INFO] [launch.py:253:main] process 5622 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-28 13:26:23,423] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-28 13:26:23,426] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-28 13:26:23,431] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-28 13:26:23,440] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b0n0: [2024-03-28 13:26:23,852] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b0n0: [2024-03-28 13:26:23,876] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b0n0: [2024-03-28 13:26:23,878] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b0n0: [2024-03-28 13:26:23,880] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: INFO: overriding default arguments for tokenizer_type:GPTSentencePieceTokenizer                    with tokenizer_type:GPT2BPETokenizer
x3006c0s19b1n0: using world size: 8, data-parallel-size: 8, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
x3006c0s19b1n0: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
x3006c0s19b1n0: using torch.bfloat16 for parameters ...
x3006c0s19b1n0: ------------------------ arguments ------------------------
x3006c0s19b1n0:   accumulate_allreduce_grads_in_fp32 .............. True
x3006c0s19b1n0:   adam_beta1 ...................................... 0.9
x3006c0s19b1n0:   adam_beta2 ...................................... 0.95
x3006c0s19b1n0:   adam_eps ........................................ 1e-08
x3006c0s19b1n0:   add_bias_linear ................................. False
x3006c0s19b1n0:   add_position_embedding .......................... False
x3006c0s19b1n0:   adlr_autoresume ................................. False
x3006c0s19b1n0:   adlr_autoresume_interval ........................ 1000
x3006c0s19b1n0:   aml_data_download_path .......................... None
x3006c0s19b1n0:   apply_layernorm_1p .............................. False
x3006c0s19b1n0:   apply_query_key_layer_scaling ................... False
x3006c0s19b1n0:   apply_residual_connection_post_layernorm ........ False
x3006c0s19b1n0:   async_tensor_model_parallel_allreduce ........... False
x3006c0s19b1n0:   attention_dropout ............................... 0.0
x3006c0s19b1n0:   attention_softmax_in_fp32 ....................... False
x3006c0s19b1n0:   barrier_with_L1_time ............................ True
x3006c0s19b1n0:   bert_binary_head ................................ True
x3006c0s19b1n0:   bert_embedder_type .............................. megatron
x3006c0s19b1n0:   bert_load ....................................... None
x3006c0s19b1n0:   bf16 ............................................ True
x3006c0s19b1n0:   bias_dropout_fusion ............................. True
x3006c0s19b1n0:   bias_gelu_fusion ................................ False
x3006c0s19b1n0:   biencoder_projection_dim ........................ 0
x3006c0s19b1n0:   biencoder_shared_query_context_model ............ False
x3006c0s19b1n0:   block_data_path ................................. None
x3006c0s19b1n0:   checkpoint_activations .......................... True
x3006c0s19b1n0:   checkpoint_in_cpu ............................... False
x3006c0s19b1n0:   checkpoint_num_layers ........................... 1
x3006c0s19b1n0:   classes_fraction ................................ 1.0
x3006c0s19b1n0:   clip_grad ....................................... 1.0
x3006c0s19b1n0:   compression_training ............................ False
x3006c0s19b1n0:   consumed_train_samples .......................... 0
x3006c0s19b1n0:   consumed_train_tokens ........................... 0
x3006c0s19b1n0:   consumed_valid_samples .......................... 0
x3006c0s19b1n0:   contigious_checkpointing ........................ False
x3006c0s19b1n0:   cpu_optimizer ................................... True
x3006c0s19b1n0:   cpu_torch_adam .................................. False
x3006c0s19b1n0:   create_moe_param_group .......................... False
x3006c0s19b1n0:   curriculum_learning_legacy ...................... False
x3006c0s19b1n0:   data_cache_path ................................. None
x3006c0s19b1n0:   data_efficiency_curriculum_learning ............. False
x3006c0s19b1n0:   data_impl ....................................... mmap
x3006c0s19b1n0:   data_parallel_random_init ....................... False
x3006c0s19b1n0:   data_parallel_size .............................. 8
x3006c0s19b1n0:   data_path ....................................... ['/home/am6429/dl-io/datasets/meg-gpt2_text_document']
x3006c0s19b1n0:   data_per_class_fraction ......................... 1.0
x3006c0s19b1n0:   data_sharding ................................... True
x3006c0s19b1n0:   dataloader_type ................................. single
x3006c0s19b1n0:   DDP_impl ........................................ local
x3006c0s19b1n0:   decoder_num_layers .............................. None
x3006c0s19b1n0:   decoder_seq_length .............................. None
x3006c0s19b1n0:   deepscale ....................................... False
x3006c0s19b1n0:   deepscale_config ................................ None
x3006c0s19b1n0:   deepspeed ....................................... True
x3006c0s19b1n0:   deepspeed_activation_checkpointing .............. True
x3006c0s19b1n0:   deepspeed_config ................................ /home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json
x3006c0s19b1n0:   dino_bottleneck_size ............................ 256
x3006c0s19b1n0:   dino_freeze_last_layer .......................... 1
x3006c0s19b1n0:   dino_head_hidden_size ........................... 2048
x3006c0s19b1n0:   dino_local_crops_number ......................... 10
x3006c0s19b1n0:   dino_local_img_size ............................. 96
x3006c0s19b1n0:   dino_norm_last_layer ............................ False
x3006c0s19b1n0:   dino_teacher_temp ............................... 0.07
x3006c0s19b1n0:   dino_warmup_teacher_temp ........................ 0.04
x3006c0s19b1n0:   dino_warmup_teacher_temp_epochs ................. 30
x3006c0s19b1n0:   distribute_checkpointed_activations ............. False
x3006c0s19b1n0:   distribute_saved_activations .................... False
x3006c0s19b1n0:   distributed_backend ............................. nccl
x3006c0s19b1n0:   distributed_timeout_minutes ..................... 10
x3006c0s19b1n0:   ds_inference .................................... False
x3006c0s19b1n0:   ds_pipeline_enabled ............................. False
x3006c0s19b1n0:   ds_sequence_parallel_size ....................... 1
x3006c0s19b1n0:   embedding_path .................................. None
x3006c0s19b1n0:   embedding_weights_in_fp32 ....................... False
x3006c0s19b1n0:   empty_unused_memory_level ....................... 0
x3006c0s19b1n0:   enable_expert_tensor_parallelism ................ False
x3006c0s19b1n0:   encoder_num_layers .............................. 60
x3006c0s19b1n0:   encoder_seq_length .............................. 2048
x3006c0s19b1n0:   end_weight_decay ................................ 0.1
x3006c0s19b1n0:   eod_mask_loss ................................... False
x3006c0s19b1n0:   eval_interval ................................... 1000
x3006c0s19b1n0:   eval_iters ...................................... 0
x3006c0s19b1n0:   evidence_data_path .............................. None
x3006c0s19b1n0:   exit_duration_in_mins ........................... None
x3006c0s19b1n0:   exit_interval ................................... 20
x3006c0s19b1n0:   exit_on_missing_checkpoint ...................... False
x3006c0s19b1n0:   exit_signal_handler ............................. False
x3006c0s19b1n0:   expert_interval ................................. 2
x3006c0s19b1n0:   ffn_hidden_size ................................. 17728
x3006c0s19b1n0:   finetune ........................................ False
x3006c0s19b1n0:   force_ds_sequence_parallel ...................... False
x3006c0s19b1n0:   fp16 ............................................ False
x3006c0s19b1n0:   fp16_lm_cross_entropy ........................... False
x3006c0s19b1n0:   fp32_residual_connection ........................ False
x3006c0s19b1n0:   fp8_amax_compute_algo ........................... most_recent
x3006c0s19b1n0:   fp8_amax_history_len ............................ 1
x3006c0s19b1n0:   fp8_e4m3 ........................................ False
x3006c0s19b1n0:   fp8_hybrid ...................................... False
x3006c0s19b1n0:   fp8_interval .................................... 1
x3006c0s19b1n0:   fp8_margin ...................................... 0
x3006c0s19b1n0:   fp8_wgrad ....................................... True
x3006c0s19b1n0:   global_batch_size ............................... 32
x3006c0s19b1n0:   gradient_accumulation_fusion .................... True
x3006c0s19b1n0:   head_lr_mult .................................... 1.0
x3006c0s19b1n0:   hidden_dropout .................................. 0.0
x3006c0s19b1n0:   hidden_size ..................................... 6656
x3006c0s19b1n0:   hidden_size_teacher ............................. None
x3006c0s19b1n0:   hysteresis ...................................... 2
x3006c0s19b1n0:   ict_head_size ................................... None
x3006c0s19b1n0:   ict_load ........................................ None
x3006c0s19b1n0:   img_h ........................................... 224
x3006c0s19b1n0:   img_w ........................................... 224
x3006c0s19b1n0:   indexer_batch_size .............................. 128
x3006c0s19b1n0:   indexer_log_interval ............................ 1000
x3006c0s19b1n0:   inference ....................................... False
x3006c0s19b1n0:   inference_batch_times_seqlen_threshold .......... 512
x3006c0s19b1n0:   init_method_std ................................. 0.02
x3006c0s19b1n0:   init_method_xavier_uniform ...................... False
x3006c0s19b1n0:   initial_loss_scale .............................. 4294967296
x3006c0s19b1n0:   iter_per_epoch .................................. 1250
x3006c0s19b1n0:   kd .............................................. False
x3006c0s19b1n0:   kd_alpha_ce ..................................... 1
x3006c0s19b1n0:   kd_beta_ce ...................................... 1
x3006c0s19b1n0:   kd_temp ......................................... 1.0
x3006c0s19b1n0:   kv_channels ..................................... 128
x3006c0s19b1n0:   layernorm_epsilon ............................... 1e-05
x3006c0s19b1n0:   lazy_mpu_init ................................... None
x3006c0s19b1n0:   load ............................................ None
x3006c0s19b1n0:   load_teacher .................................... None
x3006c0s19b1n0:   local_rank ...................................... 0
x3006c0s19b1n0:   log_batch_size_to_tensorboard ................... False
x3006c0s19b1n0:   log_interval .................................... 1
x3006c0s19b1n0:   log_learning_rate_to_tensorboard ................ True
x3006c0s19b1n0:   log_loss_scale_to_tensorboard ................... True
x3006c0s19b1n0:   log_memory_to_tensorboard ....................... False
x3006c0s19b1n0:   log_num_zeros_in_grad ........................... False
x3006c0s19b1n0:   log_optimizer_states_to_tensorboard ............. False
x3006c0s19b1n0:   log_params_norm ................................. False
x3006c0s19b1n0:   log_timers_to_tensorboard ....................... False
x3006c0s19b1n0:   log_validation_ppl_to_tensorboard ............... False
x3006c0s19b1n0:   log_world_size_to_tensorboard ................... False
x3006c0s19b1n0:   loss_scale ...................................... None
x3006c0s19b1n0:   loss_scale_window ............................... 1000
x3006c0s19b1n0:   lr .............................................. 0.0003
x3006c0s19b1n0:   lr_decay_iters .................................. None
x3006c0s19b1n0:   lr_decay_samples ................................ None
x3006c0s19b1n0:   lr_decay_style .................................. cosine
x3006c0s19b1n0:   lr_decay_tokens ................................. None
x3006c0s19b1n0:   lr_warmup_fraction .............................. None
x3006c0s19b1n0:   lr_warmup_iters ................................. 1
x3006c0s19b1n0:   lr_warmup_samples ............................... 0
x3006c0s19b1n0:   lr_warmup_tokens ................................ None
x3006c0s19b1n0:   make_vocab_size_divisible_by .................... 128
x3006c0s19b1n0:   mask_factor ..................................... 1.0
x3006c0s19b1n0:   mask_prob ....................................... 0.15
x3006c0s19b1n0:   mask_type ....................................... random
x3006c0s19b1n0:   masked_softmax_fusion ........................... True
x3006c0s19b1n0:   max_position_embeddings ......................... 2048
x3006c0s19b1n0:   max_tokens_to_oom ............................... 12000
x3006c0s19b1n0:   memory_centric_tiled_linear ..................... False
x3006c0s19b1n0:   merge_file ...................................... /home/am6429/dl-io/datasets/gpt2-merges.txt
x3006c0s19b1n0:   micro_batch_size ................................ 4
x3006c0s19b1n0:   min_loss_scale .................................. 1.0
x3006c0s19b1n0:   min_lr .......................................... 3e-05
x3006c0s19b1n0:   mlp_type ........................................ standard
x3006c0s19b1n0:   mmap_warmup ..................................... False
x3006c0s19b1n0:   moe_eval_capacity_factor ........................ 1.0
x3006c0s19b1n0:   moe_expert_parallel_size ........................ 1
x3006c0s19b1n0:   moe_loss_coeff .................................. 0.1
x3006c0s19b1n0:   moe_min_capacity ................................ 4
x3006c0s19b1n0:   moe_token_dropping .............................. True
x3006c0s19b1n0:   moe_train_capacity_factor ....................... 1.0
x3006c0s19b1n0:   mos ............................................. False
x3006c0s19b1n0:   no_load_lr_state ................................ False
x3006c0s19b1n0:   no_load_optim ................................... None
x3006c0s19b1n0:   no_load_rng ..................................... None
x3006c0s19b1n0:   no_persist_layer_norm ........................... False
x3006c0s19b1n0:   no_pipeline_parallel ............................ True
x3006c0s19b1n0:   no_save_optim ................................... None
x3006c0s19b1n0:   no_save_rng ..................................... None
x3006c0s19b1n0:   normalization ................................... rmsnorm
x3006c0s19b1n0:   num_attention_heads ............................. 52
x3006c0s19b1n0:   num_attention_heads_teacher ..................... None
x3006c0s19b1n0:   num_channels .................................... 3
x3006c0s19b1n0:   num_classes ..................................... 1000
x3006c0s19b1n0:   num_experts ..................................... [1]
x3006c0s19b1n0:   num_experts_switch .............................. None
x3006c0s19b1n0:   num_experts_teacher ............................. [1]
x3006c0s19b1n0:   num_key_value_heads ............................. 4
x3006c0s19b1n0:   num_layers ...................................... 60
x3006c0s19b1n0:   num_layers_per_virtual_pipeline_stage ........... None
x3006c0s19b1n0:   num_layers_teacher .............................. None
x3006c0s19b1n0:   num_workers ..................................... 2
x3006c0s19b1n0:   onnx_safe ....................................... None
x3006c0s19b1n0:   openai_gelu ..................................... False
x3006c0s19b1n0:   optimizer ....................................... adam
x3006c0s19b1n0:   output_bert_embeddings .......................... False
x3006c0s19b1n0:   overlap_p2p_comm ................................ False
x3006c0s19b1n0:   override_opt_param_scheduler .................... False
x3006c0s19b1n0:   params_dtype .................................... torch.bfloat16
x3006c0s19b1n0:   partition_activations ........................... False
x3006c0s19b1n0:   patch_dim ....................................... 16
x3006c0s19b1n0:   perform_initialization .......................... True
x3006c0s19b1n0:   pipeline_model_parallel_size .................... 1
x3006c0s19b1n0:   pipeline_model_parallel_split_rank .............. None
x3006c0s19b1n0:   profile_backward ................................ False
x3006c0s19b1n0:   query_in_block_prob ............................. 0.1
x3006c0s19b1n0:   rampup_batch_size ............................... None
x3006c0s19b1n0:   random_ltd ...................................... False
x3006c0s19b1n0:   rank ............................................ 0
x3006c0s19b1n0:   recompute_granularity ........................... None
x3006c0s19b1n0:   recompute_method ................................ None
x3006c0s19b1n0:   recompute_num_layers ............................ 1
x3006c0s19b1n0:   remote_device ................................... none
x3006c0s19b1n0:   reset_attention_mask ............................ False
x3006c0s19b1n0:   reset_iteration ................................. False
x3006c0s19b1n0:   reset_position_ids .............................. False
x3006c0s19b1n0:   retriever_report_topk_accuracies ................ []
x3006c0s19b1n0:   retriever_score_scaling ......................... False
x3006c0s19b1n0:   retriever_seq_length ............................ 256
x3006c0s19b1n0:   retro_add_retriever ............................. False
x3006c0s19b1n0:   retro_cyclic_train_iters ........................ None
x3006c0s19b1n0:   retro_encoder_attention_dropout ................. 0.1
x3006c0s19b1n0:   retro_encoder_hidden_dropout .................... 0.1
x3006c0s19b1n0:   retro_encoder_layers ............................ 2
x3006c0s19b1n0:   retro_num_neighbors ............................. 2
x3006c0s19b1n0:   retro_num_retrieved_chunks ...................... 2
x3006c0s19b1n0:   retro_return_doc_ids ............................ False
x3006c0s19b1n0:   retro_workdir ................................... None
x3006c0s19b1n0:   return_data_index ............................... False
x3006c0s19b1n0:   rotary_percent .................................. 1.0
x3006c0s19b1n0:   sample_rate ..................................... 1.0
x3006c0s19b1n0:   save ............................................ /local/scratch/llama2/zero3-tp1}_dp8
x3006c0s19b1n0:   save_interval ................................... 1000
x3006c0s19b1n0:   scatter_gather_tensors_in_pipeline .............. True
x3006c0s19b1n0:   scattered_embeddings ............................ False
x3006c0s19b1n0:   seed ............................................ 1234
x3006c0s19b1n0:   seq_length ...................................... 2048
x3006c0s19b1n0:   sequence_parallel ............................... False
x3006c0s19b1n0:   sgd_momentum .................................... 0.9
x3006c0s19b1n0:   short_seq_prob .................................. 0.1
x3006c0s19b1n0:   skip_train ...................................... False
x3006c0s19b1n0:   split ........................................... 949,50,1
x3006c0s19b1n0:   split_transformers .............................. False
x3006c0s19b1n0:   squared_relu .................................... False
x3006c0s19b1n0:   standalone_embedding_stage ...................... False
x3006c0s19b1n0:   start_weight_decay .............................. 0.1
x3006c0s19b1n0:   swiglu .......................................... True
x3006c0s19b1n0:   swin_backbone_type .............................. tiny
x3006c0s19b1n0:   synchronize_each_layer .......................... False
x3006c0s19b1n0:   tensor_model_parallel_size ...................... 1
x3006c0s19b1n0:   tensorboard_dir ................................. None
x3006c0s19b1n0:   tensorboard_log_interval ........................ 1
x3006c0s19b1n0:   tensorboard_queue_size .......................... 1000
x3006c0s19b1n0:   test_data_path .................................. None
x3006c0s19b1n0:   tile_factor ..................................... 1
x3006c0s19b1n0:   timing_log_level ................................ 0
x3006c0s19b1n0:   timing_log_option ............................... minmax
x3006c0s19b1n0:   titles_data_path ................................ None
x3006c0s19b1n0:   tokenizer_model ................................. /home/am6429/dl-io/datasets/tokenizer.model
x3006c0s19b1n0:   tokenizer_type .................................. GPT2BPETokenizer
x3006c0s19b1n0:   topk ............................................ 1
x3006c0s19b1n0:   train_data_exact_num_epochs ..................... None
x3006c0s19b1n0:   train_data_path ................................. None
x3006c0s19b1n0:   train_desc_path ................................. None
x3006c0s19b1n0:   train_doc_idx_path .............................. None
x3006c0s19b1n0:   train_idx_path .................................. None
x3006c0s19b1n0:   train_iters ..................................... 10
x3006c0s19b1n0:   train_sample_idx_path ........................... None
x3006c0s19b1n0:   train_samples ................................... None
x3006c0s19b1n0:   train_shuffle_idx_path .......................... None
x3006c0s19b1n0:   train_tokens .................................... None
x3006c0s19b1n0:   transformer_impl ................................ local
x3006c0s19b1n0:   transformer_pipeline_model_parallel_size ........ 1
x3006c0s19b1n0:   untie_embeddings_and_output_weights ............. True
x3006c0s19b1n0:   use_checkpoint_args ............................. False
x3006c0s19b1n0:   use_checkpoint_opt_param_scheduler .............. False
x3006c0s19b1n0:   use_contiguous_buffers_in_local_ddp ............. True
x3006c0s19b1n0:   use_cpu_initialization .......................... None
x3006c0s19b1n0:   use_dataset_only ................................ False
x3006c0s19b1n0:   use_distributed_optimizer ....................... False
x3006c0s19b1n0:   use_flash_attn .................................. False
x3006c0s19b1n0:   use_flash_attn_triton ........................... False
x3006c0s19b1n0:   use_flash_attn_v1 ............................... False
x3006c0s19b1n0:   use_flash_attn_v2 ............................... False
x3006c0s19b1n0:   use_one_sent_docs ............................... False
x3006c0s19b1n0:   use_pin_memory .................................. False
x3006c0s19b1n0:   use_ring_exchange_p2p ........................... False
x3006c0s19b1n0:   use_rotary_position_embeddings .................. True
x3006c0s19b1n0:   use_tutel ....................................... False
x3006c0s19b1n0:   valid_data_path ................................. None
x3006c0s19b1n0:   variable_seq_lengths ............................ False
x3006c0s19b1n0:   virtual_pipeline_model_parallel_size ............ None
x3006c0s19b1n0:   vision_backbone_type ............................ vit
x3006c0s19b1n0:   vision_pretraining .............................. False
x3006c0s19b1n0:   vision_pretraining_type ......................... classify
x3006c0s19b1n0:   vocab_extra_ids ................................. 0
x3006c0s19b1n0:   vocab_file ...................................... /home/am6429/dl-io/datasets/gpt2-vocab.json
x3006c0s19b1n0:   vocab_size ...................................... None
x3006c0s19b1n0:   weight_decay .................................... 0.1
x3006c0s19b1n0:   weight_decay_incr_style ......................... constant
x3006c0s19b1n0:   world_size ...................................... 8
x3006c0s19b1n0:   zero_allgather_bucket_size ...................... 0.0
x3006c0s19b1n0:   zero_contigious_gradients ....................... False
x3006c0s19b1n0:   zero_reduce_bucket_size ......................... 0.0
x3006c0s19b1n0:   zero_reduce_scatter ............................. False
x3006c0s19b1n0:   zero_stage ...................................... 3
x3006c0s19b1n0: -------------------- end of arguments ---------------------
x3006c0s19b1n0: setting number of micro-batches to constant 1
x3006c0s19b1n0: > building GPT2BPETokenizer tokenizer ...
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: [2024-03-28 13:26:26,410] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: [2024-03-28 13:26:26,422] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
x3006c0s19b1n0: > initializing torch distributed ...
x3006c0s19b1n0: [2024-03-28 13:26:26,424] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: [2024-03-28 13:26:26,424] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
x3006c0s19b1n0: [2024-03-28 13:26:26,434] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b0n0:       meet the required dependencies to JIT install the op.
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: JIT compiled ops requires ninja
x3006c0s1b0n0: ninja .................. [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: op name ................ installed .. compatible
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b0n0:       meet the required dependencies to JIT install the op.
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: JIT compiled ops requires ninja
x3006c0s1b0n0: ninja .................. [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: op name ................ installed .. compatible
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b0n0:       meet the required dependencies to JIT install the op.
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: JIT compiled ops requires ninja
x3006c0s1b0n0: ninja .................. [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: op name ................ installed .. compatible
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b0n0:       meet the required dependencies to JIT install the op.
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: JIT compiled ops requires ninja
x3006c0s1b0n0: ninja .................. [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: op name ................ installed .. compatible
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: DeepSpeed general environment info:
x3006c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b0n0: torch version .................... 2.0.1+cu118
x3006c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b0n0: torch cuda version ............... 11.8
x3006c0s1b0n0: torch hip version ................ None
x3006c0s1b0n0: nvcc version ..................... 11.8
x3006c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: DeepSpeed general environment info:
x3006c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b0n0: torch version .................... 2.0.1+cu118
x3006c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b0n0: torch cuda version ............... 11.8
x3006c0s1b0n0: torch hip version ................ None
x3006c0s1b0n0: nvcc version ..................... 11.8
x3006c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: DeepSpeed general environment info:
x3006c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b0n0: torch version .................... 2.0.1+cu118
x3006c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b0n0: torch cuda version ............... 11.8
x3006c0s1b0n0: torch hip version ................ None
x3006c0s1b0n0: nvcc version ..................... 11.8
x3006c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed general environment info:
x3006c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b0n0: torch version .................... 2.0.1+cu118
x3006c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b0n0: torch cuda version ............... 11.8
x3006c0s1b0n0: torch hip version ................ None
x3006c0s1b0n0: nvcc version ..................... 11.8
x3006c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b0n0: [2024-03-28 13:26:26,843] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b0n0: [2024-03-28 13:26:26,862] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b0n0: [2024-03-28 13:26:26,873] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b0n0: [2024-03-28 13:26:26,882] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: > initialized tensor model parallel with size 1
x3006c0s19b1n0: > initialized pipeline model parallel with size 1
x3006c0s19b1n0: > setting random seeds to 1234 ...
x3006c0s19b1n0: [2024-03-28 13:26:27,871] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
x3006c0s19b1n0: > compiling dataset index builder ...
x3006c0s19b1n0: make: Entering directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3006c0s19b1n0: make: Nothing to be done for 'default'.
x3006c0s19b1n0: make: Leaving directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3006c0s19b1n0: >>> done with dataset index builder. Compilation time: 0.083 seconds
x3006c0s19b1n0: > compiling and loading fused kernels ...
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: >>> done with compiling and loading fused kernels. Compilation time: 3.810 seconds
x3006c0s1b0n0: <<<<<<<<<<< 6
x3006c0s1b0n0: <<<<<<<<<<< 7
x3006c0s1b0n0: <<<<<<<<<<< 4
x3006c0s1b0n0: <<<<<<<<<<< 5
x3006c0s19b1n0: initialize_megatron took 6.1877546310424805
x3006c0s19b1n0: <<<<<<<<<<< 0
x3006c0s19b1n0: <<<<<<<<<<< 2
x3006c0s19b1n0: <<<<<<<<<<< 1
x3006c0s19b1n0: <<<<<<<<<<< 3
x3006c0s19b1n0: time to initialize megatron (seconds): 6.495
x3006c0s19b1n0: [after megatron is initialized] datetime: 2024-03-28 13:26:32 
x3006c0s19b1n0: get_accelerator and all_reduce  took 0.0025131702423095703
x3006c0s19b1n0: building GPT model ...
x3006c0s19b1n0: [2024-03-28 13:26:32,637] [INFO] [utils.py:800:see_memory_usage] Before Building Model
x3006c0s19b1n0: [2024-03-28 13:26:32,637] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 4.62 GB         CA 0.0 GB         Max_CA 5 GB 
x3006c0s19b1n0: [2024-03-28 13:26:32,638] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 21.6 GB, percent = 4.3%
x3006c0s19b1n0: [2024-03-28 13:26:38,947] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 423, num_elems = 27.64B
x3006c0s19b1n0: [2024-03-28 13:26:39,012] [INFO] [utils.py:800:see_memory_usage] After Building Model
x3006c0s19b1n0: [2024-03-28 13:26:39,013] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 7.07 GB         CA 21.53 GB         Max_CA 38 GB 
x3006c0s19b1n0: [2024-03-28 13:26:39,013] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 22.02 GB, percent = 4.4%
x3006c0s19b1n0:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 27635239424
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: Time to load cpu_adam op: 2.500140905380249 seconds
x3006c0s19b1n0: Time to load cpu_adam op: 2.441694498062134 seconds
x3006c0s1b0n0: Time to load cpu_adam op: 2.440873861312866 seconds
x3006c0s19b1n0: Time to load cpu_adam op: 2.4384994506835938 seconds
x3006c0s1b0n0: Time to load cpu_adam op: 2.436365842819214 seconds
x3006c0s1b0n0: Time to load cpu_adam op: 2.482638359069824 seconds
x3006c0s19b1n0: Time to load cpu_adam op: 2.5021302700042725 seconds
x3006c0s1b0n0: ninja: no work to do.
x3006c0s1b0n0: Time to load cpu_adam op: 2.527477264404297 seconds
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: > learning rate decay style: cosine
x3006c0s19b1n0: DeepSpeed is enabled.
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-28 13:26:43,496] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.3+8074cd62, git-hash=8074cd62, git-branch=hybrid_opt_offload
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-28 13:26:43,565] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After args sanity test
x3006c0s19b1n0: [2024-03-28 13:26:43,565] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 21.53 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:26:43,565] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.43 GB, percent = 5.8%
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-28 13:26:43,619] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure distributed model
x3006c0s19b1n0: [2024-03-28 13:26:43,619] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 21.53 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:26:43,619] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.43 GB, percent = 5.8%
x3006c0s19b1n0: [2024-03-28 13:26:43,679] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After configure distributed model
x3006c0s19b1n0: [2024-03-28 13:26:43,679] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 21.53 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:26:43,679] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.43 GB, percent = 5.8%
x3006c0s19b1n0: [2024-03-28 13:26:43,679] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
x3006c0s19b1n0: [2024-03-28 13:26:43,730] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After setting model parameters
x3006c0s19b1n0: [2024-03-28 13:26:43,730] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 21.53 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:26:43,731] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.43 GB, percent = 5.8%
x3006c0s19b1n0: [2024-03-28 13:26:43,782] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure optimizer
x3006c0s19b1n0: [2024-03-28 13:26:43,783] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 21.53 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:26:43,783] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.43 GB, percent = 5.8%
x3006c0s19b1n0: [2024-03-28 13:26:43,783] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
x3006c0s19b1n0: [2024-03-28 13:26:43,783] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
x3006c0s19b1n0: [2024-03-28 13:26:43,801] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
x3006c0s19b1n0: [2024-03-28 13:26:43,801] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
x3006c0s19b1n0: [2024-03-28 13:26:43,801] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
x3006c0s19b1n0: [2024-03-28 13:26:43,801] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
x3006c0s19b1n0: [2024-03-28 13:26:43,851] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
x3006c0s19b1n0: [2024-03-28 13:26:43,852] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 21.53 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:26:43,852] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.43 GB, percent = 5.8%
x3006c0s19b1n0: [2024-03-28 13:26:43,854] [INFO] [stage3.py:137:__init__] Reduce bucket size 500,000,000
x3006c0s19b1n0: [2024-03-28 13:26:43,854] [INFO] [stage3.py:138:__init__] Prefetch bucket size 50,000,000
x3006c0s19b1n0: [2024-03-28 13:26:43,906] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
x3006c0s19b1n0: [2024-03-28 13:26:43,906] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 21.53 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:26:43,906] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.43 GB, percent = 5.8%
x3006c0s19b1n0: Parameter Offload: Total persistent parameters: 805376 in 121 params
x3006c0s19b1n0: [2024-03-28 13:26:43,982] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
x3006c0s19b1n0: [2024-03-28 13:26:43,983] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 21.53 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:26:43,983] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.44 GB, percent = 5.8%
x3006c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-28 13:26:44,037] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
x3006c0s19b1n0: [2024-03-28 13:26:44,037] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 21.53 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:26:44,037] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.44 GB, percent = 5.8%
x3006c0s19b1n0: [2024-03-28 13:26:44,072] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-28 13:26:44,072] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-28 13:26:44,072] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-28 13:26:44,072] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b0n0: [2024-03-28 13:26:44,072] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b0n0: [2024-03-28 13:26:44,072] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b0n0: [2024-03-28 13:26:44,072] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b0n0: [2024-03-28 13:26:44,072] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b0n0: [2024-03-28 13:26:44,072] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-28 13:26:44,072] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b0n0: [2024-03-28 13:26:44,072] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-28 13:26:44,072] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-28 13:26:44,072] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-28 13:26:44,072] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b0n0: [2024-03-28 13:26:44,072] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b0n0: [2024-03-28 13:26:44,072] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b0n0: [2024-03-28 13:26:44,072] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,072] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,072] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,072] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,072] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,072] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,072] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,072] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,073] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,074] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,075] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,076] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b0n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b0n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b0n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b0n0: [2024-03-28 13:26:44,077] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-28 13:26:46,341] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 31
x3006c0s19b1n0: [2024-03-28 13:26:46,342] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.45 GB         CA 6.44 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:26:46,342] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 48.78 GB, percent = 9.7%
x3006c0s19b1n0: [2024-03-28 13:26:46,416] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
x3006c0s19b1n0: [2024-03-28 13:26:46,417] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-28 13:26:46,417] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 48.78 GB, percent = 9.7%
x3006c0s19b1n0: [2024-03-28 13:26:48,932] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
x3006c0s19b1n0: [2024-03-28 13:26:48,932] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-28 13:26:48,933] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 61.23 GB, percent = 12.2%
x3006c0s19b1n0: [2024-03-28 13:26:50,816] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
x3006c0s19b1n0: [2024-03-28 13:26:50,817] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-28 13:26:50,817] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 84.43 GB, percent = 16.8%
x3006c0s19b1n0: [2024-03-28 13:26:57,689] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | init_optimizer_state: 6860.87
x3006c0s19b1n0: [2024-03-28 13:26:57,756] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
x3006c0s19b1n0: [2024-03-28 13:26:57,756] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-28 13:26:57,756] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 144.42 GB, percent = 28.7%
x3006c0s19b1n0: [2024-03-28 13:26:57,756] [INFO] [stage3.py:520:_setup_for_real_optimizer] optimizer state initialized
x3006c0s19b1n0: [2024-03-28 13:27:05,981] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
x3006c0s19b1n0: [2024-03-28 13:27:05,981] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 8.61 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:27:05,981] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 177.11 GB, percent = 35.2%
x3006c0s19b1n0: [2024-03-28 13:27:05,981] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
x3006c0s19b1n0: [2024-03-28 13:27:06,048] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure LR scheduler
x3006c0s19b1n0: [2024-03-28 13:27:06,048] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:27:06,048] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 177.1 GB, percent = 35.2%
x3006c0s19b1n0: [2024-03-28 13:27:06,049] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
x3006c0s19b1n0: [2024-03-28 13:27:06,049] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7f6a1c1dc1f0>
x3006c0s19b1n0: [2024-03-28 13:27:06,049] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:27:06,111] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before rewriting optimizer step
x3006c0s19b1n0: [2024-03-28 13:27:06,111] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:27:06,111] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 177.11 GB, percent = 35.2%
x3006c0s19b1n0: [2024-03-28 13:27:06,175] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure checkpointing
x3006c0s19b1n0: [2024-03-28 13:27:06,175] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:27:06,175] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 177.11 GB, percent = 35.2%
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:998:print] DeepSpeedEngine configuration:
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   activation_checkpointing_config  {
x3006c0s19b1n0:     "partition_activations": false, 
x3006c0s19b1n0:     "contiguous_memory_optimization": false, 
x3006c0s19b1n0:     "cpu_checkpointing": false, 
x3006c0s19b1n0:     "number_checkpoints": null, 
x3006c0s19b1n0:     "synchronize_checkpoint_boundary": false, 
x3006c0s19b1n0:     "profile": false
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   amp_enabled .................. False
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   amp_params ................... False
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   autotuning_config ............ {
x3006c0s19b1n0:     "enabled": false, 
x3006c0s19b1n0:     "start_step": null, 
x3006c0s19b1n0:     "end_step": null, 
x3006c0s19b1n0:     "metric_path": null, 
x3006c0s19b1n0:     "arg_mappings": null, 
x3006c0s19b1n0:     "metric": "throughput", 
x3006c0s19b1n0:     "model_info": null, 
x3006c0s19b1n0:     "results_dir": "autotuning_results", 
x3006c0s19b1n0:     "exps_dir": "autotuning_exps", 
x3006c0s19b1n0:     "overwrite": true, 
x3006c0s19b1n0:     "fast": true, 
x3006c0s19b1n0:     "start_profile_step": 3, 
x3006c0s19b1n0:     "end_profile_step": 5, 
x3006c0s19b1n0:     "tuner_type": "gridsearch", 
x3006c0s19b1n0:     "tuner_early_stopping": 5, 
x3006c0s19b1n0:     "tuner_num_trials": 50, 
x3006c0s19b1n0:     "model_info_path": null, 
x3006c0s19b1n0:     "mp_size": 1, 
x3006c0s19b1n0:     "max_train_batch_size": null, 
x3006c0s19b1n0:     "min_train_batch_size": 1, 
x3006c0s19b1n0:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
x3006c0s19b1n0:     "min_train_micro_batch_size_per_gpu": 1, 
x3006c0s19b1n0:     "num_tuning_micro_batch_sizes": 3
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   bfloat16_enabled ............. True
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   bfloat16_immediate_grad_update  False
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   checkpoint_parallel_write_pipeline  False
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   checkpoint_tag_validation_enabled  True
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   checkpoint_tag_validation_fail  False
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f6a1c1dcb50>
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   communication_data_type ...... None
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   curriculum_enabled_legacy .... False
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   curriculum_params_legacy ..... False
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   data_efficiency_enabled ...... False
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   dataloader_drop_last ......... False
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   datastates_config ............ {
x3006c0s19b1n0:     "enabled": null, 
x3006c0s19b1n0:     "config": {
x3006c0s19b1n0:     }
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   disable_allgather ............ False
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   dump_state ................... False
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   dynamic_loss_scale_args ...... None
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   eigenvalue_enabled ........... False
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   eigenvalue_gas_boundary_resolution  1
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   eigenvalue_layer_name ........ bert.encoder.layer
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   eigenvalue_layer_num ......... 0
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   eigenvalue_max_iter .......... 100
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   eigenvalue_stability ......... 1e-06
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   eigenvalue_tol ............... 0.01
x3006c0s19b1n0: [2024-03-28 13:27:06,176] [INFO] [config.py:1002:print]   eigenvalue_verbose ........... False
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   elasticity_enabled ........... False
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   flops_profiler_config ........ {
x3006c0s19b1n0:     "enabled": false, 
x3006c0s19b1n0:     "recompute_fwd_factor": 0.0, 
x3006c0s19b1n0:     "profile_step": 1, 
x3006c0s19b1n0:     "module_depth": -1, 
x3006c0s19b1n0:     "top_modules": 1, 
x3006c0s19b1n0:     "detailed": true, 
x3006c0s19b1n0:     "output_file": null
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   fp16_auto_cast ............... None
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   fp16_enabled ................. False
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   fp16_master_weights_and_gradients  False
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   global_rank .................. 0
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   grad_accum_dtype ............. bf16
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   gradient_accumulation_steps .. 1
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   gradient_clipping ............ 0.0
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   gradient_predivide_factor .... 1.0
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   graph_harvesting ............. False
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   initial_dynamic_scale ........ 1
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   load_universal_checkpoint .... False
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   loss_scale ................... 1.0
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   memory_breakdown ............. True
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   mics_hierarchial_params_gather  False
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   mics_shard_size .............. -1
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   nebula_config ................ {
x3006c0s19b1n0:     "enabled": false, 
x3006c0s19b1n0:     "persistent_storage_path": null, 
x3006c0s19b1n0:     "persistent_time_interval": 100, 
x3006c0s19b1n0:     "num_of_version_in_retention": 2, 
x3006c0s19b1n0:     "enable_nebula_load": true, 
x3006c0s19b1n0:     "load_path": null
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   optimizer_legacy_fusion ...... False
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   optimizer_name ............... None
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   optimizer_params ............. None
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   pld_enabled .................. False
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   pld_params ................... False
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   prescale_gradients ........... False
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   scheduler_name ............... None
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   scheduler_params ............. None
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   seq_parallel_communication_data_type  torch.float32
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   sparse_attention ............. None
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   sparse_gradients_enabled ..... False
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   steps_per_print .............. 1
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   train_batch_size ............. 32
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   train_micro_batch_size_per_gpu  4
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   use_data_before_expert_parallel_  False
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   use_node_local_storage ....... False
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   wall_clock_breakdown ......... True
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   weight_quantization_config ... None
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   world_size ................... 8
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   zero_allow_untested_optimizer  False
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0, prefetch_optimizer=False, part_grads_async=False, prefetch_optimizer_gap=5) sub_group_size=100000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   zero_enabled ................. True
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   zero_force_ds_cpu_optimizer .. True
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:1002:print]   zero_optimization_stage ...... 3
x3006c0s19b1n0: [2024-03-28 13:27:06,177] [INFO] [config.py:988:print_user_config]   json = {
x3006c0s19b1n0:     "train_batch_size": 32, 
x3006c0s19b1n0:     "train_micro_batch_size_per_gpu": 4, 
x3006c0s19b1n0:     "steps_per_print": 1, 
x3006c0s19b1n0:     "zero_optimization": {
x3006c0s19b1n0:         "stage": 3, 
x3006c0s19b1n0:         "offload_optimizer": {
x3006c0s19b1n0:             "device": "cpu", 
x3006c0s19b1n0:             "ratio": 1, 
x3006c0s19b1n0:             "pin_memory": true, 
x3006c0s19b1n0:             "prefetch_optimizer": 0, 
x3006c0s19b1n0:             "part_grads_async": 0, 
x3006c0s19b1n0:             "prefetch_optimizer_gap": 5
x3006c0s19b1n0:         }, 
x3006c0s19b1n0:         "sub_group_size": 1.000000e+08
x3006c0s19b1n0:     }, 
x3006c0s19b1n0:     "bf16": {
x3006c0s19b1n0:         "enabled": true
x3006c0s19b1n0:     }, 
x3006c0s19b1n0:     "data_types": {
x3006c0s19b1n0:         "grad_accum_dtype": "bf16"
x3006c0s19b1n0:     }, 
x3006c0s19b1n0:     "wall_clock_breakdown": true, 
x3006c0s19b1n0:     "memory_breakdown": true, 
x3006c0s19b1n0:     "flops_profiler": {
x3006c0s19b1n0:         "enabled": false
x3006c0s19b1n0:     }
x3006c0s19b1n0: }
x3006c0s1b0n0: <TIMER:model-and-optimizer-setup,34.87440228462219>
x3006c0s1b0n0: <TIMER:model-and-optimizer-setup,34.87448811531067>
x3006c0s1b0n0: <TIMER:model-and-optimizer-setup,34.87580728530884>
x3006c0s1b0n0: <TIMER:model-and-optimizer-setup,34.87588715553284>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,34.875943183898926>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,34.87663006782532>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,34.87765669822693>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,34.87777519226074>
x3006c0s19b1n0: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-28 13:27:07 
x3006c0s19b1n0: > building train, validation, and test datasets ...
x3006c0s19b1n0:  > datasets target sizes (minimum size):
x3006c0s19b1n0:     train:      320
x3006c0s19b1n0:     validation: 0
x3006c0s19b1n0:     test:       0
x3006c0s19b1n0: > building train, validation, and test datasets for GPT ...
x3006c0s19b1n0: Single data path provided for train, valid & test
x3006c0s19b1n0:  > building dataset index ...
x3006c0s19b1n0:     reading sizes...
x3006c0s19b1n0:     reading pointers...
x3006c0s19b1n0:     reading document index...
x3006c0s19b1n0:     creating numpy buffer of mmap...
x3006c0s19b1n0:     creating memory view of numpy buffer...
x3006c0s19b1n0:  > finished creating indexed dataset in 0.002263 seconds
x3006c0s19b1n0:     number of documents: 79000
x3006c0s19b1n0:  > dataset split:
x3006c0s19b1n0:     train:
x3006c0s19b1n0:      document indices in [0, 74971) total of 74971 documents
x3006c0s19b1n0:     validation:
x3006c0s19b1n0:      document indices in [74971, 78921) total of 3950 documents
x3006c0s19b1n0:     test:
x3006c0s19b1n0:      document indices in [78921, 79000) total of 79 documents
x3006c0s19b1n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_doc_idx.npy
x3006c0s19b1n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_sample_idx.npy
x3006c0s19b1n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_shuffle_idx.npy
x3006c0s19b1n0:     loaded indexed file in 0.002 seconds
x3006c0s19b1n0:     total number of samples: 108448
x3006c0s19b1n0:     total number of epochs: 1
x3006c0s19b1n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_doc_idx.npy
x3006c0s19b1n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_sample_idx.npy
x3006c0s19b1n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_shuffle_idx.npy
x3006c0s19b1n0:     loaded indexed file in 0.004 seconds
x3006c0s19b1n0:     total number of samples: 5792
x3006c0s19b1n0:     total number of epochs: 1
x3006c0s19b1n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_doc_idx.npy
x3006c0s19b1n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_sample_idx.npy
x3006c0s19b1n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_shuffle_idx.npy
x3006c0s19b1n0:     loaded indexed file in 0.004 seconds
x3006c0s19b1n0:     total number of samples: 185
x3006c0s19b1n0:     total number of epochs: 1
x3006c0s19b1n0: > finished creating GPT datasets ...
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.551567554473877>
x3006c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5553188323974609>
x3006c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5560219287872314>
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5591888427734375>
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5723376274108887>
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5806732177734375>
x3006c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5936574935913086>
x3006c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.6077151298522949>
x3006c0s19b1n0: [after dataloaders are built] datetime: 2024-03-28 13:27:08 
x3006c0s19b1n0: done with setup ...
x3006c0s19b1n0: training ...
x3006c0s1b0n0: (min, max) time across ranks (ms):
x3006c0s1b0n0:     model-and-optimizer-setup ......................: (34874.40, 34877.78)
x3006c0s1b0n0:     train/valid/test-data-iterators-setup ..........: (551.57, 607.72)
x3006c0s19b1n0: [before training begins] datetime: 2024-03-28 13:27:08 
x3006c0s19b1n0: [before the start of training step] datetime: 2024-03-28 13:27:08 
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:27:08,261] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:27:08,262] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 7.45 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:27:08,262] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 180.03 GB, percent = 35.8%
x3006c0s19b1n0: [2024-03-28 13:27:08,386] [INFO] [checkpointing.py:539:forward] Activation Checkpointing Information
x3006c0s19b1n0: [2024-03-28 13:27:08,386] [INFO] [checkpointing.py:540:forward] ----Partition Activations False, CPU CHECKPOINTING False
x3006c0s19b1n0: [2024-03-28 13:27:08,386] [INFO] [checkpointing.py:541:forward] ----contiguous Memory Checkpointing False with 60 total layers
x3006c0s19b1n0: [2024-03-28 13:27:08,386] [INFO] [checkpointing.py:543:forward] ----Synchronization False
x3006c0s19b1n0: [2024-03-28 13:27:08,386] [INFO] [checkpointing.py:544:forward] ----Profiling time in checkpointing False
x3006c0s19b1n0: [2024-03-28 13:27:16,944] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:27:16,945] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 19.76 GB         CA 22.95 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:27:16,945] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 180.17 GB, percent = 35.8%
x3006c0s19b1n0: [2024-03-28 13:27:17,137] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:27:17,138] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 16.84 GB         CA 16.91 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:27:17,138] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 180.2 GB, percent = 35.8%
x3006c0s19b1n0: [2024-03-28 13:27:45,765] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:27:45,765] [INFO] [utils.py:801:see_memory_usage] MA 10.25 GB         Max_MA 20.43 GB         CA 10.33 GB         Max_CA 25 GB 
x3006c0s19b1n0: [2024-03-28 13:27:45,765] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 183.04 GB, percent = 36.4%
x3006c0s19b1n0: [2024-03-28 13:27:45,832] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:27:45,833] [INFO] [utils.py:801:see_memory_usage] MA 10.25 GB         Max_MA 10.25 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:27:45,833] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 183.08 GB, percent = 36.4%
x3006c0s1b0n0: [2024-03-28 13:27:55,125] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.223682641983032
x3006c0s1b0n0: [2024-03-28 13:27:55,126] [INFO] [stage3.py:2251:step] Full outer step loop took 9.223997592926025
x3006c0s19b1n0: [2024-03-28 13:27:55,196] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.294219732284546
x3006c0s19b1n0: [2024-03-28 13:27:55,196] [INFO] [stage3.py:2251:step] Full outer step loop took 9.29447317123413
x3006c0s1b0n0: [2024-03-28 13:27:55,285] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.366440057754517
x3006c0s1b0n0: [2024-03-28 13:27:55,286] [INFO] [stage3.py:2251:step] Full outer step loop took 9.368523359298706
x3006c0s1b0n0: [2024-03-28 13:27:55,305] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.402939319610596
x3006c0s1b0n0: [2024-03-28 13:27:55,305] [INFO] [stage3.py:2251:step] Full outer step loop took 9.40358591079712
x3006c0s1b0n0: [2024-03-28 13:27:55,369] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.467263221740723
x3006c0s1b0n0: [2024-03-28 13:27:55,369] [INFO] [stage3.py:2251:step] Full outer step loop took 9.46745777130127
x3006c0s19b1n0: [2024-03-28 13:27:55,410] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.507883548736572
x3006c0s19b1n0: [2024-03-28 13:27:55,410] [INFO] [stage3.py:2251:step] Full outer step loop took 9.508147716522217
x3006c0s19b1n0: [2024-03-28 13:27:55,419] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.517200946807861
x3006c0s19b1n0: [2024-03-28 13:27:55,422] [INFO] [stage3.py:2251:step] Full outer step loop took 9.520292520523071
x3006c0s19b1n0: [2024-03-28 13:27:55,508] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.59147572517395
x3006c0s19b1n0: [2024-03-28 13:27:55,508] [INFO] [stage3.py:2251:step] Full outer step loop took 9.591684579849243
x3006c0s1b0n0: [2024-03-28 13:27:55,519] [INFO] [stage3.py:2277:step] End to end step took 9.617527961730957
x3006c0s19b1n0: [2024-03-28 13:27:55,519] [INFO] [stage3.py:2277:step] End to end step took 9.602535247802734
x3006c0s19b1n0: [2024-03-28 13:27:55,520] [INFO] [stage3.py:2277:step] End to end step took 9.618023157119751
x3006c0s19b1n0: [2024-03-28 13:27:55,520] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 9294.74
x3006c0s19b1n0: [2024-03-28 13:27:55,520] [INFO] [stage3.py:2277:step] End to end step took 9.618196487426758
x3006c0s1b0n0: [2024-03-28 13:27:55,520] [INFO] [stage3.py:2277:step] End to end step took 9.6021089553833
x3006c0s1b0n0: [2024-03-28 13:27:55,520] [INFO] [stage3.py:2277:step] End to end step took 9.618236303329468
x3006c0s1b0n0: [2024-03-28 13:27:55,520] [INFO] [stage3.py:2277:step] End to end step took 9.618267059326172
x3006c0s19b1n0: [2024-03-28 13:27:55,520] [WARNING] [stage3.py:2267:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3006c0s19b1n0: [2024-03-28 13:27:55,520] [INFO] [stage3.py:2277:step] End to end step took 9.618327856063843
x3006c0s19b1n0: [2024-03-28 13:27:55,520] [INFO] [logging.py:96:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:27:55,521] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 8779.49 | bwd_microstep: 28514.92 | bwd_inner_microstep: 28412.43 | bwd_allreduce_microstep: 102.39 | step_microstep: 9687.42
x3006c0s19b1n0: [2024-03-28 13:27:55,521] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 8779.49 | bwd: 28514.91 | bwd_inner: 28412.43 | bwd_allreduce: 102.40 | step: 9687.43
x3006c0s19b1n0: [2024-03-28 13:27:55,626] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:27:55,626] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.25 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:27:55,626] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.85 GB, percent = 67.1%
x3006c0s19b1n0: <TIMER:interval-time,47.55828404426575>
x3006c0s19b1n0: <TIMER:interval-time,47.55829954147339>
x3006c0s19b1n0: <TIMER:interval-time,47.558305501937866>
x3006c0s1b0n0: <TIMER:interval-time,47.55826783180237><TIMER:interval-time,47.55826497077942><TIMER:interval-time,47.5582377910614><TIMER:interval-time,47.55824112892151>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s19b1n0: <TIMER:interval-time,47.558369636535645>
x3006c0s19b1n0: [Rank 0] (after 1 iterations) memory (MB) | allocated: 9224.5439453125 | max allocated: 9224.54443359375 | reserved: 9296.0 | max reserved: 9296.0
x3006c0s1b0n0:  elapsed_time 47.558265 | consumed samples:           32 | consumed tokens:        65536 | elapsed time per iteration (ms): 47558.3 | learning rate: 3.000E-04 | global batch size:    32 | lm loss: 1.215015E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.673 | TFLOPs: 46.56 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:27:55,766] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:27:55,767] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 13:27:55,767] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.95 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:28:02,695] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:28:02,695] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:28:02,695] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.94 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:28:02,780] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:28:02,780] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:28:02,780] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.94 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:28:23,359] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:28:23,360] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:28:23,360] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.71 GB, percent = 67.1%
x3006c0s19b1n0: [2024-03-28 13:28:23,432] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:28:23,433] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:28:23,433] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.71 GB, percent = 67.1%
x3006c0s1b0n0: [2024-03-28 13:28:30,218] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.760427713394165
x3006c0s1b0n0: [2024-03-28 13:28:30,218] [INFO] [stage3.py:2251:step] Full outer step loop took 6.760665416717529
x3006c0s19b1n0: [2024-03-28 13:28:30,387] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.929781675338745
x3006c0s19b1n0: [2024-03-28 13:28:30,388] [INFO] [stage3.py:2251:step] Full outer step loop took 6.93051552772522
x3006c0s1b0n0: [2024-03-28 13:28:30,448] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.990367412567139
x3006c0s1b0n0: [2024-03-28 13:28:30,448] [INFO] [stage3.py:2251:step] Full outer step loop took 6.990659713745117
x3006c0s1b0n0: [2024-03-28 13:28:30,458] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.000593900680542
x3006c0s1b0n0: [2024-03-28 13:28:30,458] [INFO] [stage3.py:2251:step] Full outer step loop took 7.00080680847168
x3006c0s1b0n0: [2024-03-28 13:28:30,532] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.074452877044678
x3006c0s1b0n0: [2024-03-28 13:28:30,532] [INFO] [stage3.py:2251:step] Full outer step loop took 7.074599027633667
x3006c0s19b1n0: [2024-03-28 13:28:30,612] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.154953479766846
x3006c0s19b1n0: [2024-03-28 13:28:30,613] [INFO] [stage3.py:2251:step] Full outer step loop took 7.155254364013672
x3006c0s19b1n0: [2024-03-28 13:28:30,624] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.1664721965789795
x3006c0s19b1n0: [2024-03-28 13:28:30,624] [INFO] [stage3.py:2251:step] Full outer step loop took 7.16666316986084
x3006c0s19b1n0: [2024-03-28 13:28:30,630] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.172663688659668
x3006c0s19b1n0: [2024-03-28 13:28:30,630] [INFO] [stage3.py:2251:step] Full outer step loop took 7.172818422317505
x3006c0s19b1n0: [2024-03-28 13:28:30,641] [INFO] [stage3.py:2277:step] End to end step took 7.183741807937622
x3006c0s19b1n0: [2024-03-28 13:28:30,641] [INFO] [stage3.py:2277:step] End to end step took 7.183763265609741
x3006c0s19b1n0: [2024-03-28 13:28:30,641] [INFO] [stage3.py:2277:step] End to end step took 7.183845043182373
x3006c0s1b0n0: [2024-03-28 13:28:30,641] [INFO] [stage3.py:2277:step] End to end step took 7.183654308319092
x3006c0s1b0n0: [2024-03-28 13:28:30,641] [INFO] [stage3.py:2277:step] End to end step took 7.183826923370361
x3006c0s19b1n0: [2024-03-28 13:28:30,641] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6930.89
x3006c0s19b1n0: [2024-03-28 13:28:30,642] [INFO] [stage3.py:2277:step] End to end step took 7.1842710971832275
x3006c0s1b0n0: [2024-03-28 13:28:30,642] [INFO] [stage3.py:2277:step] End to end step took 7.184227705001831
x3006c0s1b0n0: [2024-03-28 13:28:30,642] [INFO] [stage3.py:2277:step] End to end step took 7.184293985366821
x3006c0s19b1n0: [2024-03-28 13:28:30,642] [INFO] [logging.py:96:log_dist] [Rank 0] step=2, skipped=0, lr=[0.0002918585038060976], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:28:30,642] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6889.85 | bwd_microstep: 20425.39 | bwd_inner_microstep: 20304.64 | bwd_allreduce_microstep: 120.64 | step_microstep: 7208.97
x3006c0s19b1n0: [2024-03-28 13:28:30,642] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6889.84 | bwd: 20425.39 | bwd_inner: 20304.66 | bwd_allreduce: 120.64 | step: 7208.97
x3006c0s19b1n0: [2024-03-28 13:28:30,758] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:28:30,758] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:28:30,759] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.74 GB, percent = 67.1%
x3006c0s19b1n0: <TIMER:interval-time,35.13177299499512><TIMER:interval-time,35.13179397583008>
x3006c0s19b1n0: <TIMER:interval-time,35.131795167922974>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,35.1317880153656>
x3006c0s1b0n0: <TIMER:interval-time,35.13184070587158><TIMER:interval-time,35.13184571266174><TIMER:interval-time,35.13184309005737>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,35.131959438323975>
x3006c0s1b0n0:  elapsed_time 35.131841 | consumed samples:           64 | consumed tokens:       131072 | elapsed time per iteration (ms): 35131.8 | learning rate: 2.919E-04 | global batch size:    32 | lm loss: 1.216527E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.911 | TFLOPs: 63.02 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:28:30,899] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:28:30,899] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 13:28:30,899] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.75 GB, percent = 67.1%
x3006c0s19b1n0: [2024-03-28 13:28:37,773] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:28:37,773] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:28:37,773] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.73 GB, percent = 67.1%
x3006c0s19b1n0: [2024-03-28 13:28:37,849] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:28:37,850] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:28:37,850] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.73 GB, percent = 67.1%
x3006c0s19b1n0: [2024-03-28 13:28:58,733] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:28:58,734] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:28:58,734] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.91 GB, percent = 67.1%
x3006c0s19b1n0: [2024-03-28 13:28:58,805] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:28:58,805] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:28:58,805] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.91 GB, percent = 67.1%
x3006c0s1b0n0: [2024-03-28 13:29:05,537] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.707086086273193
x3006c0s1b0n0: [2024-03-28 13:29:05,537] [INFO] [stage3.py:2251:step] Full outer step loop took 6.707506418228149
x3006c0s1b0n0: [2024-03-28 13:29:05,800] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.970325946807861
x3006c0s1b0n0: [2024-03-28 13:29:05,800] [INFO] [stage3.py:2251:step] Full outer step loop took 6.970564842224121
x3006c0s19b1n0: [2024-03-28 13:29:05,811] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.981100797653198
x3006c0s19b1n0: [2024-03-28 13:29:05,811] [INFO] [stage3.py:2251:step] Full outer step loop took 6.981369733810425
x3006c0s19b1n0: [2024-03-28 13:29:05,836] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.006747722625732
x3006c0s19b1n0: [2024-03-28 13:29:05,837] [INFO] [stage3.py:2251:step] Full outer step loop took 7.007074356079102
x3006c0s1b0n0: [2024-03-28 13:29:05,845] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.015678882598877
x3006c0s1b0n0: [2024-03-28 13:29:05,846] [INFO] [stage3.py:2251:step] Full outer step loop took 7.0158655643463135
x3006c0s1b0n0: [2024-03-28 13:29:05,870] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.038967847824097
x3006c0s1b0n0: [2024-03-28 13:29:05,870] [INFO] [stage3.py:2251:step] Full outer step loop took 7.039117813110352
x3006c0s19b1n0: [2024-03-28 13:29:05,935] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.105109214782715
x3006c0s19b1n0: [2024-03-28 13:29:05,935] [INFO] [stage3.py:2251:step] Full outer step loop took 7.105534553527832
x3006c0s19b1n0: [2024-03-28 13:29:05,956] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.126044273376465
x3006c0s19b1n0: [2024-03-28 13:29:05,956] [INFO] [stage3.py:2251:step] Full outer step loop took 7.126199960708618
x3006c0s19b1n0: [2024-03-28 13:29:05,967] [INFO] [stage3.py:2277:step] End to end step took 7.136983156204224
x3006c0s1b0n0: [2024-03-28 13:29:05,967] [INFO] [stage3.py:2277:step] End to end step took 7.135757684707642
x3006c0s19b1n0: [2024-03-28 13:29:05,967] [INFO] [stage3.py:2277:step] End to end step took 7.137036323547363
x3006c0s1b0n0: [2024-03-28 13:29:05,967] [INFO] [stage3.py:2277:step] End to end step took 7.137165069580078
x3006c0s1b0n0: [2024-03-28 13:29:05,967] [INFO] [stage3.py:2277:step] End to end step took 7.1372387409210205
x3006c0s19b1n0: [2024-03-28 13:29:05,967] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6981.62
x3006c0s19b1n0: [2024-03-28 13:29:05,967] [INFO] [stage3.py:2277:step] End to end step took 7.137579679489136
x3006c0s19b1n0: [2024-03-28 13:29:05,967] [INFO] [stage3.py:2277:step] End to end step took 7.1377198696136475
x3006c0s1b0n0: [2024-03-28 13:29:05,967] [INFO] [stage3.py:2277:step] End to end step took 7.13755202293396
x3006c0s19b1n0: [2024-03-28 13:29:05,968] [INFO] [logging.py:96:log_dist] [Rank 0] step=3, skipped=0, lr=[0.00026841599982106197], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:29:05,968] [INFO] [timer.py:260:stop] epoch=0/micro_step=3/global_step=3, RunningAvgSamplesPerSec=0.9125294508534334, CurrSamplesPerSec=0.9125294508534334, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 13:29:05,968] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6841.42 | bwd_microstep: 20713.86 | bwd_inner_microstep: 20612.26 | bwd_allreduce_microstep: 101.46 | step_microstep: 7162.73
x3006c0s19b1n0: [2024-03-28 13:29:05,968] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6841.40 | bwd: 20713.87 | bwd_inner: 20612.28 | bwd_allreduce: 101.47 | step: 7162.73
x3006c0s19b1n0: [2024-03-28 13:29:06,085] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:29:06,085] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:29:06,086] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.93 GB, percent = 67.2%
x3006c0s19b1n0: <TIMER:interval-time,35.326497077941895><TIMER:interval-time,35.3265016078949><TIMER:interval-time,35.326505184173584>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,35.32650899887085>
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,35.32646918296814><TIMER:interval-time,35.32646918296814><TIMER:interval-time,35.32646679878235><TIMER:interval-time,35.32647132873535>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0:  elapsed_time 35.326471 | consumed samples:           96 | consumed tokens:       196608 | elapsed time per iteration (ms): 35326.5 | learning rate: 2.684E-04 | global batch size:    32 | lm loss: 3.444557E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.906 | TFLOPs: 62.68 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:29:06,220] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:29:06,220] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 13:29:06,220] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.94 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:29:13,170] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:29:13,171] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:29:13,171] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.86 GB, percent = 67.1%
x3006c0s19b1n0: [2024-03-28 13:29:13,261] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:29:13,261] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:29:13,261] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.86 GB, percent = 67.1%
x3006c0s19b1n0: [2024-03-28 13:29:33,728] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:29:33,729] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:29:33,729] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.02 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:29:33,803] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:29:33,803] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:29:33,804] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.02 GB, percent = 67.2%
x3006c0s1b0n0: [2024-03-28 13:29:40,739] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.910065650939941
x3006c0s1b0n0: [2024-03-28 13:29:40,739] [INFO] [stage3.py:2251:step] Full outer step loop took 6.910301685333252
x3006c0s19b1n0: [2024-03-28 13:29:40,743] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.914612293243408
x3006c0s19b1n0: [2024-03-28 13:29:40,744] [INFO] [stage3.py:2251:step] Full outer step loop took 6.915229320526123
x3006c0s1b0n0: [2024-03-28 13:29:40,790] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.961462020874023
x3006c0s1b0n0: [2024-03-28 13:29:40,791] [INFO] [stage3.py:2251:step] Full outer step loop took 6.962078332901001
x3006c0s19b1n0: [2024-03-28 13:29:40,883] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.054473161697388
x3006c0s19b1n0: [2024-03-28 13:29:40,883] [INFO] [stage3.py:2251:step] Full outer step loop took 7.0546534061431885
x3006c0s1b0n0: [2024-03-28 13:29:40,912] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.083069086074829
x3006c0s1b0n0: [2024-03-28 13:29:40,912] [INFO] [stage3.py:2251:step] Full outer step loop took 7.083284854888916
x3006c0s1b0n0: [2024-03-28 13:29:40,925] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.096515417098999
x3006c0s1b0n0: [2024-03-28 13:29:40,925] [INFO] [stage3.py:2251:step] Full outer step loop took 7.096676826477051
x3006c0s19b1n0: [2024-03-28 13:29:40,939] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.110059022903442
x3006c0s19b1n0: [2024-03-28 13:29:40,939] [INFO] [stage3.py:2251:step] Full outer step loop took 7.110260963439941
x3006c0s19b1n0: [2024-03-28 13:29:40,949] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.120808839797974
x3006c0s19b1n0: [2024-03-28 13:29:40,950] [INFO] [stage3.py:2251:step] Full outer step loop took 7.120963096618652
x3006c0s1b0n0: [2024-03-28 13:29:40,961] [INFO] [stage3.py:2277:step] End to end step took 7.132083415985107
x3006c0s1b0n0: [2024-03-28 13:29:40,961] [INFO] [stage3.py:2277:step] End to end step took 7.132125616073608
x3006c0s19b1n0: [2024-03-28 13:29:40,961] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 7054.83
x3006c0s19b1n0: [2024-03-28 13:29:40,961] [INFO] [stage3.py:2277:step] End to end step took 7.132127523422241
x3006c0s19b1n0: [2024-03-28 13:29:40,961] [INFO] [stage3.py:2277:step] End to end step took 7.132336378097534
x3006c0s1b0n0: [2024-03-28 13:29:40,961] [INFO] [stage3.py:2277:step] End to end step took 7.132478475570679
x3006c0s19b1n0: [2024-03-28 13:29:40,961] [INFO] [stage3.py:2277:step] End to end step took 7.132425546646118
x3006c0s19b1n0: [2024-03-28 13:29:40,961] [INFO] [logging.py:96:log_dist] [Rank 0] step=4, skipped=0, lr=[0.00023249999999999996], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:29:40,961] [INFO] [stage3.py:2277:step] End to end step took 7.132580041885376
x3006c0s1b0n0: [2024-03-28 13:29:40,961] [INFO] [stage3.py:2277:step] End to end step took 7.132370948791504
x3006c0s19b1n0: [2024-03-28 13:29:40,961] [INFO] [timer.py:260:stop] epoch=0/micro_step=4/global_step=4, RunningAvgSamplesPerSec=0.9168091924826589, CurrSamplesPerSec=0.9211292670472587, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 13:29:40,962] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6901.13 | bwd_microstep: 20312.20 | bwd_inner_microstep: 20190.44 | bwd_allreduce_microstep: 121.65 | step_microstep: 7157.97
x3006c0s19b1n0: [2024-03-28 13:29:40,962] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6901.12 | bwd: 20312.20 | bwd_inner: 20190.45 | bwd_allreduce: 121.66 | step: 7157.97
x3006c0s19b1n0: [2024-03-28 13:29:41,097] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:29:41,097] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:29:41,097] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.05 GB, percent = 67.2%
x3006c0s19b1n0: <TIMER:interval-time,35.011619091033936>
x3006c0s19b1n0: <TIMER:interval-time,35.011629819869995>
x3006c0s1b0n0: <TIMER:interval-time,35.011632204055786><TIMER:interval-time,35.01163411140442><TIMER:interval-time,35.01163411140442>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s19b1n0: <TIMER:interval-time,35.01167392730713>
x3006c0s19b1n0: <TIMER:interval-time,35.011722564697266>
x3006c0s1b0n0: <TIMER:interval-time,35.01175618171692>
x3006c0s1b0n0:  elapsed_time 35.011634 | consumed samples:          128 | consumed tokens:       262144 | elapsed time per iteration (ms): 35011.6 | learning rate: 2.325E-04 | global batch size:    32 | lm loss: 2.249373E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.914 | TFLOPs: 63.24 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:29:41,247] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:29:41,248] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 13:29:41,248] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.05 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:29:48,153] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:29:48,154] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:29:48,154] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 338.04 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:29:48,249] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:29:48,249] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:29:48,249] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.9 GB, percent = 67.1%
x3006c0s19b1n0: [2024-03-28 13:30:09,099] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:30:09,100] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:30:09,100] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.78 GB, percent = 67.1%
x3006c0s19b1n0: [2024-03-28 13:30:09,169] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:30:09,170] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:30:09,170] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.78 GB, percent = 67.1%
x3006c0s19b1n0: [2024-03-28 13:30:16,023] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.8291332721710205
x3006c0s19b1n0: [2024-03-28 13:30:16,024] [INFO] [stage3.py:2251:step] Full outer step loop took 6.83001446723938
x3006c0s1b0n0: [2024-03-28 13:30:16,028] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.8339684009552
x3006c0s1b0n0: [2024-03-28 13:30:16,028] [INFO] [stage3.py:2251:step] Full outer step loop took 6.834218740463257
x3006c0s1b0n0: [2024-03-28 13:30:16,113] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.903013467788696
x3006c0s1b0n0: [2024-03-28 13:30:16,113] [INFO] [stage3.py:2251:step] Full outer step loop took 6.90322732925415
x3006c0s1b0n0: [2024-03-28 13:30:16,170] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.976063251495361
x3006c0s1b0n0: [2024-03-28 13:30:16,170] [INFO] [stage3.py:2251:step] Full outer step loop took 6.976254463195801
x3006c0s1b0n0: [2024-03-28 13:30:16,196] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.001682996749878
x3006c0s1b0n0: [2024-03-28 13:30:16,196] [INFO] [stage3.py:2251:step] Full outer step loop took 7.001837253570557
x3006c0s19b1n0: [2024-03-28 13:30:16,207] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.012589454650879
x3006c0s19b1n0: [2024-03-28 13:30:16,207] [INFO] [stage3.py:2251:step] Full outer step loop took 7.01293158531189
x3006c0s19b1n0: [2024-03-28 13:30:16,228] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.0335164070129395
x3006c0s19b1n0: [2024-03-28 13:30:16,228] [INFO] [stage3.py:2251:step] Full outer step loop took 7.033713340759277
x3006c0s19b1n0: [2024-03-28 13:30:16,299] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.10537576675415
x3006c0s19b1n0: [2024-03-28 13:30:16,299] [INFO] [stage3.py:2251:step] Full outer step loop took 7.10553240776062
x3006c0s19b1n0: [2024-03-28 13:30:16,310] [INFO] [stage3.py:2277:step] End to end step took 7.116330862045288
x3006c0s1b0n0: [2024-03-28 13:30:16,310] [INFO] [stage3.py:2277:step] End to end step took 7.1163458824157715
x3006c0s1b0n0: [2024-03-28 13:30:16,310] [INFO] [stage3.py:2277:step] End to end step took 7.116407871246338
x3006c0s19b1n0: [2024-03-28 13:30:16,311] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 7013.26
x3006c0s19b1n0: [2024-03-28 13:30:16,311] [INFO] [stage3.py:2277:step] End to end step took 7.116703033447266
x3006c0s19b1n0: [2024-03-28 13:30:16,311] [INFO] [stage3.py:2277:step] End to end step took 7.117027759552002
x3006c0s1b0n0: [2024-03-28 13:30:16,311] [INFO] [stage3.py:2277:step] End to end step took 7.116977691650391
x3006c0s1b0n0: [2024-03-28 13:30:16,311] [INFO] [stage3.py:2277:step] End to end step took 7.100992441177368
x3006c0s19b1n0: [2024-03-28 13:30:16,311] [INFO] [stage3.py:2277:step] End to end step took 7.117088556289673
x3006c0s19b1n0: [2024-03-28 13:30:16,311] [INFO] [logging.py:96:log_dist] [Rank 0] step=5, skipped=0, lr=[0.0001884425039850356], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:30:16,312] [INFO] [timer.py:260:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=0.9154230060098996, CurrSamplesPerSec=0.9126631703727139, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 13:30:16,312] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6866.18 | bwd_microstep: 20700.91 | bwd_inner_microstep: 20583.53 | bwd_allreduce_microstep: 117.27 | step_microstep: 7141.45
x3006c0s19b1n0: [2024-03-28 13:30:16,312] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6866.17 | bwd: 20700.91 | bwd_inner: 20583.54 | bwd_allreduce: 117.27 | step: 7141.45
x3006c0s19b1n0: [2024-03-28 13:30:16,440] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:30:16,441] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:30:16,441] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.79 GB, percent = 67.1%
x3006c0s19b1n0: <TIMER:interval-time,35.34330368041992>
x3006c0s19b1n0: <TIMER:interval-time,35.34330749511719>
x3006c0s19b1n0: <TIMER:interval-time,35.34331488609314>
x3006c0s1b0n0: <TIMER:interval-time,35.343358516693115><TIMER:interval-time,35.34335947036743><TIMER:interval-time,35.343355894088745>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s19b1n0: <TIMER:interval-time,35.343376874923706>
x3006c0s1b0n0: <TIMER:interval-time,35.34345531463623>
x3006c0s1b0n0:  elapsed_time 35.343455 | consumed samples:          160 | consumed tokens:       327680 | elapsed time per iteration (ms): 35343.5 | learning rate: 1.884E-04 | global batch size:    32 | lm loss: 1.748318E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.905 | TFLOPs: 62.65 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:30:16,585] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:30:16,586] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 13:30:16,586] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.8 GB, percent = 67.1%
x3006c0s19b1n0: [2024-03-28 13:30:23,510] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:30:23,510] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:30:23,510] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.78 GB, percent = 67.1%
x3006c0s19b1n0: [2024-03-28 13:30:23,588] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:30:23,589] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:30:23,589] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.78 GB, percent = 67.1%
x3006c0s19b1n0: [2024-03-28 13:30:44,861] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:30:44,861] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:30:44,862] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.94 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:30:44,929] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:30:44,929] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:30:44,930] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.94 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:30:51,787] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.834083080291748
x3006c0s19b1n0: [2024-03-28 13:30:51,788] [INFO] [stage3.py:2251:step] Full outer step loop took 6.8344666957855225
x3006c0s1b0n0: [2024-03-28 13:30:51,981] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.027261972427368
x3006c0s1b0n0: [2024-03-28 13:30:51,981] [INFO] [stage3.py:2251:step] Full outer step loop took 7.027819871902466
x3006c0s19b1n0: [2024-03-28 13:30:52,013] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.059920072555542
x3006c0s19b1n0: [2024-03-28 13:30:52,014] [INFO] [stage3.py:2251:step] Full outer step loop took 7.060520648956299
x3006c0s1b0n0: [2024-03-28 13:30:52,084] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.130626440048218
x3006c0s1b0n0: [2024-03-28 13:30:52,084] [INFO] [stage3.py:2251:step] Full outer step loop took 7.1309332847595215
x3006c0s19b1n0: [2024-03-28 13:30:52,099] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.145372629165649
x3006c0s19b1n0: [2024-03-28 13:30:52,099] [INFO] [stage3.py:2251:step] Full outer step loop took 7.145553112030029
x3006c0s19b1n0: [2024-03-28 13:30:52,107] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.153501749038696
x3006c0s19b1n0: [2024-03-28 13:30:52,107] [INFO] [stage3.py:2251:step] Full outer step loop took 7.153658628463745
x3006c0s1b0n0: [2024-03-28 13:30:52,124] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.17086935043335
x3006c0s1b0n0: [2024-03-28 13:30:52,124] [INFO] [stage3.py:2251:step] Full outer step loop took 7.171094655990601
x3006c0s1b0n0: [2024-03-28 13:30:52,150] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.196381330490112
x3006c0s1b0n0: [2024-03-28 13:30:52,150] [INFO] [stage3.py:2251:step] Full outer step loop took 7.196529388427734
x3006c0s1b0n0: [2024-03-28 13:30:52,161] [INFO] [stage3.py:2277:step] End to end step took 7.207401990890503
x3006c0s19b1n0: [2024-03-28 13:30:52,161] [INFO] [stage3.py:2277:step] End to end step took 7.2075793743133545
x3006c0s1b0n0: [2024-03-28 13:30:52,161] [INFO] [stage3.py:2277:step] End to end step took 7.207716941833496
x3006c0s19b1n0: [2024-03-28 13:30:52,161] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6834.90
x3006c0s19b1n0: [2024-03-28 13:30:52,161] [INFO] [stage3.py:2277:step] End to end step took 7.208017110824585
x3006c0s19b1n0: [2024-03-28 13:30:52,162] [INFO] [stage3.py:2277:step] End to end step took 7.20821475982666
x3006c0s1b0n0: [2024-03-28 13:30:52,161] [INFO] [stage3.py:2277:step] End to end step took 7.208094835281372
x3006c0s1b0n0: [2024-03-28 13:30:52,162] [INFO] [stage3.py:2277:step] End to end step took 7.208168983459473
x3006c0s19b1n0: [2024-03-28 13:30:52,162] [INFO] [stage3.py:2277:step] End to end step took 7.207796335220337
x3006c0s19b1n0: [2024-03-28 13:30:52,162] [INFO] [logging.py:96:log_dist] [Rank 0] step=6, skipped=0, lr=[0.0001415574960149644], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:30:52,162] [INFO] [timer.py:260:stop] epoch=0/micro_step=6/global_step=6, RunningAvgSamplesPerSec=0.9113906105095482, CurrSamplesPerSec=0.8995037941580938, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 13:30:52,162] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6887.09 | bwd_microstep: 21116.43 | bwd_inner_microstep: 20997.12 | bwd_allreduce_microstep: 119.20 | step_microstep: 7232.66
x3006c0s19b1n0: [2024-03-28 13:30:52,163] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6887.08 | bwd: 21116.43 | bwd_inner: 20997.13 | bwd_allreduce: 119.21 | step: 7232.66
x3006c0s19b1n0: [2024-03-28 13:30:52,281] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:30:52,281] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:30:52,281] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.96 GB, percent = 67.2%
x3006c0s19b1n0: <TIMER:interval-time,35.83973741531372><TIMER:interval-time,35.839739084243774><TIMER:interval-time,35.839733839035034>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,35.839757442474365>
x3006c0s1b0n0: <TIMER:interval-time,35.83976864814758><TIMER:interval-time,35.83976745605469>
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,35.83977031707764>
x3006c0s1b0n0: <TIMER:interval-time,35.8398699760437>
x3006c0s1b0n0:  elapsed_time 35.839770 | consumed samples:          192 | consumed tokens:       393216 | elapsed time per iteration (ms): 35839.8 | learning rate: 1.416E-04 | global batch size:    32 | lm loss: 1.497432E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.893 | TFLOPs: 61.78 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:30:52,409] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:30:52,409] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 13:30:52,409] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.85 GB, percent = 67.1%
x3006c0s19b1n0: [2024-03-28 13:30:59,338] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:30:59,339] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:30:59,339] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.84 GB, percent = 67.1%
x3006c0s19b1n0: [2024-03-28 13:30:59,422] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:30:59,423] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:30:59,423] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.84 GB, percent = 67.1%
x3006c0s19b1n0: [2024-03-28 13:31:20,279] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:31:20,280] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:31:20,280] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.83 GB, percent = 67.1%
x3006c0s19b1n0: [2024-03-28 13:31:20,352] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:31:20,352] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:31:20,352] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.83 GB, percent = 67.1%
x3006c0s1b0n0: [2024-03-28 13:31:27,210] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.833124876022339
x3006c0s1b0n0: [2024-03-28 13:31:27,210] [INFO] [stage3.py:2251:step] Full outer step loop took 6.833335876464844
x3006c0s19b1n0: [2024-03-28 13:31:27,262] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.885239124298096
x3006c0s19b1n0: [2024-03-28 13:31:27,262] [INFO] [stage3.py:2251:step] Full outer step loop took 6.885483741760254
x3006c0s1b0n0: [2024-03-28 13:31:27,410] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.03328013420105
x3006c0s1b0n0: [2024-03-28 13:31:27,410] [INFO] [stage3.py:2251:step] Full outer step loop took 7.033493280410767
x3006c0s19b1n0: [2024-03-28 13:31:27,428] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.0519044399261475
x3006c0s19b1n0: [2024-03-28 13:31:27,429] [INFO] [stage3.py:2251:step] Full outer step loop took 7.052163124084473
x3006c0s1b0n0: [2024-03-28 13:31:27,430] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.053687810897827
x3006c0s1b0n0: [2024-03-28 13:31:27,430] [INFO] [stage3.py:2251:step] Full outer step loop took 7.053898096084595
x3006c0s19b1n0: [2024-03-28 13:31:27,437] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.060668230056763
x3006c0s19b1n0: [2024-03-28 13:31:27,437] [INFO] [stage3.py:2251:step] Full outer step loop took 7.060839653015137
x3006c0s1b0n0: [2024-03-28 13:31:27,437] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.0481696128845215
x3006c0s1b0n0: [2024-03-28 13:31:27,438] [INFO] [stage3.py:2251:step] Full outer step loop took 7.048317909240723
x3006c0s19b1n0: [2024-03-28 13:31:27,441] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.064750671386719
x3006c0s19b1n0: [2024-03-28 13:31:27,441] [INFO] [stage3.py:2251:step] Full outer step loop took 7.064905405044556
x3006c0s1b0n0: [2024-03-28 13:31:27,452] [INFO] [stage3.py:2277:step] End to end step took 7.062507390975952
x3006c0s1b0n0: [2024-03-28 13:31:27,452] [INFO] [stage3.py:2277:step] End to end step took 7.075372934341431
x3006c0s19b1n0: [2024-03-28 13:31:27,452] [INFO] [stage3.py:2277:step] End to end step took 7.075417757034302
x3006c0s19b1n0: [2024-03-28 13:31:27,452] [INFO] [stage3.py:2277:step] End to end step took 7.075412750244141
x3006c0s19b1n0: [2024-03-28 13:31:27,452] [INFO] [stage3.py:2277:step] End to end step took 7.075588703155518
x3006c0s1b0n0: [2024-03-28 13:31:27,452] [INFO] [stage3.py:2277:step] End to end step took 7.075744152069092
x3006c0s19b1n0: [2024-03-28 13:31:27,452] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6885.73
x3006c0s19b1n0: [2024-03-28 13:31:27,453] [INFO] [stage3.py:2277:step] End to end step took 7.0761096477508545
x3006c0s1b0n0: [2024-03-28 13:31:27,453] [INFO] [stage3.py:2277:step] End to end step took 7.076127767562866
x3006c0s19b1n0: [2024-03-28 13:31:27,453] [INFO] [logging.py:96:log_dist] [Rank 0] step=7, skipped=0, lr=[9.750000000000001e-05], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:31:27,453] [INFO] [timer.py:260:stop] epoch=0/micro_step=7/global_step=7, RunningAvgSamplesPerSec=0.9117463564835213, CurrSamplesPerSec=0.9131721219112903, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 13:31:27,453] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6888.40 | bwd_microstep: 20701.02 | bwd_inner_microstep: 20582.90 | bwd_allreduce_microstep: 118.01 | step_microstep: 7100.66
x3006c0s19b1n0: [2024-03-28 13:31:27,453] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6888.39 | bwd: 20701.02 | bwd_inner: 20582.92 | bwd_allreduce: 118.01 | step: 7100.67
x3006c0s19b1n0: [2024-03-28 13:31:27,578] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:31:27,579] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:31:27,579] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.85 GB, percent = 67.1%
x3006c0s19b1n0: <TIMER:interval-time,35.297160625457764><TIMER:interval-time,35.29715657234192><TIMER:interval-time,35.297163009643555>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,35.297184228897095><TIMER:interval-time,35.297184228897095><TIMER:interval-time,35.29718208312988><TIMER:interval-time,35.29718351364136>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s19b1n0: <TIMER:interval-time,35.2972149848938>
x3006c0s1b0n0:  elapsed_time 35.297184 | consumed samples:          224 | consumed tokens:       458752 | elapsed time per iteration (ms): 35297.2 | learning rate: 9.750E-05 | global batch size:    32 | lm loss: 1.319906E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.907 | TFLOPs: 62.73 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:31:27,710] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:31:27,711] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 13:31:27,711] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.86 GB, percent = 67.1%
x3006c0s19b1n0: [2024-03-28 13:31:34,747] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:31:34,747] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:31:34,748] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.84 GB, percent = 67.1%
x3006c0s19b1n0: [2024-03-28 13:31:34,838] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:31:34,839] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:31:34,839] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.84 GB, percent = 67.1%
x3006c0s19b1n0: [2024-03-28 13:31:55,812] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:31:55,813] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:31:55,813] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.94 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:31:55,882] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:31:55,882] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:31:55,882] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.94 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:32:02,736] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.829251527786255
x3006c0s19b1n0: [2024-03-28 13:32:02,736] [INFO] [stage3.py:2251:step] Full outer step loop took 6.8296799659729
x3006c0s1b0n0: [2024-03-28 13:32:02,782] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.875092029571533
x3006c0s1b0n0: [2024-03-28 13:32:02,783] [INFO] [stage3.py:2251:step] Full outer step loop took 6.876699924468994
x3006c0s1b0n0: [2024-03-28 13:32:02,918] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.011904239654541
x3006c0s1b0n0: [2024-03-28 13:32:02,918] [INFO] [stage3.py:2251:step] Full outer step loop took 7.012186288833618
x3006c0s1b0n0: [2024-03-28 13:32:02,938] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.0321855545043945
x3006c0s1b0n0: [2024-03-28 13:32:02,939] [INFO] [stage3.py:2251:step] Full outer step loop took 7.032425880432129
x3006c0s1b0n0: [2024-03-28 13:32:02,969] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.06303071975708
x3006c0s1b0n0: [2024-03-28 13:32:02,969] [INFO] [stage3.py:2251:step] Full outer step loop took 7.063194990158081
x3006c0s19b1n0: [2024-03-28 13:32:03,106] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.199518442153931
x3006c0s19b1n0: [2024-03-28 13:32:03,106] [INFO] [stage3.py:2251:step] Full outer step loop took 7.199728488922119
x3006c0s19b1n0: [2024-03-28 13:32:03,121] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.214664936065674
x3006c0s19b1n0: [2024-03-28 13:32:03,121] [INFO] [stage3.py:2251:step] Full outer step loop took 7.214859485626221
x3006c0s19b1n0: [2024-03-28 13:32:03,123] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.1967103481292725
x3006c0s19b1n0: [2024-03-28 13:32:03,123] [INFO] [stage3.py:2251:step] Full outer step loop took 7.196847915649414
x3006c0s19b1n0: [2024-03-28 13:32:03,134] [INFO] [stage3.py:2277:step] End to end step took 7.207536220550537
x3006c0s1b0n0: [2024-03-28 13:32:03,134] [INFO] [stage3.py:2277:step] End to end step took 7.227304697036743
x3006c0s19b1n0: [2024-03-28 13:32:03,134] [INFO] [stage3.py:2277:step] End to end step took 7.227416276931763
x3006c0s1b0n0: [2024-03-28 13:32:03,134] [INFO] [stage3.py:2277:step] End to end step took 7.227427244186401
x3006c0s19b1n0: [2024-03-28 13:32:03,134] [INFO] [stage3.py:2277:step] End to end step took 7.227482795715332
x3006c0s19b1n0: [2024-03-28 13:32:03,134] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6829.98
x3006c0s19b1n0: [2024-03-28 13:32:03,134] [INFO] [stage3.py:2277:step] End to end step took 7.227968215942383
x3006c0s1b0n0: [2024-03-28 13:32:03,134] [INFO] [stage3.py:2277:step] End to end step took 7.227874517440796
x3006c0s1b0n0: [2024-03-28 13:32:03,134] [INFO] [stage3.py:2277:step] End to end step took 7.227883815765381
x3006c0s19b1n0: [2024-03-28 13:32:03,135] [INFO] [logging.py:96:log_dist] [Rank 0] step=8, skipped=0, lr=[6.158400017893797e-05], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:32:03,135] [INFO] [timer.py:260:stop] epoch=0/micro_step=8/global_step=8, RunningAvgSamplesPerSec=0.9103408549660171, CurrSamplesPerSec=0.9033778496872676, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 13:32:03,135] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6998.65 | bwd_microstep: 20812.87 | bwd_inner_microstep: 20706.12 | bwd_allreduce_microstep: 106.62 | step_microstep: 7252.57
x3006c0s19b1n0: [2024-03-28 13:32:03,135] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6998.64 | bwd: 20812.87 | bwd_inner: 20706.14 | bwd_allreduce: 106.64 | step: 7252.57
x3006c0s19b1n0: [2024-03-28 13:32:03,266] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:32:03,266] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:32:03,266] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.96 GB, percent = 67.2%
x3006c0s19b1n0: <TIMER:interval-time,35.687411069869995>
x3006c0s19b1n0: <TIMER:interval-time,35.687434911727905>
x3006c0s1b0n0: <TIMER:interval-time,35.68746852874756><TIMER:interval-time,35.68746852874756><TIMER:interval-time,35.687469720840454>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,35.687477111816406>
x3006c0s19b1n0: <TIMER:interval-time,35.68747878074646>
x3006c0s19b1n0: <TIMER:interval-time,35.687522411346436>
x3006c0s1b0n0:  elapsed_time 35.687477 | consumed samples:          256 | consumed tokens:       524288 | elapsed time per iteration (ms): 35687.5 | learning rate: 6.158E-05 | global batch size:    32 | lm loss: 1.228736E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.897 | TFLOPs: 62.04 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:32:03,409] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:32:03,410] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 13:32:03,410] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.97 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:32:10,139] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:32:10,140] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:32:10,140] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.97 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:32:10,230] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:32:10,230] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:32:10,231] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.97 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:32:31,108] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:32:31,108] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:32:31,108] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.95 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:32:31,181] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:32:31,182] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:32:31,182] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.95 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:32:37,892] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.686531066894531
x3006c0s19b1n0: [2024-03-28 13:32:37,892] [INFO] [stage3.py:2251:step] Full outer step loop took 6.686713457107544
x3006c0s1b0n0: [2024-03-28 13:32:37,955] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.749828338623047
x3006c0s1b0n0: [2024-03-28 13:32:37,956] [INFO] [stage3.py:2251:step] Full outer step loop took 6.750565052032471
x3006c0s1b0n0: [2024-03-28 13:32:38,048] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.843335151672363
x3006c0s1b0n0: [2024-03-28 13:32:38,049] [INFO] [stage3.py:2251:step] Full outer step loop took 6.843610525131226
x3006c0s1b0n0: [2024-03-28 13:32:38,124] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.919394254684448
x3006c0s1b0n0: [2024-03-28 13:32:38,125] [INFO] [stage3.py:2251:step] Full outer step loop took 6.919668436050415
x3006c0s1b0n0: [2024-03-28 13:32:38,210] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.004822015762329
x3006c0s1b0n0: [2024-03-28 13:32:38,210] [INFO] [stage3.py:2251:step] Full outer step loop took 7.004966735839844
x3006c0s19b1n0: [2024-03-28 13:32:38,322] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.116734981536865
x3006c0s19b1n0: [2024-03-28 13:32:38,322] [INFO] [stage3.py:2251:step] Full outer step loop took 7.117198705673218
x3006c0s19b1n0: [2024-03-28 13:32:38,340] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.133924722671509
x3006c0s19b1n0: [2024-03-28 13:32:38,340] [INFO] [stage3.py:2251:step] Full outer step loop took 7.134076118469238
x3006c0s19b1n0: [2024-03-28 13:32:38,340] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.134664535522461
x3006c0s19b1n0: [2024-03-28 13:32:38,340] [INFO] [stage3.py:2251:step] Full outer step loop took 7.134822130203247
x3006c0s19b1n0: [2024-03-28 13:32:38,352] [INFO] [stage3.py:2277:step] End to end step took 7.147358655929565
x3006c0s19b1n0: [2024-03-28 13:32:38,352] [INFO] [stage3.py:2277:step] End to end step took 7.146871328353882
x3006c0s1b0n0: [2024-03-28 13:32:38,352] [INFO] [stage3.py:2277:step] End to end step took 7.147346019744873
x3006c0s19b1n0: [2024-03-28 13:32:38,353] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6686.89
x3006c0s19b1n0: [2024-03-28 13:32:38,353] [INFO] [stage3.py:2277:step] End to end step took 7.147906064987183
x3006c0s1b0n0: [2024-03-28 13:32:38,353] [INFO] [stage3.py:2277:step] End to end step took 7.147854804992676
x3006c0s1b0n0: [2024-03-28 13:32:38,353] [INFO] [stage3.py:2277:step] End to end step took 7.147967338562012
x3006c0s19b1n0: [2024-03-28 13:32:38,353] [INFO] [stage3.py:2277:step] End to end step took 7.1480629444122314
x3006c0s1b0n0: [2024-03-28 13:32:38,353] [INFO] [stage3.py:2277:step] End to end step took 7.148058176040649
x3006c0s19b1n0: [2024-03-28 13:32:38,353] [INFO] [logging.py:96:log_dist] [Rank 0] step=9, skipped=0, lr=[3.814149619390238e-05], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:32:38,354] [INFO] [timer.py:260:stop] epoch=0/micro_step=9/global_step=9, RunningAvgSamplesPerSec=0.9111163315644278, CurrSamplesPerSec=0.9157970785578669, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 13:32:38,354] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6690.51 | bwd_microstep: 20714.59 | bwd_inner_microstep: 20591.36 | bwd_allreduce_microstep: 123.10 | step_microstep: 7172.05
x3006c0s19b1n0: [2024-03-28 13:32:38,354] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6690.50 | bwd: 20714.58 | bwd_inner: 20591.38 | bwd_allreduce: 123.10 | step: 7172.05
x3006c0s19b1n0: [2024-03-28 13:32:38,471] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:32:38,471] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:32:38,471] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.97 GB, percent = 67.2%
x3006c0s19b1n0: <TIMER:interval-time,35.2046275138855>
x3006c0s19b1n0: <TIMER:interval-time,35.204628705978394><TIMER:interval-time,35.20462989807129>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,35.20462489128113>
x3006c0s1b0n0: <TIMER:interval-time,35.20441007614136><TIMER:interval-time,35.204410791397095><TIMER:interval-time,35.20441222190857>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,35.20442247390747>
x3006c0s1b0n0:  elapsed_time 35.204422 | consumed samples:          288 | consumed tokens:       589824 | elapsed time per iteration (ms): 35204.4 | learning rate: 3.814E-05 | global batch size:    32 | lm loss: 1.242802E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.909 | TFLOPs: 62.89 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:32:38,595] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:32:38,596] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 13:32:38,596] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.92 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:32:45,436] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:32:45,437] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:32:45,437] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.9 GB, percent = 67.1%
x3006c0s19b1n0: [2024-03-28 13:32:45,520] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:32:45,520] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:32:45,520] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.9 GB, percent = 67.1%
x3006c0s19b1n0: [2024-03-28 13:33:06,175] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:33:06,175] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:33:06,175] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.92 GB, percent = 67.2%
x3006c0s19b1n0: [2024-03-28 13:33:06,245] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:33:06,245] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:33:06,245] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.92 GB, percent = 67.2%
x3006c0s1b0n0: [2024-03-28 13:33:12,813] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.5428147315979
x3006c0s1b0n0: [2024-03-28 13:33:12,815] [INFO] [stage3.py:2251:step] Full outer step loop took 6.545382738113403
x3006c0s1b0n0: [2024-03-28 13:33:13,207] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.9379563331604
x3006c0s1b0n0: [2024-03-28 13:33:13,207] [INFO] [stage3.py:2251:step] Full outer step loop took 6.938211917877197
x3006c0s19b1n0: [2024-03-28 13:33:13,257] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.988274574279785
x3006c0s19b1n0: [2024-03-28 13:33:13,258] [INFO] [stage3.py:2251:step] Full outer step loop took 6.988776922225952
x3006c0s19b1n0: [2024-03-28 13:33:13,315] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.04573392868042
x3006c0s19b1n0: [2024-03-28 13:33:13,315] [INFO] [stage3.py:2251:step] Full outer step loop took 7.046107053756714
x3006c0s1b0n0: [2024-03-28 13:33:13,330] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.044682025909424
x3006c0s1b0n0: [2024-03-28 13:33:13,331] [INFO] [stage3.py:2251:step] Full outer step loop took 7.044950008392334
x3006c0s19b1n0: [2024-03-28 13:33:13,386] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.116381645202637
x3006c0s19b1n0: [2024-03-28 13:33:13,386] [INFO] [stage3.py:2251:step] Full outer step loop took 7.116570472717285
x3006c0s1b0n0: [2024-03-28 13:33:13,408] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.138401508331299
x3006c0s1b0n0: [2024-03-28 13:33:13,408] [INFO] [stage3.py:2251:step] Full outer step loop took 7.138548851013184
x3006c0s19b1n0: [2024-03-28 13:33:13,463] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.194228649139404
x3006c0s19b1n0: [2024-03-28 13:33:13,464] [INFO] [stage3.py:2251:step] Full outer step loop took 7.194386959075928
x3006c0s19b1n0: [2024-03-28 13:33:13,474] [INFO] [stage3.py:2277:step] End to end step took 7.205122709274292
x3006c0s1b0n0: [2024-03-28 13:33:13,475] [INFO] [stage3.py:2277:step] End to end step took 7.205374002456665
x3006c0s19b1n0: [2024-03-28 13:33:13,474] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6989.09
x3006c0s19b1n0: [2024-03-28 13:33:13,475] [INFO] [stage3.py:2277:step] End to end step took 7.205383062362671
x3006c0s19b1n0: [2024-03-28 13:33:13,475] [INFO] [stage3.py:2277:step] End to end step took 7.205687046051025
x3006c0s1b0n0: [2024-03-28 13:33:13,475] [INFO] [stage3.py:2277:step] End to end step took 7.205632209777832
x3006c0s1b0n0: [2024-03-28 13:33:13,475] [INFO] [stage3.py:2277:step] End to end step took 7.205733776092529
x3006c0s19b1n0: [2024-03-28 13:33:13,475] [INFO] [stage3.py:2277:step] End to end step took 7.205771446228027
x3006c0s1b0n0: [2024-03-28 13:33:13,475] [INFO] [stage3.py:2277:step] End to end step took 7.189282655715942
x3006c0s19b1n0: [2024-03-28 13:33:13,475] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[3e-05], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:33:13,476] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=0.9119062077514299, CurrSamplesPerSec=0.9174739222660445, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 13:33:13,476] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6802.71 | bwd_microstep: 20500.42 | bwd_inner_microstep: 20386.64 | bwd_allreduce_microstep: 113.67 | step_microstep: 7230.10
x3006c0s19b1n0: [2024-03-28 13:33:13,476] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6802.70 | bwd: 20500.42 | bwd_inner: 20386.66 | bwd_allreduce: 113.67 | step: 7230.11
x3006c0s19b1n0: [2024-03-28 13:33:13,590] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:33:13,591] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:33:13,591] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 337.94 GB, percent = 67.2%
x3006c0s19b1n0: <TIMER:interval-time,35.11931538581848><TIMER:interval-time,35.119311809539795><TIMER:interval-time,35.11931347846985><TIMER:interval-time,35.1193151473999>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,35.119346618652344>
x3006c0s1b0n0: <TIMER:interval-time,35.11935067176819><TIMER:interval-time,35.11935210227966><TIMER:interval-time,35.11935043334961>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0:  elapsed_time 35.119352 | consumed samples:          320 | consumed tokens:       655360 | elapsed time per iteration (ms): 35119.4 | learning rate: 3.000E-05 | global batch size:    32 | lm loss: 1.122073E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.911 | TFLOPs: 63.05 |
x3006c0s1b0n0: <<<only_train:365.5307478904724>>>
x3006c0s19b1n0: <<<only_train:365.5309271812439>>>
x3006c0s19b1n0: <<<only_train:365.53100967407227>>>
x3006c0s19b1n0: <<<only_train:365.53093218803406>>>
x3006c0s1b0n0: <<<only_train:365.5307216644287>>><<<only_train:365.53091526031494>>>
x3006c0s1b0n0: 
x3006c0s1b0n0: <<<only_train:365.5308210849762>>>
x3006c0s19b1n0: <<<only_train:365.5307536125183>>>
x3006c0s19b1n0: [after training ends] datetime: 2024-03-28 13:33:13 
x3006c0s19b1n0: <<<full_time:365.53115034103394>>><<<full_time:365.53123354911804>>><<<full_time:365.53094840049744>>>
x3006c0s19b1n0: <<<full_time:365.531188249588>>>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b0n0: <<<full_time:365.53106021881104>>><<<full_time:365.53106451034546>>>
x3006c0s1b0n0: <<<full_time:365.5311782360077>>>
x3006c0s1b0n0: 
x3006c0s1b0n0: <<<full_time:365.53099155426025>>>
x3006c0s19b1n0: [2024-03-28 13:33:17,112] [INFO] [launch.py:348:main] Process 12548 exits successfully.
x3006c0s1b0n0: [2024-03-28 13:33:17,590] [INFO] [launch.py:348:main] Process 5621 exits successfully.
x3006c0s19b1n0: [2024-03-28 13:33:20,115] [INFO] [launch.py:348:main] Process 12549 exits successfully.
x3006c0s19b1n0: [2024-03-28 13:33:20,115] [INFO] [launch.py:348:main] Process 12547 exits successfully.
x3006c0s19b1n0: [2024-03-28 13:33:20,116] [INFO] [launch.py:348:main] Process 12550 exits successfully.
x3006c0s1b0n0: [2024-03-28 13:33:20,593] [INFO] [launch.py:348:main] Process 5620 exits successfully.
x3006c0s1b0n0: [2024-03-28 13:33:20,593] [INFO] [launch.py:348:main] Process 5619 exits successfully.
x3006c0s1b0n0: [2024-03-28 13:33:20,593] [INFO] [launch.py:348:main] Process 5622 exits successfully.

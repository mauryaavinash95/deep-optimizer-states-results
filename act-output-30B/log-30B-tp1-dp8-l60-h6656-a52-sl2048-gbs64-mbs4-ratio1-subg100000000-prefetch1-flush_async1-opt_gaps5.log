[2024-03-28 13:40:09,741] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-28 13:40:12,983] [INFO] [runner.py:463:main] Using IP address of 10.140.57.107 for node x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-28 13:40:12,985] [INFO] [runner.py:557:main] deepspeed_env file = ./.deepspeed_env
[2024-03-28 13:40:12,985] [INFO] [multinode_runner.py:80:get_cmd] Running on the following workers: x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov,x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-28 13:40:12,985] [INFO] [runner.py:568:main] cmd = pdsh -S -f 1024 -w x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov,x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov export PYTHONUSERBASE=/home/am6429/.local/polaris/conda/2023-10-04; export PYTHONPATH=/home/am6429/dl-io/Megatron-DeepSpeed; export PATH=/home/am6429/.conda/envs/dspeed_env/bin:/soft/datascience/conda/2023-10-04/mconda3/condabin:/soft/compilers/cudatoolkit/cuda-11.8.0/bin:/soft/buildtools/cmake/cmake-3.23.2/cmake-3.23.2-linux-x86_64/bin:/opt/cray/pe/gcc/11.2.0/bin:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/include:/opt/cray/pe/pals/1.2.11/bin:/opt/cray/pe/craype/2.7.20/bin:/opt/cray/libfabric/1.15.2.0/bin:/home/am6429/.conda/envs/dspeed_env/bin:/opt/cray/pe/perftools/23.03.0/bin:/opt/cray/pe/papi/7.0.0.1/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/am6429/.local/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/am6429/veloc-build/include:/home/am6429/veloc-build/bin:/soft/datascience/conda/2023-01-10/mconda3/include:/opt/cray/pe/bin:/soft/datascience/conda/2023-01-10/mconda3/include; export LD_LIBRARY_PATH=/usr/lib64/:/soft/compilers/cudatoolkit/cuda-11.8.0/lib64:/opt/cray/pe/gcc/11.2.0/snos/lib64:/soft/compilers/cudatoolkit/cuda-11.8.0/extras/CUPTI/lib64:/soft/libraries/trt/TensorRT-8.5.3.1.Linux.x86_64-gnu.cuda-11.8.cudnn8.6/lib:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/lib:/soft/libraries/cudnn/cudnn-11-linux-x64-v8.6.0.163/lib:/opt/cray/libfabric/1.15.2.0/lib64:/usr/lib64:/opt/cray/pe/papi/7.0.0.1/lib64:/home/am6429/veloc-build/lib:/home/am6429/veloc-build/lib64:/home/am6429/nvcomp/lib:/soft/datascience/conda/2023-01-10/mconda3/lib:/soft/datascience/conda/2023-01-10/mconda3/lib/; export http_proxy=http://proxy.alcf.anl.gov:3128; export https_proxy=http://proxy.alcf.anl.gov:3128; export CC=gcc; export CXX=g++; export IBV_FORK_SAFE=1; export CFLAGS=-I/soft/datascience/conda/2023-01-10/mconda3/include/; export LDFLAGS=-L/soft/datascience/conda/2023-01-10/mconda3/lib/; export CUDA_DEVICE_MAX_CONNECTIONS=1; export TORCHSNAPSHOT_PER_RANK_MEMORY_BUDGET_BYTES=34359738368; export _DEFAULT_MAX_PER_RANK_IO_CONCURRENCY=1; export _MAX_PER_RANK_IO_CONCURRENCY=1; export NSYS_REPORT_DIR=/home/am6429/dl-io/dl-io-outputs/act-output-30B//rep-30B-tp1-dp8-l60-h6656-a52-sl2048-gbs64-mbs4-ratio1-subg100000000-prefetch1-flush_async1-opt_gaps5-%n;  cd /home/am6429/dl-io/Megatron-DeepSpeed; /home/am6429/.conda/envs/dspeed_env/bin/python -u -m deepspeed.launcher.launch --world_info=eyJ4MzAwNmMwczE5YjFuMC5oc24uY20ucG9sYXJpcy5hbGNmLmFubC5nb3YiOiBbMCwgMSwgMiwgM10sICJ4MzAwNmMwczFiMG4wLmhzbi5jbS5wb2xhcmlzLmFsY2YuYW5sLmdvdiI6IFswLCAxLCAyLCAzXX0= --node_rank=%n --master_addr=10.140.57.107 --master_port=29700 /home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py --tensor-model-parallel-size 1 --num-layers 60 --hidden-size 6656 --num-attention-heads 52 --micro-batch-size 4 --global-batch-size 64 --seq-length 2048 --max-position-embeddings 2048 --train-iters 10 --save /local/scratch/llama2/zero3-tp1}_dp8 --data-path /home/am6429/dl-io/datasets/meg-gpt2_text_document --vocab-file /home/am6429/dl-io/datasets/gpt2-vocab.json --merge-file /home/am6429/dl-io/datasets/gpt2-merges.txt --data-impl mmap --tokenizer-type GPTSentencePieceTokenizer --tokenizer-model /home/am6429/dl-io/datasets/tokenizer.model --split 949,50,1 --distributed-backend nccl --lr 3e-4 --lr-decay-style cosine --min-lr 3e-5 --weight-decay 0.1 --clip-grad 1 --lr-warmup-iters 1 --optimizer adam --adam-beta1 0.9 --adam-beta2 0.95 --log-interval 1 --save-interval 1000 --eval-interval 1000 --eval-iters 0 --bf16 --no-query-key-layer-scaling --attention-dropout 0 --hidden-dropout 0 --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization rmsnorm --disable-bias-linear --num-key-value-heads 4 --deepspeed --exit-interval 20 --deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json --zero-stage=3 --no-pipeline-parallel --cpu-optimizer --checkpoint-activations --deepspeed-activation-checkpointing
x3006c0s19b1n0: [2024-03-28 13:40:15,010] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b0n0: [2024-03-28 13:40:15,015] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-28 13:40:16,840] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3006c0s19b1n0: [2024-03-28 13:40:16,840] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=0
x3006c0s19b1n0: [2024-03-28 13:40:16,840] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3006c0s19b1n0: [2024-03-28 13:40:16,840] [INFO] [launch.py:163:main] dist_world_size=8
x3006c0s19b1n0: [2024-03-28 13:40:16,840] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3006c0s19b1n0: [2024-03-28 13:40:16,841] [INFO] [launch.py:253:main] process 19490 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-28 13:40:16,842] [INFO] [launch.py:253:main] process 19491 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-28 13:40:16,842] [INFO] [launch.py:253:main] process 19492 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-28 13:40:16,843] [INFO] [launch.py:253:main] process 19493 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b0n0: [2024-03-28 13:40:17,317] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3006c0s1b0n0: [2024-03-28 13:40:17,317] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=1
x3006c0s1b0n0: [2024-03-28 13:40:17,317] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3006c0s1b0n0: [2024-03-28 13:40:17,318] [INFO] [launch.py:163:main] dist_world_size=8
x3006c0s1b0n0: [2024-03-28 13:40:17,318] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3006c0s1b0n0: [2024-03-28 13:40:17,318] [INFO] [launch.py:253:main] process 11655 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b0n0: [2024-03-28 13:40:17,319] [INFO] [launch.py:253:main] process 11656 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b0n0: [2024-03-28 13:40:17,319] [INFO] [launch.py:253:main] process 11657 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b0n0: [2024-03-28 13:40:17,320] [INFO] [launch.py:253:main] process 11658 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-28 13:40:18,657] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-28 13:40:18,659] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-28 13:40:18,667] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-28 13:40:18,673] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b0n0: [2024-03-28 13:40:19,122] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b0n0: [2024-03-28 13:40:19,137] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b0n0: [2024-03-28 13:40:19,139] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b0n0: [2024-03-28 13:40:19,145] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [2024-03-28 13:40:21,600] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: [2024-03-28 13:40:21,631] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: INFO: overriding default arguments for tokenizer_type:GPTSentencePieceTokenizer                    with tokenizer_type:GPT2BPETokenizer
x3006c0s19b1n0: using world size: 8, data-parallel-size: 8, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
x3006c0s19b1n0: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
x3006c0s19b1n0: using torch.bfloat16 for parameters ...
x3006c0s19b1n0: ------------------------ arguments ------------------------
x3006c0s19b1n0:   accumulate_allreduce_grads_in_fp32 .............. True
x3006c0s19b1n0:   adam_beta1 ...................................... 0.9
x3006c0s19b1n0:   adam_beta2 ...................................... 0.95
x3006c0s19b1n0:   adam_eps ........................................ 1e-08
x3006c0s19b1n0:   add_bias_linear ................................. False
x3006c0s19b1n0:   add_position_embedding .......................... False
x3006c0s19b1n0:   adlr_autoresume ................................. False
x3006c0s19b1n0:   adlr_autoresume_interval ........................ 1000
x3006c0s19b1n0:   aml_data_download_path .......................... None
x3006c0s19b1n0:   apply_layernorm_1p .............................. False
x3006c0s19b1n0:   apply_query_key_layer_scaling ................... False
x3006c0s19b1n0:   apply_residual_connection_post_layernorm ........ False
x3006c0s19b1n0:   async_tensor_model_parallel_allreduce ........... False
x3006c0s19b1n0:   attention_dropout ............................... 0.0
x3006c0s19b1n0:   attention_softmax_in_fp32 ....................... False
x3006c0s19b1n0:   barrier_with_L1_time ............................ True
x3006c0s19b1n0:   bert_binary_head ................................ True
x3006c0s19b1n0:   bert_embedder_type .............................. megatron
x3006c0s19b1n0:   bert_load ....................................... None
x3006c0s19b1n0:   bf16 ............................................ True
x3006c0s19b1n0:   bias_dropout_fusion ............................. True
x3006c0s19b1n0:   bias_gelu_fusion ................................ False
x3006c0s19b1n0:   biencoder_projection_dim ........................ 0
x3006c0s19b1n0:   biencoder_shared_query_context_model ............ False
x3006c0s19b1n0:   block_data_path ................................. None
x3006c0s19b1n0:   checkpoint_activations .......................... True
x3006c0s19b1n0:   checkpoint_in_cpu ............................... False
x3006c0s19b1n0:   checkpoint_num_layers ........................... 1
x3006c0s19b1n0:   classes_fraction ................................ 1.0
x3006c0s19b1n0:   clip_grad ....................................... 1.0
x3006c0s19b1n0:   compression_training ............................ False
x3006c0s19b1n0:   consumed_train_samples .......................... 0
x3006c0s19b1n0:   consumed_train_tokens ........................... 0
x3006c0s19b1n0:   consumed_valid_samples .......................... 0
x3006c0s19b1n0:   contigious_checkpointing ........................ False
x3006c0s19b1n0:   cpu_optimizer ................................... True
x3006c0s19b1n0:   cpu_torch_adam .................................. False
x3006c0s19b1n0:   create_moe_param_group .......................... False
x3006c0s19b1n0:   curriculum_learning_legacy ...................... False
x3006c0s19b1n0:   data_cache_path ................................. None
x3006c0s19b1n0:   data_efficiency_curriculum_learning ............. False
x3006c0s19b1n0:   data_impl ....................................... mmap
x3006c0s19b1n0:   data_parallel_random_init ....................... False
x3006c0s19b1n0:   data_parallel_size .............................. 8
x3006c0s19b1n0:   data_path ....................................... ['/home/am6429/dl-io/datasets/meg-gpt2_text_document']
x3006c0s19b1n0:   data_per_class_fraction ......................... 1.0
x3006c0s19b1n0:   data_sharding ................................... True
x3006c0s19b1n0:   dataloader_type ................................. single
x3006c0s19b1n0:   DDP_impl ........................................ local
x3006c0s19b1n0:   decoder_num_layers .............................. None
x3006c0s19b1n0:   decoder_seq_length .............................. None
x3006c0s19b1n0:   deepscale ....................................... False
x3006c0s19b1n0:   deepscale_config ................................ None
x3006c0s19b1n0:   deepspeed ....................................... True
x3006c0s19b1n0:   deepspeed_activation_checkpointing .............. True
x3006c0s19b1n0:   deepspeed_config ................................ /home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json
x3006c0s19b1n0:   dino_bottleneck_size ............................ 256
x3006c0s19b1n0:   dino_freeze_last_layer .......................... 1
x3006c0s19b1n0:   dino_head_hidden_size ........................... 2048
x3006c0s19b1n0:   dino_local_crops_number ......................... 10
x3006c0s19b1n0:   dino_local_img_size ............................. 96
x3006c0s19b1n0:   dino_norm_last_layer ............................ False
x3006c0s19b1n0:   dino_teacher_temp ............................... 0.07
x3006c0s19b1n0:   dino_warmup_teacher_temp ........................ 0.04
x3006c0s19b1n0:   dino_warmup_teacher_temp_epochs ................. 30
x3006c0s19b1n0:   distribute_checkpointed_activations ............. False
x3006c0s19b1n0:   distribute_saved_activations .................... False
x3006c0s19b1n0:   distributed_backend ............................. nccl
x3006c0s19b1n0:   distributed_timeout_minutes ..................... 10
x3006c0s19b1n0:   ds_inference .................................... False
x3006c0s19b1n0:   ds_pipeline_enabled ............................. False
x3006c0s19b1n0:   ds_sequence_parallel_size ....................... 1
x3006c0s19b1n0:   embedding_path .................................. None
x3006c0s19b1n0:   embedding_weights_in_fp32 ....................... False
x3006c0s19b1n0:   empty_unused_memory_level ....................... 0
x3006c0s19b1n0:   enable_expert_tensor_parallelism ................ False
x3006c0s19b1n0:   encoder_num_layers .............................. 60
x3006c0s19b1n0:   encoder_seq_length .............................. 2048
x3006c0s19b1n0:   end_weight_decay ................................ 0.1
x3006c0s19b1n0:   eod_mask_loss ................................... False
x3006c0s19b1n0:   eval_interval ................................... 1000
x3006c0s19b1n0:   eval_iters ...................................... 0
x3006c0s19b1n0:   evidence_data_path .............................. None
x3006c0s19b1n0:   exit_duration_in_mins ........................... None
x3006c0s19b1n0:   exit_interval ................................... 20
x3006c0s19b1n0:   exit_on_missing_checkpoint ...................... False
x3006c0s19b1n0:   exit_signal_handler ............................. False
x3006c0s19b1n0:   expert_interval ................................. 2
x3006c0s19b1n0:   ffn_hidden_size ................................. 17728
x3006c0s19b1n0:   finetune ........................................ False
x3006c0s19b1n0:   force_ds_sequence_parallel ...................... False
x3006c0s19b1n0:   fp16 ............................................ False
x3006c0s19b1n0:   fp16_lm_cross_entropy ........................... False
x3006c0s19b1n0:   fp32_residual_connection ........................ False
x3006c0s19b1n0:   fp8_amax_compute_algo ........................... most_recent
x3006c0s19b1n0:   fp8_amax_history_len ............................ 1
x3006c0s19b1n0:   fp8_e4m3 ........................................ False
x3006c0s19b1n0:   fp8_hybrid ...................................... False
x3006c0s19b1n0:   fp8_interval .................................... 1
x3006c0s19b1n0:   fp8_margin ...................................... 0
x3006c0s19b1n0:   fp8_wgrad ....................................... True
x3006c0s19b1n0:   global_batch_size ............................... 64
x3006c0s19b1n0:   gradient_accumulation_fusion .................... True
x3006c0s19b1n0:   head_lr_mult .................................... 1.0
x3006c0s19b1n0:   hidden_dropout .................................. 0.0
x3006c0s19b1n0:   hidden_size ..................................... 6656
x3006c0s19b1n0:   hidden_size_teacher ............................. None
x3006c0s19b1n0:   hysteresis ...................................... 2
x3006c0s19b1n0:   ict_head_size ................................... None
x3006c0s19b1n0:   ict_load ........................................ None
x3006c0s19b1n0:   img_h ........................................... 224
x3006c0s19b1n0:   img_w ........................................... 224
x3006c0s19b1n0:   indexer_batch_size .............................. 128
x3006c0s19b1n0:   indexer_log_interval ............................ 1000
x3006c0s19b1n0:   inference ....................................... False
x3006c0s19b1n0:   inference_batch_times_seqlen_threshold .......... 512
x3006c0s19b1n0:   init_method_std ................................. 0.02
x3006c0s19b1n0:   init_method_xavier_uniform ...................... False
x3006c0s19b1n0:   initial_loss_scale .............................. 4294967296
x3006c0s19b1n0:   iter_per_epoch .................................. 1250
x3006c0s19b1n0:   kd .............................................. False
x3006c0s19b1n0:   kd_alpha_ce ..................................... 1
x3006c0s19b1n0:   kd_beta_ce ...................................... 1
x3006c0s19b1n0:   kd_temp ......................................... 1.0
x3006c0s19b1n0:   kv_channels ..................................... 128
x3006c0s19b1n0:   layernorm_epsilon ............................... 1e-05
x3006c0s19b1n0:   lazy_mpu_init ................................... None
x3006c0s19b1n0:   load ............................................ None
x3006c0s19b1n0:   load_teacher .................................... None
x3006c0s19b1n0:   local_rank ...................................... 0
x3006c0s19b1n0:   log_batch_size_to_tensorboard ................... False
x3006c0s19b1n0:   log_interval .................................... 1
x3006c0s19b1n0:   log_learning_rate_to_tensorboard ................ True
x3006c0s19b1n0:   log_loss_scale_to_tensorboard ................... True
x3006c0s19b1n0:   log_memory_to_tensorboard ....................... False
x3006c0s19b1n0:   log_num_zeros_in_grad ........................... False
x3006c0s19b1n0:   log_optimizer_states_to_tensorboard ............. False
x3006c0s19b1n0:   log_params_norm ................................. False
x3006c0s19b1n0:   log_timers_to_tensorboard ....................... False
x3006c0s19b1n0:   log_validation_ppl_to_tensorboard ............... False
x3006c0s19b1n0:   log_world_size_to_tensorboard ................... False
x3006c0s19b1n0:   loss_scale ...................................... None
x3006c0s19b1n0:   loss_scale_window ............................... 1000
x3006c0s19b1n0:   lr .............................................. 0.0003
x3006c0s19b1n0:   lr_decay_iters .................................. None
x3006c0s19b1n0:   lr_decay_samples ................................ None
x3006c0s19b1n0:   lr_decay_style .................................. cosine
x3006c0s19b1n0:   lr_decay_tokens ................................. None
x3006c0s19b1n0:   lr_warmup_fraction .............................. None
x3006c0s19b1n0:   lr_warmup_iters ................................. 1
x3006c0s19b1n0:   lr_warmup_samples ............................... 0
x3006c0s19b1n0:   lr_warmup_tokens ................................ None
x3006c0s19b1n0:   make_vocab_size_divisible_by .................... 128
x3006c0s19b1n0:   mask_factor ..................................... 1.0
x3006c0s19b1n0:   mask_prob ....................................... 0.15
x3006c0s19b1n0:   mask_type ....................................... random
x3006c0s19b1n0:   masked_softmax_fusion ........................... True
x3006c0s19b1n0:   max_position_embeddings ......................... 2048
x3006c0s19b1n0:   max_tokens_to_oom ............................... 12000
x3006c0s19b1n0:   memory_centric_tiled_linear ..................... False
x3006c0s19b1n0:   merge_file ...................................... /home/am6429/dl-io/datasets/gpt2-merges.txt
x3006c0s19b1n0:   micro_batch_size ................................ 4
x3006c0s19b1n0:   min_loss_scale .................................. 1.0
x3006c0s19b1n0:   min_lr .......................................... 3e-05
x3006c0s19b1n0:   mlp_type ........................................ standard
x3006c0s19b1n0:   mmap_warmup ..................................... False
x3006c0s19b1n0:   moe_eval_capacity_factor ........................ 1.0
x3006c0s19b1n0:   moe_expert_parallel_size ........................ 1
x3006c0s19b1n0:   moe_loss_coeff .................................. 0.1
x3006c0s19b1n0:   moe_min_capacity ................................ 4
x3006c0s19b1n0:   moe_token_dropping .............................. True
x3006c0s19b1n0:   moe_train_capacity_factor ....................... 1.0
x3006c0s19b1n0:   mos ............................................. False
x3006c0s19b1n0:   no_load_lr_state ................................ False
x3006c0s19b1n0:   no_load_optim ................................... None
x3006c0s19b1n0:   no_load_rng ..................................... None
x3006c0s19b1n0:   no_persist_layer_norm ........................... False
x3006c0s19b1n0:   no_pipeline_parallel ............................ True
x3006c0s19b1n0:   no_save_optim ................................... None
x3006c0s19b1n0:   no_save_rng ..................................... None
x3006c0s19b1n0:   normalization ................................... rmsnorm
x3006c0s19b1n0:   num_attention_heads ............................. 52
x3006c0s19b1n0:   num_attention_heads_teacher ..................... None
x3006c0s19b1n0:   num_channels .................................... 3
x3006c0s19b1n0:   num_classes ..................................... 1000
x3006c0s19b1n0:   num_experts ..................................... [1]
x3006c0s19b1n0:   num_experts_switch .............................. None
x3006c0s19b1n0:   num_experts_teacher ............................. [1]
x3006c0s19b1n0:   num_key_value_heads ............................. 4
x3006c0s19b1n0:   num_layers ...................................... 60
x3006c0s19b1n0:   num_layers_per_virtual_pipeline_stage ........... None
x3006c0s19b1n0:   num_layers_teacher .............................. None
x3006c0s19b1n0:   num_workers ..................................... 2
x3006c0s19b1n0:   onnx_safe ....................................... None
x3006c0s19b1n0:   openai_gelu ..................................... False
x3006c0s19b1n0:   optimizer ....................................... adam
x3006c0s19b1n0:   output_bert_embeddings .......................... False
x3006c0s19b1n0:   overlap_p2p_comm ................................ False
x3006c0s19b1n0:   override_opt_param_scheduler .................... False
x3006c0s19b1n0:   params_dtype .................................... torch.bfloat16
x3006c0s19b1n0:   partition_activations ........................... False
x3006c0s19b1n0:   patch_dim ....................................... 16
x3006c0s19b1n0:   perform_initialization .......................... True
x3006c0s19b1n0:   pipeline_model_parallel_size .................... 1
x3006c0s19b1n0:   pipeline_model_parallel_split_rank .............. None
x3006c0s19b1n0:   profile_backward ................................ False
x3006c0s19b1n0:   query_in_block_prob ............................. 0.1
x3006c0s19b1n0:   rampup_batch_size ............................... None
x3006c0s19b1n0:   random_ltd ...................................... False
x3006c0s19b1n0:   rank ............................................ 0
x3006c0s19b1n0:   recompute_granularity ........................... None
x3006c0s19b1n0:   recompute_method ................................ None
x3006c0s19b1n0:   recompute_num_layers ............................ 1
x3006c0s19b1n0:   remote_device ................................... none
x3006c0s19b1n0:   reset_attention_mask ............................ False
x3006c0s19b1n0:   reset_iteration ................................. False
x3006c0s19b1n0:   reset_position_ids .............................. False
x3006c0s19b1n0:   retriever_report_topk_accuracies ................ []
x3006c0s19b1n0:   retriever_score_scaling ......................... False
x3006c0s19b1n0:   retriever_seq_length ............................ 256
x3006c0s19b1n0:   retro_add_retriever ............................. False
x3006c0s19b1n0:   retro_cyclic_train_iters ........................ None
x3006c0s19b1n0:   retro_encoder_attention_dropout ................. 0.1
x3006c0s19b1n0:   retro_encoder_hidden_dropout .................... 0.1
x3006c0s19b1n0:   retro_encoder_layers ............................ 2
x3006c0s19b1n0:   retro_num_neighbors ............................. 2
x3006c0s19b1n0:   retro_num_retrieved_chunks ...................... 2
x3006c0s19b1n0:   retro_return_doc_ids ............................ False
x3006c0s19b1n0:   retro_workdir ................................... None
x3006c0s19b1n0:   return_data_index ............................... False
x3006c0s19b1n0:   rotary_percent .................................. 1.0
x3006c0s19b1n0:   sample_rate ..................................... 1.0
x3006c0s19b1n0:   save ............................................ /local/scratch/llama2/zero3-tp1}_dp8
x3006c0s19b1n0:   save_interval ................................... 1000
x3006c0s19b1n0:   scatter_gather_tensors_in_pipeline .............. True
x3006c0s19b1n0:   scattered_embeddings ............................ False
x3006c0s19b1n0:   seed ............................................ 1234
x3006c0s19b1n0:   seq_length ...................................... 2048
x3006c0s19b1n0:   sequence_parallel ............................... False
x3006c0s19b1n0:   sgd_momentum .................................... 0.9
x3006c0s19b1n0:   short_seq_prob .................................. 0.1
x3006c0s19b1n0:   skip_train ...................................... False
x3006c0s19b1n0:   split ........................................... 949,50,1
x3006c0s19b1n0:   split_transformers .............................. False
x3006c0s19b1n0:   squared_relu .................................... False
x3006c0s19b1n0:   standalone_embedding_stage ...................... False
x3006c0s19b1n0:   start_weight_decay .............................. 0.1
x3006c0s19b1n0:   swiglu .......................................... True
x3006c0s19b1n0:   swin_backbone_type .............................. tiny
x3006c0s19b1n0:   synchronize_each_layer .......................... False
x3006c0s19b1n0:   tensor_model_parallel_size ...................... 1
x3006c0s19b1n0:   tensorboard_dir ................................. None
x3006c0s19b1n0:   tensorboard_log_interval ........................ 1
x3006c0s19b1n0:   tensorboard_queue_size .......................... 1000
x3006c0s19b1n0:   test_data_path .................................. None
x3006c0s19b1n0:   tile_factor ..................................... 1
x3006c0s19b1n0:   timing_log_level ................................ 0
x3006c0s19b1n0:   timing_log_option ............................... minmax
x3006c0s19b1n0:   titles_data_path ................................ None
x3006c0s19b1n0:   tokenizer_model ................................. /home/am6429/dl-io/datasets/tokenizer.model
x3006c0s19b1n0:   tokenizer_type .................................. GPT2BPETokenizer
x3006c0s19b1n0:   topk ............................................ 1
x3006c0s19b1n0:   train_data_exact_num_epochs ..................... None
x3006c0s19b1n0:   train_data_path ................................. None
x3006c0s19b1n0:   train_desc_path ................................. None
x3006c0s19b1n0:   train_doc_idx_path .............................. None
x3006c0s19b1n0:   train_idx_path .................................. None
x3006c0s19b1n0:   train_iters ..................................... 10
x3006c0s19b1n0:   train_sample_idx_path ........................... None
x3006c0s19b1n0:   train_samples ................................... None
x3006c0s19b1n0:   train_shuffle_idx_path .......................... None
x3006c0s19b1n0:   train_tokens .................................... None
x3006c0s19b1n0:   transformer_impl ................................ local
x3006c0s19b1n0:   transformer_pipeline_model_parallel_size ........ 1
x3006c0s19b1n0:   untie_embeddings_and_output_weights ............. True
x3006c0s19b1n0:   use_checkpoint_args ............................. False
x3006c0s19b1n0:   use_checkpoint_opt_param_scheduler .............. False
x3006c0s19b1n0:   use_contiguous_buffers_in_local_ddp ............. True
x3006c0s19b1n0:   use_cpu_initialization .......................... None
x3006c0s19b1n0:   use_dataset_only ................................ False
x3006c0s19b1n0:   use_distributed_optimizer ....................... False
x3006c0s19b1n0:   use_flash_attn .................................. False
x3006c0s19b1n0:   use_flash_attn_triton ........................... False
x3006c0s19b1n0:   use_flash_attn_v1 ............................... False
x3006c0s19b1n0:   use_flash_attn_v2 ............................... False
x3006c0s19b1n0:   use_one_sent_docs ............................... False
x3006c0s19b1n0:   use_pin_memory .................................. False
x3006c0s19b1n0:   use_ring_exchange_p2p ........................... False
x3006c0s19b1n0:   use_rotary_position_embeddings .................. True
x3006c0s19b1n0:   use_tutel ....................................... False
x3006c0s19b1n0:   valid_data_path ................................. None
x3006c0s19b1n0:   variable_seq_lengths ............................ False
x3006c0s19b1n0:   virtual_pipeline_model_parallel_size ............ None
x3006c0s19b1n0:   vision_backbone_type ............................ vit
x3006c0s19b1n0:   vision_pretraining .............................. False
x3006c0s19b1n0:   vision_pretraining_type ......................... classify
x3006c0s19b1n0:   vocab_extra_ids ................................. 0
x3006c0s19b1n0:   vocab_file ...................................... /home/am6429/dl-io/datasets/gpt2-vocab.json
x3006c0s19b1n0:   vocab_size ...................................... None
x3006c0s19b1n0:   weight_decay .................................... 0.1
x3006c0s19b1n0:   weight_decay_incr_style ......................... constant
x3006c0s19b1n0:   world_size ...................................... 8
x3006c0s19b1n0:   zero_allgather_bucket_size ...................... 0.0
x3006c0s19b1n0:   zero_contigious_gradients ....................... False
x3006c0s19b1n0:   zero_reduce_bucket_size ......................... 0.0
x3006c0s19b1n0:   zero_reduce_scatter ............................. False
x3006c0s19b1n0:   zero_stage ...................................... 3
x3006c0s19b1n0: -------------------- end of arguments ---------------------
x3006c0s19b1n0: setting number of micro-batches to constant 2
x3006c0s19b1n0: > building GPT2BPETokenizer tokenizer ...
x3006c0s19b1n0: [2024-03-28 13:40:21,685] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
x3006c0s19b1n0: > initializing torch distributed ...
x3006c0s19b1n0: [2024-03-28 13:40:21,698] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: [2024-03-28 13:40:21,698] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b0n0:       meet the required dependencies to JIT install the op.
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: JIT compiled ops requires ninja
x3006c0s1b0n0: ninja .................. [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: op name ................ installed .. compatible
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b0n0:       meet the required dependencies to JIT install the op.
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: JIT compiled ops requires ninja
x3006c0s1b0n0: ninja .................. [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: op name ................ installed .. compatible
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b0n0:       meet the required dependencies to JIT install the op.
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: JIT compiled ops requires ninja
x3006c0s1b0n0: ninja .................. [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: op name ................ installed .. compatible
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b0n0:       meet the required dependencies to JIT install the op.
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: JIT compiled ops requires ninja
x3006c0s1b0n0: ninja .................. [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: op name ................ installed .. compatible
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: DeepSpeed general environment info:
x3006c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b0n0: torch version .................... 2.0.1+cu118
x3006c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b0n0: torch cuda version ............... 11.8
x3006c0s1b0n0: torch hip version ................ None
x3006c0s1b0n0: nvcc version ..................... 11.8
x3006c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: DeepSpeed general environment info:
x3006c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b0n0: torch version .................... 2.0.1+cu118
x3006c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b0n0: torch cuda version ............... 11.8
x3006c0s1b0n0: torch hip version ................ None
x3006c0s1b0n0: nvcc version ..................... 11.8
x3006c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: DeepSpeed general environment info:
x3006c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b0n0: torch version .................... 2.0.1+cu118
x3006c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b0n0: torch cuda version ............... 11.8
x3006c0s1b0n0: torch hip version ................ None
x3006c0s1b0n0: nvcc version ..................... 11.8
x3006c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed general environment info:
x3006c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b0n0: torch version .................... 2.0.1+cu118
x3006c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b0n0: torch cuda version ............... 11.8
x3006c0s1b0n0: torch hip version ................ None
x3006c0s1b0n0: nvcc version ..................... 11.8
x3006c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b0n0: [2024-03-28 13:40:22,118] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b0n0: [2024-03-28 13:40:22,142] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b0n0: [2024-03-28 13:40:22,151] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b0n0: [2024-03-28 13:40:22,154] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: > initialized tensor model parallel with size 1
x3006c0s19b1n0: > initialized pipeline model parallel with size 1
x3006c0s19b1n0: > setting random seeds to 1234 ...
x3006c0s19b1n0: [2024-03-28 13:40:23,148] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
x3006c0s19b1n0: > compiling dataset index builder ...
x3006c0s19b1n0: make: Entering directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3006c0s19b1n0: make: Nothing to be done for 'default'.
x3006c0s19b1n0: make: Leaving directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3006c0s19b1n0: >>> done with dataset index builder. Compilation time: 0.085 seconds
x3006c0s19b1n0: > compiling and loading fused kernels ...
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: >>> done with compiling and loading fused kernels. Compilation time: 3.304 seconds
x3006c0s1b0n0: <<<<<<<<<<< 6
x3006c0s19b1n0: initialize_megatron took 5.670438528060913
x3006c0s19b1n0: <<<<<<<<<<< 0
x3006c0s1b0n0: <<<<<<<<<<< 7
x3006c0s1b0n0: <<<<<<<<<<< 5
x3006c0s1b0n0: <<<<<<<<<<< 4
x3006c0s19b1n0: <<<<<<<<<<< 2
x3006c0s19b1n0: <<<<<<<<<<< 3
x3006c0s19b1n0: <<<<<<<<<<< 1
x3006c0s19b1n0: time to initialize megatron (seconds): 6.063
x3006c0s19b1n0: [after megatron is initialized] datetime: 2024-03-28 13:40:27 
x3006c0s19b1n0: get_accelerator and all_reduce  took 0.011090993881225586
x3006c0s19b1n0: building GPT model ...
x3006c0s19b1n0: [2024-03-28 13:40:27,404] [INFO] [utils.py:800:see_memory_usage] Before Building Model
x3006c0s19b1n0: [2024-03-28 13:40:27,405] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 4.62 GB         CA 0.0 GB         Max_CA 5 GB 
x3006c0s19b1n0: [2024-03-28 13:40:27,405] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 21.66 GB, percent = 4.3%
x3006c0s19b1n0: [2024-03-28 13:40:33,327] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 423, num_elems = 27.64B
x3006c0s19b1n0: [2024-03-28 13:40:33,392] [INFO] [utils.py:800:see_memory_usage] After Building Model
x3006c0s19b1n0: [2024-03-28 13:40:33,392] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 7.07 GB         CA 21.86 GB         Max_CA 38 GB 
x3006c0s19b1n0: [2024-03-28 13:40:33,392] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 22.07 GB, percent = 4.4%
x3006c0s19b1n0:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 27635239424
x3006c0s1b0n0: ninja: no work to do.
x3006c0s1b0n0: Time to load cpu_adam op: 2.4420528411865234 seconds
x3006c0s1b0n0: Time to load cpu_adam op: 2.472646713256836 seconds
x3006c0s19b1n0: Time to load cpu_adam op: 2.4327259063720703 seconds
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: Time to load cpu_adam op: 2.436189651489258 seconds
x3006c0s19b1n0: Time to load cpu_adam op: 2.4602713584899902 seconds
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: Time to load cpu_adam op: 2.4734835624694824 seconds
x3006c0s1b0n0: ninja: no work to do.
x3006c0s1b0n0: Time to load cpu_adam op: 2.4356985092163086 seconds
x3006c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s1b0n0: Time to load cpu_adam op: 2.472111463546753 seconds
x3006c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: > learning rate decay style: cosine
x3006c0s19b1n0: DeepSpeed is enabled.
x3006c0s19b1n0: [2024-03-28 13:40:37,792] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.3+8074cd62, git-hash=8074cd62, git-branch=hybrid_opt_offload
x3006c0s19b1n0: [2024-03-28 13:40:37,862] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After args sanity test
x3006c0s19b1n0: [2024-03-28 13:40:37,862] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:40:37,862] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 28.11 GB, percent = 5.6%
x3006c0s19b1n0: [2024-03-28 13:40:37,918] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure distributed model
x3006c0s19b1n0: [2024-03-28 13:40:37,919] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:40:37,919] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 28.39 GB, percent = 5.6%
x3006c0s19b1n0: [2024-03-28 13:40:37,980] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After configure distributed model
x3006c0s19b1n0: [2024-03-28 13:40:37,980] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:40:37,981] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 28.69 GB, percent = 5.7%
x3006c0s19b1n0: [2024-03-28 13:40:37,981] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
x3006c0s19b1n0: [2024-03-28 13:40:38,033] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After setting model parameters
x3006c0s19b1n0: [2024-03-28 13:40:38,034] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:40:38,034] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 28.94 GB, percent = 5.7%
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-28 13:40:38,087] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure optimizer
x3006c0s19b1n0: [2024-03-28 13:40:38,087] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:40:38,087] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.15 GB, percent = 5.8%
x3006c0s19b1n0: [2024-03-28 13:40:38,088] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
x3006c0s19b1n0: [2024-03-28 13:40:38,088] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
x3006c0s19b1n0: [2024-03-28 13:40:38,106] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
x3006c0s19b1n0: [2024-03-28 13:40:38,106] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
x3006c0s19b1n0: [2024-03-28 13:40:38,106] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
x3006c0s19b1n0: [2024-03-28 13:40:38,106] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-28 13:40:38,158] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
x3006c0s19b1n0: [2024-03-28 13:40:38,159] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:40:38,159] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.36 GB, percent = 5.8%
x3006c0s19b1n0: [2024-03-28 13:40:38,160] [INFO] [stage3.py:137:__init__] Reduce bucket size 500,000,000
x3006c0s19b1n0: [2024-03-28 13:40:38,161] [INFO] [stage3.py:138:__init__] Prefetch bucket size 50,000,000
x3006c0s19b1n0: [2024-03-28 13:40:38,212] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
x3006c0s19b1n0: [2024-03-28 13:40:38,213] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:40:38,213] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.45 GB, percent = 5.9%
x3006c0s19b1n0: Parameter Offload: Total persistent parameters: 805376 in 121 params
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-28 13:40:38,290] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
x3006c0s19b1n0: [2024-03-28 13:40:38,290] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:40:38,290] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.51 GB, percent = 5.9%
x3006c0s19b1n0: [2024-03-28 13:40:38,345] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
x3006c0s19b1n0: [2024-03-28 13:40:38,345] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.86 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:40:38,345] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.51 GB, percent = 5.9%
x3006c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s1b0n0: [2024-03-28 13:40:38,575] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b0n0: [2024-03-28 13:40:38,575] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-28 13:40:38,575] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b0n0: [2024-03-28 13:40:38,575] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-28 13:40:38,575] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-28 13:40:38,575] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-28 13:40:38,575] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b0n0: [2024-03-28 13:40:38,575] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b0n0: [2024-03-28 13:40:38,575] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-28 13:40:38,575] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b0n0: [2024-03-28 13:40:38,575] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-28 13:40:38,575] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-28 13:40:38,575] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-28 13:40:38,575] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b0n0: [2024-03-28 13:40:38,575] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b0n0: [2024-03-28 13:40:38,575] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-28 13:40:38,575] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,575] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,575] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,575] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,575] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,575] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,575] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,575] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,576] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,577] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,578] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,579] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b0n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b0n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b0n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b0n0: [2024-03-28 13:40:38,580] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-28 13:40:41,882] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 31
x3006c0s19b1n0: [2024-03-28 13:40:41,883] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.44 GB         CA 6.44 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:40:41,883] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 34.19 GB, percent = 6.8%
x3006c0s19b1n0: [2024-03-28 13:40:41,962] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
x3006c0s19b1n0: [2024-03-28 13:40:41,963] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-28 13:40:41,963] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 35.34 GB, percent = 7.0%
x3006c0s19b1n0: [2024-03-28 13:41:01,626] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
x3006c0s19b1n0: [2024-03-28 13:41:01,627] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-28 13:41:01,627] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 219.04 GB, percent = 43.5%
x3006c0s19b1n0: [2024-03-28 13:41:01,896] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
x3006c0s19b1n0: [2024-03-28 13:41:01,896] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-28 13:41:01,896] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 222.97 GB, percent = 44.3%
x3006c0s19b1n0: [2024-03-28 13:41:08,013] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | init_optimizer_state: 6102.88
x3006c0s19b1n0: [2024-03-28 13:41:08,191] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
x3006c0s19b1n0: [2024-03-28 13:41:08,192] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-28 13:41:08,192] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 283.26 GB, percent = 56.3%
x3006c0s19b1n0: [2024-03-28 13:41:08,658] [INFO] [stage3.py:520:_setup_for_real_optimizer] optimizer state initialized
x3006c0s19b1n0: [2024-03-28 13:41:17,087] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
x3006c0s19b1n0: [2024-03-28 13:41:17,088] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 8.61 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:41:17,088] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 315.61 GB, percent = 62.7%
x3006c0s19b1n0: [2024-03-28 13:41:17,088] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
x3006c0s19b1n0: [2024-03-28 13:41:17,156] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure LR scheduler
x3006c0s19b1n0: [2024-03-28 13:41:17,156] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:41:17,157] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 315.6 GB, percent = 62.7%
x3006c0s19b1n0: [2024-03-28 13:41:17,157] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
x3006c0s19b1n0: [2024-03-28 13:41:17,157] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7f3d51e781f0>
x3006c0s19b1n0: [2024-03-28 13:41:17,157] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:41:17,222] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before rewriting optimizer step
x3006c0s19b1n0: [2024-03-28 13:41:17,222] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:41:17,222] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 315.61 GB, percent = 62.7%
x3006c0s19b1n0: [2024-03-28 13:41:17,288] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure checkpointing
x3006c0s19b1n0: [2024-03-28 13:41:17,289] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:41:17,289] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 315.6 GB, percent = 62.7%
x3006c0s19b1n0: [2024-03-28 13:41:17,289] [INFO] [config.py:998:print] DeepSpeedEngine configuration:
x3006c0s19b1n0: [2024-03-28 13:41:17,289] [INFO] [config.py:1002:print]   activation_checkpointing_config  {
x3006c0s19b1n0:     "partition_activations": false, 
x3006c0s19b1n0:     "contiguous_memory_optimization": false, 
x3006c0s19b1n0:     "cpu_checkpointing": false, 
x3006c0s19b1n0:     "number_checkpoints": null, 
x3006c0s19b1n0:     "synchronize_checkpoint_boundary": false, 
x3006c0s19b1n0:     "profile": false
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 13:41:17,289] [INFO] [config.py:1002:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
x3006c0s19b1n0: [2024-03-28 13:41:17,289] [INFO] [config.py:1002:print]   amp_enabled .................. False
x3006c0s19b1n0: [2024-03-28 13:41:17,289] [INFO] [config.py:1002:print]   amp_params ................... False
x3006c0s19b1n0: [2024-03-28 13:41:17,289] [INFO] [config.py:1002:print]   autotuning_config ............ {
x3006c0s19b1n0:     "enabled": false, 
x3006c0s19b1n0:     "start_step": null, 
x3006c0s19b1n0:     "end_step": null, 
x3006c0s19b1n0:     "metric_path": null, 
x3006c0s19b1n0:     "arg_mappings": null, 
x3006c0s19b1n0:     "metric": "throughput", 
x3006c0s19b1n0:     "model_info": null, 
x3006c0s19b1n0:     "results_dir": "autotuning_results", 
x3006c0s19b1n0:     "exps_dir": "autotuning_exps", 
x3006c0s19b1n0:     "overwrite": true, 
x3006c0s19b1n0:     "fast": true, 
x3006c0s19b1n0:     "start_profile_step": 3, 
x3006c0s19b1n0:     "end_profile_step": 5, 
x3006c0s19b1n0:     "tuner_type": "gridsearch", 
x3006c0s19b1n0:     "tuner_early_stopping": 5, 
x3006c0s19b1n0:     "tuner_num_trials": 50, 
x3006c0s19b1n0:     "model_info_path": null, 
x3006c0s19b1n0:     "mp_size": 1, 
x3006c0s19b1n0:     "max_train_batch_size": null, 
x3006c0s19b1n0:     "min_train_batch_size": 1, 
x3006c0s19b1n0:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
x3006c0s19b1n0:     "min_train_micro_batch_size_per_gpu": 1, 
x3006c0s19b1n0:     "num_tuning_micro_batch_sizes": 3
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   bfloat16_enabled ............. True
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   bfloat16_immediate_grad_update  False
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   checkpoint_parallel_write_pipeline  False
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   checkpoint_tag_validation_enabled  True
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   checkpoint_tag_validation_fail  False
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f3d51e78460>
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   communication_data_type ...... None
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   curriculum_enabled_legacy .... False
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   curriculum_params_legacy ..... False
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   data_efficiency_enabled ...... False
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   dataloader_drop_last ......... False
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   datastates_config ............ {
x3006c0s19b1n0:     "enabled": null, 
x3006c0s19b1n0:     "config": {
x3006c0s19b1n0:     }
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   disable_allgather ............ False
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   dump_state ................... False
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   dynamic_loss_scale_args ...... None
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   eigenvalue_enabled ........... False
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   eigenvalue_gas_boundary_resolution  1
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   eigenvalue_layer_name ........ bert.encoder.layer
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   eigenvalue_layer_num ......... 0
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   eigenvalue_max_iter .......... 100
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   eigenvalue_stability ......... 1e-06
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   eigenvalue_tol ............... 0.01
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   eigenvalue_verbose ........... False
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   elasticity_enabled ........... False
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   flops_profiler_config ........ {
x3006c0s19b1n0:     "enabled": false, 
x3006c0s19b1n0:     "recompute_fwd_factor": 0.0, 
x3006c0s19b1n0:     "profile_step": 1, 
x3006c0s19b1n0:     "module_depth": -1, 
x3006c0s19b1n0:     "top_modules": 1, 
x3006c0s19b1n0:     "detailed": true, 
x3006c0s19b1n0:     "output_file": null
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   fp16_auto_cast ............... None
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   fp16_enabled ................. False
x3006c0s19b1n0: [2024-03-28 13:41:17,290] [INFO] [config.py:1002:print]   fp16_master_weights_and_gradients  False
x3006c0s19b1n0: [2024-03-28 13:41:17,291] [INFO] [config.py:1002:print]   global_rank .................. 0
x3006c0s19b1n0: [2024-03-28 13:41:17,291] [INFO] [config.py:1002:print]   grad_accum_dtype ............. bf16
x3006c0s19b1n0: [2024-03-28 13:41:17,291] [INFO] [config.py:1002:print]   gradient_accumulation_steps .. 2
x3006c0s19b1n0: [2024-03-28 13:41:17,291] [INFO] [config.py:1002:print]   gradient_clipping ............ 0.0
x3006c0s19b1n0: [2024-03-28 13:41:17,291] [INFO] [config.py:1002:print]   gradient_predivide_factor .... 1.0
x3006c0s19b1n0: [2024-03-28 13:41:17,291] [INFO] [config.py:1002:print]   graph_harvesting ............. False
x3006c0s19b1n0: [2024-03-28 13:41:17,291] [INFO] [config.py:1002:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
x3006c0s19b1n0: [2024-03-28 13:41:17,291] [INFO] [config.py:1002:print]   initial_dynamic_scale ........ 1
x3006c0s19b1n0: [2024-03-28 13:41:17,291] [INFO] [config.py:1002:print]   load_universal_checkpoint .... False
x3006c0s19b1n0: [2024-03-28 13:41:17,291] [INFO] [config.py:1002:print]   loss_scale ................... 1.0
x3006c0s19b1n0: [2024-03-28 13:41:17,291] [INFO] [config.py:1002:print]   memory_breakdown ............. True
x3006c0s19b1n0: [2024-03-28 13:41:17,291] [INFO] [config.py:1002:print]   mics_hierarchial_params_gather  False
x3006c0s19b1n0: [2024-03-28 13:41:17,291] [INFO] [config.py:1002:print]   mics_shard_size .............. -1
x3006c0s19b1n0: [2024-03-28 13:41:17,291] [INFO] [config.py:1002:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
x3006c0s19b1n0: [2024-03-28 13:41:17,291] [INFO] [config.py:1002:print]   nebula_config ................ {
x3006c0s19b1n0:     "enabled": false, 
x3006c0s19b1n0:     "persistent_storage_path": null, 
x3006c0s19b1n0:     "persistent_time_interval": 100, 
x3006c0s19b1n0:     "num_of_version_in_retention": 2, 
x3006c0s19b1n0:     "enable_nebula_load": true, 
x3006c0s19b1n0:     "load_path": null
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 13:41:17,291] [INFO] [config.py:1002:print]   optimizer_legacy_fusion ...... False
x3006c0s19b1n0: [2024-03-28 13:41:17,291] [INFO] [config.py:1002:print]   optimizer_name ............... None
x3006c0s19b1n0: [2024-03-28 13:41:17,291] [INFO] [config.py:1002:print]   optimizer_params ............. None
x3006c0s19b1n0: [2024-03-28 13:41:17,291] [INFO] [config.py:1002:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
x3006c0s19b1n0: [2024-03-28 13:41:17,291] [INFO] [config.py:1002:print]   pld_enabled .................. False
x3006c0s19b1n0: [2024-03-28 13:41:17,291] [INFO] [config.py:1002:print]   pld_params ................... False
x3006c0s19b1n0: [2024-03-28 13:41:17,291] [INFO] [config.py:1002:print]   prescale_gradients ........... False
x3006c0s19b1n0: [2024-03-28 13:41:17,291] [INFO] [config.py:1002:print]   scheduler_name ............... None
x3006c0s19b1n0: [2024-03-28 13:41:17,291] [INFO] [config.py:1002:print]   scheduler_params ............. None
x3006c0s19b1n0: [2024-03-28 13:41:17,291] [INFO] [config.py:1002:print]   seq_parallel_communication_data_type  torch.float32
x3006c0s19b1n0: [2024-03-28 13:41:17,291] [INFO] [config.py:1002:print]   sparse_attention ............. None
x3006c0s19b1n0: [2024-03-28 13:41:17,291] [INFO] [config.py:1002:print]   sparse_gradients_enabled ..... False
x3006c0s19b1n0: [2024-03-28 13:41:17,291] [INFO] [config.py:1002:print]   steps_per_print .............. 1
x3006c0s19b1n0: [2024-03-28 13:41:17,291] [INFO] [config.py:1002:print]   train_batch_size ............. 64
x3006c0s19b1n0: [2024-03-28 13:41:17,292] [INFO] [config.py:1002:print]   train_micro_batch_size_per_gpu  4
x3006c0s19b1n0: [2024-03-28 13:41:17,292] [INFO] [config.py:1002:print]   use_data_before_expert_parallel_  False
x3006c0s19b1n0: [2024-03-28 13:41:17,292] [INFO] [config.py:1002:print]   use_node_local_storage ....... False
x3006c0s19b1n0: [2024-03-28 13:41:17,292] [INFO] [config.py:1002:print]   wall_clock_breakdown ......... True
x3006c0s19b1n0: [2024-03-28 13:41:17,292] [INFO] [config.py:1002:print]   weight_quantization_config ... None
x3006c0s19b1n0: [2024-03-28 13:41:17,292] [INFO] [config.py:1002:print]   world_size ................... 8
x3006c0s19b1n0: [2024-03-28 13:41:17,292] [INFO] [config.py:1002:print]   zero_allow_untested_optimizer  False
x3006c0s19b1n0: [2024-03-28 13:41:17,292] [INFO] [config.py:1002:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0, prefetch_optimizer=True, part_grads_async=True, prefetch_optimizer_gap=5) sub_group_size=100000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
x3006c0s19b1n0: [2024-03-28 13:41:17,292] [INFO] [config.py:1002:print]   zero_enabled ................. True
x3006c0s19b1n0: [2024-03-28 13:41:17,292] [INFO] [config.py:1002:print]   zero_force_ds_cpu_optimizer .. True
x3006c0s19b1n0: [2024-03-28 13:41:17,292] [INFO] [config.py:1002:print]   zero_optimization_stage ...... 3
x3006c0s19b1n0: [2024-03-28 13:41:17,292] [INFO] [config.py:988:print_user_config]   json = {
x3006c0s19b1n0:     "train_batch_size": 64, 
x3006c0s19b1n0:     "train_micro_batch_size_per_gpu": 4, 
x3006c0s19b1n0:     "steps_per_print": 1, 
x3006c0s19b1n0:     "zero_optimization": {
x3006c0s19b1n0:         "stage": 3, 
x3006c0s19b1n0:         "offload_optimizer": {
x3006c0s19b1n0:             "device": "cpu", 
x3006c0s19b1n0:             "ratio": 1, 
x3006c0s19b1n0:             "pin_memory": true, 
x3006c0s19b1n0:             "prefetch_optimizer": 1, 
x3006c0s19b1n0:             "part_grads_async": 1, 
x3006c0s19b1n0:             "prefetch_optimizer_gap": 5
x3006c0s19b1n0:         }, 
x3006c0s19b1n0:         "sub_group_size": 1.000000e+08
x3006c0s19b1n0:     }, 
x3006c0s19b1n0:     "bf16": {
x3006c0s19b1n0:         "enabled": true
x3006c0s19b1n0:     }, 
x3006c0s19b1n0:     "data_types": {
x3006c0s19b1n0:         "grad_accum_dtype": "bf16"
x3006c0s19b1n0:     }, 
x3006c0s19b1n0:     "wall_clock_breakdown": true, 
x3006c0s19b1n0:     "memory_breakdown": true, 
x3006c0s19b1n0:     "flops_profiler": {
x3006c0s19b1n0:         "enabled": false
x3006c0s19b1n0:     }
x3006c0s19b1n0: }
x3006c0s1b0n0: <TIMER:model-and-optimizer-setup,51.24045276641846>
x3006c0s1b0n0: <TIMER:model-and-optimizer-setup,51.2409245967865>
x3006c0s1b0n0: <TIMER:model-and-optimizer-setup,51.24096417427063>
x3006c0s1b0n0: <TIMER:model-and-optimizer-setup,51.24102330207825>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,51.241008281707764>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,51.24108624458313>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,51.24277949333191>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,51.24284648895264>
x3006c0s19b1n0: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-28 13:41:18 
x3006c0s19b1n0: > building train, validation, and test datasets ...
x3006c0s19b1n0:  > datasets target sizes (minimum size):
x3006c0s19b1n0:     train:      640
x3006c0s19b1n0:     validation: 0
x3006c0s19b1n0:     test:       0
x3006c0s19b1n0: > building train, validation, and test datasets for GPT ...
x3006c0s19b1n0: Single data path provided for train, valid & test
x3006c0s19b1n0:  > building dataset index ...
x3006c0s19b1n0:     reading sizes...
x3006c0s19b1n0:     reading pointers...
x3006c0s19b1n0:     reading document index...
x3006c0s19b1n0:     creating numpy buffer of mmap...
x3006c0s19b1n0:     creating memory view of numpy buffer...
x3006c0s19b1n0:  > finished creating indexed dataset in 0.002176 seconds
x3006c0s19b1n0:     number of documents: 79000
x3006c0s19b1n0:  > dataset split:
x3006c0s19b1n0:     train:
x3006c0s19b1n0:      document indices in [0, 74971) total of 74971 documents
x3006c0s19b1n0:     validation:
x3006c0s19b1n0:      document indices in [74971, 78921) total of 3950 documents
x3006c0s19b1n0:     test:
x3006c0s19b1n0:      document indices in [78921, 79000) total of 79 documents
x3006c0s19b1n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/d5d1daec41eb416469c3827ed48205ed_doc_idx.npy
x3006c0s19b1n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/d5d1daec41eb416469c3827ed48205ed_sample_idx.npy
x3006c0s19b1n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/d5d1daec41eb416469c3827ed48205ed_shuffle_idx.npy
x3006c0s19b1n0:     loaded indexed file in 0.010 seconds
x3006c0s19b1n0:     total number of samples: 108448
x3006c0s19b1n0:     total number of epochs: 1
x3006c0s19b1n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_doc_idx.npy
x3006c0s19b1n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_sample_idx.npy
x3006c0s19b1n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_shuffle_idx.npy
x3006c0s19b1n0:     loaded indexed file in 0.004 seconds
x3006c0s19b1n0:     total number of samples: 5792
x3006c0s19b1n0:     total number of epochs: 1
x3006c0s19b1n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_doc_idx.npy
x3006c0s19b1n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_sample_idx.npy
x3006c0s19b1n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_shuffle_idx.npy
x3006c0s19b1n0:     loaded indexed file in 0.003 seconds
x3006c0s19b1n0:     total number of samples: 185
x3006c0s19b1n0:     total number of epochs: 1
x3006c0s19b1n0: > finished creating GPT datasets ...
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5379593372344971>
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5407536029815674>
x3006c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5576536655426025>
x3006c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5597457885742188>
x3006c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5629820823669434>
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5638723373413086>
x3006c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5738747119903564>
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.6798429489135742>
x3006c0s19b1n0: [after dataloaders are built] datetime: 2024-03-28 13:41:19 
x3006c0s19b1n0: done with setup ...
x3006c0s19b1n0: training ...
x3006c0s1b0n0: (min, max) time across ranks (ms):
x3006c0s1b0n0:     model-and-optimizer-setup ......................: (51240.45, 51242.85)
x3006c0s1b0n0:     train/valid/test-data-iterators-setup ..........: (537.96, 679.84)
x3006c0s19b1n0: [before training begins] datetime: 2024-03-28 13:41:19 
x3006c0s19b1n0: [before the start of training step] datetime: 2024-03-28 13:41:19 
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:41:19,467] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:41:19,467] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 7.45 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:41:19,468] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 317.93 GB, percent = 63.2%
x3006c0s19b1n0: [2024-03-28 13:41:19,603] [INFO] [checkpointing.py:539:forward] Activation Checkpointing Information
x3006c0s19b1n0: [2024-03-28 13:41:19,603] [INFO] [checkpointing.py:540:forward] ----Partition Activations False, CPU CHECKPOINTING False
x3006c0s19b1n0: [2024-03-28 13:41:19,603] [INFO] [checkpointing.py:541:forward] ----contiguous Memory Checkpointing False with 60 total layers
x3006c0s19b1n0: [2024-03-28 13:41:19,603] [INFO] [checkpointing.py:543:forward] ----Synchronization False
x3006c0s19b1n0: [2024-03-28 13:41:19,603] [INFO] [checkpointing.py:544:forward] ----Profiling time in checkpointing False
x3006c0s19b1n0: [2024-03-28 13:41:28,335] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:41:28,335] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 19.76 GB         CA 22.95 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:41:28,336] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 318.1 GB, percent = 63.2%
x3006c0s19b1n0: [2024-03-28 13:41:28,513] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:41:28,514] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 16.84 GB         CA 16.91 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:41:28,514] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 318.08 GB, percent = 63.2%
x3006c0s19b1n0: [2024-03-28 13:41:55,192] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:41:55,193] [INFO] [utils.py:801:see_memory_usage] MA 10.25 GB         Max_MA 20.22 GB         CA 10.33 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:41:55,193] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 318.14 GB, percent = 63.2%
x3006c0s19b1n0: [2024-03-28 13:41:55,273] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:41:55,273] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:41:55,273] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 318.17 GB, percent = 63.2%
x3006c0s19b1n0: [2024-03-28 13:42:02,207] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:42:02,208] [INFO] [utils.py:801:see_memory_usage] MA 20.07 GB         Max_MA 22.37 GB         CA 25.45 GB         Max_CA 25 GB 
x3006c0s19b1n0: [2024-03-28 13:42:02,208] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 318.16 GB, percent = 63.2%
x3006c0s19b1n0: [2024-03-28 13:42:02,283] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:42:02,284] [INFO] [utils.py:801:see_memory_usage] MA 20.07 GB         Max_MA 20.07 GB         CA 20.68 GB         Max_CA 25 GB 
x3006c0s19b1n0: [2024-03-28 13:42:02,284] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 318.16 GB, percent = 63.2%
x3006c0s19b1n0: [2024-03-28 13:42:21,407] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:42:21,408] [INFO] [utils.py:801:see_memory_usage] MA 11.51 GB         Max_MA 22.69 GB         CA 11.75 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-28 13:42:21,408] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 318.18 GB, percent = 63.2%
x3006c0s19b1n0: [2024-03-28 13:42:21,479] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:42:21,480] [INFO] [utils.py:801:see_memory_usage] MA 11.51 GB         Max_MA 11.51 GB         CA 11.75 GB         Max_CA 12 GB 
x3006c0s19b1n0: [2024-03-28 13:42:21,480] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 318.28 GB, percent = 63.2%
x3006c0s19b1n0: [2024-03-28 13:42:28,176] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.514916658401489
x3006c0s19b1n0: [2024-03-28 13:42:28,199] [INFO] [stage3.py:2251:step] Full outer step loop took 6.538867950439453
x3006c0s1b0n0: [2024-03-28 13:42:28,220] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.559636116027832
x3006c0s19b1n0: [2024-03-28 13:42:28,229] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.568281888961792
x3006c0s19b1n0: [2024-03-28 13:42:28,250] [INFO] [stage3.py:2251:step] Full outer step loop took 6.589619398117065
x3006c0s1b0n0: [2024-03-28 13:42:28,255] [INFO] [stage3.py:2251:step] Full outer step loop took 6.594744920730591
x3006c0s1b0n0: [2024-03-28 13:42:28,298] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.637916803359985
x3006c0s1b0n0: [2024-03-28 13:42:28,304] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.64342188835144
x3006c0s1b0n0: [2024-03-28 13:42:28,309] [INFO] [stage3.py:2251:step] Full outer step loop took 6.648576974868774
x3006c0s1b0n0: [2024-03-28 13:42:28,326] [INFO] [stage3.py:2251:step] Full outer step loop took 6.665325880050659
x3006c0s1b0n0: [2024-03-28 13:42:28,341] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.681040287017822
x3006c0s1b0n0: [2024-03-28 13:42:28,350] [INFO] [stage3.py:2251:step] Full outer step loop took 6.6900694370269775
x3006c0s19b1n0: [2024-03-28 13:42:28,359] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.6985695362091064
x3006c0s19b1n0: [2024-03-28 13:42:28,366] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.70563530921936
x3006c0s19b1n0: [2024-03-28 13:42:28,371] [INFO] [stage3.py:2251:step] Full outer step loop took 6.710351943969727
x3006c0s19b1n0: [2024-03-28 13:42:28,375] [INFO] [stage3.py:2251:step] Full outer step loop took 6.714709997177124
x3006c0s19b1n0: [2024-03-28 13:42:28,385] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6710.58
x3006c0s19b1n0: [2024-03-28 13:42:28,385] [INFO] [stage3.py:2277:step] End to end step took 6.7250285148620605
x3006c0s1b0n0: [2024-03-28 13:42:28,385] [INFO] [stage3.py:2277:step] End to end step took 6.725001811981201
x3006c0s19b1n0: [2024-03-28 13:42:28,385] [WARNING] [stage3.py:2267:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3006c0s19b1n0: [2024-03-28 13:42:28,385] [INFO] [stage3.py:2277:step] End to end step took 6.725191831588745
x3006c0s1b0n0: [2024-03-28 13:42:28,385] [INFO] [stage3.py:2277:step] End to end step took 6.725006818771362
x3006c0s1b0n0: [2024-03-28 13:42:28,386] [INFO] [stage3.py:2277:step] End to end step took 6.725207567214966
x3006c0s19b1n0: [2024-03-28 13:42:28,386] [INFO] [logging.py:96:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
x3006c0s1b0n0: [2024-03-28 13:42:28,386] [INFO] [stage3.py:2277:step] End to end step took 6.725394248962402
x3006c0s19b1n0: [2024-03-28 13:42:28,386] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 15846.84 | bwd_microstep: 45487.77 | bwd_inner_microstep: 45323.51 | bwd_allreduce_microstep: 164.06 | step_microstep: 6905.98
x3006c0s19b1n0: [2024-03-28 13:42:28,386] [INFO] [stage3.py:2277:step] End to end step took 6.725722551345825
x3006c0s19b1n0: [2024-03-28 13:42:28,386] [INFO] [stage3.py:2277:step] End to end step took 6.725632429122925
x3006c0s19b1n0: [2024-03-28 13:42:28,386] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 15846.84 | bwd: 45487.76 | bwd_inner: 45323.52 | bwd_allreduce: 164.09 | step: 6905.97
x3006c0s19b1n0: [2024-03-28 13:42:28,481] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:42:28,481] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.56 GB         CA 10.34 GB         Max_CA 14 GB 
x3006c0s19b1n0: [2024-03-28 13:42:28,482] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.33 GB, percent = 79.8%
x3006c0s19b1n0: <TIMER:interval-time,69.21078443527222><TIMER:interval-time,69.21079659461975>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,69.21080732345581>
x3006c0s1b0n0: <TIMER:interval-time,69.21084094047546><TIMER:interval-time,69.21083378791809><TIMER:interval-time,69.21083927154541><TIMER:interval-time,69.21082949638367>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s19b1n0: <TIMER:interval-time,69.21092510223389>
x3006c0s19b1n0: [Rank 0] (after 1 iterations) memory (MB) | allocated: 10510.36328125 | max allocated: 10510.3642578125 | reserved: 10586.0 | max reserved: 10586.0
x3006c0s1b0n0:  elapsed_time 69.210834 | consumed samples:           64 | consumed tokens:       131072 | elapsed time per iteration (ms): 69210.8 | learning rate: 3.000E-04 | global batch size:    64 | lm loss: 1.215771E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.925 | TFLOPs: 63.98 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:42:28,612] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:42:28,612] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:42:28,613] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.36 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:42:35,554] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:42:35,554] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:42:35,555] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.35 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:42:35,644] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:42:35,645] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:42:35,646] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.35 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:42:54,145] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:42:54,146] [INFO] [utils.py:801:see_memory_usage] MA 11.51 GB         Max_MA 23.77 GB         CA 11.59 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-28 13:42:54,146] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.38 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:42:54,236] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:42:54,236] [INFO] [utils.py:801:see_memory_usage] MA 11.52 GB         Max_MA 11.52 GB         CA 11.59 GB         Max_CA 12 GB 
x3006c0s19b1n0: [2024-03-28 13:42:54,236] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.38 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:43:01,107] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:43:01,107] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:43:01,108] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.37 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:43:01,187] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:43:01,188] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:43:01,188] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.37 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:43:20,174] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:43:20,175] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 23.87 GB         CA 13.01 GB         Max_CA 32 GB 
x3006c0s19b1n0: [2024-03-28 13:43:20,175] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.38 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:43:20,243] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:43:20,244] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 12.77 GB         CA 13.01 GB         Max_CA 13 GB 
x3006c0s19b1n0: [2024-03-28 13:43:20,244] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.38 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:43:24,815] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.547516822814941
x3006c0s1b0n0: [2024-03-28 13:43:24,818] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.550264120101929
x3006c0s1b0n0: [2024-03-28 13:43:24,827] [INFO] [stage3.py:2251:step] Full outer step loop took 4.559300422668457
x3006c0s19b1n0: [2024-03-28 13:43:24,833] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.565417051315308
x3006c0s19b1n0: [2024-03-28 13:43:24,834] [INFO] [stage3.py:2251:step] Full outer step loop took 4.566184759140015
x3006c0s19b1n0: [2024-03-28 13:43:24,850] [INFO] [stage3.py:2251:step] Full outer step loop took 4.581925868988037
x3006c0s1b0n0: [2024-03-28 13:43:24,922] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.653994560241699
x3006c0s19b1n0: [2024-03-28 13:43:24,922] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.654491901397705
x3006c0s19b1n0: [2024-03-28 13:43:24,923] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.6556456089019775
x3006c0s19b1n0: [2024-03-28 13:43:24,931] [INFO] [stage3.py:2251:step] Full outer step loop took 4.663537979125977
x3006c0s1b0n0: [2024-03-28 13:43:24,932] [INFO] [stage3.py:2251:step] Full outer step loop took 4.664324760437012
x3006c0s19b1n0: [2024-03-28 13:43:24,932] [INFO] [stage3.py:2251:step] Full outer step loop took 4.664687871932983
x3006c0s1b0n0: [2024-03-28 13:43:24,968] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.69978404045105
x3006c0s1b0n0: [2024-03-28 13:43:24,981] [INFO] [stage3.py:2251:step] Full outer step loop took 4.713869333267212
x3006c0s1b0n0: [2024-03-28 13:43:24,985] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.7177276611328125
x3006c0s1b0n0: [2024-03-28 13:43:24,994] [INFO] [stage3.py:2251:step] Full outer step loop took 4.726773500442505
x3006c0s19b1n0: [2024-03-28 13:43:25,005] [INFO] [stage3.py:2277:step] End to end step took 4.7374327182769775
x3006c0s19b1n0: [2024-03-28 13:43:25,005] [INFO] [stage3.py:2277:step] End to end step took 4.737428903579712
x3006c0s1b0n0: [2024-03-28 13:43:25,005] [INFO] [stage3.py:2277:step] End to end step took 4.737452030181885
x3006c0s1b0n0: [2024-03-28 13:43:25,005] [INFO] [stage3.py:2277:step] End to end step took 4.7374372482299805
x3006c0s1b0n0: [2024-03-28 13:43:25,005] [INFO] [stage3.py:2277:step] End to end step took 4.737552881240845
x3006c0s19b1n0: [2024-03-28 13:43:25,005] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4566.45
x3006c0s1b0n0: [2024-03-28 13:43:25,005] [INFO] [stage3.py:2277:step] End to end step took 4.737821578979492
x3006c0s19b1n0: [2024-03-28 13:43:25,005] [INFO] [stage3.py:2277:step] End to end step took 4.7379374504089355
x3006c0s19b1n0: [2024-03-28 13:43:25,005] [INFO] [stage3.py:2277:step] End to end step took 4.7378249168396
x3006c0s19b1n0: [2024-03-28 13:43:25,006] [INFO] [logging.py:96:log_dist] [Rank 0] step=2, skipped=0, lr=[0.0002918585038060976], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:43:25,006] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 13742.06 | bwd_microstep: 37143.46 | bwd_inner_microstep: 36981.45 | bwd_allreduce_microstep: 161.84 | step_microstep: 4762.05
x3006c0s19b1n0: [2024-03-28 13:43:25,006] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 13742.04 | bwd: 37143.44 | bwd_inner: 36981.43 | bwd_allreduce: 161.87 | step: 4762.05
x3006c0s19b1n0: [2024-03-28 13:43:25,118] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:43:25,119] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.77 GB         CA 10.34 GB         Max_CA 14 GB 
x3006c0s19b1n0: [2024-03-28 13:43:25,119] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.38 GB, percent = 79.8%
x3006c0s19b1n0: <TIMER:interval-time,56.6374990940094><TIMER:interval-time,56.63745331764221><TIMER:interval-time,56.637500286102295>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,56.63753318786621><TIMER:interval-time,56.63753414154053><TIMER:interval-time,56.63753581047058>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,56.63753843307495>
x3006c0s19b1n0: <TIMER:interval-time,56.63756227493286>
x3006c0s1b0n0:  elapsed_time 56.637533 | consumed samples:          128 | consumed tokens:       262144 | elapsed time per iteration (ms): 56637.5 | learning rate: 2.919E-04 | global batch size:    64 | lm loss: 1.082609E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.130 | TFLOPs: 78.19 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:43:25,257] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:43:25,257] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:43:25,258] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.36 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:43:32,190] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:43:32,191] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:43:32,191] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.35 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:43:32,269] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:43:32,270] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:43:32,270] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.35 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:43:50,687] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:43:50,688] [INFO] [utils.py:801:see_memory_usage] MA 11.51 GB         Max_MA 23.77 GB         CA 11.59 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-28 13:43:50,688] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.37 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:43:50,782] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:43:50,783] [INFO] [utils.py:801:see_memory_usage] MA 11.52 GB         Max_MA 11.52 GB         CA 11.59 GB         Max_CA 12 GB 
x3006c0s19b1n0: [2024-03-28 13:43:50,783] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.39 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:43:57,643] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:43:57,643] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:43:57,643] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.38 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:43:57,722] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:43:57,722] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:43:57,722] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.38 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:44:16,677] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:44:16,677] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 23.87 GB         CA 13.01 GB         Max_CA 32 GB 
x3006c0s19b1n0: [2024-03-28 13:44:16,678] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.38 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:44:16,746] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:44:16,747] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 12.77 GB         CA 13.01 GB         Max_CA 13 GB 
x3006c0s19b1n0: [2024-03-28 13:44:16,747] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.38 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:44:21,326] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.555198669433594
x3006c0s19b1n0: [2024-03-28 13:44:21,348] [INFO] [stage3.py:2251:step] Full outer step loop took 4.577591896057129
x3006c0s19b1n0: [2024-03-28 13:44:21,370] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.599325180053711
x3006c0s19b1n0: [2024-03-28 13:44:21,379] [INFO] [stage3.py:2251:step] Full outer step loop took 4.6083338260650635
x3006c0s1b0n0: [2024-03-28 13:44:21,472] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.701659440994263
x3006c0s19b1n0: [2024-03-28 13:44:21,480] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.709463596343994
x3006c0s19b1n0: [2024-03-28 13:44:21,480] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.709782838821411
x3006c0s19b1n0: [2024-03-28 13:44:21,490] [INFO] [stage3.py:2251:step] Full outer step loop took 4.719350099563599
x3006c0s19b1n0: [2024-03-28 13:44:21,490] [INFO] [stage3.py:2251:step] Full outer step loop took 4.719758033752441
x3006c0s1b0n0: [2024-03-28 13:44:21,507] [INFO] [stage3.py:2251:step] Full outer step loop took 4.736008167266846
x3006c0s1b0n0: [2024-03-28 13:44:21,542] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.770814657211304
x3006c0s1b0n0: [2024-03-28 13:44:21,542] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.771660089492798
x3006c0s1b0n0: [2024-03-28 13:44:21,555] [INFO] [stage3.py:2251:step] Full outer step loop took 4.784772872924805
x3006c0s1b0n0: [2024-03-28 13:44:21,575] [INFO] [stage3.py:2251:step] Full outer step loop took 4.804291486740112
x3006c0s1b0n0: [2024-03-28 13:44:21,579] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.808932542800903
x3006c0s1b0n0: [2024-03-28 13:44:21,588] [INFO] [stage3.py:2251:step] Full outer step loop took 4.817930698394775
x3006c0s19b1n0: [2024-03-28 13:44:21,599] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4719.96
x3006c0s1b0n0: [2024-03-28 13:44:21,599] [INFO] [stage3.py:2277:step] End to end step took 4.828430414199829
x3006c0s1b0n0: [2024-03-28 13:44:21,599] [INFO] [stage3.py:2277:step] End to end step took 4.828468322753906
x3006c0s1b0n0: [2024-03-28 13:44:21,599] [INFO] [stage3.py:2277:step] End to end step took 4.828351736068726
x3006c0s19b1n0: [2024-03-28 13:44:21,599] [INFO] [stage3.py:2277:step] End to end step took 4.828646898269653
x3006c0s19b1n0: [2024-03-28 13:44:21,599] [INFO] [logging.py:96:log_dist] [Rank 0] step=3, skipped=0, lr=[0.00026841599982106197], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:44:21,599] [INFO] [stage3.py:2277:step] End to end step took 4.829001426696777
x3006c0s1b0n0: [2024-03-28 13:44:21,599] [INFO] [stage3.py:2277:step] End to end step took 4.828949689865112
x3006c0s19b1n0: [2024-03-28 13:44:21,599] [INFO] [stage3.py:2277:step] End to end step took 4.829028606414795
x3006c0s19b1n0: [2024-03-28 13:44:21,600] [INFO] [timer.py:260:stop] epoch=0/micro_step=3/global_step=3, RunningAvgSamplesPerSec=2.076902171377473, CurrSamplesPerSec=2.076902171377473, MemAllocated=10.26GB, MaxMemAllocated=12.77GB
x3006c0s19b1n0: [2024-03-28 13:44:21,600] [INFO] [stage3.py:2277:step] End to end step took 4.829240560531616
x3006c0s19b1n0: [2024-03-28 13:44:21,600] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 13712.96 | bwd_microstep: 37030.88 | bwd_inner_microstep: 36871.11 | bwd_allreduce_microstep: 159.60 | step_microstep: 4852.93
x3006c0s19b1n0: [2024-03-28 13:44:21,600] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 13712.95 | bwd: 37030.89 | bwd_inner: 36871.10 | bwd_allreduce: 159.63 | step: 4852.93
x3006c0s19b1n0: [2024-03-28 13:44:21,697] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:44:21,698] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.77 GB         CA 10.34 GB         Max_CA 14 GB 
x3006c0s19b1n0: [2024-03-28 13:44:21,698] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.38 GB, percent = 79.8%
x3006c0s19b1n0: <TIMER:interval-time,56.578479528427124>
x3006c0s19b1n0: <TIMER:interval-time,56.57845973968506><TIMER:interval-time,56.57848501205444>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,56.57851719856262>
x3006c0s1b0n0: <TIMER:interval-time,56.578518867492676><TIMER:interval-time,56.57851982116699><TIMER:interval-time,56.57852101325989>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,56.578641176223755>
x3006c0s1b0n0:  elapsed_time 56.578520 | consumed samples:          192 | consumed tokens:       393216 | elapsed time per iteration (ms): 56578.5 | learning rate: 2.684E-04 | global batch size:    64 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.131 | TFLOPs: 78.27 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:44:21,811] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:44:21,812] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:44:21,812] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.36 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:44:28,756] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:44:28,757] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:44:28,757] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.36 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:44:28,836] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:44:28,836] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:44:28,836] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.36 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:44:47,212] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:44:47,213] [INFO] [utils.py:801:see_memory_usage] MA 11.51 GB         Max_MA 23.77 GB         CA 11.59 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-28 13:44:47,213] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.39 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:44:47,294] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:44:47,295] [INFO] [utils.py:801:see_memory_usage] MA 11.52 GB         Max_MA 11.52 GB         CA 11.59 GB         Max_CA 12 GB 
x3006c0s19b1n0: [2024-03-28 13:44:47,295] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.39 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:44:54,074] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:44:54,075] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:44:54,075] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.38 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:44:54,151] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:44:54,152] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:44:54,152] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.38 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:45:12,983] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:45:12,983] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 23.87 GB         CA 13.01 GB         Max_CA 32 GB 
x3006c0s19b1n0: [2024-03-28 13:45:12,984] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.38 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:45:13,052] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:45:13,052] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 12.77 GB         CA 13.01 GB         Max_CA 13 GB 
x3006c0s19b1n0: [2024-03-28 13:45:13,052] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.38 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:45:17,567] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.490365505218506
x3006c0s19b1n0: [2024-03-28 13:45:17,578] [INFO] [stage3.py:2251:step] Full outer step loop took 4.5014495849609375
x3006c0s1b0n0: [2024-03-28 13:45:17,636] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.559679746627808
x3006c0s1b0n0: [2024-03-28 13:45:17,648] [INFO] [stage3.py:2251:step] Full outer step loop took 4.571457862854004
x3006c0s19b1n0: [2024-03-28 13:45:17,686] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.60919976234436
x3006c0s19b1n0: [2024-03-28 13:45:17,689] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.612502336502075
x3006c0s19b1n0: [2024-03-28 13:45:17,697] [INFO] [stage3.py:2251:step] Full outer step loop took 4.620019197463989
x3006c0s19b1n0: [2024-03-28 13:45:17,706] [INFO] [stage3.py:2251:step] Full outer step loop took 4.629105806350708
x3006c0s19b1n0: [2024-03-28 13:45:17,737] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.660707473754883
x3006c0s19b1n0: [2024-03-28 13:45:17,746] [INFO] [stage3.py:2251:step] Full outer step loop took 4.669718980789185
x3006c0s1b0n0: [2024-03-28 13:45:17,791] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.714740991592407
x3006c0s1b0n0: [2024-03-28 13:45:17,798] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.721587896347046
x3006c0s1b0n0: [2024-03-28 13:45:17,801] [INFO] [stage3.py:2251:step] Full outer step loop took 4.724501609802246
x3006c0s1b0n0: [2024-03-28 13:45:17,802] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.725428342819214
x3006c0s1b0n0: [2024-03-28 13:45:17,808] [INFO] [stage3.py:2251:step] Full outer step loop took 4.731827259063721
x3006c0s1b0n0: [2024-03-28 13:45:17,811] [INFO] [stage3.py:2251:step] Full outer step loop took 4.734452486038208
x3006c0s1b0n0: [2024-03-28 13:45:17,821] [INFO] [stage3.py:2277:step] End to end step took 4.744743347167969
x3006c0s19b1n0: [2024-03-28 13:45:17,821] [INFO] [stage3.py:2277:step] End to end step took 4.744955539703369
x3006c0s19b1n0: [2024-03-28 13:45:17,821] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4620.95
x3006c0s19b1n0: [2024-03-28 13:45:17,822] [INFO] [stage3.py:2277:step] End to end step took 4.745201587677002
x3006c0s1b0n0: [2024-03-28 13:45:17,822] [INFO] [stage3.py:2277:step] End to end step took 4.745150327682495
x3006c0s1b0n0: [2024-03-28 13:45:17,822] [INFO] [stage3.py:2277:step] End to end step took 4.745273590087891
x3006c0s1b0n0: [2024-03-28 13:45:17,822] [INFO] [stage3.py:2277:step] End to end step took 4.745323181152344
x3006c0s19b1n0: [2024-03-28 13:45:17,822] [INFO] [stage3.py:2277:step] End to end step took 4.745336294174194
x3006c0s19b1n0: [2024-03-28 13:45:17,822] [INFO] [logging.py:96:log_dist] [Rank 0] step=4, skipped=0, lr=[0.00023249999999999996], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:45:17,822] [INFO] [stage3.py:2277:step] End to end step took 4.745525360107422
x3006c0s19b1n0: [2024-03-28 13:45:17,822] [INFO] [timer.py:260:stop] epoch=0/micro_step=4/global_step=4, RunningAvgSamplesPerSec=2.08669230808517, CurrSamplesPerSec=2.0965751797536702, MemAllocated=10.26GB, MaxMemAllocated=12.77GB
x3006c0s19b1n0: [2024-03-28 13:45:17,823] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 13649.40 | bwd_microstep: 36871.82 | bwd_inner_microstep: 36707.28 | bwd_allreduce_microstep: 164.36 | step_microstep: 4769.85
x3006c0s19b1n0: [2024-03-28 13:45:17,823] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 13649.39 | bwd: 36871.82 | bwd_inner: 36707.27 | bwd_allreduce: 164.41 | step: 4769.86
x3006c0s19b1n0: [2024-03-28 13:45:17,926] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:45:17,926] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.77 GB         CA 10.34 GB         Max_CA 14 GB 
x3006c0s19b1n0: [2024-03-28 13:45:17,927] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.38 GB, percent = 79.8%
x3006c0s19b1n0: <TIMER:interval-time,56.228089570999146>
x3006c0s19b1n0: <TIMER:interval-time,56.22809338569641><TIMER:interval-time,56.228100061416626><TIMER:interval-time,56.228092193603516>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,56.22807550430298><TIMER:interval-time,56.22805714607239>
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,56.228086948394775>
x3006c0s1b0n0: <TIMER:interval-time,56.22820281982422>
x3006c0s1b0n0:  elapsed_time 56.228076 | consumed samples:          256 | consumed tokens:       524288 | elapsed time per iteration (ms): 56228.1 | learning rate: 2.325E-04 | global batch size:    64 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.138 | TFLOPs: 78.75 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:45:18,051] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:45:18,052] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:45:18,052] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.38 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:45:24,925] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:45:24,926] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:45:24,926] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.37 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:45:25,010] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:45:25,011] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:45:25,011] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.37 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:45:43,357] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:45:43,358] [INFO] [utils.py:801:see_memory_usage] MA 11.51 GB         Max_MA 23.77 GB         CA 11.59 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-28 13:45:43,358] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.38 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:45:43,443] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:45:43,443] [INFO] [utils.py:801:see_memory_usage] MA 11.52 GB         Max_MA 11.52 GB         CA 11.59 GB         Max_CA 12 GB 
x3006c0s19b1n0: [2024-03-28 13:45:43,444] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.38 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:45:50,175] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:45:50,175] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:45:50,175] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.37 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:45:50,260] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:45:50,261] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:45:50,261] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.37 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:46:09,106] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:46:09,107] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 23.87 GB         CA 13.01 GB         Max_CA 32 GB 
x3006c0s19b1n0: [2024-03-28 13:46:09,107] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.37 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:46:09,182] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:46:09,182] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 12.77 GB         CA 13.01 GB         Max_CA 13 GB 
x3006c0s19b1n0: [2024-03-28 13:46:09,182] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.37 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:46:13,772] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.565524578094482
x3006c0s19b1n0: [2024-03-28 13:46:13,779] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.572138071060181
x3006c0s19b1n0: [2024-03-28 13:46:13,786] [INFO] [stage3.py:2251:step] Full outer step loop took 4.5791919231414795
x3006c0s1b0n0: [2024-03-28 13:46:13,789] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.567997455596924
x3006c0s19b1n0: [2024-03-28 13:46:13,791] [INFO] [stage3.py:2251:step] Full outer step loop took 4.584707975387573
x3006c0s1b0n0: [2024-03-28 13:46:13,811] [INFO] [stage3.py:2251:step] Full outer step loop took 4.6043102741241455
x3006c0s1b0n0: [2024-03-28 13:46:13,840] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.633205413818359
x3006c0s1b0n0: [2024-03-28 13:46:13,851] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.644397497177124
x3006c0s1b0n0: [2024-03-28 13:46:13,860] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.653902530670166
x3006c0s19b1n0: [2024-03-28 13:46:13,861] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.654430150985718
x3006c0s1b0n0: [2024-03-28 13:46:13,861] [INFO] [stage3.py:2251:step] Full outer step loop took 4.654170751571655
x3006c0s1b0n0: [2024-03-28 13:46:13,861] [INFO] [stage3.py:2251:step] Full outer step loop took 4.654916524887085
x3006c0s1b0n0: [2024-03-28 13:46:13,869] [INFO] [stage3.py:2251:step] Full outer step loop took 4.662935256958008
x3006c0s19b1n0: [2024-03-28 13:46:13,872] [INFO] [stage3.py:2251:step] Full outer step loop took 4.6651716232299805
x3006c0s19b1n0: [2024-03-28 13:46:13,897] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.690695524215698
x3006c0s19b1n0: [2024-03-28 13:46:13,906] [INFO] [stage3.py:2251:step] Full outer step loop took 4.69969916343689
x3006c0s1b0n0: [2024-03-28 13:46:13,918] [INFO] [stage3.py:2277:step] End to end step took 4.71140193939209
x3006c0s1b0n0: [2024-03-28 13:46:13,918] [INFO] [stage3.py:2277:step] End to end step took 4.711416244506836
x3006c0s1b0n0: [2024-03-28 13:46:13,918] [INFO] [stage3.py:2277:step] End to end step took 4.711411952972412
x3006c0s19b1n0: [2024-03-28 13:46:13,918] [INFO] [stage3.py:2277:step] End to end step took 4.711501359939575
x3006c0s1b0n0: [2024-03-28 13:46:13,918] [INFO] [stage3.py:2277:step] End to end step took 4.711615324020386
x3006c0s19b1n0: [2024-03-28 13:46:13,918] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4579.49
x3006c0s19b1n0: [2024-03-28 13:46:13,918] [INFO] [stage3.py:2277:step] End to end step took 4.711895942687988
x3006c0s19b1n0: [2024-03-28 13:46:13,918] [INFO] [stage3.py:2277:step] End to end step took 4.71184515953064
x3006c0s19b1n0: [2024-03-28 13:46:13,919] [INFO] [stage3.py:2277:step] End to end step took 4.7120680809021
x3006c0s19b1n0: [2024-03-28 13:46:13,919] [INFO] [logging.py:96:log_dist] [Rank 0] step=5, skipped=0, lr=[0.0001884425039850356], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:46:13,919] [INFO] [timer.py:260:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=2.091155149722266, CurrSamplesPerSec=2.1001383475081643, MemAllocated=10.26GB, MaxMemAllocated=12.77GB
x3006c0s19b1n0: [2024-03-28 13:46:13,920] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 13522.58 | bwd_microstep: 36848.06 | bwd_inner_microstep: 36688.70 | bwd_allreduce_microstep: 159.21 | step_microstep: 4736.78
x3006c0s19b1n0: [2024-03-28 13:46:13,920] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 13522.57 | bwd: 36848.04 | bwd_inner: 36688.68 | bwd_allreduce: 159.23 | step: 4736.79
x3006c0s19b1n0: [2024-03-28 13:46:14,019] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:46:14,020] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.77 GB         CA 10.34 GB         Max_CA 14 GB 
x3006c0s19b1n0: [2024-03-28 13:46:14,020] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.37 GB, percent = 79.8%
x3006c0s19b1n0: <TIMER:interval-time,56.092814922332764><TIMER:interval-time,56.092814922332764><TIMER:interval-time,56.09281349182129>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,56.09281826019287>
x3006c0s1b0n0: <TIMER:interval-time,56.09285259246826><TIMER:interval-time,56.09285235404968><TIMER:interval-time,56.0928373336792><TIMER:interval-time,56.09285521507263>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0:  elapsed_time 56.092852 | consumed samples:          320 | consumed tokens:       655360 | elapsed time per iteration (ms): 56092.9 | learning rate: 1.884E-04 | global batch size:    64 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.141 | TFLOPs: 78.94 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:46:14,151] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:46:14,152] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:46:14,152] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.38 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:46:20,576] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:46:20,576] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:46:20,576] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.37 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:46:20,656] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:46:20,656] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:46:20,657] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.37 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:46:38,897] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:46:38,897] [INFO] [utils.py:801:see_memory_usage] MA 11.51 GB         Max_MA 23.77 GB         CA 11.59 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-28 13:46:38,898] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.38 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:46:38,981] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:46:38,981] [INFO] [utils.py:801:see_memory_usage] MA 11.52 GB         Max_MA 11.52 GB         CA 11.59 GB         Max_CA 12 GB 
x3006c0s19b1n0: [2024-03-28 13:46:38,981] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.4 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:46:45,295] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:46:45,296] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:46:45,296] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.39 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:46:45,376] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:46:45,377] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:46:45,377] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.39 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:47:04,136] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:47:04,137] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 23.87 GB         CA 13.01 GB         Max_CA 32 GB 
x3006c0s19b1n0: [2024-03-28 13:47:04,137] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.39 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:47:04,208] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:47:04,209] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 12.77 GB         CA 13.01 GB         Max_CA 13 GB 
x3006c0s19b1n0: [2024-03-28 13:47:04,209] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.39 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:47:08,695] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.462487459182739
x3006c0s19b1n0: [2024-03-28 13:47:08,709] [INFO] [stage3.py:2251:step] Full outer step loop took 4.4768288135528564
x3006c0s19b1n0: [2024-03-28 13:47:08,789] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.555566787719727
x3006c0s1b0n0: [2024-03-28 13:47:08,814] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.581567764282227
x3006c0s1b0n0: [2024-03-28 13:47:08,833] [INFO] [stage3.py:2251:step] Full outer step loop took 4.600051641464233
x3006c0s19b1n0: [2024-03-28 13:47:08,836] [INFO] [stage3.py:2251:step] Full outer step loop took 4.603153944015503
x3006c0s19b1n0: [2024-03-28 13:47:08,840] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.607159614562988
x3006c0s19b1n0: [2024-03-28 13:47:08,843] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.610114812850952
x3006c0s19b1n0: [2024-03-28 13:47:08,849] [INFO] [stage3.py:2251:step] Full outer step loop took 4.616652011871338
x3006c0s19b1n0: [2024-03-28 13:47:08,852] [INFO] [stage3.py:2251:step] Full outer step loop took 4.619281530380249
x3006c0s1b0n0: [2024-03-28 13:47:08,899] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.665700912475586
x3006c0s1b0n0: [2024-03-28 13:47:08,916] [INFO] [stage3.py:2251:step] Full outer step loop took 4.682971000671387
x3006c0s1b0n0: [2024-03-28 13:47:08,927] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.694615364074707
x3006c0s1b0n0: [2024-03-28 13:47:08,936] [INFO] [stage3.py:2251:step] Full outer step loop took 4.7036824226379395
x3006c0s1b0n0: [2024-03-28 13:47:08,974] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.741214752197266
x3006c0s1b0n0: [2024-03-28 13:47:08,983] [INFO] [stage3.py:2251:step] Full outer step loop took 4.750243186950684
x3006c0s1b0n0: [2024-03-28 13:47:08,993] [INFO] [stage3.py:2277:step] End to end step took 4.760660886764526
x3006c0s19b1n0: [2024-03-28 13:47:08,993] [INFO] [stage3.py:2277:step] End to end step took 4.760659694671631
x3006c0s19b1n0: [2024-03-28 13:47:08,993] [INFO] [stage3.py:2277:step] End to end step took 4.760695695877075
x3006c0s19b1n0: [2024-03-28 13:47:08,993] [INFO] [stage3.py:2277:step] End to end step took 4.760484933853149
x3006c0s1b0n0: [2024-03-28 13:47:08,993] [INFO] [stage3.py:2277:step] End to end step took 4.760687351226807
x3006c0s1b0n0: [2024-03-28 13:47:08,993] [INFO] [stage3.py:2277:step] End to end step took 4.760715961456299
x3006c0s19b1n0: [2024-03-28 13:47:08,993] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4477.14
x3006c0s1b0n0: [2024-03-28 13:47:08,994] [INFO] [stage3.py:2277:step] End to end step took 4.76096248626709
x3006c0s19b1n0: [2024-03-28 13:47:08,994] [INFO] [stage3.py:2277:step] End to end step took 4.761191129684448
x3006c0s19b1n0: [2024-03-28 13:47:08,994] [INFO] [logging.py:96:log_dist] [Rank 0] step=6, skipped=0, lr=[0.0001415574960149644], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:47:08,994] [INFO] [timer.py:260:stop] epoch=0/micro_step=6/global_step=6, RunningAvgSamplesPerSec=2.101348178895079, CurrSamplesPerSec=2.132532327370735, MemAllocated=10.26GB, MaxMemAllocated=12.77GB
x3006c0s19b1n0: [2024-03-28 13:47:08,995] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 12664.25 | bwd_microstep: 36658.24 | bwd_inner_microstep: 36501.11 | bwd_allreduce_microstep: 156.97 | step_microstep: 4785.56
x3006c0s19b1n0: [2024-03-28 13:47:08,995] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 12664.24 | bwd: 36658.23 | bwd_inner: 36501.09 | bwd_allreduce: 157.00 | step: 4785.56
x3006c0s19b1n0: [2024-03-28 13:47:09,086] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:47:09,087] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.77 GB         CA 10.34 GB         Max_CA 14 GB 
x3006c0s19b1n0: [2024-03-28 13:47:09,087] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.39 GB, percent = 79.8%
x3006c0s19b1n0: <TIMER:interval-time,55.067138671875><TIMER:interval-time,55.06714105606079><TIMER:interval-time,55.067140340805054><TIMER:interval-time,55.067142724990845>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,55.06715393066406><TIMER:interval-time,55.06715393066406><TIMER:interval-time,55.06715393066406>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,55.06726312637329>
x3006c0s1b0n0:  elapsed_time 55.067154 | consumed samples:          384 | consumed tokens:       786432 | elapsed time per iteration (ms): 55067.2 | learning rate: 1.416E-04 | global batch size:    64 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.162 | TFLOPs: 80.41 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:47:09,187] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:47:09,188] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:47:09,188] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.38 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:47:16,134] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:47:16,135] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:47:16,135] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.37 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:47:16,220] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:47:16,221] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:47:16,221] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.37 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:47:34,581] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:47:34,582] [INFO] [utils.py:801:see_memory_usage] MA 11.51 GB         Max_MA 23.77 GB         CA 11.59 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-28 13:47:34,582] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.39 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:47:34,663] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:47:34,663] [INFO] [utils.py:801:see_memory_usage] MA 11.52 GB         Max_MA 11.52 GB         CA 11.59 GB         Max_CA 12 GB 
x3006c0s19b1n0: [2024-03-28 13:47:34,664] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.39 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:47:41,498] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:47:41,499] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:47:41,499] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.39 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:47:41,583] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:47:41,584] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:47:41,584] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.39 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:48:00,446] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:48:00,447] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 23.87 GB         CA 13.01 GB         Max_CA 32 GB 
x3006c0s19b1n0: [2024-03-28 13:48:00,447] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.39 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:48:00,522] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:48:00,522] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 12.77 GB         CA 13.01 GB         Max_CA 13 GB 
x3006c0s19b1n0: [2024-03-28 13:48:00,523] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.39 GB, percent = 79.8%
x3006c0s1b0n0: [2024-03-28 13:48:05,118] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.570834636688232
x3006c0s1b0n0: [2024-03-28 13:48:05,134] [INFO] [stage3.py:2251:step] Full outer step loop took 4.586690664291382
x3006c0s1b0n0: [2024-03-28 13:48:05,143] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.596180438995361
x3006c0s19b1n0: [2024-03-28 13:48:05,148] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.600460052490234
x3006c0s19b1n0: [2024-03-28 13:48:05,157] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.609968185424805
x3006c0s1b0n0: [2024-03-28 13:48:05,166] [INFO] [stage3.py:2251:step] Full outer step loop took 4.618914365768433
x3006c0s19b1n0: [2024-03-28 13:48:05,185] [INFO] [stage3.py:2251:step] Full outer step loop took 4.6377036571502686
x3006c0s19b1n0: [2024-03-28 13:48:05,191] [INFO] [stage3.py:2251:step] Full outer step loop took 4.643714189529419
x3006c0s19b1n0: [2024-03-28 13:48:05,192] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.645252466201782
x3006c0s19b1n0: [2024-03-28 13:48:05,195] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.648115634918213
x3006c0s19b1n0: [2024-03-28 13:48:05,202] [INFO] [stage3.py:2251:step] Full outer step loop took 4.654756546020508
x3006c0s19b1n0: [2024-03-28 13:48:05,204] [INFO] [stage3.py:2251:step] Full outer step loop took 4.657169580459595
x3006c0s1b0n0: [2024-03-28 13:48:05,236] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.689107656478882
x3006c0s1b0n0: [2024-03-28 13:48:05,252] [INFO] [stage3.py:2251:step] Full outer step loop took 4.7049994468688965
x3006c0s1b0n0: [2024-03-28 13:48:05,258] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.71128249168396
x3006c0s1b0n0: [2024-03-28 13:48:05,267] [INFO] [stage3.py:2251:step] Full outer step loop took 4.720291376113892
x3006c0s19b1n0: [2024-03-28 13:48:05,278] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4644.01
x3006c0s19b1n0: [2024-03-28 13:48:05,278] [INFO] [stage3.py:2277:step] End to end step took 4.7308666706085205
x3006c0s1b0n0: [2024-03-28 13:48:05,278] [INFO] [stage3.py:2277:step] End to end step took 4.730849742889404
x3006c0s1b0n0: [2024-03-28 13:48:05,278] [INFO] [stage3.py:2277:step] End to end step took 4.73091983795166
x3006c0s19b1n0: [2024-03-28 13:48:05,278] [INFO] [stage3.py:2277:step] End to end step took 4.730997085571289
x3006c0s19b1n0: [2024-03-28 13:48:05,278] [INFO] [stage3.py:2277:step] End to end step took 4.73103141784668
x3006c0s19b1n0: [2024-03-28 13:48:05,278] [INFO] [stage3.py:2277:step] End to end step took 4.731021881103516
x3006c0s1b0n0: [2024-03-28 13:48:05,278] [INFO] [stage3.py:2277:step] End to end step took 4.730879783630371
x3006c0s19b1n0: [2024-03-28 13:48:05,278] [INFO] [logging.py:96:log_dist] [Rank 0] step=7, skipped=0, lr=[9.750000000000001e-05], mom=[(0.9, 0.95)]
x3006c0s1b0n0: [2024-03-28 13:48:05,278] [INFO] [stage3.py:2277:step] End to end step took 4.731350660324097
x3006c0s19b1n0: [2024-03-28 13:48:05,279] [INFO] [timer.py:260:stop] epoch=0/micro_step=7/global_step=7, RunningAvgSamplesPerSec=2.0991899986101505, CurrSamplesPerSec=2.090601427084703, MemAllocated=10.26GB, MaxMemAllocated=12.77GB
x3006c0s19b1n0: [2024-03-28 13:48:05,279] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 13700.74 | bwd_microstep: 36875.17 | bwd_inner_microstep: 36716.08 | bwd_allreduce_microstep: 158.95 | step_microstep: 4755.89
x3006c0s19b1n0: [2024-03-28 13:48:05,279] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 13700.73 | bwd: 36875.17 | bwd_inner: 36716.07 | bwd_allreduce: 158.98 | step: 4755.89
x3006c0s19b1n0: [2024-03-28 13:48:05,385] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:48:05,385] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.77 GB         CA 10.34 GB         Max_CA 14 GB 
x3006c0s19b1n0: [2024-03-28 13:48:05,386] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.39 GB, percent = 79.8%
x3006c0s19b1n0: <TIMER:interval-time,56.29806971549988>
x3006c0s19b1n0: <TIMER:interval-time,56.298073530197144><TIMER:interval-time,56.2980740070343>
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,56.29809880256653><TIMER:interval-time,56.29808783531189><TIMER:interval-time,56.29810118675232><TIMER:interval-time,56.29810404777527>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s19b1n0: <TIMER:interval-time,56.29812502861023>
x3006c0s1b0n0:  elapsed_time 56.298099 | consumed samples:          448 | consumed tokens:       917504 | elapsed time per iteration (ms): 56298.1 | learning rate: 9.750E-05 | global batch size:    64 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.137 | TFLOPs: 78.66 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:48:05,511] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:48:05,512] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:48:05,512] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.38 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:48:12,306] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:48:12,306] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:48:12,307] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.37 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:48:12,388] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:48:12,388] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:48:12,388] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.37 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:48:30,350] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:48:30,351] [INFO] [utils.py:801:see_memory_usage] MA 11.51 GB         Max_MA 23.77 GB         CA 11.59 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-28 13:48:30,351] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.4 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:48:30,437] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:48:30,437] [INFO] [utils.py:801:see_memory_usage] MA 11.52 GB         Max_MA 11.52 GB         CA 11.59 GB         Max_CA 12 GB 
x3006c0s19b1n0: [2024-03-28 13:48:30,437] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.4 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:48:37,080] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:48:37,081] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:48:37,081] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.39 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:48:37,182] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:48:37,183] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:48:37,183] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.39 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:48:56,157] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:48:56,157] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 23.87 GB         CA 13.01 GB         Max_CA 32 GB 
x3006c0s19b1n0: [2024-03-28 13:48:56,158] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.39 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:48:56,231] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:48:56,232] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 12.77 GB         CA 13.01 GB         Max_CA 13 GB 
x3006c0s19b1n0: [2024-03-28 13:48:56,232] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.39 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:49:00,838] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.581596374511719
x3006c0s19b1n0: [2024-03-28 13:49:00,867] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.6112236976623535
x3006c0s19b1n0: [2024-03-28 13:49:00,867] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.611407518386841
x3006c0s19b1n0: [2024-03-28 13:49:00,868] [INFO] [stage3.py:2251:step] Full outer step loop took 4.611790180206299
x3006c0s1b0n0: [2024-03-28 13:49:00,878] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.621250867843628
x3006c0s19b1n0: [2024-03-28 13:49:00,878] [INFO] [stage3.py:2251:step] Full outer step loop took 4.622120380401611
x3006c0s19b1n0: [2024-03-28 13:49:00,890] [INFO] [stage3.py:2251:step] Full outer step loop took 4.63393759727478
x3006c0s1b0n0: [2024-03-28 13:49:00,899] [INFO] [stage3.py:2251:step] Full outer step loop took 4.642320156097412
x3006c0s19b1n0: [2024-03-28 13:49:00,922] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.665795087814331
x3006c0s19b1n0: [2024-03-28 13:49:00,932] [INFO] [stage3.py:2251:step] Full outer step loop took 4.67553973197937
x3006c0s1b0n0: [2024-03-28 13:49:00,995] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.739341735839844
x3006c0s1b0n0: [2024-03-28 13:49:01,002] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.745614767074585
x3006c0s1b0n0: [2024-03-28 13:49:01,028] [INFO] [stage3.py:2251:step] Full outer step loop took 4.7714362144470215
x3006c0s1b0n0: [2024-03-28 13:49:01,029] [INFO] [stage3.py:2251:step] Full outer step loop took 4.7729833126068115
x3006c0s1b0n0: [2024-03-28 13:49:01,033] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.776470184326172
x3006c0s1b0n0: [2024-03-28 13:49:01,042] [INFO] [stage3.py:2251:step] Full outer step loop took 4.785482883453369
x3006c0s1b0n0: [2024-03-28 13:49:01,052] [INFO] [stage3.py:2277:step] End to end step took 4.796157121658325
x3006c0s1b0n0: [2024-03-28 13:49:01,052] [INFO] [stage3.py:2277:step] End to end step took 4.79636287689209
x3006c0s19b1n0: [2024-03-28 13:49:01,052] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4612.25
x3006c0s19b1n0: [2024-03-28 13:49:01,052] [INFO] [stage3.py:2277:step] End to end step took 4.796432018280029
x3006c0s19b1n0: [2024-03-28 13:49:01,052] [INFO] [stage3.py:2277:step] End to end step took 4.79642391204834
x3006c0s19b1n0: [2024-03-28 13:49:01,053] [INFO] [stage3.py:2277:step] End to end step took 4.796608924865723
x3006c0s1b0n0: [2024-03-28 13:49:01,053] [INFO] [stage3.py:2277:step] End to end step took 4.7965593338012695
x3006c0s19b1n0: [2024-03-28 13:49:01,053] [INFO] [stage3.py:2277:step] End to end step took 4.79685640335083
x3006c0s19b1n0: [2024-03-28 13:49:01,053] [INFO] [logging.py:96:log_dist] [Rank 0] step=8, skipped=0, lr=[6.158400017893797e-05], mom=[(0.9, 0.95)]
x3006c0s1b0n0: [2024-03-28 13:49:01,053] [INFO] [stage3.py:2277:step] End to end step took 4.796874284744263
x3006c0s19b1n0: [2024-03-28 13:49:01,053] [INFO] [timer.py:260:stop] epoch=0/micro_step=8/global_step=8, RunningAvgSamplesPerSec=2.0977446155853414, CurrSamplesPerSec=2.0905474542874445, MemAllocated=10.26GB, MaxMemAllocated=12.77GB
x3006c0s19b1n0: [2024-03-28 13:49:01,054] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 13355.11 | bwd_microstep: 36589.43 | bwd_inner_microstep: 36433.07 | bwd_allreduce_microstep: 156.20 | step_microstep: 4821.63
x3006c0s19b1n0: [2024-03-28 13:49:01,054] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 13355.10 | bwd: 36589.43 | bwd_inner: 36433.06 | bwd_allreduce: 156.24 | step: 4821.64
x3006c0s19b1n0: [2024-03-28 13:49:01,147] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:49:01,148] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.77 GB         CA 10.34 GB         Max_CA 14 GB 
x3006c0s19b1n0: [2024-03-28 13:49:01,148] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.34 GB, percent = 79.8%
x3006c0s19b1n0: <TIMER:interval-time,55.76207685470581><TIMER:interval-time,55.76206970214844><TIMER:interval-time,55.762070655822754><TIMER:interval-time,55.76207900047302>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,55.76208567619324>
x3006c0s1b0n0: <TIMER:interval-time,55.762091398239136>
x3006c0s1b0n0: <TIMER:interval-time,55.76210021972656>
x3006c0s1b0n0: <TIMER:interval-time,55.76220703125>
x3006c0s1b0n0:  elapsed_time 55.762100 | consumed samples:          512 | consumed tokens:      1048576 | elapsed time per iteration (ms): 55762.1 | learning rate: 6.158E-05 | global batch size:    64 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.148 | TFLOPs: 79.41 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:49:01,287] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:49:01,288] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:49:01,288] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.38 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:49:08,209] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:49:08,209] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:49:08,210] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.37 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:49:08,290] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:49:08,291] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:49:08,291] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.37 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:49:26,859] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:49:26,860] [INFO] [utils.py:801:see_memory_usage] MA 11.51 GB         Max_MA 23.77 GB         CA 11.59 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-28 13:49:26,860] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.4 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:49:26,942] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:49:26,942] [INFO] [utils.py:801:see_memory_usage] MA 11.52 GB         Max_MA 11.52 GB         CA 11.59 GB         Max_CA 12 GB 
x3006c0s19b1n0: [2024-03-28 13:49:26,942] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.4 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:49:33,612] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:49:33,612] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:49:33,613] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.39 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:49:33,700] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:49:33,701] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:49:33,701] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.39 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:49:52,927] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:49:52,928] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 23.87 GB         CA 13.01 GB         Max_CA 32 GB 
x3006c0s19b1n0: [2024-03-28 13:49:52,928] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.39 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:49:53,003] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:49:53,004] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 12.77 GB         CA 13.01 GB         Max_CA 13 GB 
x3006c0s19b1n0: [2024-03-28 13:49:53,004] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.39 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:49:57,548] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.520265579223633
x3006c0s19b1n0: [2024-03-28 13:49:57,557] [INFO] [stage3.py:2251:step] Full outer step loop took 4.529317855834961
x3006c0s1b0n0: [2024-03-28 13:49:57,571] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.543050527572632
x3006c0s1b0n0: [2024-03-28 13:49:57,580] [INFO] [stage3.py:2251:step] Full outer step loop took 4.55209755897522
x3006c0s19b1n0: [2024-03-28 13:49:57,669] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.641093492507935
x3006c0s1b0n0: [2024-03-28 13:49:57,703] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.674646854400635
x3006c0s1b0n0: [2024-03-28 13:49:57,713] [INFO] [stage3.py:2251:step] Full outer step loop took 4.685311794281006
x3006c0s19b1n0: [2024-03-28 13:49:57,724] [INFO] [stage3.py:2251:step] Full outer step loop took 4.695678949356079
x3006c0s1b0n0: [2024-03-28 13:49:57,729] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.7007834911346436
x3006c0s19b1n0: [2024-03-28 13:49:57,736] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.707433700561523
x3006c0s19b1n0: [2024-03-28 13:49:57,742] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.713739633560181
x3006c0s1b0n0: [2024-03-28 13:49:57,743] [INFO] [stage3.py:2251:step] Full outer step loop took 4.7155280113220215
x3006c0s19b1n0: [2024-03-28 13:49:57,745] [INFO] [stage3.py:2251:step] Full outer step loop took 4.716670751571655
x3006c0s1b0n0: [2024-03-28 13:49:57,747] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.718977212905884
x3006c0s19b1n0: [2024-03-28 13:49:57,751] [INFO] [stage3.py:2251:step] Full outer step loop took 4.722832441329956
x3006c0s1b0n0: [2024-03-28 13:49:57,756] [INFO] [stage3.py:2251:step] Full outer step loop took 4.727983474731445
x3006c0s19b1n0: [2024-03-28 13:49:57,766] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4696.26
x3006c0s1b0n0: [2024-03-28 13:49:57,766] [INFO] [stage3.py:2277:step] End to end step took 4.738520860671997
x3006c0s19b1n0: [2024-03-28 13:49:57,766] [INFO] [stage3.py:2277:step] End to end step took 4.738329172134399
x3006c0s19b1n0: [2024-03-28 13:49:57,766] [INFO] [stage3.py:2277:step] End to end step took 4.7385573387146
x3006c0s19b1n0: [2024-03-28 13:49:57,766] [INFO] [stage3.py:2277:step] End to end step took 4.738632678985596
x3006c0s1b0n0: [2024-03-28 13:49:57,767] [INFO] [stage3.py:2277:step] End to end step took 4.7386345863342285
x3006c0s1b0n0: [2024-03-28 13:49:57,767] [INFO] [stage3.py:2277:step] End to end step took 4.738693714141846
x3006c0s19b1n0: [2024-03-28 13:49:57,767] [INFO] [logging.py:96:log_dist] [Rank 0] step=9, skipped=0, lr=[3.814149619390238e-05], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:49:57,767] [INFO] [stage3.py:2277:step] End to end step took 4.7390007972717285
x3006c0s1b0n0: [2024-03-28 13:49:57,767] [INFO] [stage3.py:2277:step] End to end step took 4.738996505737305
x3006c0s19b1n0: [2024-03-28 13:49:57,767] [INFO] [timer.py:260:stop] epoch=0/micro_step=9/global_step=9, RunningAvgSamplesPerSec=2.094664058089415, CurrSamplesPerSec=2.0763690543775715, MemAllocated=10.26GB, MaxMemAllocated=12.77GB
x3006c0s19b1n0: [2024-03-28 13:49:57,767] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 13509.71 | bwd_microstep: 37450.92 | bwd_inner_microstep: 37288.32 | bwd_allreduce_microstep: 162.45 | step_microstep: 4763.20
x3006c0s19b1n0: [2024-03-28 13:49:57,768] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 13509.70 | bwd: 37450.92 | bwd_inner: 37288.30 | bwd_allreduce: 162.48 | step: 4763.21
x3006c0s19b1n0: [2024-03-28 13:49:57,867] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:49:57,868] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.77 GB         CA 10.34 GB         Max_CA 14 GB 
x3006c0s19b1n0: [2024-03-28 13:49:57,868] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.34 GB, percent = 79.8%
x3006c0s19b1n0: <TIMER:interval-time,56.719643115997314><TIMER:interval-time,56.71964430809021><TIMER:interval-time,56.71964168548584>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,56.71964979171753><TIMER:interval-time,56.719661474227905><TIMER:interval-time,56.71966481208801>
x3006c0s1b0n0: <TIMER:interval-time,56.71966910362244>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s19b1n0: <TIMER:interval-time,56.71975302696228>
x3006c0s1b0n0:  elapsed_time 56.719661 | consumed samples:          576 | consumed tokens:      1179648 | elapsed time per iteration (ms): 56719.7 | learning rate: 3.814E-05 | global batch size:    64 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.128 | TFLOPs: 78.07 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:49:57,998] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:49:57,999] [INFO] [utils.py:801:see_memory_usage] MA 10.27 GB         Max_MA 10.27 GB         CA 10.34 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:49:57,999] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.38 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:50:04,942] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:50:04,943] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:50:04,943] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.37 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:50:05,024] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:50:05,025] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:50:05,025] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.37 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:50:23,469] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:50:23,470] [INFO] [utils.py:801:see_memory_usage] MA 11.51 GB         Max_MA 23.77 GB         CA 11.59 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-28 13:50:23,470] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.41 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:50:23,557] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:50:23,557] [INFO] [utils.py:801:see_memory_usage] MA 11.52 GB         Max_MA 11.52 GB         CA 11.59 GB         Max_CA 12 GB 
x3006c0s19b1n0: [2024-03-28 13:50:23,557] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.41 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:50:30,374] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:50:30,375] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 23.54 GB         CA 27.11 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:50:30,375] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.4 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:50:30,464] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:50:30,465] [INFO] [utils.py:801:see_memory_usage] MA 21.24 GB         Max_MA 21.24 GB         CA 21.4 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:50:30,465] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.4 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:50:49,352] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:50:49,353] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 23.87 GB         CA 13.01 GB         Max_CA 32 GB 
x3006c0s19b1n0: [2024-03-28 13:50:49,353] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.41 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:50:49,425] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:50:49,426] [INFO] [utils.py:801:see_memory_usage] MA 12.77 GB         Max_MA 12.77 GB         CA 13.01 GB         Max_CA 13 GB 
x3006c0s19b1n0: [2024-03-28 13:50:49,426] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.41 GB, percent = 79.8%
x3006c0s19b1n0: [2024-03-28 13:50:53,921] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.471498012542725
x3006c0s19b1n0: [2024-03-28 13:50:53,934] [INFO] [stage3.py:2251:step] Full outer step loop took 4.4842445850372314
x3006c0s19b1n0: [2024-03-28 13:50:54,031] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.5809266567230225
x3006c0s19b1n0: [2024-03-28 13:50:54,050] [INFO] [stage3.py:2251:step] Full outer step loop took 4.600102424621582
x3006c0s1b0n0: [2024-03-28 13:50:54,070] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.620378494262695
x3006c0s1b0n0: [2024-03-28 13:50:54,071] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.6209845542907715
x3006c0s1b0n0: [2024-03-28 13:50:54,079] [INFO] [stage3.py:2251:step] Full outer step loop took 4.629401206970215
x3006c0s1b0n0: [2024-03-28 13:50:54,080] [INFO] [stage3.py:2251:step] Full outer step loop took 4.630033731460571
x3006c0s19b1n0: [2024-03-28 13:50:54,089] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.639307260513306
x3006c0s19b1n0: [2024-03-28 13:50:54,094] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.644558429718018
x3006c0s19b1n0: [2024-03-28 13:50:54,099] [INFO] [stage3.py:2251:step] Full outer step loop took 4.648879528045654
x3006c0s19b1n0: [2024-03-28 13:50:54,103] [INFO] [stage3.py:2251:step] Full outer step loop took 4.653617858886719
x3006c0s1b0n0: [2024-03-28 13:50:54,144] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.69375205039978
x3006c0s1b0n0: [2024-03-28 13:50:54,161] [INFO] [stage3.py:2251:step] Full outer step loop took 4.710953950881958
x3006c0s1b0n0: [2024-03-28 13:50:54,198] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.748584985733032
x3006c0s1b0n0: [2024-03-28 13:50:54,207] [INFO] [stage3.py:2251:step] Full outer step loop took 4.757596015930176
x3006c0s1b0n0: [2024-03-28 13:50:54,218] [INFO] [stage3.py:2277:step] End to end step took 4.767945289611816
x3006c0s19b1n0: [2024-03-28 13:50:54,218] [INFO] [stage3.py:2277:step] End to end step took 4.767983675003052
x3006c0s19b1n0: [2024-03-28 13:50:54,218] [INFO] [stage3.py:2277:step] End to end step took 4.767841100692749
x3006c0s1b0n0: [2024-03-28 13:50:54,218] [INFO] [stage3.py:2277:step] End to end step took 4.768145561218262
x3006c0s19b1n0: [2024-03-28 13:50:54,218] [INFO] [stage3.py:2277:step] End to end step took 4.768189191818237
x3006c0s1b0n0: [2024-03-28 13:50:54,218] [INFO] [stage3.py:2277:step] End to end step took 4.768226861953735
x3006c0s19b1n0: [2024-03-28 13:50:54,218] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4484.72
x3006c0s19b1n0: [2024-03-28 13:50:54,218] [INFO] [stage3.py:2277:step] End to end step took 4.7686004638671875
x3006c0s1b0n0: [2024-03-28 13:50:54,218] [INFO] [stage3.py:2277:step] End to end step took 4.768476724624634
x3006c0s19b1n0: [2024-03-28 13:50:54,219] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[3e-05], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:50:54,219] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=2.0937550113091565, CurrSamplesPerSec=2.087413709499676, MemAllocated=10.26GB, MaxMemAllocated=12.77GB
x3006c0s19b1n0: [2024-03-28 13:50:54,219] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 13676.89 | bwd_microstep: 36985.35 | bwd_inner_microstep: 36821.96 | bwd_allreduce_microstep: 163.25 | step_microstep: 4793.22
x3006c0s19b1n0: [2024-03-28 13:50:54,220] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 13676.87 | bwd: 36985.34 | bwd_inner: 36821.94 | bwd_allreduce: 163.27 | step: 4793.22
x3006c0s19b1n0: [2024-03-28 13:50:54,312] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:50:54,312] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 12.77 GB         CA 10.34 GB         Max_CA 14 GB 
x3006c0s19b1n0: [2024-03-28 13:50:54,312] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 401.36 GB, percent = 79.8%
x3006c0s19b1n0: <TIMER:interval-time,56.444032192230225><TIMER:interval-time,56.44403386116028><TIMER:interval-time,56.44403290748596><TIMER:interval-time,56.4439971446991>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,56.44404363632202><TIMER:interval-time,56.44404196739197><TIMER:interval-time,56.44404745101929><TIMER:interval-time,56.44404602050781>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0:  elapsed_time 56.444042 | consumed samples:          640 | consumed tokens:      1310720 | elapsed time per iteration (ms): 56444.0 | learning rate: 3.000E-05 | global batch size:    64 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.134 | TFLOPs: 78.45 |
x3006c0s1b0n0: <<<only_train:575.0495388507843>>><<<only_train:575.049791097641>>>
x3006c0s1b0n0: 
x3006c0s19b1n0: <<<only_train:575.0492639541626>>><<<only_train:575.0496273040771>>><<<only_train:575.0496320724487>>>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: <<<only_train:575.0495908260345>>>
x3006c0s1b0n0: <<<only_train:575.04958319664>>>
x3006c0s1b0n0: <<<only_train:575.0494592189789>>>
x3006c0s19b1n0: [after training ends] datetime: 2024-03-28 13:50:54 
x3006c0s1b0n0: <<<full_time:575.0498373508453>>><<<full_time:575.049845457077>>><<<full_time:575.0496928691864>>>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: <<<full_time:575.0501027107239>>>
x3006c0s19b1n0: <<<full_time:575.0498609542847>>><<<full_time:575.0495414733887>>><<<full_time:575.0499103069305>>>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: <<<full_time:575.0499114990234>>>
x3006c0s1b0n0: [2024-03-28 13:51:03,084] [INFO] [launch.py:348:main] Process 11657 exits successfully.
x3006c0s19b1n0: [2024-03-28 13:51:03,641] [INFO] [launch.py:348:main] Process 19490 exits successfully.
x3006c0s1b0n0: [2024-03-28 13:51:11,093] [INFO] [launch.py:348:main] Process 11655 exits successfully.
x3006c0s1b0n0: [2024-03-28 13:51:11,093] [INFO] [launch.py:348:main] Process 11658 exits successfully.
x3006c0s1b0n0: [2024-03-28 13:51:11,093] [INFO] [launch.py:348:main] Process 11656 exits successfully.
x3006c0s19b1n0: [2024-03-28 13:51:11,650] [INFO] [launch.py:348:main] Process 19492 exits successfully.
x3006c0s19b1n0: [2024-03-28 13:51:11,651] [INFO] [launch.py:348:main] Process 19493 exits successfully.
x3006c0s19b1n0: [2024-03-28 13:51:11,651] [INFO] [launch.py:348:main] Process 19491 exits successfully.

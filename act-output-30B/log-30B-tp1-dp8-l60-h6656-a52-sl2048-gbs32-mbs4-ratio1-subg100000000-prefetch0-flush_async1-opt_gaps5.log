[2024-03-28 13:33:26,090] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-28 13:33:29,176] [INFO] [runner.py:463:main] Using IP address of 10.140.57.107 for node x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-28 13:33:29,178] [INFO] [runner.py:557:main] deepspeed_env file = ./.deepspeed_env
[2024-03-28 13:33:29,178] [INFO] [multinode_runner.py:80:get_cmd] Running on the following workers: x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov,x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-28 13:33:29,178] [INFO] [runner.py:568:main] cmd = pdsh -S -f 1024 -w x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov,x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov export PYTHONUSERBASE=/home/am6429/.local/polaris/conda/2023-10-04; export PYTHONPATH=/home/am6429/dl-io/Megatron-DeepSpeed; export PATH=/home/am6429/.conda/envs/dspeed_env/bin:/soft/datascience/conda/2023-10-04/mconda3/condabin:/soft/compilers/cudatoolkit/cuda-11.8.0/bin:/soft/buildtools/cmake/cmake-3.23.2/cmake-3.23.2-linux-x86_64/bin:/opt/cray/pe/gcc/11.2.0/bin:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/include:/opt/cray/pe/pals/1.2.11/bin:/opt/cray/pe/craype/2.7.20/bin:/opt/cray/libfabric/1.15.2.0/bin:/home/am6429/.conda/envs/dspeed_env/bin:/opt/cray/pe/perftools/23.03.0/bin:/opt/cray/pe/papi/7.0.0.1/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/am6429/.local/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/am6429/veloc-build/include:/home/am6429/veloc-build/bin:/soft/datascience/conda/2023-01-10/mconda3/include:/opt/cray/pe/bin:/soft/datascience/conda/2023-01-10/mconda3/include; export LD_LIBRARY_PATH=/usr/lib64/:/soft/compilers/cudatoolkit/cuda-11.8.0/lib64:/opt/cray/pe/gcc/11.2.0/snos/lib64:/soft/compilers/cudatoolkit/cuda-11.8.0/extras/CUPTI/lib64:/soft/libraries/trt/TensorRT-8.5.3.1.Linux.x86_64-gnu.cuda-11.8.cudnn8.6/lib:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/lib:/soft/libraries/cudnn/cudnn-11-linux-x64-v8.6.0.163/lib:/opt/cray/libfabric/1.15.2.0/lib64:/usr/lib64:/opt/cray/pe/papi/7.0.0.1/lib64:/home/am6429/veloc-build/lib:/home/am6429/veloc-build/lib64:/home/am6429/nvcomp/lib:/soft/datascience/conda/2023-01-10/mconda3/lib:/soft/datascience/conda/2023-01-10/mconda3/lib/; export http_proxy=http://proxy.alcf.anl.gov:3128; export https_proxy=http://proxy.alcf.anl.gov:3128; export CC=gcc; export CXX=g++; export IBV_FORK_SAFE=1; export CFLAGS=-I/soft/datascience/conda/2023-01-10/mconda3/include/; export LDFLAGS=-L/soft/datascience/conda/2023-01-10/mconda3/lib/; export CUDA_DEVICE_MAX_CONNECTIONS=1; export TORCHSNAPSHOT_PER_RANK_MEMORY_BUDGET_BYTES=34359738368; export _DEFAULT_MAX_PER_RANK_IO_CONCURRENCY=1; export _MAX_PER_RANK_IO_CONCURRENCY=1; export NSYS_REPORT_DIR=/home/am6429/dl-io/dl-io-outputs/act-output-30B//rep-30B-tp1-dp8-l60-h6656-a52-sl2048-gbs32-mbs4-ratio1-subg100000000-prefetch0-flush_async1-opt_gaps5-%n;  cd /home/am6429/dl-io/Megatron-DeepSpeed; /home/am6429/.conda/envs/dspeed_env/bin/python -u -m deepspeed.launcher.launch --world_info=eyJ4MzAwNmMwczE5YjFuMC5oc24uY20ucG9sYXJpcy5hbGNmLmFubC5nb3YiOiBbMCwgMSwgMiwgM10sICJ4MzAwNmMwczFiMG4wLmhzbi5jbS5wb2xhcmlzLmFsY2YuYW5sLmdvdiI6IFswLCAxLCAyLCAzXX0= --node_rank=%n --master_addr=10.140.57.107 --master_port=29700 /home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py --tensor-model-parallel-size 1 --num-layers 60 --hidden-size 6656 --num-attention-heads 52 --micro-batch-size 4 --global-batch-size 32 --seq-length 2048 --max-position-embeddings 2048 --train-iters 10 --save /local/scratch/llama2/zero3-tp1}_dp8 --data-path /home/am6429/dl-io/datasets/meg-gpt2_text_document --vocab-file /home/am6429/dl-io/datasets/gpt2-vocab.json --merge-file /home/am6429/dl-io/datasets/gpt2-merges.txt --data-impl mmap --tokenizer-type GPTSentencePieceTokenizer --tokenizer-model /home/am6429/dl-io/datasets/tokenizer.model --split 949,50,1 --distributed-backend nccl --lr 3e-4 --lr-decay-style cosine --min-lr 3e-5 --weight-decay 0.1 --clip-grad 1 --lr-warmup-iters 1 --optimizer adam --adam-beta1 0.9 --adam-beta2 0.95 --log-interval 1 --save-interval 1000 --eval-interval 1000 --eval-iters 0 --bf16 --no-query-key-layer-scaling --attention-dropout 0 --hidden-dropout 0 --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization rmsnorm --disable-bias-linear --num-key-value-heads 4 --deepspeed --exit-interval 20 --deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json --zero-stage=3 --no-pipeline-parallel --cpu-optimizer --checkpoint-activations --deepspeed-activation-checkpointing
x3006c0s1b0n0: [2024-03-28 13:33:31,209] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-28 13:33:31,217] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-28 13:33:33,070] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3006c0s19b1n0: [2024-03-28 13:33:33,070] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=0
x3006c0s19b1n0: [2024-03-28 13:33:33,070] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3006c0s19b1n0: [2024-03-28 13:33:33,070] [INFO] [launch.py:163:main] dist_world_size=8
x3006c0s19b1n0: [2024-03-28 13:33:33,070] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3006c0s19b1n0: [2024-03-28 13:33:33,071] [INFO] [launch.py:253:main] process 16035 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-28 13:33:33,072] [INFO] [launch.py:253:main] process 16036 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-28 13:33:33,072] [INFO] [launch.py:253:main] process 16037 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-28 13:33:33,073] [INFO] [launch.py:253:main] process 16038 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b0n0: [2024-03-28 13:33:33,545] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3006c0s1b0n0: [2024-03-28 13:33:33,545] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=4, node_rank=1
x3006c0s1b0n0: [2024-03-28 13:33:33,545] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3006c0s1b0n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7]})
x3006c0s1b0n0: [2024-03-28 13:33:33,545] [INFO] [launch.py:163:main] dist_world_size=8
x3006c0s1b0n0: [2024-03-28 13:33:33,545] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3006c0s1b0n0: [2024-03-28 13:33:33,546] [INFO] [launch.py:253:main] process 8736 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b0n0: [2024-03-28 13:33:33,546] [INFO] [launch.py:253:main] process 8737 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b0n0: [2024-03-28 13:33:33,547] [INFO] [launch.py:253:main] process 8738 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s1b0n0: [2024-03-28 13:33:33,547] [INFO] [launch.py:253:main] process 8739 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '60', '--hidden-size', '6656', '--num-attention-heads', '52', '--micro-batch-size', '4', '--global-batch-size', '32', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp8', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3006c0s19b1n0: [2024-03-28 13:33:34,840] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-28 13:33:34,899] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-28 13:33:34,902] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: [2024-03-28 13:33:34,915] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b0n0: [2024-03-28 13:33:35,358] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b0n0: [2024-03-28 13:33:35,374] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b0n0: [2024-03-28 13:33:35,384] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s1b0n0: [2024-03-28 13:33:35,389] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: [2024-03-28 13:33:37,553] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed C++/CUDA extension op report
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s19b1n0:       runtime if needed. Op compatibility means that your system
x3006c0s19b1n0:       meet the required dependencies to JIT install the op.
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: JIT compiled ops requires ninja
x3006c0s19b1n0: ninja .................. [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: op name ................ installed .. compatible
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s19b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: INFO: overriding default arguments for tokenizer_type:GPTSentencePieceTokenizer                    with tokenizer_type:GPT2BPETokenizer
x3006c0s19b1n0: using world size: 8, data-parallel-size: 8, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
x3006c0s19b1n0: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
x3006c0s19b1n0: using torch.bfloat16 for parameters ...
x3006c0s19b1n0: ------------------------ arguments ------------------------
x3006c0s19b1n0:   accumulate_allreduce_grads_in_fp32 .............. True
x3006c0s19b1n0:   adam_beta1 ...................................... 0.9
x3006c0s19b1n0:   adam_beta2 ...................................... 0.95
x3006c0s19b1n0:   adam_eps ........................................ 1e-08
x3006c0s19b1n0:   add_bias_linear ................................. False
x3006c0s19b1n0:   add_position_embedding .......................... False
x3006c0s19b1n0:   adlr_autoresume ................................. False
x3006c0s19b1n0:   adlr_autoresume_interval ........................ 1000
x3006c0s19b1n0:   aml_data_download_path .......................... None
x3006c0s19b1n0:   apply_layernorm_1p .............................. False
x3006c0s19b1n0:   apply_query_key_layer_scaling ................... False
x3006c0s19b1n0:   apply_residual_connection_post_layernorm ........ False
x3006c0s19b1n0:   async_tensor_model_parallel_allreduce ........... False
x3006c0s19b1n0:   attention_dropout ............................... 0.0
x3006c0s19b1n0:   attention_softmax_in_fp32 ....................... False
x3006c0s19b1n0:   barrier_with_L1_time ............................ True
x3006c0s19b1n0:   bert_binary_head ................................ True
x3006c0s19b1n0:   bert_embedder_type .............................. megatron
x3006c0s19b1n0:   bert_load ....................................... None
x3006c0s19b1n0:   bf16 ............................................ True
x3006c0s19b1n0:   bias_dropout_fusion ............................. True
x3006c0s19b1n0:   bias_gelu_fusion ................................ False
x3006c0s19b1n0:   biencoder_projection_dim ........................ 0
x3006c0s19b1n0:   biencoder_shared_query_context_model ............ False
x3006c0s19b1n0:   block_data_path ................................. None
x3006c0s19b1n0:   checkpoint_activations .......................... True
x3006c0s19b1n0:   checkpoint_in_cpu ............................... False
x3006c0s19b1n0:   checkpoint_num_layers ........................... 1
x3006c0s19b1n0:   classes_fraction ................................ 1.0
x3006c0s19b1n0:   clip_grad ....................................... 1.0
x3006c0s19b1n0:   compression_training ............................ False
x3006c0s19b1n0:   consumed_train_samples .......................... 0
x3006c0s19b1n0:   consumed_train_tokens ........................... 0
x3006c0s19b1n0:   consumed_valid_samples .......................... 0
x3006c0s19b1n0:   contigious_checkpointing ........................ False
x3006c0s19b1n0:   cpu_optimizer ................................... True
x3006c0s19b1n0:   cpu_torch_adam .................................. False
x3006c0s19b1n0:   create_moe_param_group .......................... False
x3006c0s19b1n0:   curriculum_learning_legacy ...................... False
x3006c0s19b1n0:   data_cache_path ................................. None
x3006c0s19b1n0:   data_efficiency_curriculum_learning ............. False
x3006c0s19b1n0:   data_impl ....................................... mmap
x3006c0s19b1n0:   data_parallel_random_init ....................... False
x3006c0s19b1n0:   data_parallel_size .............................. 8
x3006c0s19b1n0:   data_path ....................................... ['/home/am6429/dl-io/datasets/meg-gpt2_text_document']
x3006c0s19b1n0:   data_per_class_fraction ......................... 1.0
x3006c0s19b1n0:   data_sharding ................................... True
x3006c0s19b1n0:   dataloader_type ................................. single
x3006c0s19b1n0:   DDP_impl ........................................ local
x3006c0s19b1n0:   decoder_num_layers .............................. None
x3006c0s19b1n0:   decoder_seq_length .............................. None
x3006c0s19b1n0:   deepscale ....................................... False
x3006c0s19b1n0:   deepscale_config ................................ None
x3006c0s19b1n0:   deepspeed ....................................... True
x3006c0s19b1n0:   deepspeed_activation_checkpointing .............. True
x3006c0s19b1n0:   deepspeed_config ................................ /home/am6429/dl-io/dl-io-outputs/act-output-30B//ds_config.json
x3006c0s19b1n0:   dino_bottleneck_size ............................ 256
x3006c0s19b1n0:   dino_freeze_last_layer .......................... 1
x3006c0s19b1n0:   dino_head_hidden_size ........................... 2048
x3006c0s19b1n0:   dino_local_crops_number ......................... 10
x3006c0s19b1n0:   dino_local_img_size ............................. 96
x3006c0s19b1n0:   dino_norm_last_layer ............................ False
x3006c0s19b1n0:   dino_teacher_temp ............................... 0.07
x3006c0s19b1n0:   dino_warmup_teacher_temp ........................ 0.04
x3006c0s19b1n0:   dino_warmup_teacher_temp_epochs ................. 30
x3006c0s19b1n0:   distribute_checkpointed_activations ............. False
x3006c0s19b1n0:   distribute_saved_activations .................... False
x3006c0s19b1n0:   distributed_backend ............................. nccl
x3006c0s19b1n0:   distributed_timeout_minutes ..................... 10
x3006c0s19b1n0:   ds_inference .................................... False
x3006c0s19b1n0:   ds_pipeline_enabled ............................. False
x3006c0s19b1n0:   ds_sequence_parallel_size ....................... 1
x3006c0s19b1n0:   embedding_path .................................. None
x3006c0s19b1n0:   embedding_weights_in_fp32 ....................... False
x3006c0s19b1n0:   empty_unused_memory_level ....................... 0
x3006c0s19b1n0:   enable_expert_tensor_parallelism ................ False
x3006c0s19b1n0:   encoder_num_layers .............................. 60
x3006c0s19b1n0:   encoder_seq_length .............................. 2048
x3006c0s19b1n0:   end_weight_decay ................................ 0.1
x3006c0s19b1n0:   eod_mask_loss ................................... False
x3006c0s19b1n0:   eval_interval ................................... 1000
x3006c0s19b1n0:   eval_iters ...................................... 0
x3006c0s19b1n0:   evidence_data_path .............................. None
x3006c0s19b1n0:   exit_duration_in_mins ........................... None
x3006c0s19b1n0:   exit_interval ................................... 20
x3006c0s19b1n0:   exit_on_missing_checkpoint ...................... False
x3006c0s19b1n0:   exit_signal_handler ............................. False
x3006c0s19b1n0:   expert_interval ................................. 2
x3006c0s19b1n0:   ffn_hidden_size ................................. 17728
x3006c0s19b1n0:   finetune ........................................ False
x3006c0s19b1n0:   force_ds_sequence_parallel ...................... False
x3006c0s19b1n0:   fp16 ............................................ False
x3006c0s19b1n0:   fp16_lm_cross_entropy ........................... False
x3006c0s19b1n0:   fp32_residual_connection ........................ False
x3006c0s19b1n0:   fp8_amax_compute_algo ........................... most_recent
x3006c0s19b1n0:   fp8_amax_history_len ............................ 1
x3006c0s19b1n0:   fp8_e4m3 ........................................ False
x3006c0s19b1n0:   fp8_hybrid ...................................... False
x3006c0s19b1n0:   fp8_interval .................................... 1
x3006c0s19b1n0:   fp8_margin ...................................... 0
x3006c0s19b1n0:   fp8_wgrad ....................................... True
x3006c0s19b1n0:   global_batch_size ............................... 32
x3006c0s19b1n0:   gradient_accumulation_fusion .................... True
x3006c0s19b1n0:   head_lr_mult .................................... 1.0
x3006c0s19b1n0:   hidden_dropout .................................. 0.0
x3006c0s19b1n0:   hidden_size ..................................... 6656
x3006c0s19b1n0:   hidden_size_teacher ............................. None
x3006c0s19b1n0:   hysteresis ...................................... 2
x3006c0s19b1n0:   ict_head_size ................................... None
x3006c0s19b1n0:   ict_load ........................................ None
x3006c0s19b1n0:   img_h ........................................... 224
x3006c0s19b1n0:   img_w ........................................... 224
x3006c0s19b1n0:   indexer_batch_size .............................. 128
x3006c0s19b1n0:   indexer_log_interval ............................ 1000
x3006c0s19b1n0:   inference ....................................... False
x3006c0s19b1n0:   inference_batch_times_seqlen_threshold .......... 512
x3006c0s19b1n0:   init_method_std ................................. 0.02
x3006c0s19b1n0:   init_method_xavier_uniform ...................... False
x3006c0s19b1n0:   initial_loss_scale .............................. 4294967296
x3006c0s19b1n0:   iter_per_epoch .................................. 1250
x3006c0s19b1n0:   kd .............................................. False
x3006c0s19b1n0:   kd_alpha_ce ..................................... 1
x3006c0s19b1n0:   kd_beta_ce ...................................... 1
x3006c0s19b1n0:   kd_temp ......................................... 1.0
x3006c0s19b1n0:   kv_channels ..................................... 128
x3006c0s19b1n0:   layernorm_epsilon ............................... 1e-05
x3006c0s19b1n0:   lazy_mpu_init ................................... None
x3006c0s19b1n0:   load ............................................ None
x3006c0s19b1n0:   load_teacher .................................... None
x3006c0s19b1n0:   local_rank ...................................... 0
x3006c0s19b1n0:   log_batch_size_to_tensorboard ................... False
x3006c0s19b1n0:   log_interval .................................... 1
x3006c0s19b1n0:   log_learning_rate_to_tensorboard ................ True
x3006c0s19b1n0:   log_loss_scale_to_tensorboard ................... True
x3006c0s19b1n0:   log_memory_to_tensorboard ....................... False
x3006c0s19b1n0:   log_num_zeros_in_grad ........................... False
x3006c0s19b1n0:   log_optimizer_states_to_tensorboard ............. False
x3006c0s19b1n0:   log_params_norm ................................. False
x3006c0s19b1n0:   log_timers_to_tensorboard ....................... False
x3006c0s19b1n0:   log_validation_ppl_to_tensorboard ............... False
x3006c0s19b1n0:   log_world_size_to_tensorboard ................... False
x3006c0s19b1n0:   loss_scale ...................................... None
x3006c0s19b1n0:   loss_scale_window ............................... 1000
x3006c0s19b1n0:   lr .............................................. 0.0003
x3006c0s19b1n0:   lr_decay_iters .................................. None
x3006c0s19b1n0:   lr_decay_samples ................................ None
x3006c0s19b1n0:   lr_decay_style .................................. cosine
x3006c0s19b1n0:   lr_decay_tokens ................................. None
x3006c0s19b1n0:   lr_warmup_fraction .............................. None
x3006c0s19b1n0:   lr_warmup_iters ................................. 1
x3006c0s19b1n0:   lr_warmup_samples ............................... 0
x3006c0s19b1n0:   lr_warmup_tokens ................................ None
x3006c0s19b1n0:   make_vocab_size_divisible_by .................... 128
x3006c0s19b1n0:   mask_factor ..................................... 1.0
x3006c0s19b1n0:   mask_prob ....................................... 0.15
x3006c0s19b1n0:   mask_type ....................................... random
x3006c0s19b1n0:   masked_softmax_fusion ........................... True
x3006c0s19b1n0:   max_position_embeddings ......................... 2048
x3006c0s19b1n0:   max_tokens_to_oom ............................... 12000
x3006c0s19b1n0:   memory_centric_tiled_linear ..................... False
x3006c0s19b1n0:   merge_file ...................................... /home/am6429/dl-io/datasets/gpt2-merges.txt
x3006c0s19b1n0:   micro_batch_size ................................ 4
x3006c0s19b1n0:   min_loss_scale .................................. 1.0
x3006c0s19b1n0:   min_lr .......................................... 3e-05
x3006c0s19b1n0:   mlp_type ........................................ standard
x3006c0s19b1n0:   mmap_warmup ..................................... False
x3006c0s19b1n0:   moe_eval_capacity_factor ........................ 1.0
x3006c0s19b1n0:   moe_expert_parallel_size ........................ 1
x3006c0s19b1n0:   moe_loss_coeff .................................. 0.1
x3006c0s19b1n0:   moe_min_capacity ................................ 4
x3006c0s19b1n0:   moe_token_dropping .............................. True
x3006c0s19b1n0:   moe_train_capacity_factor ....................... 1.0
x3006c0s19b1n0:   mos ............................................. False
x3006c0s19b1n0:   no_load_lr_state ................................ False
x3006c0s19b1n0:   no_load_optim ................................... None
x3006c0s19b1n0:   no_load_rng ..................................... None
x3006c0s19b1n0:   no_persist_layer_norm ........................... False
x3006c0s19b1n0:   no_pipeline_parallel ............................ True
x3006c0s19b1n0:   no_save_optim ................................... None
x3006c0s19b1n0:   no_save_rng ..................................... None
x3006c0s19b1n0:   normalization ................................... rmsnorm
x3006c0s19b1n0:   num_attention_heads ............................. 52
x3006c0s19b1n0:   num_attention_heads_teacher ..................... None
x3006c0s19b1n0:   num_channels .................................... 3
x3006c0s19b1n0:   num_classes ..................................... 1000
x3006c0s19b1n0:   num_experts ..................................... [1]
x3006c0s19b1n0:   num_experts_switch .............................. None
x3006c0s19b1n0:   num_experts_teacher ............................. [1]
x3006c0s19b1n0:   num_key_value_heads ............................. 4
x3006c0s19b1n0:   num_layers ...................................... 60
x3006c0s19b1n0:   num_layers_per_virtual_pipeline_stage ........... None
x3006c0s19b1n0:   num_layers_teacher .............................. None
x3006c0s19b1n0:   num_workers ..................................... 2
x3006c0s19b1n0:   onnx_safe ....................................... None
x3006c0s19b1n0:   openai_gelu ..................................... False
x3006c0s19b1n0:   optimizer ....................................... adam
x3006c0s19b1n0:   output_bert_embeddings .......................... False
x3006c0s19b1n0:   overlap_p2p_comm ................................ False
x3006c0s19b1n0:   override_opt_param_scheduler .................... False
x3006c0s19b1n0:   params_dtype .................................... torch.bfloat16
x3006c0s19b1n0:   partition_activations ........................... False
x3006c0s19b1n0:   patch_dim ....................................... 16
x3006c0s19b1n0:   perform_initialization .......................... True
x3006c0s19b1n0:   pipeline_model_parallel_size .................... 1
x3006c0s19b1n0:   pipeline_model_parallel_split_rank .............. None
x3006c0s19b1n0:   profile_backward ................................ False
x3006c0s19b1n0:   query_in_block_prob ............................. 0.1
x3006c0s19b1n0:   rampup_batch_size ............................... None
x3006c0s19b1n0:   random_ltd ...................................... False
x3006c0s19b1n0:   rank ............................................ 0
x3006c0s19b1n0:   recompute_granularity ........................... None
x3006c0s19b1n0:   recompute_method ................................ None
x3006c0s19b1n0:   recompute_num_layers ............................ 1
x3006c0s19b1n0:   remote_device ................................... none
x3006c0s19b1n0:   reset_attention_mask ............................ False
x3006c0s19b1n0:   reset_iteration ................................. False
x3006c0s19b1n0:   reset_position_ids .............................. False
x3006c0s19b1n0:   retriever_report_topk_accuracies ................ []
x3006c0s19b1n0:   retriever_score_scaling ......................... False
x3006c0s19b1n0:   retriever_seq_length ............................ 256
x3006c0s19b1n0:   retro_add_retriever ............................. False
x3006c0s19b1n0:   retro_cyclic_train_iters ........................ None
x3006c0s19b1n0:   retro_encoder_attention_dropout ................. 0.1
x3006c0s19b1n0:   retro_encoder_hidden_dropout .................... 0.1
x3006c0s19b1n0:   retro_encoder_layers ............................ 2
x3006c0s19b1n0:   retro_num_neighbors ............................. 2
x3006c0s19b1n0:   retro_num_retrieved_chunks ...................... 2
x3006c0s19b1n0:   retro_return_doc_ids ............................ False
x3006c0s19b1n0:   retro_workdir ................................... None
x3006c0s19b1n0:   return_data_index ............................... False
x3006c0s19b1n0:   rotary_percent .................................. 1.0
x3006c0s19b1n0:   sample_rate ..................................... 1.0
x3006c0s19b1n0:   save ............................................ /local/scratch/llama2/zero3-tp1}_dp8
x3006c0s19b1n0:   save_interval ................................... 1000
x3006c0s19b1n0:   scatter_gather_tensors_in_pipeline .............. True
x3006c0s19b1n0:   scattered_embeddings ............................ False
x3006c0s19b1n0:   seed ............................................ 1234
x3006c0s19b1n0:   seq_length ...................................... 2048
x3006c0s19b1n0:   sequence_parallel ............................... False
x3006c0s19b1n0:   sgd_momentum .................................... 0.9
x3006c0s19b1n0:   short_seq_prob .................................. 0.1
x3006c0s19b1n0:   skip_train ...................................... False
x3006c0s19b1n0:   split ........................................... 949,50,1
x3006c0s19b1n0:   split_transformers .............................. False
x3006c0s19b1n0:   squared_relu .................................... False
x3006c0s19b1n0:   standalone_embedding_stage ...................... False
x3006c0s19b1n0:   start_weight_decay .............................. 0.1
x3006c0s19b1n0:   swiglu .......................................... True
x3006c0s19b1n0:   swin_backbone_type .............................. tiny
x3006c0s19b1n0:   synchronize_each_layer .......................... False
x3006c0s19b1n0:   tensor_model_parallel_size ...................... 1
x3006c0s19b1n0:   tensorboard_dir ................................. None
x3006c0s19b1n0:   tensorboard_log_interval ........................ 1
x3006c0s19b1n0:   tensorboard_queue_size .......................... 1000
x3006c0s19b1n0:   test_data_path .................................. None
x3006c0s19b1n0:   tile_factor ..................................... 1
x3006c0s19b1n0:   timing_log_level ................................ 0
x3006c0s19b1n0:   timing_log_option ............................... minmax
x3006c0s19b1n0:   titles_data_path ................................ None
x3006c0s19b1n0:   tokenizer_model ................................. /home/am6429/dl-io/datasets/tokenizer.model
x3006c0s19b1n0:   tokenizer_type .................................. GPT2BPETokenizer
x3006c0s19b1n0:   topk ............................................ 1
x3006c0s19b1n0:   train_data_exact_num_epochs ..................... None
x3006c0s19b1n0:   train_data_path ................................. None
x3006c0s19b1n0:   train_desc_path ................................. None
x3006c0s19b1n0:   train_doc_idx_path .............................. None
x3006c0s19b1n0:   train_idx_path .................................. None
x3006c0s19b1n0:   train_iters ..................................... 10
x3006c0s19b1n0:   train_sample_idx_path ........................... None
x3006c0s19b1n0:   train_samples ................................... None
x3006c0s19b1n0:   train_shuffle_idx_path .......................... None
x3006c0s19b1n0:   train_tokens .................................... None
x3006c0s19b1n0:   transformer_impl ................................ local
x3006c0s19b1n0:   transformer_pipeline_model_parallel_size ........ 1
x3006c0s19b1n0:   untie_embeddings_and_output_weights ............. True
x3006c0s19b1n0:   use_checkpoint_args ............................. False
x3006c0s19b1n0:   use_checkpoint_opt_param_scheduler .............. False
x3006c0s19b1n0:   use_contiguous_buffers_in_local_ddp ............. True
x3006c0s19b1n0:   use_cpu_initialization .......................... None
x3006c0s19b1n0:   use_dataset_only ................................ False
x3006c0s19b1n0:   use_distributed_optimizer ....................... False
x3006c0s19b1n0:   use_flash_attn .................................. False
x3006c0s19b1n0:   use_flash_attn_triton ........................... False
x3006c0s19b1n0:   use_flash_attn_v1 ............................... False
x3006c0s19b1n0:   use_flash_attn_v2 ............................... False
x3006c0s19b1n0:   use_one_sent_docs ............................... False
x3006c0s19b1n0:   use_pin_memory .................................. False
x3006c0s19b1n0:   use_ring_exchange_p2p ........................... False
x3006c0s19b1n0:   use_rotary_position_embeddings .................. True
x3006c0s19b1n0:   use_tutel ....................................... False
x3006c0s19b1n0:   valid_data_path ................................. None
x3006c0s19b1n0:   variable_seq_lengths ............................ False
x3006c0s19b1n0:   virtual_pipeline_model_parallel_size ............ None
x3006c0s19b1n0:   vision_backbone_type ............................ vit
x3006c0s19b1n0:   vision_pretraining .............................. False
x3006c0s19b1n0:   vision_pretraining_type ......................... classify
x3006c0s19b1n0:   vocab_extra_ids ................................. 0
x3006c0s19b1n0:   vocab_file ...................................... /home/am6429/dl-io/datasets/gpt2-vocab.json
x3006c0s19b1n0:   vocab_size ...................................... None
x3006c0s19b1n0:   weight_decay .................................... 0.1
x3006c0s19b1n0:   weight_decay_incr_style ......................... constant
x3006c0s19b1n0:   world_size ...................................... 8
x3006c0s19b1n0:   zero_allgather_bucket_size ...................... 0.0
x3006c0s19b1n0:   zero_contigious_gradients ....................... False
x3006c0s19b1n0:   zero_reduce_bucket_size ......................... 0.0
x3006c0s19b1n0:   zero_reduce_scatter ............................. False
x3006c0s19b1n0:   zero_stage ...................................... 3
x3006c0s19b1n0: -------------------- end of arguments ---------------------
x3006c0s19b1n0: setting number of micro-batches to constant 1
x3006c0s19b1n0: > building GPT2BPETokenizer tokenizer ...
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s19b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s19b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s19b1n0: --------------------------------------------------
x3006c0s19b1n0: DeepSpeed general environment info:
x3006c0s19b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s19b1n0: torch version .................... 2.0.1+cu118
x3006c0s19b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s19b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s19b1n0: torch cuda version ............... 11.8
x3006c0s19b1n0: torch hip version ................ None
x3006c0s19b1n0: nvcc version ..................... 11.8
x3006c0s19b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s19b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s19b1n0:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
x3006c0s19b1n0: > initializing torch distributed ...
x3006c0s19b1n0: [2024-03-28 13:33:37,821] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: [2024-03-28 13:33:37,821] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s19b1n0: [2024-03-28 13:33:37,870] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: [2024-03-28 13:33:37,874] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b0n0:       meet the required dependencies to JIT install the op.
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: JIT compiled ops requires ninja
x3006c0s1b0n0: ninja .................. [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: op name ................ installed .. compatible
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b0n0:       meet the required dependencies to JIT install the op.
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: JIT compiled ops requires ninja
x3006c0s1b0n0: ninja .................. [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: op name ................ installed .. compatible
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b0n0:       meet the required dependencies to JIT install the op.
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: JIT compiled ops requires ninja
x3006c0s1b0n0: ninja .................. [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: op name ................ installed .. compatible
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed C++/CUDA extension op report
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3006c0s1b0n0:       runtime if needed. Op compatibility means that your system
x3006c0s1b0n0:       meet the required dependencies to JIT install the op.
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: JIT compiled ops requires ninja
x3006c0s1b0n0: ninja .................. [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: op name ................ installed .. compatible
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3006c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed general environment info:
x3006c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b0n0: torch version .................... 2.0.1+cu118
x3006c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b0n0: torch cuda version ............... 11.8
x3006c0s1b0n0: torch hip version ................ None
x3006c0s1b0n0: nvcc version ..................... 11.8
x3006c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3006c0s1b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed general environment info:
x3006c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b0n0: torch version .................... 2.0.1+cu118
x3006c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b0n0: torch cuda version ............... 11.8
x3006c0s1b0n0: torch hip version ................ None
x3006c0s1b0n0: nvcc version ..................... 11.8
x3006c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b0n0: [2024-03-28 13:33:38,270] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: DeepSpeed general environment info:
x3006c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b0n0: torch version .................... 2.0.1+cu118
x3006c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b0n0: torch cuda version ............... 11.8
x3006c0s1b0n0: torch hip version ................ None
x3006c0s1b0n0: nvcc version ..................... 11.8
x3006c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3006c0s1b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3006c0s1b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3006c0s1b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3006c0s1b0n0: --------------------------------------------------
x3006c0s1b0n0: DeepSpeed general environment info:
x3006c0s1b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3006c0s1b0n0: torch version .................... 2.0.1+cu118
x3006c0s1b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3006c0s1b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3006c0s1b0n0: torch cuda version ............... 11.8
x3006c0s1b0n0: torch hip version ................ None
x3006c0s1b0n0: nvcc version ..................... 11.8
x3006c0s1b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3006c0s1b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3006c0s1b0n0: [2024-03-28 13:33:38,314] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3006c0s1b0n0: [2024-03-28 13:33:38,360] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s1b0n0: [2024-03-28 13:33:38,370] [INFO] [comm.py:637:init_distributed] cdb=None
x3006c0s19b1n0: > initialized tensor model parallel with size 1
x3006c0s19b1n0: > initialized pipeline model parallel with size 1
x3006c0s19b1n0: > setting random seeds to 1234 ...
x3006c0s19b1n0: [2024-03-28 13:33:39,019] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
x3006c0s19b1n0: > compiling dataset index builder ...
x3006c0s19b1n0: make: Entering directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3006c0s19b1n0: make: Nothing to be done for 'default'.
x3006c0s19b1n0: make: Leaving directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3006c0s19b1n0: >>> done with dataset index builder. Compilation time: 0.084 seconds
x3006c0s19b1n0: > compiling and loading fused kernels ...
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: >>> done with compiling and loading fused kernels. Compilation time: 3.592 seconds
x3006c0s19b1n0: initialize_megatron took 5.720499515533447
x3006c0s19b1n0: <<<<<<<<<<< 0
x3006c0s19b1n0: <<<<<<<<<<< 2
x3006c0s19b1n0: <<<<<<<<<<< 1
x3006c0s1b0n0: <<<<<<<<<<< 4
x3006c0s1b0n0: <<<<<<<<<<< 7
x3006c0s1b0n0: <<<<<<<<<<< 6
x3006c0s1b0n0: <<<<<<<<<<< 5
x3006c0s19b1n0: <<<<<<<<<<< 3
x3006c0s19b1n0: time to initialize megatron (seconds): 6.260
x3006c0s19b1n0: [after megatron is initialized] datetime: 2024-03-28 13:33:43 
x3006c0s19b1n0: get_accelerator and all_reduce  took 0.006369113922119141
x3006c0s19b1n0: building GPT model ...
x3006c0s19b1n0: [2024-03-28 13:33:43,563] [INFO] [utils.py:800:see_memory_usage] Before Building Model
x3006c0s19b1n0: [2024-03-28 13:33:43,564] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 4.62 GB         CA 0.0 GB         Max_CA 5 GB 
x3006c0s19b1n0: [2024-03-28 13:33:43,564] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 21.6 GB, percent = 4.3%
x3006c0s19b1n0: [2024-03-28 13:33:49,857] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 423, num_elems = 27.64B
x3006c0s19b1n0: [2024-03-28 13:33:49,919] [INFO] [utils.py:800:see_memory_usage] After Building Model
x3006c0s19b1n0: [2024-03-28 13:33:49,919] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 7.07 GB         CA 21.88 GB         Max_CA 38 GB 
x3006c0s19b1n0: [2024-03-28 13:33:49,920] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 22.01 GB, percent = 4.4%
x3006c0s19b1n0:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 27635239424
x3006c0s1b0n0: ninja: no work to do.
x3006c0s1b0n0: Time to load cpu_adam op: 2.512718439102173 seconds
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: Time to load cpu_adam op: 2.4861180782318115 seconds
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: Time to load cpu_adam op: 2.4359958171844482 seconds
x3006c0s19b1n0: Time to load cpu_adam op: 2.432861089706421 seconds
x3006c0s19b1n0: ninja: no work to do.
x3006c0s19b1n0: Time to load cpu_adam op: 2.463324785232544 seconds
x3006c0s1b0n0: ninja: no work to do.
x3006c0s1b0n0: Time to load cpu_adam op: 2.4535531997680664 seconds
x3006c0s1b0n0: Time to load cpu_adam op: 2.4767730236053467 seconds
x3006c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s1b0n0: Time to load cpu_adam op: 2.5540127754211426 seconds
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: > learning rate decay style: cosine
x3006c0s19b1n0: DeepSpeed is enabled.
x3006c0s19b1n0: [2024-03-28 13:33:54,394] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.3+8074cd62, git-hash=8074cd62, git-branch=hybrid_opt_offload
x3006c0s19b1n0: [2024-03-28 13:33:54,464] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After args sanity test
x3006c0s19b1n0: [2024-03-28 13:33:54,464] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.88 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:33:54,464] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 28.45 GB, percent = 5.7%
x3006c0s19b1n0: [2024-03-28 13:33:54,520] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure distributed model
x3006c0s19b1n0: [2024-03-28 13:33:54,521] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.88 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:33:54,521] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 28.73 GB, percent = 5.7%
x3006c0s19b1n0: [2024-03-28 13:33:54,581] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After configure distributed model
x3006c0s19b1n0: [2024-03-28 13:33:54,582] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.88 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:33:54,582] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.01 GB, percent = 5.8%
x3006c0s19b1n0: [2024-03-28 13:33:54,582] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-28 13:33:54,634] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After setting model parameters
x3006c0s19b1n0: [2024-03-28 13:33:54,634] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.88 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:33:54,634] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.14 GB, percent = 5.8%
x3006c0s19b1n0: [2024-03-28 13:33:54,685] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure optimizer
x3006c0s19b1n0: [2024-03-28 13:33:54,686] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.88 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:33:54,686] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.23 GB, percent = 5.8%
x3006c0s19b1n0: [2024-03-28 13:33:54,686] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
x3006c0s19b1n0: [2024-03-28 13:33:54,687] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
x3006c0s19b1n0: [2024-03-28 13:33:54,705] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
x3006c0s19b1n0: [2024-03-28 13:33:54,705] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
x3006c0s19b1n0: [2024-03-28 13:33:54,705] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
x3006c0s19b1n0: [2024-03-28 13:33:54,705] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
x3006c0s19b1n0: [2024-03-28 13:33:54,755] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
x3006c0s19b1n0: [2024-03-28 13:33:54,755] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.88 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:33:54,756] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.34 GB, percent = 5.8%
x3006c0s19b1n0: [2024-03-28 13:33:54,757] [INFO] [stage3.py:137:__init__] Reduce bucket size 500,000,000
x3006c0s19b1n0: [2024-03-28 13:33:54,757] [INFO] [stage3.py:138:__init__] Prefetch bucket size 50,000,000
x3006c0s19b1n0: [2024-03-28 13:33:54,809] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
x3006c0s19b1n0: [2024-03-28 13:33:54,809] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.88 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:33:54,809] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.42 GB, percent = 5.8%
x3006c0s19b1n0: Parameter Offload: Total persistent parameters: 805376 in 121 params
x3006c0s19b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s19b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s19b1n0: [2024-03-28 13:33:54,885] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
x3006c0s19b1n0: [2024-03-28 13:33:54,885] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.88 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:33:54,885] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.44 GB, percent = 5.9%
x3006c0s19b1n0: [2024-03-28 13:33:54,939] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
x3006c0s19b1n0: [2024-03-28 13:33:54,939] [INFO] [utils.py:801:see_memory_usage] MA 6.44 GB         Max_MA 6.44 GB         CA 21.88 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:33:54,939] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.44 GB, percent = 5.9%
x3006c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s1b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3006c0s1b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3006c0s1b0n0: [2024-03-28 13:33:55,092] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b0n0: [2024-03-28 13:33:55,092] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-28 13:33:55,092] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b0n0: [2024-03-28 13:33:55,092] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b0n0: [2024-03-28 13:33:55,092] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-28 13:33:55,092] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-28 13:33:55,092] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s19b1n0: [2024-03-28 13:33:55,092] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 0, numel: 103569856
x3006c0s1b0n0: [2024-03-28 13:33:55,092] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-28 13:33:55,092] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b0n0: [2024-03-28 13:33:55,092] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b0n0: [2024-03-28 13:33:55,092] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-28 13:33:55,092] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-28 13:33:55,092] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-28 13:33:55,092] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s1b0n0: [2024-03-28 13:33:55,092] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 1, numel: 106817984
x3006c0s19b1n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 2, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 3, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 4, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 5, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,093] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 6, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 7, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 8, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 9, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 10, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 11, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,094] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 12, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 13, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 14, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 15, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 16, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 17, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,095] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 18, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 19, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 20, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 21, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 22, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 23, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,096] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 24, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 25, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 26, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 27, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 28, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s1b0n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 29, numel: 112356608
x3006c0s19b1n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b0n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b0n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b0n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s1b0n0: [2024-03-28 13:33:55,097] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 30, numel: 98032064
x3006c0s19b1n0: [2024-03-28 13:33:59,073] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 31
x3006c0s19b1n0: [2024-03-28 13:33:59,074] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.44 GB         CA 6.44 GB         Max_CA 22 GB 
x3006c0s19b1n0: [2024-03-28 13:33:59,074] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 45.74 GB, percent = 9.1%
x3006c0s19b1n0: [2024-03-28 13:33:59,287] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
x3006c0s19b1n0: [2024-03-28 13:33:59,287] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-28 13:33:59,287] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 48.59 GB, percent = 9.7%
x3006c0s19b1n0: [2024-03-28 13:34:01,843] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
x3006c0s19b1n0: [2024-03-28 13:34:01,843] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-28 13:34:01,843] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 81.05 GB, percent = 16.1%
x3006c0s19b1n0: [2024-03-28 13:34:01,972] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
x3006c0s19b1n0: [2024-03-28 13:34:01,972] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-28 13:34:01,972] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 84.57 GB, percent = 16.8%
x3006c0s19b1n0: [2024-03-28 13:34:08,197] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | init_optimizer_state: 6209.59
x3006c0s19b1n0: [2024-03-28 13:34:08,306] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
x3006c0s19b1n0: [2024-03-28 13:34:08,306] [INFO] [utils.py:801:see_memory_usage] MA 6.43 GB         Max_MA 6.43 GB         CA 6.44 GB         Max_CA 6 GB 
x3006c0s19b1n0: [2024-03-28 13:34:08,306] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 146.45 GB, percent = 29.1%
x3006c0s19b1n0: [2024-03-28 13:34:08,415] [INFO] [stage3.py:520:_setup_for_real_optimizer] optimizer state initialized
x3006c0s19b1n0: [2024-03-28 13:34:16,567] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
x3006c0s19b1n0: [2024-03-28 13:34:16,568] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 8.61 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:34:16,568] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 177.12 GB, percent = 35.2%
x3006c0s19b1n0: [2024-03-28 13:34:16,568] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
x3006c0s19b1n0: [2024-03-28 13:34:16,638] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure LR scheduler
x3006c0s19b1n0: [2024-03-28 13:34:16,639] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:34:16,639] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 177.14 GB, percent = 35.2%
x3006c0s19b1n0: [2024-03-28 13:34:16,639] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
x3006c0s19b1n0: [2024-03-28 13:34:16,639] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7efa367a81f0>
x3006c0s19b1n0: [2024-03-28 13:34:16,639] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:34:16,706] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before rewriting optimizer step
x3006c0s19b1n0: [2024-03-28 13:34:16,707] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:34:16,707] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 177.13 GB, percent = 35.2%
x3006c0s19b1n0: [2024-03-28 13:34:16,774] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure checkpointing
x3006c0s19b1n0: [2024-03-28 13:34:16,775] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 22.75 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:34:16,775] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 177.12 GB, percent = 35.2%
x3006c0s19b1n0: [2024-03-28 13:34:16,775] [INFO] [config.py:998:print] DeepSpeedEngine configuration:
x3006c0s19b1n0: [2024-03-28 13:34:16,775] [INFO] [config.py:1002:print]   activation_checkpointing_config  {
x3006c0s19b1n0:     "partition_activations": false, 
x3006c0s19b1n0:     "contiguous_memory_optimization": false, 
x3006c0s19b1n0:     "cpu_checkpointing": false, 
x3006c0s19b1n0:     "number_checkpoints": null, 
x3006c0s19b1n0:     "synchronize_checkpoint_boundary": false, 
x3006c0s19b1n0:     "profile": false
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 13:34:16,775] [INFO] [config.py:1002:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
x3006c0s19b1n0: [2024-03-28 13:34:16,775] [INFO] [config.py:1002:print]   amp_enabled .................. False
x3006c0s19b1n0: [2024-03-28 13:34:16,775] [INFO] [config.py:1002:print]   amp_params ................... False
x3006c0s19b1n0: [2024-03-28 13:34:16,775] [INFO] [config.py:1002:print]   autotuning_config ............ {
x3006c0s19b1n0:     "enabled": false, 
x3006c0s19b1n0:     "start_step": null, 
x3006c0s19b1n0:     "end_step": null, 
x3006c0s19b1n0:     "metric_path": null, 
x3006c0s19b1n0:     "arg_mappings": null, 
x3006c0s19b1n0:     "metric": "throughput", 
x3006c0s19b1n0:     "model_info": null, 
x3006c0s19b1n0:     "results_dir": "autotuning_results", 
x3006c0s19b1n0:     "exps_dir": "autotuning_exps", 
x3006c0s19b1n0:     "overwrite": true, 
x3006c0s19b1n0:     "fast": true, 
x3006c0s19b1n0:     "start_profile_step": 3, 
x3006c0s19b1n0:     "end_profile_step": 5, 
x3006c0s19b1n0:     "tuner_type": "gridsearch", 
x3006c0s19b1n0:     "tuner_early_stopping": 5, 
x3006c0s19b1n0:     "tuner_num_trials": 50, 
x3006c0s19b1n0:     "model_info_path": null, 
x3006c0s19b1n0:     "mp_size": 1, 
x3006c0s19b1n0:     "max_train_batch_size": null, 
x3006c0s19b1n0:     "min_train_batch_size": 1, 
x3006c0s19b1n0:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
x3006c0s19b1n0:     "min_train_micro_batch_size_per_gpu": 1, 
x3006c0s19b1n0:     "num_tuning_micro_batch_sizes": 3
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 13:34:16,775] [INFO] [config.py:1002:print]   bfloat16_enabled ............. True
x3006c0s19b1n0: [2024-03-28 13:34:16,775] [INFO] [config.py:1002:print]   bfloat16_immediate_grad_update  False
x3006c0s19b1n0: [2024-03-28 13:34:16,775] [INFO] [config.py:1002:print]   checkpoint_parallel_write_pipeline  False
x3006c0s19b1n0: [2024-03-28 13:34:16,775] [INFO] [config.py:1002:print]   checkpoint_tag_validation_enabled  True
x3006c0s19b1n0: [2024-03-28 13:34:16,775] [INFO] [config.py:1002:print]   checkpoint_tag_validation_fail  False
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7efa367a8b50>
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   communication_data_type ...... None
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   curriculum_enabled_legacy .... False
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   curriculum_params_legacy ..... False
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   data_efficiency_enabled ...... False
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   dataloader_drop_last ......... False
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   datastates_config ............ {
x3006c0s19b1n0:     "enabled": null, 
x3006c0s19b1n0:     "config": {
x3006c0s19b1n0:     }
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   disable_allgather ............ False
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   dump_state ................... False
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   dynamic_loss_scale_args ...... None
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   eigenvalue_enabled ........... False
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   eigenvalue_gas_boundary_resolution  1
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   eigenvalue_layer_name ........ bert.encoder.layer
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   eigenvalue_layer_num ......... 0
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   eigenvalue_max_iter .......... 100
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   eigenvalue_stability ......... 1e-06
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   eigenvalue_tol ............... 0.01
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   eigenvalue_verbose ........... False
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   elasticity_enabled ........... False
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   flops_profiler_config ........ {
x3006c0s19b1n0:     "enabled": false, 
x3006c0s19b1n0:     "recompute_fwd_factor": 0.0, 
x3006c0s19b1n0:     "profile_step": 1, 
x3006c0s19b1n0:     "module_depth": -1, 
x3006c0s19b1n0:     "top_modules": 1, 
x3006c0s19b1n0:     "detailed": true, 
x3006c0s19b1n0:     "output_file": null
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   fp16_auto_cast ............... None
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   fp16_enabled ................. False
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   fp16_master_weights_and_gradients  False
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   global_rank .................. 0
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   grad_accum_dtype ............. bf16
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   gradient_accumulation_steps .. 1
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   gradient_clipping ............ 0.0
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   gradient_predivide_factor .... 1.0
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   graph_harvesting ............. False
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   initial_dynamic_scale ........ 1
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   load_universal_checkpoint .... False
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   loss_scale ................... 1.0
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   memory_breakdown ............. True
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   mics_hierarchial_params_gather  False
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   mics_shard_size .............. -1
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   nebula_config ................ {
x3006c0s19b1n0:     "enabled": false, 
x3006c0s19b1n0:     "persistent_storage_path": null, 
x3006c0s19b1n0:     "persistent_time_interval": 100, 
x3006c0s19b1n0:     "num_of_version_in_retention": 2, 
x3006c0s19b1n0:     "enable_nebula_load": true, 
x3006c0s19b1n0:     "load_path": null
x3006c0s19b1n0: }
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   optimizer_legacy_fusion ...... False
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   optimizer_name ............... None
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   optimizer_params ............. None
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   pld_enabled .................. False
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   pld_params ................... False
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   prescale_gradients ........... False
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   scheduler_name ............... None
x3006c0s19b1n0: [2024-03-28 13:34:16,776] [INFO] [config.py:1002:print]   scheduler_params ............. None
x3006c0s19b1n0: [2024-03-28 13:34:16,777] [INFO] [config.py:1002:print]   seq_parallel_communication_data_type  torch.float32
x3006c0s19b1n0: [2024-03-28 13:34:16,777] [INFO] [config.py:1002:print]   sparse_attention ............. None
x3006c0s19b1n0: [2024-03-28 13:34:16,777] [INFO] [config.py:1002:print]   sparse_gradients_enabled ..... False
x3006c0s19b1n0: [2024-03-28 13:34:16,777] [INFO] [config.py:1002:print]   steps_per_print .............. 1
x3006c0s19b1n0: [2024-03-28 13:34:16,777] [INFO] [config.py:1002:print]   train_batch_size ............. 32
x3006c0s19b1n0: [2024-03-28 13:34:16,777] [INFO] [config.py:1002:print]   train_micro_batch_size_per_gpu  4
x3006c0s19b1n0: [2024-03-28 13:34:16,777] [INFO] [config.py:1002:print]   use_data_before_expert_parallel_  False
x3006c0s19b1n0: [2024-03-28 13:34:16,777] [INFO] [config.py:1002:print]   use_node_local_storage ....... False
x3006c0s19b1n0: [2024-03-28 13:34:16,777] [INFO] [config.py:1002:print]   wall_clock_breakdown ......... True
x3006c0s19b1n0: [2024-03-28 13:34:16,777] [INFO] [config.py:1002:print]   weight_quantization_config ... None
x3006c0s19b1n0: [2024-03-28 13:34:16,777] [INFO] [config.py:1002:print]   world_size ................... 8
x3006c0s19b1n0: [2024-03-28 13:34:16,777] [INFO] [config.py:1002:print]   zero_allow_untested_optimizer  False
x3006c0s19b1n0: [2024-03-28 13:34:16,777] [INFO] [config.py:1002:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0, prefetch_optimizer=False, part_grads_async=True, prefetch_optimizer_gap=5) sub_group_size=100000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
x3006c0s19b1n0: [2024-03-28 13:34:16,777] [INFO] [config.py:1002:print]   zero_enabled ................. True
x3006c0s19b1n0: [2024-03-28 13:34:16,777] [INFO] [config.py:1002:print]   zero_force_ds_cpu_optimizer .. True
x3006c0s19b1n0: [2024-03-28 13:34:16,777] [INFO] [config.py:1002:print]   zero_optimization_stage ...... 3
x3006c0s19b1n0: [2024-03-28 13:34:16,777] [INFO] [config.py:988:print_user_config]   json = {
x3006c0s19b1n0:     "train_batch_size": 32, 
x3006c0s19b1n0:     "train_micro_batch_size_per_gpu": 4, 
x3006c0s19b1n0:     "steps_per_print": 1, 
x3006c0s19b1n0:     "zero_optimization": {
x3006c0s19b1n0:         "stage": 3, 
x3006c0s19b1n0:         "offload_optimizer": {
x3006c0s19b1n0:             "device": "cpu", 
x3006c0s19b1n0:             "ratio": 1, 
x3006c0s19b1n0:             "pin_memory": true, 
x3006c0s19b1n0:             "prefetch_optimizer": 0, 
x3006c0s19b1n0:             "part_grads_async": 1, 
x3006c0s19b1n0:             "prefetch_optimizer_gap": 5
x3006c0s19b1n0:         }, 
x3006c0s19b1n0:         "sub_group_size": 1.000000e+08
x3006c0s19b1n0:     }, 
x3006c0s19b1n0:     "bf16": {
x3006c0s19b1n0:         "enabled": true
x3006c0s19b1n0:     }, 
x3006c0s19b1n0:     "data_types": {
x3006c0s19b1n0:         "grad_accum_dtype": "bf16"
x3006c0s19b1n0:     }, 
x3006c0s19b1n0:     "wall_clock_breakdown": true, 
x3006c0s19b1n0:     "memory_breakdown": true, 
x3006c0s19b1n0:     "flops_profiler": {
x3006c0s19b1n0:         "enabled": false
x3006c0s19b1n0:     }
x3006c0s19b1n0: }
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,34.50370502471924>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,34.504074811935425>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,34.50524878501892>
x3006c0s1b0n0: <TIMER:model-and-optimizer-setup,34.50616121292114>
x3006c0s19b1n0: <TIMER:model-and-optimizer-setup,34.50629734992981>
x3006c0s1b0n0: <TIMER:model-and-optimizer-setup,34.50708818435669>
x3006c0s1b0n0: <TIMER:model-and-optimizer-setup,34.50800156593323>
x3006c0s1b0n0: <TIMER:model-and-optimizer-setup,34.508368730545044>
x3006c0s19b1n0: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-28 13:34:18 
x3006c0s19b1n0: > building train, validation, and test datasets ...
x3006c0s19b1n0:  > datasets target sizes (minimum size):
x3006c0s19b1n0:     train:      320
x3006c0s19b1n0:     validation: 0
x3006c0s19b1n0:     test:       0
x3006c0s19b1n0: > building train, validation, and test datasets for GPT ...
x3006c0s19b1n0: Single data path provided for train, valid & test
x3006c0s19b1n0:  > building dataset index ...
x3006c0s19b1n0:     reading sizes...
x3006c0s19b1n0:     reading pointers...
x3006c0s19b1n0:     reading document index...
x3006c0s19b1n0:     creating numpy buffer of mmap...
x3006c0s19b1n0:     creating memory view of numpy buffer...
x3006c0s19b1n0:  > finished creating indexed dataset in 0.002447 seconds
x3006c0s19b1n0:     number of documents: 79000
x3006c0s19b1n0:  > dataset split:
x3006c0s19b1n0:     train:
x3006c0s19b1n0:      document indices in [0, 74971) total of 74971 documents
x3006c0s19b1n0:     validation:
x3006c0s19b1n0:      document indices in [74971, 78921) total of 3950 documents
x3006c0s19b1n0:     test:
x3006c0s19b1n0:      document indices in [78921, 79000) total of 79 documents
x3006c0s19b1n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_doc_idx.npy
x3006c0s19b1n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_sample_idx.npy
x3006c0s19b1n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/386626d17d59d628069060be9633441e_shuffle_idx.npy
x3006c0s19b1n0:     loaded indexed file in 0.002 seconds
x3006c0s19b1n0:     total number of samples: 108448
x3006c0s19b1n0:     total number of epochs: 1
x3006c0s19b1n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_doc_idx.npy
x3006c0s19b1n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_sample_idx.npy
x3006c0s19b1n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_shuffle_idx.npy
x3006c0s19b1n0:     loaded indexed file in 0.004 seconds
x3006c0s19b1n0:     total number of samples: 5792
x3006c0s19b1n0:     total number of epochs: 1
x3006c0s19b1n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_doc_idx.npy
x3006c0s19b1n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_sample_idx.npy
x3006c0s19b1n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_shuffle_idx.npy
x3006c0s19b1n0:     loaded indexed file in 0.003 seconds
x3006c0s19b1n0:     total number of samples: 185
x3006c0s19b1n0:     total number of epochs: 1
x3006c0s19b1n0: > finished creating GPT datasets ...
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5282065868377686>
x3006c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5439672470092773>
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5464234352111816>
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5531384944915771>
x3006c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5575745105743408>
x3006c0s19b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5629861354827881>
x3006c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5733475685119629>
x3006c0s1b0n0: <TIMER:train/valid/test-data-iterators-setup,0.590557336807251>
x3006c0s19b1n0: [after dataloaders are built] datetime: 2024-03-28 13:34:18 
x3006c0s19b1n0: done with setup ...
x3006c0s19b1n0: training ...
x3006c0s1b0n0: (min, max) time across ranks (ms):
x3006c0s1b0n0:     model-and-optimizer-setup ......................: (34503.70, 34508.37)
x3006c0s1b0n0:     train/valid/test-data-iterators-setup ..........: (528.21, 590.56)
x3006c0s19b1n0: [before training begins] datetime: 2024-03-28 13:34:18 
x3006c0s19b1n0: [before the start of training step] datetime: 2024-03-28 13:34:18 
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:34:18,809] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:34:18,810] [INFO] [utils.py:801:see_memory_usage] MA 7.37 GB         Max_MA 7.37 GB         CA 7.45 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:34:18,810] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 179.98 GB, percent = 35.8%
x3006c0s19b1n0: [2024-03-28 13:34:18,942] [INFO] [checkpointing.py:539:forward] Activation Checkpointing Information
x3006c0s19b1n0: [2024-03-28 13:34:18,942] [INFO] [checkpointing.py:540:forward] ----Partition Activations False, CPU CHECKPOINTING False
x3006c0s19b1n0: [2024-03-28 13:34:18,942] [INFO] [checkpointing.py:541:forward] ----contiguous Memory Checkpointing False with 60 total layers
x3006c0s19b1n0: [2024-03-28 13:34:18,942] [INFO] [checkpointing.py:543:forward] ----Synchronization False
x3006c0s19b1n0: [2024-03-28 13:34:18,942] [INFO] [checkpointing.py:544:forward] ----Profiling time in checkpointing False
x3006c0s19b1n0: [2024-03-28 13:34:27,099] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:34:27,099] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 19.76 GB         CA 22.95 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:34:27,100] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 180.15 GB, percent = 35.8%
x3006c0s19b1n0: [2024-03-28 13:34:27,272] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:34:27,273] [INFO] [utils.py:801:see_memory_usage] MA 16.84 GB         Max_MA 16.84 GB         CA 16.91 GB         Max_CA 23 GB 
x3006c0s19b1n0: [2024-03-28 13:34:27,273] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 180.15 GB, percent = 35.8%
x3006c0s19b1n0: [2024-03-28 13:34:52,882] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:34:52,883] [INFO] [utils.py:801:see_memory_usage] MA 10.25 GB         Max_MA 20.43 GB         CA 10.33 GB         Max_CA 27 GB 
x3006c0s19b1n0: [2024-03-28 13:34:52,883] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 180.17 GB, percent = 35.8%
x3006c0s19b1n0: [2024-03-28 13:34:52,960] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:34:52,961] [INFO] [utils.py:801:see_memory_usage] MA 10.25 GB         Max_MA 10.25 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:34:52,961] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 180.27 GB, percent = 35.8%
x3006c0s1b0n0: [2024-03-28 13:35:02,310] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.215471267700195
x3006c0s1b0n0: [2024-03-28 13:35:02,310] [INFO] [stage3.py:2251:step] Full outer step loop took 9.215766191482544
x3006c0s19b1n0: [2024-03-28 13:35:02,338] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.244404792785645
x3006c0s19b1n0: [2024-03-28 13:35:02,339] [INFO] [stage3.py:2251:step] Full outer step loop took 9.244733572006226
x3006c0s19b1n0: [2024-03-28 13:35:02,494] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.400208950042725
x3006c0s19b1n0: [2024-03-28 13:35:02,495] [INFO] [stage3.py:2251:step] Full outer step loop took 9.400515079498291
x3006c0s1b0n0: [2024-03-28 13:35:02,506] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.411928415298462
x3006c0s1b0n0: [2024-03-28 13:35:02,506] [INFO] [stage3.py:2251:step] Full outer step loop took 9.412272214889526
x3006c0s1b0n0: [2024-03-28 13:35:02,557] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.463270902633667
x3006c0s1b0n0: [2024-03-28 13:35:02,558] [INFO] [stage3.py:2251:step] Full outer step loop took 9.463565587997437
x3006c0s19b1n0: [2024-03-28 13:35:02,596] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.502186059951782
x3006c0s19b1n0: [2024-03-28 13:35:02,596] [INFO] [stage3.py:2251:step] Full outer step loop took 9.502432584762573
x3006c0s1b0n0: [2024-03-28 13:35:02,610] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.515614032745361
x3006c0s1b0n0: [2024-03-28 13:35:02,610] [INFO] [stage3.py:2251:step] Full outer step loop took 9.515811681747437
x3006c0s19b1n0: [2024-03-28 13:35:02,625] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.531005144119263
x3006c0s19b1n0: [2024-03-28 13:35:02,625] [INFO] [stage3.py:2251:step] Full outer step loop took 9.531208276748657
x3006c0s19b1n0: [2024-03-28 13:35:02,637] [INFO] [stage3.py:2277:step] End to end step took 9.542684316635132
x3006c0s1b0n0: [2024-03-28 13:35:02,637] [INFO] [stage3.py:2277:step] End to end step took 9.542636394500732
x3006c0s19b1n0: [2024-03-28 13:35:02,637] [INFO] [stage3.py:2277:step] End to end step took 9.542853832244873
x3006c0s1b0n0: [2024-03-28 13:35:02,637] [INFO] [stage3.py:2277:step] End to end step took 9.543067455291748
x3006c0s1b0n0: [2024-03-28 13:35:02,637] [INFO] [stage3.py:2277:step] End to end step took 9.542885065078735
x3006c0s1b0n0: [2024-03-28 13:35:02,637] [INFO] [stage3.py:2277:step] End to end step took 9.543101072311401
x3006c0s19b1n0: [2024-03-28 13:35:02,637] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 9245.00
x3006c0s19b1n0: [2024-03-28 13:35:02,637] [INFO] [stage3.py:2277:step] End to end step took 9.54318618774414
x3006c0s19b1n0: [2024-03-28 13:35:02,637] [WARNING] [stage3.py:2267:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3006c0s19b1n0: [2024-03-28 13:35:02,638] [INFO] [stage3.py:2277:step] End to end step took 9.54352593421936
x3006c0s19b1n0: [2024-03-28 13:35:02,638] [INFO] [logging.py:96:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:35:02,638] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 8365.65 | bwd_microstep: 25462.88 | bwd_inner_microstep: 25380.30 | bwd_allreduce_microstep: 82.45 | step_microstep: 9677.18
x3006c0s19b1n0: [2024-03-28 13:35:02,638] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 8365.65 | bwd: 25462.87 | bwd_inner: 25380.31 | bwd_allreduce: 82.46 | step: 9677.18
x3006c0s19b1n0: [2024-03-28 13:35:02,744] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:35:02,745] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.25 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:35:02,745] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 334.99 GB, percent = 66.6%
x3006c0s19b1n0: <TIMER:interval-time,44.13239622116089><TIMER:interval-time,44.132397413253784><TIMER:interval-time,44.13240313529968><TIMER:interval-time,44.132405519485474>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,44.13234496116638><TIMER:interval-time,44.13243556022644>
x3006c0s1b0n0: <TIMER:interval-time,44.132429122924805><TIMER:interval-time,44.13243341445923>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0:  elapsed_time 44.132433 | consumed samples:           32 | consumed tokens:        65536 | elapsed time per iteration (ms): 44132.4 | learning rate: 3.000E-04 | global batch size:    32 | lm loss: 1.215015E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.725 | TFLOPs: 50.17 |
x3006c0s19b1n0: [Rank 0] (after 1 iterations) memory (MB) | allocated: 9224.5439453125 | max allocated: 9224.54443359375 | reserved: 9296.0 | max reserved: 9296.0
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:35:02,875] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:35:02,876] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 13:35:02,876] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.09 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:35:09,407] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:35:09,407] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:35:09,407] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.07 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:35:09,487] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:35:09,488] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:35:09,488] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.07 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:35:28,012] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:35:28,012] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-28 13:35:28,013] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.07 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:35:28,086] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:35:28,087] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:35:28,087] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.06 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:35:34,852] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.741361379623413
x3006c0s19b1n0: [2024-03-28 13:35:34,853] [INFO] [stage3.py:2251:step] Full outer step loop took 6.741841554641724
x3006c0s19b1n0: [2024-03-28 13:35:35,062] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.9515016078948975
x3006c0s19b1n0: [2024-03-28 13:35:35,063] [INFO] [stage3.py:2251:step] Full outer step loop took 6.952052116394043
x3006c0s1b0n0: [2024-03-28 13:35:35,079] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.967979192733765
x3006c0s1b0n0: [2024-03-28 13:35:35,079] [INFO] [stage3.py:2251:step] Full outer step loop took 6.968435525894165
x3006c0s19b1n0: [2024-03-28 13:35:35,085] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.9746413230896
x3006c0s19b1n0: [2024-03-28 13:35:35,085] [INFO] [stage3.py:2251:step] Full outer step loop took 6.974835395812988
x3006c0s1b0n0: [2024-03-28 13:35:35,159] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.04871678352356
x3006c0s1b0n0: [2024-03-28 13:35:35,160] [INFO] [stage3.py:2251:step] Full outer step loop took 7.049070119857788
x3006c0s1b0n0: [2024-03-28 13:35:35,161] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.050314664840698
x3006c0s1b0n0: [2024-03-28 13:35:35,161] [INFO] [stage3.py:2251:step] Full outer step loop took 7.050494432449341
x3006c0s19b1n0: [2024-03-28 13:35:35,165] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.054362535476685
x3006c0s19b1n0: [2024-03-28 13:35:35,165] [INFO] [stage3.py:2251:step] Full outer step loop took 7.054513931274414
x3006c0s1b0n0: [2024-03-28 13:35:35,239] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.128068208694458
x3006c0s1b0n0: [2024-03-28 13:35:35,239] [INFO] [stage3.py:2251:step] Full outer step loop took 7.128213882446289
x3006c0s1b0n0: [2024-03-28 13:35:35,250] [INFO] [stage3.py:2277:step] End to end step took 7.1396965980529785
x3006c0s19b1n0: [2024-03-28 13:35:35,250] [INFO] [stage3.py:2277:step] End to end step took 7.139857053756714
x3006c0s1b0n0: [2024-03-28 13:35:35,251] [INFO] [stage3.py:2277:step] End to end step took 7.139994859695435
x3006c0s19b1n0: [2024-03-28 13:35:35,251] [INFO] [stage3.py:2277:step] End to end step took 7.14001727104187
x3006c0s1b0n0: [2024-03-28 13:35:35,251] [INFO] [stage3.py:2277:step] End to end step took 7.140081882476807
x3006c0s19b1n0: [2024-03-28 13:35:35,251] [INFO] [stage3.py:2277:step] End to end step took 7.140148878097534
x3006c0s19b1n0: [2024-03-28 13:35:35,251] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6742.14
x3006c0s1b0n0: [2024-03-28 13:35:35,251] [INFO] [stage3.py:2277:step] End to end step took 7.140333414077759
x3006c0s19b1n0: [2024-03-28 13:35:35,251] [INFO] [stage3.py:2277:step] End to end step took 7.1402974128723145
x3006c0s19b1n0: [2024-03-28 13:35:35,251] [INFO] [logging.py:96:log_dist] [Rank 0] step=2, skipped=0, lr=[0.0002918585038060976], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:35:35,252] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6494.15 | bwd_microstep: 18361.05 | bwd_inner_microstep: 18276.22 | bwd_allreduce_microstep: 84.75 | step_microstep: 7164.79
x3006c0s19b1n0: [2024-03-28 13:35:35,252] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6494.14 | bwd: 18361.04 | bwd_inner: 18276.22 | bwd_allreduce: 84.77 | step: 7164.80
x3006c0s19b1n0: [2024-03-28 13:35:35,374] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:35:35,375] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:35:35,375] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.11 GB, percent = 66.6%
x3006c0s19b1n0: <TIMER:interval-time,32.63006019592285><TIMER:interval-time,32.63006377220154>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,32.63007116317749>
x3006c0s1b0n0: <TIMER:interval-time,32.63007140159607>
x3006c0s1b0n0: <TIMER:interval-time,32.63007593154907><TIMER:interval-time,32.63007593154907><TIMER:interval-time,32.630078077316284>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s19b1n0: <TIMER:interval-time,32.63017702102661>
x3006c0s1b0n0:  elapsed_time 32.630076 | consumed samples:           64 | consumed tokens:       131072 | elapsed time per iteration (ms): 32630.1 | learning rate: 2.919E-04 | global batch size:    32 | lm loss: 1.216527E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.981 | TFLOPs: 67.85 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:35:35,510] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:35:35,511] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 13:35:35,511] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.11 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:35:42,736] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:35:42,737] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:35:42,737] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.1 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:35:42,822] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:35:42,822] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:35:42,822] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.1 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:36:00,817] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:36:00,818] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-28 13:36:00,818] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.08 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:36:00,892] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:36:00,893] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:36:00,893] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.08 GB, percent = 66.6%
x3006c0s1b0n0: [2024-03-28 13:36:07,507] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.590125322341919
x3006c0s1b0n0: [2024-03-28 13:36:07,507] [INFO] [stage3.py:2251:step] Full outer step loop took 6.590498208999634
x3006c0s1b0n0: [2024-03-28 13:36:07,897] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.980274438858032
x3006c0s1b0n0: [2024-03-28 13:36:07,897] [INFO] [stage3.py:2251:step] Full outer step loop took 6.980458974838257
x3006c0s1b0n0: [2024-03-28 13:36:07,903] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.9854326248168945
x3006c0s1b0n0: [2024-03-28 13:36:07,903] [INFO] [stage3.py:2251:step] Full outer step loop took 6.985639572143555
x3006c0s1b0n0: [2024-03-28 13:36:07,910] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.99256443977356
x3006c0s1b0n0: [2024-03-28 13:36:07,910] [INFO] [stage3.py:2251:step] Full outer step loop took 6.992715835571289
x3006c0s19b1n0: [2024-03-28 13:36:08,052] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.134736776351929
x3006c0s19b1n0: [2024-03-28 13:36:08,052] [INFO] [stage3.py:2251:step] Full outer step loop took 7.135107040405273
x3006c0s19b1n0: [2024-03-28 13:36:08,078] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.160876035690308
x3006c0s19b1n0: [2024-03-28 13:36:08,078] [INFO] [stage3.py:2251:step] Full outer step loop took 7.161105155944824
x3006c0s19b1n0: [2024-03-28 13:36:08,081] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.164368152618408
x3006c0s19b1n0: [2024-03-28 13:36:08,081] [INFO] [stage3.py:2251:step] Full outer step loop took 7.164517879486084
x3006c0s19b1n0: [2024-03-28 13:36:08,153] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.236255645751953
x3006c0s19b1n0: [2024-03-28 13:36:08,153] [INFO] [stage3.py:2251:step] Full outer step loop took 7.236415386199951
x3006c0s19b1n0: [2024-03-28 13:36:08,165] [INFO] [stage3.py:2277:step] End to end step took 7.248404502868652
x3006c0s1b0n0: [2024-03-28 13:36:08,165] [INFO] [stage3.py:2277:step] End to end step took 7.248291015625
x3006c0s1b0n0: [2024-03-28 13:36:08,165] [INFO] [stage3.py:2277:step] End to end step took 7.248027563095093
x3006c0s1b0n0: [2024-03-28 13:36:08,165] [INFO] [stage3.py:2277:step] End to end step took 7.248374938964844
x3006c0s19b1n0: [2024-03-28 13:36:08,165] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 7161.38
x3006c0s19b1n0: [2024-03-28 13:36:08,166] [INFO] [stage3.py:2277:step] End to end step took 7.248612403869629
x3006c0s19b1n0: [2024-03-28 13:36:08,166] [INFO] [stage3.py:2277:step] End to end step took 7.2485551834106445
x3006c0s19b1n0: [2024-03-28 13:36:08,166] [INFO] [stage3.py:2277:step] End to end step took 7.2486512660980225
x3006c0s1b0n0: [2024-03-28 13:36:08,166] [INFO] [stage3.py:2277:step] End to end step took 7.248856067657471
x3006c0s19b1n0: [2024-03-28 13:36:08,166] [INFO] [logging.py:96:log_dist] [Rank 0] step=3, skipped=0, lr=[0.00026841599982106197], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:36:08,166] [INFO] [timer.py:260:stop] epoch=0/micro_step=3/global_step=3, RunningAvgSamplesPerSec=0.979975895777164, CurrSamplesPerSec=0.979975895777164, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 13:36:08,167] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 7185.95 | bwd_microstep: 17827.22 | bwd_inner_microstep: 17748.23 | bwd_allreduce_microstep: 78.92 | step_microstep: 7273.13
x3006c0s19b1n0: [2024-03-28 13:36:08,167] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 7185.94 | bwd: 17827.21 | bwd_inner: 17748.22 | bwd_allreduce: 78.93 | step: 7273.13
x3006c0s19b1n0: [2024-03-28 13:36:08,301] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:36:08,301] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:36:08,301] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.11 GB, percent = 66.6%
x3006c0s19b1n0: <TIMER:interval-time,32.9259979724884><TIMER:interval-time,32.92600083351135><TIMER:interval-time,32.9260036945343>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,32.926037073135376><TIMER:interval-time,32.92603874206543><TIMER:interval-time,32.92604160308838>
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,32.926044940948486>
x3006c0s1b0n0: 
x3006c0s19b1n0: <TIMER:interval-time,32.92604041099548>
x3006c0s1b0n0:  elapsed_time 32.926045 | consumed samples:           96 | consumed tokens:       196608 | elapsed time per iteration (ms): 32926.0 | learning rate: 2.684E-04 | global batch size:    32 | lm loss: 3.444557E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.972 | TFLOPs: 67.24 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:36:08,459] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:36:08,459] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 13:36:08,460] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.11 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:36:15,162] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:36:15,162] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:36:15,162] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.09 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:36:15,268] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:36:15,269] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:36:15,269] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.09 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:36:33,767] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:36:33,768] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-28 13:36:33,768] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.08 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:36:33,843] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:36:33,844] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:36:33,844] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.08 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:36:40,373] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.504886865615845
x3006c0s19b1n0: [2024-03-28 13:36:40,374] [INFO] [stage3.py:2251:step] Full outer step loop took 6.50540018081665
x3006c0s1b0n0: [2024-03-28 13:36:40,683] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.815335988998413
x3006c0s1b0n0: [2024-03-28 13:36:40,684] [INFO] [stage3.py:2251:step] Full outer step loop took 6.8162150382995605
x3006c0s1b0n0: [2024-03-28 13:36:40,741] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.873234748840332
x3006c0s1b0n0: [2024-03-28 13:36:40,741] [INFO] [stage3.py:2251:step] Full outer step loop took 6.873454809188843
x3006c0s1b0n0: [2024-03-28 13:36:40,746] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.8785624504089355
x3006c0s1b0n0: [2024-03-28 13:36:40,746] [INFO] [stage3.py:2251:step] Full outer step loop took 6.878756999969482
x3006c0s19b1n0: [2024-03-28 13:36:40,774] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.9061291217803955
x3006c0s19b1n0: [2024-03-28 13:36:40,774] [INFO] [stage3.py:2251:step] Full outer step loop took 6.906312704086304
x3006c0s19b1n0: [2024-03-28 13:36:40,831] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.963106870651245
x3006c0s19b1n0: [2024-03-28 13:36:40,831] [INFO] [stage3.py:2251:step] Full outer step loop took 6.963454008102417
x3006c0s19b1n0: [2024-03-28 13:36:40,862] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.994108200073242
x3006c0s19b1n0: [2024-03-28 13:36:40,862] [INFO] [stage3.py:2251:step] Full outer step loop took 6.994263648986816
x3006c0s1b0n0: [2024-03-28 13:36:40,896] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.028512239456177
x3006c0s1b0n0: [2024-03-28 13:36:40,896] [INFO] [stage3.py:2251:step] Full outer step loop took 7.028681755065918
x3006c0s1b0n0: [2024-03-28 13:36:40,909] [INFO] [stage3.py:2277:step] End to end step took 7.041180372238159
x3006c0s19b1n0: [2024-03-28 13:36:40,909] [INFO] [stage3.py:2277:step] End to end step took 7.041279315948486
x3006c0s19b1n0: [2024-03-28 13:36:40,909] [INFO] [stage3.py:2277:step] End to end step took 7.041374683380127
x3006c0s19b1n0: [2024-03-28 13:36:40,909] [INFO] [stage3.py:2277:step] End to end step took 7.041479825973511
x3006c0s19b1n0: [2024-03-28 13:36:40,909] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6505.93
x3006c0s1b0n0: [2024-03-28 13:36:40,909] [INFO] [stage3.py:2277:step] End to end step took 7.041670083999634
x3006c0s19b1n0: [2024-03-28 13:36:40,909] [INFO] [stage3.py:2277:step] End to end step took 7.0411763191223145
x3006c0s1b0n0: [2024-03-28 13:36:40,909] [INFO] [stage3.py:2277:step] End to end step took 7.0417115688323975
x3006c0s1b0n0: [2024-03-28 13:36:40,909] [INFO] [stage3.py:2277:step] End to end step took 7.041814804077148
x3006c0s19b1n0: [2024-03-28 13:36:40,910] [INFO] [logging.py:96:log_dist] [Rank 0] step=4, skipped=0, lr=[0.00023249999999999996], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:36:40,910] [INFO] [timer.py:260:stop] epoch=0/micro_step=4/global_step=4, RunningAvgSamplesPerSec=0.9830632481497881, CurrSamplesPerSec=0.9861701150202269, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 13:36:40,910] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6657.90 | bwd_microstep: 18326.22 | bwd_inner_microstep: 18243.34 | bwd_allreduce_microstep: 82.81 | step_microstep: 7066.03
x3006c0s19b1n0: [2024-03-28 13:36:40,910] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6657.89 | bwd: 18326.23 | bwd_inner: 18243.33 | bwd_allreduce: 82.83 | step: 7066.03
x3006c0s19b1n0: [2024-03-28 13:36:41,022] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:36:41,022] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:36:41,023] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.11 GB, percent = 66.6%
x3006c0s19b1n0: <TIMER:interval-time,32.72096633911133><TIMER:interval-time,32.720946073532104><TIMER:interval-time,32.720969676971436>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,32.72047448158264><TIMER:interval-time,32.72047448158264>
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,32.72047781944275><TIMER:interval-time,32.720481395721436>
x3006c0s1b0n0: 
x3006c0s19b1n0: <TIMER:interval-time,32.72109293937683>
x3006c0s1b0n0:  elapsed_time 32.720481 | consumed samples:          128 | consumed tokens:       262144 | elapsed time per iteration (ms): 32720.5 | learning rate: 2.325E-04 | global batch size:    32 | lm loss: 2.249373E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.978 | TFLOPs: 67.67 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:36:41,155] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:36:41,156] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 13:36:41,156] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.12 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:36:47,873] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:36:47,873] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:36:47,873] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.11 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:36:47,978] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:36:47,978] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:36:47,978] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.1 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:37:06,452] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:37:06,453] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-28 13:37:06,453] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.09 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:37:06,528] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:37:06,529] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:37:06,529] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.09 GB, percent = 66.6%
x3006c0s1b0n0: [2024-03-28 13:37:13,275] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.722841024398804
x3006c0s1b0n0: [2024-03-28 13:37:13,276] [INFO] [stage3.py:2251:step] Full outer step loop took 6.723124027252197
x3006c0s19b1n0: [2024-03-28 13:37:13,310] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.741988658905029
x3006c0s19b1n0: [2024-03-28 13:37:13,310] [INFO] [stage3.py:2251:step] Full outer step loop took 6.742746591567993
x3006c0s1b0n0: [2024-03-28 13:37:13,398] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.845733165740967
x3006c0s1b0n0: [2024-03-28 13:37:13,399] [INFO] [stage3.py:2251:step] Full outer step loop took 6.8459765911102295
x3006c0s19b1n0: [2024-03-28 13:37:13,519] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.96621561050415
x3006c0s19b1n0: [2024-03-28 13:37:13,519] [INFO] [stage3.py:2251:step] Full outer step loop took 6.966554403305054
x3006c0s19b1n0: [2024-03-28 13:37:13,536] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.9830076694488525
x3006c0s19b1n0: [2024-03-28 13:37:13,536] [INFO] [stage3.py:2251:step] Full outer step loop took 6.983156204223633
x3006c0s1b0n0: [2024-03-28 13:37:13,582] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.029617071151733
x3006c0s1b0n0: [2024-03-28 13:37:13,583] [INFO] [stage3.py:2251:step] Full outer step loop took 7.029811382293701
x3006c0s1b0n0: [2024-03-28 13:37:13,594] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.040877342224121
x3006c0s1b0n0: [2024-03-28 13:37:13,594] [INFO] [stage3.py:2251:step] Full outer step loop took 7.041030406951904
x3006c0s19b1n0: [2024-03-28 13:37:13,740] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.187767267227173
x3006c0s19b1n0: [2024-03-28 13:37:13,740] [INFO] [stage3.py:2251:step] Full outer step loop took 7.187917947769165
x3006c0s19b1n0: [2024-03-28 13:37:13,752] [INFO] [stage3.py:2277:step] End to end step took 7.199360370635986
x3006c0s1b0n0: [2024-03-28 13:37:13,752] [INFO] [stage3.py:2277:step] End to end step took 7.199245929718018
x3006c0s1b0n0: [2024-03-28 13:37:13,752] [INFO] [stage3.py:2277:step] End to end step took 7.199233531951904
x3006c0s1b0n0: [2024-03-28 13:37:13,752] [INFO] [stage3.py:2277:step] End to end step took 7.199641942977905
x3006c0s19b1n0: [2024-03-28 13:37:13,752] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6743.50
x3006c0s1b0n0: [2024-03-28 13:37:13,753] [INFO] [stage3.py:2277:step] End to end step took 7.199917316436768
x3006c0s19b1n0: [2024-03-28 13:37:13,752] [INFO] [stage3.py:2277:step] End to end step took 7.199753522872925
x3006c0s19b1n0: [2024-03-28 13:37:13,753] [INFO] [stage3.py:2277:step] End to end step took 7.199913501739502
x3006c0s19b1n0: [2024-03-28 13:37:13,753] [INFO] [stage3.py:2277:step] End to end step took 7.185130596160889
x3006c0s19b1n0: [2024-03-28 13:37:13,753] [INFO] [logging.py:96:log_dist] [Rank 0] step=5, skipped=0, lr=[0.0001884425039850356], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:37:13,753] [INFO] [timer.py:260:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=0.9826113070298675, CurrSamplesPerSec=0.9817086702631906, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 13:37:13,754] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6675.41 | bwd_microstep: 18305.82 | bwd_inner_microstep: 18222.66 | bwd_allreduce_microstep: 83.09 | step_microstep: 7224.06
x3006c0s19b1n0: [2024-03-28 13:37:13,754] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6675.40 | bwd: 18305.82 | bwd_inner: 18222.66 | bwd_allreduce: 83.10 | step: 7224.07
x3006c0s19b1n0: [2024-03-28 13:37:13,872] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:37:13,872] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:37:13,872] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.12 GB, percent = 66.6%
x3006c0s19b1n0: <TIMER:interval-time,32.849186420440674>
x3006c0s19b1n0: <TIMER:interval-time,32.849170446395874>
x3006c0s19b1n0: <TIMER:interval-time,32.849193811416626><TIMER:interval-time,32.84919190406799>
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,32.84924006462097><TIMER:interval-time,32.84923982620239><TIMER:interval-time,32.84924077987671>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,32.84924507141113>
x3006c0s1b0n0:  elapsed_time 32.849240 | consumed samples:          160 | consumed tokens:       327680 | elapsed time per iteration (ms): 32849.2 | learning rate: 1.884E-04 | global batch size:    32 | lm loss: 1.748318E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.974 | TFLOPs: 67.40 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:37:14,006] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:37:14,007] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 13:37:14,007] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.13 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:37:20,686] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:37:20,687] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:37:20,687] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.1 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:37:20,768] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:37:20,769] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:37:20,769] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.1 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:37:39,214] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:37:39,214] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-28 13:37:39,215] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.09 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:37:39,286] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:37:39,287] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:37:39,287] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.09 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:37:46,082] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.750572443008423
x3006c0s19b1n0: [2024-03-28 13:37:46,083] [INFO] [stage3.py:2251:step] Full outer step loop took 6.7517409324646
x3006c0s1b0n0: [2024-03-28 13:37:46,201] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.890251874923706
x3006c0s1b0n0: [2024-03-28 13:37:46,201] [INFO] [stage3.py:2251:step] Full outer step loop took 6.890471696853638
x3006c0s19b1n0: [2024-03-28 13:37:46,214] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.903411626815796
x3006c0s19b1n0: [2024-03-28 13:37:46,214] [INFO] [stage3.py:2251:step] Full outer step loop took 6.9036688804626465
x3006c0s1b0n0: [2024-03-28 13:37:46,340] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.029015779495239
x3006c0s1b0n0: [2024-03-28 13:37:46,340] [INFO] [stage3.py:2251:step] Full outer step loop took 7.029258966445923
x3006c0s1b0n0: [2024-03-28 13:37:46,344] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.033527374267578
x3006c0s1b0n0: [2024-03-28 13:37:46,344] [INFO] [stage3.py:2251:step] Full outer step loop took 7.0337207317352295
x3006c0s1b0n0: [2024-03-28 13:37:46,354] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.043062210083008
x3006c0s1b0n0: [2024-03-28 13:37:46,354] [INFO] [stage3.py:2251:step] Full outer step loop took 7.043212175369263
x3006c0s19b1n0: [2024-03-28 13:37:46,367] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.056745767593384
x3006c0s19b1n0: [2024-03-28 13:37:46,368] [INFO] [stage3.py:2251:step] Full outer step loop took 7.056936740875244
x3006c0s19b1n0: [2024-03-28 13:37:46,374] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.063466787338257
x3006c0s19b1n0: [2024-03-28 13:37:46,374] [INFO] [stage3.py:2251:step] Full outer step loop took 7.0636305809021
x3006c0s1b0n0: [2024-03-28 13:37:46,386] [INFO] [stage3.py:2277:step] End to end step took 7.075578451156616
x3006c0s19b1n0: [2024-03-28 13:37:46,386] [INFO] [stage3.py:2277:step] End to end step took 7.075555324554443
x3006c0s1b0n0: [2024-03-28 13:37:46,386] [INFO] [stage3.py:2277:step] End to end step took 7.075644254684448
x3006c0s1b0n0: [2024-03-28 13:37:46,386] [INFO] [stage3.py:2277:step] End to end step took 7.07577109336853
x3006c0s19b1n0: [2024-03-28 13:37:46,387] [INFO] [stage3.py:2277:step] End to end step took 7.075834035873413
x3006c0s19b1n0: [2024-03-28 13:37:46,387] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6752.78
x3006c0s1b0n0: [2024-03-28 13:37:46,387] [INFO] [stage3.py:2277:step] End to end step took 7.076074600219727
x3006c0s19b1n0: [2024-03-28 13:37:46,387] [INFO] [stage3.py:2277:step] End to end step took 7.076113939285278
x3006c0s19b1n0: [2024-03-28 13:37:46,387] [INFO] [stage3.py:2277:step] End to end step took 7.055934190750122
x3006c0s19b1n0: [2024-03-28 13:37:46,387] [INFO] [logging.py:96:log_dist] [Rank 0] step=6, skipped=0, lr=[0.0001415574960149644], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:37:46,388] [INFO] [timer.py:260:stop] epoch=0/micro_step=6/global_step=6, RunningAvgSamplesPerSec=0.9840232160094651, CurrSamplesPerSec=0.9882833935213164, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 13:37:46,388] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6638.52 | bwd_microstep: 18280.31 | bwd_inner_microstep: 18199.00 | bwd_allreduce_microstep: 81.25 | step_microstep: 7100.68
x3006c0s19b1n0: [2024-03-28 13:37:46,388] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6638.51 | bwd: 18280.32 | bwd_inner: 18198.99 | bwd_allreduce: 81.26 | step: 7100.69
x3006c0s19b1n0: [2024-03-28 13:37:46,513] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:37:46,513] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:37:46,514] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.11 GB, percent = 66.6%
x3006c0s19b1n0: <TIMER:interval-time,32.64114713668823><TIMER:interval-time,32.64115118980408>
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,32.641162157058716>
x3006c0s19b1n0: <TIMER:interval-time,32.641209840774536>
x3006c0s1b0n0: <TIMER:interval-time,32.64092969894409><TIMER:interval-time,32.640931606292725><TIMER:interval-time,32.640929222106934><TIMER:interval-time,32.6409330368042>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0:  elapsed_time 32.640933 | consumed samples:          192 | consumed tokens:       393216 | elapsed time per iteration (ms): 32640.9 | learning rate: 1.416E-04 | global batch size:    32 | lm loss: 1.497432E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.980 | TFLOPs: 67.83 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:37:46,658] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:37:46,658] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 13:37:46,658] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.12 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:37:53,342] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:37:53,343] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:37:53,343] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.09 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:37:53,420] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:37:53,421] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:37:53,421] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.09 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:38:11,870] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:38:11,871] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-28 13:38:11,871] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.08 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:38:11,940] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:38:11,940] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:38:11,941] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.08 GB, percent = 66.6%
x3006c0s1b0n0: [2024-03-28 13:38:18,792] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.828016757965088
x3006c0s1b0n0: [2024-03-28 13:38:18,792] [INFO] [stage3.py:2251:step] Full outer step loop took 6.828293085098267
x3006c0s19b1n0: [2024-03-28 13:38:18,821] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.857423782348633
x3006c0s19b1n0: [2024-03-28 13:38:18,822] [INFO] [stage3.py:2251:step] Full outer step loop took 6.857633113861084
x3006c0s1b0n0: [2024-03-28 13:38:18,830] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.866628170013428
x3006c0s1b0n0: [2024-03-28 13:38:18,831] [INFO] [stage3.py:2251:step] Full outer step loop took 6.8670594692230225
x3006c0s1b0n0: [2024-03-28 13:38:18,932] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.96817946434021
x3006c0s1b0n0: [2024-03-28 13:38:18,932] [INFO] [stage3.py:2251:step] Full outer step loop took 6.968380451202393
x3006c0s1b0n0: [2024-03-28 13:38:19,006] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.042505979537964
x3006c0s1b0n0: [2024-03-28 13:38:19,006] [INFO] [stage3.py:2251:step] Full outer step loop took 7.042657375335693
x3006c0s19b1n0: [2024-03-28 13:38:19,008] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.043930768966675
x3006c0s19b1n0: [2024-03-28 13:38:19,008] [INFO] [stage3.py:2251:step] Full outer step loop took 7.044157266616821
x3006c0s19b1n0: [2024-03-28 13:38:19,061] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.097176790237427
x3006c0s19b1n0: [2024-03-28 13:38:19,061] [INFO] [stage3.py:2251:step] Full outer step loop took 7.097451686859131
x3006c0s19b1n0: [2024-03-28 13:38:19,087] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.123241901397705
x3006c0s19b1n0: [2024-03-28 13:38:19,087] [INFO] [stage3.py:2251:step] Full outer step loop took 7.123396158218384
x3006c0s1b0n0: [2024-03-28 13:38:19,098] [INFO] [stage3.py:2277:step] End to end step took 7.1345250606536865
x3006c0s19b1n0: [2024-03-28 13:38:19,098] [INFO] [stage3.py:2277:step] End to end step took 7.134703159332275
x3006c0s19b1n0: [2024-03-28 13:38:19,098] [INFO] [stage3.py:2277:step] End to end step took 7.134429454803467
x3006c0s19b1n0: [2024-03-28 13:38:19,099] [INFO] [stage3.py:2277:step] End to end step took 7.135092258453369
x3006c0s1b0n0: [2024-03-28 13:38:19,099] [INFO] [stage3.py:2277:step] End to end step took 7.135227918624878
x3006c0s19b1n0: [2024-03-28 13:38:19,099] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6857.87
x3006c0s1b0n0: [2024-03-28 13:38:19,099] [INFO] [stage3.py:2277:step] End to end step took 7.135354280471802
x3006c0s19b1n0: [2024-03-28 13:38:19,099] [INFO] [stage3.py:2277:step] End to end step took 7.135143995285034
x3006c0s1b0n0: [2024-03-28 13:38:19,099] [INFO] [stage3.py:2277:step] End to end step took 7.1354265213012695
x3006c0s19b1n0: [2024-03-28 13:38:19,099] [INFO] [logging.py:96:log_dist] [Rank 0] step=7, skipped=0, lr=[9.750000000000001e-05], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:38:19,100] [INFO] [timer.py:260:stop] epoch=0/micro_step=7/global_step=7, RunningAvgSamplesPerSec=0.9845053747186828, CurrSamplesPerSec=0.9864387438657788, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 13:38:19,100] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6648.19 | bwd_microstep: 18288.23 | bwd_inner_microstep: 18206.51 | bwd_allreduce_microstep: 81.65 | step_microstep: 7159.25
x3006c0s19b1n0: [2024-03-28 13:38:19,100] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6648.17 | bwd: 18288.23 | bwd_inner: 18206.51 | bwd_allreduce: 81.66 | step: 7159.25
x3006c0s19b1n0: [2024-03-28 13:38:19,216] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:38:19,216] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:38:19,216] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.11 GB, percent = 66.6%
x3006c0s19b1n0: <TIMER:interval-time,32.702394008636475><TIMER:interval-time,32.702409505844116><TIMER:interval-time,32.70241117477417>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,32.70242357254028>
x3006c0s1b0n0: <TIMER:interval-time,32.70241832733154><TIMER:interval-time,32.702420711517334><TIMER:interval-time,32.70242118835449><TIMER:interval-time,32.70242476463318>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0:  elapsed_time 32.702425 | consumed samples:          224 | consumed tokens:       458752 | elapsed time per iteration (ms): 32702.4 | learning rate: 9.750E-05 | global batch size:    32 | lm loss: 1.319906E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.979 | TFLOPs: 67.70 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:38:19,349] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:38:19,350] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 13:38:19,350] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.12 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:38:25,792] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:38:25,792] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:38:25,793] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.1 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:38:25,870] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:38:25,870] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:38:25,870] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.1 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:38:44,216] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:38:44,217] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-28 13:38:44,217] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.08 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:38:44,287] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:38:44,288] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:38:44,288] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.08 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:38:51,011] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.699077606201172
x3006c0s19b1n0: [2024-03-28 13:38:51,012] [INFO] [stage3.py:2251:step] Full outer step loop took 6.700666904449463
x3006c0s1b0n0: [2024-03-28 13:38:51,027] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.7155234813690186
x3006c0s1b0n0: [2024-03-28 13:38:51,028] [INFO] [stage3.py:2251:step] Full outer step loop took 6.716612100601196
x3006c0s1b0n0: [2024-03-28 13:38:51,098] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.786423206329346
x3006c0s1b0n0: [2024-03-28 13:38:51,098] [INFO] [stage3.py:2251:step] Full outer step loop took 6.786789894104004
x3006c0s19b1n0: [2024-03-28 13:38:51,297] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.9852166175842285
x3006c0s19b1n0: [2024-03-28 13:38:51,297] [INFO] [stage3.py:2251:step] Full outer step loop took 6.985431671142578
x3006c0s19b1n0: [2024-03-28 13:38:51,304] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.993045806884766
x3006c0s19b1n0: [2024-03-28 13:38:51,304] [INFO] [stage3.py:2251:step] Full outer step loop took 6.993232488632202
x3006c0s19b1n0: [2024-03-28 13:38:51,373] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.061339378356934
x3006c0s19b1n0: [2024-03-28 13:38:51,373] [INFO] [stage3.py:2251:step] Full outer step loop took 7.061494827270508
x3006c0s1b0n0: [2024-03-28 13:38:51,376] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.0649425983428955
x3006c0s1b0n0: [2024-03-28 13:38:51,376] [INFO] [stage3.py:2251:step] Full outer step loop took 7.065112113952637
x3006c0s1b0n0: [2024-03-28 13:38:51,376] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.065275430679321
x3006c0s1b0n0: [2024-03-28 13:38:51,377] [INFO] [stage3.py:2251:step] Full outer step loop took 7.065420866012573
x3006c0s1b0n0: [2024-03-28 13:38:51,388] [INFO] [stage3.py:2277:step] End to end step took 7.076926946640015
x3006c0s19b1n0: [2024-03-28 13:38:51,388] [INFO] [stage3.py:2277:step] End to end step took 7.077032566070557
x3006c0s1b0n0: [2024-03-28 13:38:51,388] [INFO] [stage3.py:2277:step] End to end step took 7.077035903930664
x3006c0s19b1n0: [2024-03-28 13:38:51,388] [INFO] [stage3.py:2277:step] End to end step took 7.076546669006348
x3006c0s19b1n0: [2024-03-28 13:38:51,389] [INFO] [stage3.py:2277:step] End to end step took 7.077416181564331
x3006c0s19b1n0: [2024-03-28 13:38:51,388] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6701.92
x3006c0s1b0n0: [2024-03-28 13:38:51,389] [INFO] [stage3.py:2277:step] End to end step took 7.077408075332642
x3006c0s1b0n0: [2024-03-28 13:38:51,389] [INFO] [stage3.py:2277:step] End to end step took 7.077410697937012
x3006c0s19b1n0: [2024-03-28 13:38:51,389] [INFO] [stage3.py:2277:step] End to end step took 7.077512264251709
x3006c0s19b1n0: [2024-03-28 13:38:51,389] [INFO] [logging.py:96:log_dist] [Rank 0] step=8, skipped=0, lr=[6.158400017893797e-05], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:38:51,389] [INFO] [timer.py:260:stop] epoch=0/micro_step=8/global_step=8, RunningAvgSamplesPerSec=0.9868606868044366, CurrSamplesPerSec=0.9988083379300107, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 13:38:51,390] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6407.99 | bwd_microstep: 18183.05 | bwd_inner_microstep: 18102.21 | bwd_allreduce_microstep: 80.78 | step_microstep: 7101.68
x3006c0s19b1n0: [2024-03-28 13:38:51,390] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6407.98 | bwd: 18183.05 | bwd_inner: 18102.20 | bwd_allreduce: 80.79 | step: 7101.69
x3006c0s19b1n0: [2024-03-28 13:38:51,505] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:38:51,506] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:38:51,506] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.11 GB, percent = 66.6%
x3006c0s19b1n0: <TIMER:interval-time,32.28936767578125><TIMER:interval-time,32.289369344711304><TIMER:interval-time,32.28937268257141>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: <TIMER:interval-time,32.28938698768616>
x3006c0s1b0n0: <TIMER:interval-time,32.28938388824463><TIMER:interval-time,32.2893750667572><TIMER:interval-time,32.289387464523315><TIMER:interval-time,32.289387464523315>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0:  elapsed_time 32.289387 | consumed samples:          256 | consumed tokens:       524288 | elapsed time per iteration (ms): 32289.4 | learning rate: 6.158E-05 | global batch size:    32 | lm loss: 1.228736E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.991 | TFLOPs: 68.57 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:38:51,627] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:38:51,628] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 13:38:51,628] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.11 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:38:58,470] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:38:58,470] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:38:58,471] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.1 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:38:58,548] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:38:58,549] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:38:58,549] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.1 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:39:17,401] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:39:17,402] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-28 13:39:17,402] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.09 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:39:17,471] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:39:17,472] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:39:17,472] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.09 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:39:24,193] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.683599948883057
x3006c0s19b1n0: [2024-03-28 13:39:24,194] [INFO] [stage3.py:2251:step] Full outer step loop took 6.684024333953857
x3006c0s19b1n0: [2024-03-28 13:39:24,386] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.891675233840942
x3006c0s19b1n0: [2024-03-28 13:39:24,387] [INFO] [stage3.py:2251:step] Full outer step loop took 6.8919103145599365
x3006c0s1b0n0: [2024-03-28 13:39:24,475] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.979840517044067
x3006c0s1b0n0: [2024-03-28 13:39:24,475] [INFO] [stage3.py:2251:step] Full outer step loop took 6.980194091796875
x3006c0s1b0n0: [2024-03-28 13:39:24,501] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.005797863006592
x3006c0s1b0n0: [2024-03-28 13:39:24,501] [INFO] [stage3.py:2251:step] Full outer step loop took 7.006032705307007
x3006c0s1b0n0: [2024-03-28 13:39:24,535] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.039787769317627
x3006c0s1b0n0: [2024-03-28 13:39:24,536] [INFO] [stage3.py:2251:step] Full outer step loop took 7.040888547897339
x3006c0s1b0n0: [2024-03-28 13:39:24,583] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.088561058044434
x3006c0s1b0n0: [2024-03-28 13:39:24,584] [INFO] [stage3.py:2251:step] Full outer step loop took 7.0887181758880615
x3006c0s19b1n0: [2024-03-28 13:39:24,585] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.090625286102295
x3006c0s19b1n0: [2024-03-28 13:39:24,586] [INFO] [stage3.py:2251:step] Full outer step loop took 7.090893268585205
x3006c0s19b1n0: [2024-03-28 13:39:24,613] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.118331432342529
x3006c0s19b1n0: [2024-03-28 13:39:24,613] [INFO] [stage3.py:2251:step] Full outer step loop took 7.118484258651733
x3006c0s1b0n0: [2024-03-28 13:39:24,625] [INFO] [stage3.py:2277:step] End to end step took 7.129886150360107
x3006c0s19b1n0: [2024-03-28 13:39:24,625] [INFO] [stage3.py:2277:step] End to end step took 7.130182981491089
x3006c0s1b0n0: [2024-03-28 13:39:24,625] [INFO] [stage3.py:2277:step] End to end step took 7.13027024269104
x3006c0s1b0n0: [2024-03-28 13:39:24,625] [INFO] [stage3.py:2277:step] End to end step took 7.130288362503052
x3006c0s19b1n0: [2024-03-28 13:39:24,625] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6684.31
x3006c0s19b1n0: [2024-03-28 13:39:24,625] [INFO] [stage3.py:2277:step] End to end step took 7.130601644515991
x3006c0s19b1n0: [2024-03-28 13:39:24,625] [INFO] [stage3.py:2277:step] End to end step took 7.130611181259155
x3006c0s19b1n0: [2024-03-28 13:39:24,625] [INFO] [stage3.py:2277:step] End to end step took 7.115669012069702
x3006c0s1b0n0: [2024-03-28 13:39:24,625] [INFO] [stage3.py:2277:step] End to end step took 7.130549669265747
x3006c0s19b1n0: [2024-03-28 13:39:24,626] [INFO] [logging.py:96:log_dist] [Rank 0] step=9, skipped=0, lr=[3.814149619390238e-05], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:39:24,626] [INFO] [timer.py:260:stop] epoch=0/micro_step=9/global_step=9, RunningAvgSamplesPerSec=0.9843849939330173, CurrSamplesPerSec=0.9697878163426334, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 13:39:24,626] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6804.09 | bwd_microstep: 18692.99 | bwd_inner_microstep: 18608.83 | bwd_allreduce_microstep: 84.09 | step_microstep: 7154.44
x3006c0s19b1n0: [2024-03-28 13:39:24,627] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6804.07 | bwd: 18692.99 | bwd_inner: 18608.83 | bwd_allreduce: 84.10 | step: 7154.44
x3006c0s19b1n0: [2024-03-28 13:39:24,744] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:39:24,744] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:39:24,744] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.11 GB, percent = 66.6%
x3006c0s19b1n0: <TIMER:interval-time,33.238428354263306>
x3006c0s19b1n0: <TIMER:interval-time,33.23844075202942>
x3006c0s19b1n0: <TIMER:interval-time,33.238463163375854>
x3006c0s1b0n0: <TIMER:interval-time,33.23844385147095><TIMER:interval-time,33.23844265937805><TIMER:interval-time,33.238447427749634><TIMER:interval-time,33.238447427749634>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s19b1n0: <TIMER:interval-time,33.238563537597656>
x3006c0s1b0n0:  elapsed_time 33.238447 | consumed samples:          288 | consumed tokens:       589824 | elapsed time per iteration (ms): 33238.4 | learning rate: 3.814E-05 | global batch size:    32 | lm loss: 1.242802E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.963 | TFLOPs: 66.61 |
x3006c0s19b1n0: In train_step in training.py!!!!!... True, False
x3006c0s19b1n0: [2024-03-28 13:39:24,861] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3006c0s19b1n0: [2024-03-28 13:39:24,862] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 9.01 GB         CA 9.08 GB         Max_CA 9 GB 
x3006c0s19b1n0: [2024-03-28 13:39:24,862] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.12 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:39:31,537] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3006c0s19b1n0: [2024-03-28 13:39:31,537] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 22.29 GB         CA 25.85 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:39:31,538] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.1 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:39:31,620] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3006c0s19b1n0: [2024-03-28 13:39:31,620] [INFO] [utils.py:801:see_memory_usage] MA 19.98 GB         Max_MA 19.98 GB         CA 20.14 GB         Max_CA 26 GB 
x3006c0s19b1n0: [2024-03-28 13:39:31,620] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.1 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:39:50,111] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3006c0s19b1n0: [2024-03-28 13:39:50,111] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 22.51 GB         CA 10.33 GB         Max_CA 30 GB 
x3006c0s19b1n0: [2024-03-28 13:39:50,112] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.08 GB, percent = 66.6%
x3006c0s19b1n0: [2024-03-28 13:39:50,178] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3006c0s19b1n0: [2024-03-28 13:39:50,178] [INFO] [utils.py:801:see_memory_usage] MA 10.26 GB         Max_MA 10.26 GB         CA 10.33 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:39:50,178] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.08 GB, percent = 66.6%
x3006c0s1b0n0: [2024-03-28 13:39:56,861] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.659533977508545
x3006c0s1b0n0: [2024-03-28 13:39:56,861] [INFO] [stage3.py:2251:step] Full outer step loop took 6.659772634506226
x3006c0s19b1n0: [2024-03-28 13:39:57,027] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.825189828872681
x3006c0s19b1n0: [2024-03-28 13:39:57,027] [INFO] [stage3.py:2251:step] Full outer step loop took 6.825402498245239
x3006c0s1b0n0: [2024-03-28 13:39:57,047] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.844928503036499
x3006c0s1b0n0: [2024-03-28 13:39:57,047] [INFO] [stage3.py:2251:step] Full outer step loop took 6.845134973526001
x3006c0s1b0n0: [2024-03-28 13:39:57,158] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.9559855461120605
x3006c0s1b0n0: [2024-03-28 13:39:57,158] [INFO] [stage3.py:2251:step] Full outer step loop took 6.95617413520813
x3006c0s1b0n0: [2024-03-28 13:39:57,181] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.979581832885742
x3006c0s1b0n0: [2024-03-28 13:39:57,181] [INFO] [stage3.py:2251:step] Full outer step loop took 6.979731559753418
x3006c0s19b1n0: [2024-03-28 13:39:57,195] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.993387460708618
x3006c0s19b1n0: [2024-03-28 13:39:57,195] [INFO] [stage3.py:2251:step] Full outer step loop took 6.993625640869141
x3006c0s19b1n0: [2024-03-28 13:39:57,276] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.074540615081787
x3006c0s19b1n0: [2024-03-28 13:39:57,276] [INFO] [stage3.py:2251:step] Full outer step loop took 7.0747230052948
x3006c0s19b1n0: [2024-03-28 13:39:57,292] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.09009313583374
x3006c0s19b1n0: [2024-03-28 13:39:57,292] [INFO] [stage3.py:2251:step] Full outer step loop took 7.090251445770264
x3006c0s1b0n0: [2024-03-28 13:39:57,303] [INFO] [stage3.py:2277:step] End to end step took 7.101225852966309
x3006c0s19b1n0: [2024-03-28 13:39:57,303] [INFO] [stage3.py:2277:step] End to end step took 7.101404666900635
x3006c0s19b1n0: [2024-03-28 13:39:57,303] [INFO] [stage3.py:2277:step] End to end step took 7.101442575454712
x3006c0s19b1n0: [2024-03-28 13:39:57,303] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6825.62
x3006c0s19b1n0: [2024-03-28 13:39:57,303] [INFO] [stage3.py:2277:step] End to end step took 7.101691722869873
x3006c0s19b1n0: [2024-03-28 13:39:57,303] [INFO] [stage3.py:2277:step] End to end step took 7.101630687713623
x3006c0s1b0n0: [2024-03-28 13:39:57,303] [INFO] [stage3.py:2277:step] End to end step took 7.101778030395508
x3006c0s1b0n0: [2024-03-28 13:39:57,303] [INFO] [stage3.py:2277:step] End to end step took 7.10181736946106
x3006c0s1b0n0: [2024-03-28 13:39:57,303] [INFO] [stage3.py:2277:step] End to end step took 7.10185694694519
x3006c0s19b1n0: [2024-03-28 13:39:57,304] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[3e-05], mom=[(0.9, 0.95)]
x3006c0s19b1n0: [2024-03-28 13:39:57,304] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=0.9846374308990976, CurrSamplesPerSec=0.9864081213556226, MemAllocated=9.01GB, MaxMemAllocated=10.26GB
x3006c0s19b1n0: [2024-03-28 13:39:57,304] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 6628.90 | bwd_microstep: 18334.24 | bwd_inner_microstep: 18251.62 | bwd_allreduce_microstep: 82.54 | step_microstep: 7125.81
x3006c0s19b1n0: [2024-03-28 13:39:57,304] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 6628.89 | bwd: 18334.23 | bwd_inner: 18251.62 | bwd_allreduce: 82.56 | step: 7125.82
x3006c0s19b1n0: [2024-03-28 13:39:57,418] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3006c0s19b1n0: [2024-03-28 13:39:57,418] [INFO] [utils.py:801:see_memory_usage] MA 9.01 GB         Max_MA 10.26 GB         CA 9.08 GB         Max_CA 10 GB 
x3006c0s19b1n0: [2024-03-28 13:39:57,419] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 335.11 GB, percent = 66.6%
x3006c0s19b1n0: <TIMER:interval-time,32.6739342212677><TIMER:interval-time,32.67396259307861><TIMER:interval-time,32.67396068572998>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s1b0n0: <TIMER:interval-time,32.67399883270264><TIMER:interval-time,32.67400002479553>
x3006c0s1b0n0: 
x3006c0s1b0n0: <TIMER:interval-time,32.67400336265564><TIMER:interval-time,32.67400336265564>
x3006c0s1b0n0: 
x3006c0s19b1n0: <TIMER:interval-time,32.674081802368164>
x3006c0s1b0n0:  elapsed_time 32.674003 | consumed samples:          320 | consumed tokens:       655360 | elapsed time per iteration (ms): 32674.0 | learning rate: 3.000E-05 | global batch size:    32 | lm loss: 1.122073E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.979 | TFLOPs: 67.76 |
x3006c0s19b1n0: <<<only_train:338.81493496894836>>>
x3006c0s19b1n0: <<<only_train:338.8149540424347>>>
x3006c0s1b0n0: <<<only_train:338.8149392604828>>>
x3006c0s19b1n0: <<<only_train:338.8146162033081>>><<<only_train:338.8150010108948>>>
x3006c0s1b0n0: <<<only_train:338.81514263153076>>>
x3006c0s1b0n0: <<<only_train:338.8144648075104>>>
x3006c0s1b0n0: <<<only_train:338.81496477127075>>>
x3006c0s19b1n0: 
x3006c0s19b1n0: [after training ends] datetime: 2024-03-28 13:39:57 
x3006c0s19b1n0: <<<full_time:338.8151652812958>>><<<full_time:338.8151578903198>>><<<full_time:338.81519770622253>>>
x3006c0s19b1n0: 
x3006c0s19b1n0: 
x3006c0s19b1n0: <<<full_time:338.81483221054077>>>
x3006c0s1b0n0: <<<full_time:338.81521224975586>>><<<full_time:338.81523036956787>>><<<full_time:338.81471991539>>><<<full_time:338.8154242038727>>>
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s1b0n0: 
x3006c0s19b1n0: [2024-03-28 13:40:01,559] [INFO] [launch.py:348:main] Process 16035 exits successfully.
x3006c0s1b0n0: [2024-03-28 13:40:02,004] [INFO] [launch.py:348:main] Process 8738 exits successfully.
x3006c0s1b0n0: [2024-03-28 13:40:04,006] [INFO] [launch.py:348:main] Process 8736 exits successfully.
x3006c0s1b0n0: [2024-03-28 13:40:04,007] [INFO] [launch.py:348:main] Process 8739 exits successfully.
x3006c0s1b0n0: [2024-03-28 13:40:04,007] [INFO] [launch.py:348:main] Process 8737 exits successfully.
x3006c0s19b1n0: [2024-03-28 13:40:04,562] [INFO] [launch.py:348:main] Process 16037 exits successfully.
x3006c0s19b1n0: [2024-03-28 13:40:04,563] [INFO] [launch.py:348:main] Process 16038 exits successfully.
x3006c0s19b1n0: [2024-03-28 13:40:04,563] [INFO] [launch.py:348:main] Process 16036 exits successfully.

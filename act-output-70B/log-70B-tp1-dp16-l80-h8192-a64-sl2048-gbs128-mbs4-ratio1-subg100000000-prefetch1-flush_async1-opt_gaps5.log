[2024-03-28 13:22:25,235] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-28 13:22:28,331] [INFO] [runner.py:463:main] Using IP address of 10.140.57.42 for node x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-28 13:22:28,333] [INFO] [runner.py:557:main] deepspeed_env file = ./.deepspeed_env
[2024-03-28 13:22:28,334] [INFO] [multinode_runner.py:80:get_cmd] Running on the following workers: x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov,x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov,x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov,x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-28 13:22:28,334] [INFO] [runner.py:568:main] cmd = pdsh -S -f 1024 -w x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov,x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov,x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov,x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov export PYTHONUSERBASE=/home/am6429/.local/polaris/conda/2023-10-04; export PYTHONPATH=/home/am6429/dl-io/Megatron-DeepSpeed; export PATH=/home/am6429/.conda/envs/dspeed_env/bin:/soft/datascience/conda/2023-10-04/mconda3/condabin:/soft/compilers/cudatoolkit/cuda-11.8.0/bin:/soft/buildtools/cmake/cmake-3.23.2/cmake-3.23.2-linux-x86_64/bin:/opt/cray/pe/gcc/11.2.0/bin:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/include:/opt/cray/pe/pals/1.2.11/bin:/opt/cray/pe/craype/2.7.20/bin:/opt/cray/libfabric/1.15.2.0/bin:/home/am6429/.conda/envs/dspeed_env/bin:/opt/cray/pe/perftools/23.03.0/bin:/opt/cray/pe/papi/7.0.0.1/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/am6429/.local/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/am6429/veloc-build/include:/home/am6429/veloc-build/bin:/soft/datascience/conda/2023-01-10/mconda3/include:/opt/cray/pe/bin:/soft/datascience/conda/2023-01-10/mconda3/include; export LD_LIBRARY_PATH=/usr/lib64/:/soft/compilers/cudatoolkit/cuda-11.8.0/lib64:/opt/cray/pe/gcc/11.2.0/snos/lib64:/soft/compilers/cudatoolkit/cuda-11.8.0/extras/CUPTI/lib64:/soft/libraries/trt/TensorRT-8.5.3.1.Linux.x86_64-gnu.cuda-11.8.cudnn8.6/lib:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/lib:/soft/libraries/cudnn/cudnn-11-linux-x64-v8.6.0.163/lib:/opt/cray/libfabric/1.15.2.0/lib64:/usr/lib64:/opt/cray/pe/papi/7.0.0.1/lib64:/home/am6429/veloc-build/lib:/home/am6429/veloc-build/lib64:/home/am6429/nvcomp/lib:/soft/datascience/conda/2023-01-10/mconda3/lib:/soft/datascience/conda/2023-01-10/mconda3/lib/; export http_proxy=http://proxy.alcf.anl.gov:3128; export https_proxy=http://proxy.alcf.anl.gov:3128; export CC=gcc; export CXX=g++; export IBV_FORK_SAFE=1; export CFLAGS=-I/soft/datascience/conda/2023-01-10/mconda3/include/; export LDFLAGS=-L/soft/datascience/conda/2023-01-10/mconda3/lib/; export CUDA_DEVICE_MAX_CONNECTIONS=1; export TORCHSNAPSHOT_PER_RANK_MEMORY_BUDGET_BYTES=34359738368; export _DEFAULT_MAX_PER_RANK_IO_CONCURRENCY=1; export _MAX_PER_RANK_IO_CONCURRENCY=1; export NSYS_REPORT_DIR=/home/am6429/dl-io/dl-io-outputs/act-output-70B//rep-70B-tp1-dp16-l80-h8192-a64-sl2048-gbs128-mbs4-ratio1-subg100000000-prefetch1-flush_async1-opt_gaps5-%n;  cd /home/am6429/dl-io/Megatron-DeepSpeed; /home/am6429/.conda/envs/dspeed_env/bin/python -u -m deepspeed.launcher.launch --world_info=eyJ4MzAwM2MwczM3YjBuMC5oc24uY20ucG9sYXJpcy5hbGNmLmFubC5nb3YiOiBbMCwgMSwgMiwgM10sICJ4MzAwM2MwczM3YjFuMC5oc24uY20ucG9sYXJpcy5hbGNmLmFubC5nb3YiOiBbMCwgMSwgMiwgM10sICJ4MzAwM2MwczdiMG4wLmhzbi5jbS5wb2xhcmlzLmFsY2YuYW5sLmdvdiI6IFswLCAxLCAyLCAzXSwgIngzMDAzYzBzN2IxbjAuaHNuLmNtLnBvbGFyaXMuYWxjZi5hbmwuZ292IjogWzAsIDEsIDIsIDNdfQ== --node_rank=%n --master_addr=10.140.57.42 --master_port=29700 /home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py --tensor-model-parallel-size 1 --num-layers 80 --hidden-size 8192 --num-attention-heads 64 --micro-batch-size 4 --global-batch-size 128 --seq-length 2048 --max-position-embeddings 2048 --train-iters 10 --save /local/scratch/llama2/zero3-tp1}_dp16 --data-path /home/am6429/dl-io/datasets/meg-gpt2_text_document --vocab-file /home/am6429/dl-io/datasets/gpt2-vocab.json --merge-file /home/am6429/dl-io/datasets/gpt2-merges.txt --data-impl mmap --tokenizer-type GPTSentencePieceTokenizer --tokenizer-model /home/am6429/dl-io/datasets/tokenizer.model --split 949,50,1 --distributed-backend nccl --lr 3e-4 --lr-decay-style cosine --min-lr 3e-5 --weight-decay 0.1 --clip-grad 1 --lr-warmup-iters 1 --optimizer adam --adam-beta1 0.9 --adam-beta2 0.95 --log-interval 1 --save-interval 1000 --eval-interval 1000 --eval-iters 0 --bf16 --no-query-key-layer-scaling --attention-dropout 0 --hidden-dropout 0 --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization rmsnorm --disable-bias-linear --num-key-value-heads 4 --deepspeed --exit-interval 20 --deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json --zero-stage=3 --no-pipeline-parallel --cpu-optimizer --checkpoint-activations --deepspeed-activation-checkpointing
x3003c0s37b1n0: [2024-03-28 13:22:30,064] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b1n0: [2024-03-28 13:22:30,346] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b0n0: [2024-03-28 13:22:30,355] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b0n0: [2024-03-28 13:22:30,368] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b1n0: [2024-03-28 13:22:31,871] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3003c0s37b1n0: [2024-03-28 13:22:31,871] [INFO] [launch.py:151:main] nnodes=4, num_local_procs=4, node_rank=1
x3003c0s37b1n0: [2024-03-28 13:22:31,871] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [8, 9, 10, 11], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [12, 13, 14, 15]})
x3003c0s37b1n0: [2024-03-28 13:22:31,871] [INFO] [launch.py:163:main] dist_world_size=16
x3003c0s37b1n0: [2024-03-28 13:22:31,871] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3003c0s37b1n0: [2024-03-28 13:22:31,871] [INFO] [launch.py:253:main] process 41952 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '128', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b1n0: [2024-03-28 13:22:31,872] [INFO] [launch.py:253:main] process 41953 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '128', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b1n0: [2024-03-28 13:22:31,872] [INFO] [launch.py:253:main] process 41954 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '128', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b1n0: [2024-03-28 13:22:31,873] [INFO] [launch.py:253:main] process 41955 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '128', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b0n0: [2024-03-28 13:22:32,187] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3003c0s37b0n0: [2024-03-28 13:22:32,187] [INFO] [launch.py:151:main] nnodes=4, num_local_procs=4, node_rank=0
x3003c0s37b0n0: [2024-03-28 13:22:32,187] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [8, 9, 10, 11], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [12, 13, 14, 15]})
x3003c0s37b0n0: [2024-03-28 13:22:32,187] [INFO] [launch.py:163:main] dist_world_size=16
x3003c0s37b0n0: [2024-03-28 13:22:32,187] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3003c0s37b0n0: [2024-03-28 13:22:32,188] [INFO] [launch.py:253:main] process 49916 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '128', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b0n0: [2024-03-28 13:22:32,189] [INFO] [launch.py:253:main] process 49917 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '128', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b0n0: [2024-03-28 13:22:32,189] [INFO] [launch.py:253:main] process 49918 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '128', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b0n0: [2024-03-28 13:22:32,190] [INFO] [launch.py:253:main] process 49919 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '128', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b1n0: [2024-03-28 13:22:32,700] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3003c0s7b1n0: [2024-03-28 13:22:32,700] [INFO] [launch.py:151:main] nnodes=4, num_local_procs=4, node_rank=3
x3003c0s7b1n0: [2024-03-28 13:22:32,700] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [8, 9, 10, 11], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [12, 13, 14, 15]})
x3003c0s7b1n0: [2024-03-28 13:22:32,700] [INFO] [launch.py:163:main] dist_world_size=16
x3003c0s7b1n0: [2024-03-28 13:22:32,700] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3003c0s7b1n0: [2024-03-28 13:22:32,701] [INFO] [launch.py:253:main] process 61380 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '128', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b1n0: [2024-03-28 13:22:32,702] [INFO] [launch.py:253:main] process 61381 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '128', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b1n0: [2024-03-28 13:22:32,702] [INFO] [launch.py:253:main] process 61382 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '128', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b1n0: [2024-03-28 13:22:32,703] [INFO] [launch.py:253:main] process 61383 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '128', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b0n0: [2024-03-28 13:22:32,705] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3003c0s7b0n0: [2024-03-28 13:22:32,705] [INFO] [launch.py:151:main] nnodes=4, num_local_procs=4, node_rank=2
x3003c0s7b0n0: [2024-03-28 13:22:32,705] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [8, 9, 10, 11], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [12, 13, 14, 15]})
x3003c0s7b0n0: [2024-03-28 13:22:32,705] [INFO] [launch.py:163:main] dist_world_size=16
x3003c0s7b0n0: [2024-03-28 13:22:32,705] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3003c0s7b0n0: [2024-03-28 13:22:32,706] [INFO] [launch.py:253:main] process 53598 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '128', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b0n0: [2024-03-28 13:22:32,706] [INFO] [launch.py:253:main] process 53599 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '128', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b0n0: [2024-03-28 13:22:32,707] [INFO] [launch.py:253:main] process 53600 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '128', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b0n0: [2024-03-28 13:22:32,707] [INFO] [launch.py:253:main] process 53601 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '128', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b1n0: [2024-03-28 13:22:33,365] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b1n0: [2024-03-28 13:22:33,368] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b1n0: [2024-03-28 13:22:33,380] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b1n0: [2024-03-28 13:22:33,398] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b0n0: [2024-03-28 13:22:33,959] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b0n0: [2024-03-28 13:22:33,982] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b0n0: [2024-03-28 13:22:33,998] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b0n0: [2024-03-28 13:22:34,003] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b1n0: [2024-03-28 13:22:34,487] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b0n0: [2024-03-28 13:22:34,505] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b1n0: [2024-03-28 13:22:34,520] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b0n0: [2024-03-28 13:22:34,521] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b1n0: [2024-03-28 13:22:34,523] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b0n0: [2024-03-28 13:22:34,524] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b0n0: [2024-03-28 13:22:34,529] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b1n0: [2024-03-28 13:22:34,529] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b1n0:       meet the required dependencies to JIT install the op.
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: JIT compiled ops requires ninja
x3003c0s37b1n0: ninja .................. [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: op name ................ installed .. compatible
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b1n0:       meet the required dependencies to JIT install the op.
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: JIT compiled ops requires ninja
x3003c0s37b1n0: ninja .................. [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: op name ................ installed .. compatible
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b1n0:       meet the required dependencies to JIT install the op.
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: JIT compiled ops requires ninja
x3003c0s37b1n0: ninja .................. [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: op name ................ installed .. compatible
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: DeepSpeed general environment info:
x3003c0s37b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b1n0: torch version .................... 2.0.1+cu118
x3003c0s37b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b1n0: torch cuda version ............... 11.8
x3003c0s37b1n0: torch hip version ................ None
x3003c0s37b1n0: nvcc version ..................... 11.8
x3003c0s37b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b1n0:       meet the required dependencies to JIT install the op.
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: JIT compiled ops requires ninja
x3003c0s37b1n0: ninja .................. [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: op name ................ installed .. compatible
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: DeepSpeed general environment info:
x3003c0s37b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b1n0: torch version .................... 2.0.1+cu118
x3003c0s37b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b1n0: torch cuda version ............... 11.8
x3003c0s37b1n0: torch hip version ................ None
x3003c0s37b1n0: nvcc version ..................... 11.8
x3003c0s37b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: DeepSpeed general environment info:
x3003c0s37b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b1n0: torch version .................... 2.0.1+cu118
x3003c0s37b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b1n0: torch cuda version ............... 11.8
x3003c0s37b1n0: torch hip version ................ None
x3003c0s37b1n0: nvcc version ..................... 11.8
x3003c0s37b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b1n0: [2024-03-28 13:22:35,555] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b1n0: [2024-03-28 13:22:35,588] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: DeepSpeed general environment info:
x3003c0s37b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b1n0: torch version .................... 2.0.1+cu118
x3003c0s37b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b1n0: torch cuda version ............... 11.8
x3003c0s37b1n0: torch hip version ................ None
x3003c0s37b1n0: nvcc version ..................... 11.8
x3003c0s37b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b1n0: [2024-03-28 13:22:35,606] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b1n0: [2024-03-28 13:22:35,657] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b0n0:       meet the required dependencies to JIT install the op.
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: JIT compiled ops requires ninja
x3003c0s37b0n0: ninja .................. [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: op name ................ installed .. compatible
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b0n0:       meet the required dependencies to JIT install the op.
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: JIT compiled ops requires ninja
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b0n0:       meet the required dependencies to JIT install the op.
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: JIT compiled ops requires ninja
x3003c0s37b0n0: ninja .................. [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: op name ................ installed .. compatible
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b0n0: ninja .................. [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: op name ................ installed .. compatible
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b0n0:       meet the required dependencies to JIT install the op.
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: JIT compiled ops requires ninja
x3003c0s37b0n0: ninja .................. [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: op name ................ installed .. compatible
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: DeepSpeed general environment info:
x3003c0s37b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b0n0: torch version .................... 2.0.1+cu118
x3003c0s37b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b0n0: torch cuda version ............... 11.8
x3003c0s37b0n0: torch hip version ................ None
x3003c0s37b0n0: nvcc version ..................... 11.8
x3003c0s37b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [2024-03-28 13:22:36,875] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: DeepSpeed general environment info:
x3003c0s37b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b0n0: torch version .................... 2.0.1+cu118
x3003c0s37b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b0n0: torch cuda version ............... 11.8
x3003c0s37b0n0: torch hip version ................ None
x3003c0s37b0n0: nvcc version ..................... 11.8
x3003c0s37b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b0n0: DeepSpeed general environment info:
x3003c0s37b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b0n0: torch version .................... 2.0.1+cu118
x3003c0s37b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b0n0: torch cuda version ............... 11.8
x3003c0s37b0n0: torch hip version ................ None
x3003c0s37b0n0: nvcc version ..................... 11.8
x3003c0s37b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b0n0: DeepSpeed general environment info:
x3003c0s37b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b0n0: torch version .................... 2.0.1+cu118
x3003c0s37b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b0n0: torch cuda version ............... 11.8
x3003c0s37b0n0: torch hip version ................ None
x3003c0s37b0n0: nvcc version ..................... 11.8
x3003c0s37b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b0n0: INFO: overriding default arguments for tokenizer_type:GPTSentencePieceTokenizer                    with tokenizer_type:GPT2BPETokenizer
x3003c0s37b0n0: using world size: 16, data-parallel-size: 16, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
x3003c0s37b0n0: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
x3003c0s37b0n0: using torch.bfloat16 for parameters ...
x3003c0s37b0n0: ------------------------ arguments ------------------------
x3003c0s37b0n0:   accumulate_allreduce_grads_in_fp32 .............. True
x3003c0s37b0n0:   adam_beta1 ...................................... 0.9
x3003c0s37b0n0:   adam_beta2 ...................................... 0.95
x3003c0s37b0n0:   adam_eps ........................................ 1e-08
x3003c0s37b0n0:   add_bias_linear ................................. False
x3003c0s37b0n0:   add_position_embedding .......................... False
x3003c0s37b0n0:   adlr_autoresume ................................. False
x3003c0s37b0n0:   adlr_autoresume_interval ........................ 1000
x3003c0s37b0n0:   aml_data_download_path .......................... None
x3003c0s37b0n0:   apply_layernorm_1p .............................. False
x3003c0s37b0n0:   apply_query_key_layer_scaling ................... False
x3003c0s37b0n0:   apply_residual_connection_post_layernorm ........ False
x3003c0s37b0n0:   async_tensor_model_parallel_allreduce ........... False
x3003c0s37b0n0:   attention_dropout ............................... 0.0
x3003c0s37b0n0:   attention_softmax_in_fp32 ....................... False
x3003c0s37b0n0:   barrier_with_L1_time ............................ True
x3003c0s37b0n0:   bert_binary_head ................................ True
x3003c0s37b0n0:   bert_embedder_type .............................. megatron
x3003c0s37b0n0:   bert_load ....................................... None
x3003c0s37b0n0:   bf16 ............................................ True
x3003c0s37b0n0:   bias_dropout_fusion ............................. True
x3003c0s37b0n0:   bias_gelu_fusion ................................ False
x3003c0s37b0n0:   biencoder_projection_dim ........................ 0
x3003c0s37b0n0:   biencoder_shared_query_context_model ............ False
x3003c0s37b0n0:   block_data_path ................................. None
x3003c0s37b0n0:   checkpoint_activations .......................... True
x3003c0s37b0n0:   checkpoint_in_cpu ............................... False
x3003c0s37b0n0:   checkpoint_num_layers ........................... 1
x3003c0s37b0n0:   classes_fraction ................................ 1.0
x3003c0s37b0n0:   clip_grad ....................................... 1.0
x3003c0s37b0n0:   compression_training ............................ False
x3003c0s37b0n0:   consumed_train_samples .......................... 0
x3003c0s37b0n0:   consumed_train_tokens ........................... 0
x3003c0s37b0n0:   consumed_valid_samples .......................... 0
x3003c0s37b0n0:   contigious_checkpointing ........................ False
x3003c0s37b0n0:   cpu_optimizer ................................... True
x3003c0s37b0n0:   cpu_torch_adam .................................. False
x3003c0s37b0n0:   create_moe_param_group .......................... False
x3003c0s37b0n0:   curriculum_learning_legacy ...................... False
x3003c0s37b0n0:   data_cache_path ................................. None
x3003c0s37b0n0:   data_efficiency_curriculum_learning ............. False
x3003c0s37b0n0:   data_impl ....................................... mmap
x3003c0s37b0n0:   data_parallel_random_init ....................... False
x3003c0s37b0n0:   data_parallel_size .............................. 16
x3003c0s37b0n0:   data_path ....................................... ['/home/am6429/dl-io/datasets/meg-gpt2_text_document']
x3003c0s37b0n0:   data_per_class_fraction ......................... 1.0
x3003c0s37b0n0:   data_sharding ................................... True
x3003c0s37b0n0:   dataloader_type ................................. single
x3003c0s37b0n0:   DDP_impl ........................................ local
x3003c0s37b0n0:   decoder_num_layers .............................. None
x3003c0s37b0n0:   decoder_seq_length .............................. None
x3003c0s37b0n0:   deepscale ....................................... False
x3003c0s37b0n0:   deepscale_config ................................ None
x3003c0s37b0n0:   deepspeed ....................................... True
x3003c0s37b0n0:   deepspeed_activation_checkpointing .............. True
x3003c0s37b0n0:   deepspeed_config ................................ /home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json
x3003c0s37b0n0:   dino_bottleneck_size ............................ 256
x3003c0s37b0n0:   dino_freeze_last_layer .......................... 1
x3003c0s37b0n0:   dino_head_hidden_size ........................... 2048
x3003c0s37b0n0:   dino_local_crops_number ......................... 10
x3003c0s37b0n0:   dino_local_img_size ............................. 96
x3003c0s37b0n0:   dino_norm_last_layer ............................ False
x3003c0s37b0n0:   dino_teacher_temp ............................... 0.07
x3003c0s37b0n0:   dino_warmup_teacher_temp ........................ 0.04
x3003c0s37b0n0:   dino_warmup_teacher_temp_epochs ................. 30
x3003c0s37b0n0:   distribute_checkpointed_activations ............. False
x3003c0s37b0n0:   distribute_saved_activations .................... False
x3003c0s37b0n0:   distributed_backend ............................. nccl
x3003c0s37b0n0:   distributed_timeout_minutes ..................... 10
x3003c0s37b0n0:   ds_inference .................................... False
x3003c0s37b0n0:   ds_pipeline_enabled ............................. False
x3003c0s37b0n0:   ds_sequence_parallel_size ....................... 1
x3003c0s37b0n0:   embedding_path .................................. None
x3003c0s37b0n0:   embedding_weights_in_fp32 ....................... False
x3003c0s37b0n0:   empty_unused_memory_level ....................... 0
x3003c0s37b0n0:   enable_expert_tensor_parallelism ................ False
x3003c0s37b0n0:   encoder_num_layers .............................. 80
x3003c0s37b0n0:   encoder_seq_length .............................. 2048
x3003c0s37b0n0:   end_weight_decay ................................ 0.1
x3003c0s37b0n0:   eod_mask_loss ................................... False
x3003c0s37b0n0:   eval_interval ................................... 1000
x3003c0s37b0n0:   eval_iters ...................................... 0
x3003c0s37b0n0:   evidence_data_path .............................. None
x3003c0s37b0n0:   exit_duration_in_mins ........................... None
x3003c0s37b0n0:   exit_interval ................................... 20
x3003c0s37b0n0:   exit_on_missing_checkpoint ...................... False
x3003c0s37b0n0:   exit_signal_handler ............................. False
x3003c0s37b0n0:   expert_interval ................................. 2
x3003c0s37b0n0:   ffn_hidden_size ................................. 21824
x3003c0s37b0n0:   finetune ........................................ False
x3003c0s37b0n0:   force_ds_sequence_parallel ...................... False
x3003c0s37b0n0:   fp16 ............................................ False
x3003c0s37b0n0:   fp16_lm_cross_entropy ........................... False
x3003c0s37b0n0:   fp32_residual_connection ........................ False
x3003c0s37b0n0:   fp8_amax_compute_algo ........................... most_recent
x3003c0s37b0n0:   fp8_amax_history_len ............................ 1
x3003c0s37b0n0:   fp8_e4m3 ........................................ False
x3003c0s37b0n0:   fp8_hybrid ...................................... False
x3003c0s37b0n0:   fp8_interval .................................... 1
x3003c0s37b0n0:   fp8_margin ...................................... 0
x3003c0s37b0n0:   fp8_wgrad ....................................... True
x3003c0s37b0n0:   global_batch_size ............................... 128
x3003c0s37b0n0:   gradient_accumulation_fusion .................... True
x3003c0s37b0n0:   head_lr_mult .................................... 1.0
x3003c0s37b0n0:   hidden_dropout .................................. 0.0
x3003c0s37b0n0:   hidden_size ..................................... 8192
x3003c0s37b0n0:   hidden_size_teacher ............................. None
x3003c0s37b0n0:   hysteresis ...................................... 2
x3003c0s37b0n0:   ict_head_size ................................... None
x3003c0s37b0n0:   ict_load ........................................ None
x3003c0s37b0n0:   img_h ........................................... 224
x3003c0s37b0n0:   img_w ........................................... 224
x3003c0s37b0n0:   indexer_batch_size .............................. 128
x3003c0s37b0n0:   indexer_log_interval ............................ 1000
x3003c0s37b0n0:   inference ....................................... False
x3003c0s37b0n0:   inference_batch_times_seqlen_threshold .......... 512
x3003c0s37b0n0:   init_method_std ................................. 0.02
x3003c0s37b0n0:   init_method_xavier_uniform ...................... False
x3003c0s37b0n0:   initial_loss_scale .............................. 4294967296
x3003c0s37b0n0:   iter_per_epoch .................................. 1250
x3003c0s37b0n0:   kd .............................................. False
x3003c0s37b0n0:   kd_alpha_ce ..................................... 1
x3003c0s37b0n0:   kd_beta_ce ...................................... 1
x3003c0s37b0n0:   kd_temp ......................................... 1.0
x3003c0s37b0n0:   kv_channels ..................................... 128
x3003c0s37b0n0:   layernorm_epsilon ............................... 1e-05
x3003c0s37b0n0:   lazy_mpu_init ................................... None
x3003c0s37b0n0:   load ............................................ None
x3003c0s37b0n0:   load_teacher .................................... None
x3003c0s37b0n0:   local_rank ...................................... 0
x3003c0s37b0n0:   log_batch_size_to_tensorboard ................... False
x3003c0s37b0n0:   log_interval .................................... 1
x3003c0s37b0n0:   log_learning_rate_to_tensorboard ................ True
x3003c0s37b0n0:   log_loss_scale_to_tensorboard ................... True
x3003c0s37b0n0:   log_memory_to_tensorboard ....................... False
x3003c0s37b0n0:   log_num_zeros_in_grad ........................... False
x3003c0s37b0n0:   log_optimizer_states_to_tensorboard ............. False
x3003c0s37b0n0:   log_params_norm ................................. False
x3003c0s37b0n0:   log_timers_to_tensorboard ....................... False
x3003c0s37b0n0:   log_validation_ppl_to_tensorboard ............... False
x3003c0s37b0n0:   log_world_size_to_tensorboard ................... False
x3003c0s37b0n0:   loss_scale ...................................... None
x3003c0s37b0n0:   loss_scale_window ............................... 1000
x3003c0s37b0n0:   lr .............................................. 0.0003
x3003c0s37b0n0:   lr_decay_iters .................................. None
x3003c0s37b0n0:   lr_decay_samples ................................ None
x3003c0s37b0n0:   lr_decay_style .................................. cosine
x3003c0s37b0n0:   lr_decay_tokens ................................. None
x3003c0s37b0n0:   lr_warmup_fraction .............................. None
x3003c0s37b0n0:   lr_warmup_iters ................................. 1
x3003c0s37b0n0:   lr_warmup_samples ............................... 0
x3003c0s37b0n0:   lr_warmup_tokens ................................ None
x3003c0s37b0n0:   make_vocab_size_divisible_by .................... 128
x3003c0s37b0n0:   mask_factor ..................................... 1.0
x3003c0s37b0n0:   mask_prob ....................................... 0.15
x3003c0s37b0n0:   mask_type ....................................... random
x3003c0s37b0n0:   masked_softmax_fusion ........................... True
x3003c0s37b0n0:   max_position_embeddings ......................... 2048
x3003c0s37b0n0:   max_tokens_to_oom ............................... 12000
x3003c0s37b0n0:   memory_centric_tiled_linear ..................... False
x3003c0s37b0n0:   merge_file ...................................... /home/am6429/dl-io/datasets/gpt2-merges.txt
x3003c0s37b0n0:   micro_batch_size ................................ 4
x3003c0s37b0n0:   min_loss_scale .................................. 1.0
x3003c0s37b0n0:   min_lr .......................................... 3e-05
x3003c0s37b0n0:   mlp_type ........................................ standard
x3003c0s37b0n0:   mmap_warmup ..................................... False
x3003c0s37b0n0:   moe_eval_capacity_factor ........................ 1.0
x3003c0s37b0n0:   moe_expert_parallel_size ........................ 1
x3003c0s37b0n0:   moe_loss_coeff .................................. 0.1
x3003c0s37b0n0:   moe_min_capacity ................................ 4
x3003c0s37b0n0:   moe_token_dropping .............................. True
x3003c0s37b0n0:   moe_train_capacity_factor ....................... 1.0
x3003c0s37b0n0:   mos ............................................. False
x3003c0s37b0n0:   no_load_lr_state ................................ False
x3003c0s37b0n0:   no_load_optim ................................... None
x3003c0s37b0n0:   no_load_rng ..................................... None
x3003c0s37b0n0:   no_persist_layer_norm ........................... False
x3003c0s37b0n0:   no_pipeline_parallel ............................ True
x3003c0s37b0n0:   no_save_optim ................................... None
x3003c0s37b0n0:   no_save_rng ..................................... None
x3003c0s37b0n0:   normalization ................................... rmsnorm
x3003c0s37b0n0:   num_attention_heads ............................. 64
x3003c0s37b0n0:   num_attention_heads_teacher ..................... None
x3003c0s37b0n0:   num_channels .................................... 3
x3003c0s37b0n0:   num_classes ..................................... 1000
x3003c0s37b0n0:   num_experts ..................................... [1]
x3003c0s37b0n0:   num_experts_switch .............................. None
x3003c0s37b0n0:   num_experts_teacher ............................. [1]
x3003c0s37b0n0:   num_key_value_heads ............................. 4
x3003c0s37b0n0:   num_layers ...................................... 80
x3003c0s37b0n0:   num_layers_per_virtual_pipeline_stage ........... None
x3003c0s37b0n0:   num_layers_teacher .............................. None
x3003c0s37b0n0:   num_workers ..................................... 2
x3003c0s37b0n0:   onnx_safe ....................................... None
x3003c0s37b0n0:   openai_gelu ..................................... False
x3003c0s37b0n0:   optimizer ....................................... adam
x3003c0s37b0n0:   output_bert_embeddings .......................... False
x3003c0s37b0n0:   overlap_p2p_comm ................................ False
x3003c0s37b0n0:   override_opt_param_scheduler .................... False
x3003c0s37b0n0:   params_dtype .................................... torch.bfloat16
x3003c0s37b0n0:   partition_activations ........................... False
x3003c0s37b0n0:   patch_dim ....................................... 16
x3003c0s37b0n0:   perform_initialization .......................... True
x3003c0s37b0n0:   pipeline_model_parallel_size .................... 1
x3003c0s37b0n0:   pipeline_model_parallel_split_rank .............. None
x3003c0s37b0n0:   profile_backward ................................ False
x3003c0s37b0n0:   query_in_block_prob ............................. 0.1
x3003c0s37b0n0:   rampup_batch_size ............................... None
x3003c0s37b0n0:   random_ltd ...................................... False
x3003c0s37b0n0:   rank ............................................ 0
x3003c0s37b0n0:   recompute_granularity ........................... None
x3003c0s37b0n0:   recompute_method ................................ None
x3003c0s37b0n0:   recompute_num_layers ............................ 1
x3003c0s37b0n0:   remote_device ................................... none
x3003c0s37b0n0:   reset_attention_mask ............................ False
x3003c0s37b0n0:   reset_iteration ................................. False
x3003c0s37b0n0:   reset_position_ids .............................. False
x3003c0s37b0n0:   retriever_report_topk_accuracies ................ []
x3003c0s37b0n0:   retriever_score_scaling ......................... False
x3003c0s37b0n0:   retriever_seq_length ............................ 256
x3003c0s37b0n0:   retro_add_retriever ............................. False
x3003c0s37b0n0:   retro_cyclic_train_iters ........................ None
x3003c0s37b0n0:   retro_encoder_attention_dropout ................. 0.1
x3003c0s37b0n0:   retro_encoder_hidden_dropout .................... 0.1
x3003c0s37b0n0:   retro_encoder_layers ............................ 2
x3003c0s37b0n0:   retro_num_neighbors ............................. 2
x3003c0s37b0n0:   retro_num_retrieved_chunks ...................... 2
x3003c0s37b0n0:   retro_return_doc_ids ............................ False
x3003c0s37b0n0:   retro_workdir ................................... None
x3003c0s37b0n0:   return_data_index ............................... False
x3003c0s37b0n0:   rotary_percent .................................. 1.0
x3003c0s37b0n0:   sample_rate ..................................... 1.0
x3003c0s37b0n0:   save ............................................ /local/scratch/llama2/zero3-tp1}_dp16
x3003c0s37b0n0:   save_interval ................................... 1000
x3003c0s37b0n0:   scatter_gather_tensors_in_pipeline .............. True
x3003c0s37b0n0:   scattered_embeddings ............................ False
x3003c0s37b0n0:   seed ............................................ 1234
x3003c0s37b0n0:   seq_length ...................................... 2048
x3003c0s37b0n0:   sequence_parallel ............................... False
x3003c0s37b0n0:   sgd_momentum .................................... 0.9
x3003c0s37b0n0:   short_seq_prob .................................. 0.1
x3003c0s37b0n0:   skip_train ...................................... False
x3003c0s37b0n0:   split ........................................... 949,50,1
x3003c0s37b0n0:   split_transformers .............................. False
x3003c0s37b0n0:   squared_relu .................................... False
x3003c0s37b0n0:   standalone_embedding_stage ...................... False
x3003c0s37b0n0:   start_weight_decay .............................. 0.1
x3003c0s37b0n0:   swiglu .......................................... True
x3003c0s37b0n0:   swin_backbone_type .............................. tiny
x3003c0s37b0n0:   synchronize_each_layer .......................... False
x3003c0s37b0n0:   tensor_model_parallel_size ...................... 1
x3003c0s37b0n0:   tensorboard_dir ................................. None
x3003c0s37b0n0:   tensorboard_log_interval ........................ 1
x3003c0s37b0n0:   tensorboard_queue_size .......................... 1000
x3003c0s37b0n0:   test_data_path .................................. None
x3003c0s37b0n0:   tile_factor ..................................... 1
x3003c0s37b0n0:   timing_log_level ................................ 0
x3003c0s37b0n0:   timing_log_option ............................... minmax
x3003c0s37b0n0:   titles_data_path ................................ None
x3003c0s37b0n0:   tokenizer_model ................................. /home/am6429/dl-io/datasets/tokenizer.model
x3003c0s37b0n0:   tokenizer_type .................................. GPT2BPETokenizer
x3003c0s37b0n0:   topk ............................................ 1
x3003c0s37b0n0:   train_data_exact_num_epochs ..................... None
x3003c0s37b0n0:   train_data_path ................................. None
x3003c0s37b0n0:   train_desc_path ................................. None
x3003c0s37b0n0:   train_doc_idx_path .............................. None
x3003c0s37b0n0:   train_idx_path .................................. None
x3003c0s37b0n0:   train_iters ..................................... 10
x3003c0s37b0n0:   train_sample_idx_path ........................... None
x3003c0s37b0n0:   train_samples ................................... None
x3003c0s37b0n0:   train_shuffle_idx_path .......................... None
x3003c0s37b0n0:   train_tokens .................................... None
x3003c0s37b0n0:   transformer_impl ................................ local
x3003c0s37b0n0:   transformer_pipeline_model_parallel_size ........ 1
x3003c0s37b0n0:   untie_embeddings_and_output_weights ............. True
x3003c0s37b0n0:   use_checkpoint_args ............................. False
x3003c0s37b0n0:   use_checkpoint_opt_param_scheduler .............. False
x3003c0s37b0n0:   use_contiguous_buffers_in_local_ddp ............. True
x3003c0s37b0n0:   use_cpu_initialization .......................... None
x3003c0s37b0n0:   use_dataset_only ................................ False
x3003c0s37b0n0:   use_distributed_optimizer ....................... False
x3003c0s37b0n0:   use_flash_attn .................................. False
x3003c0s37b0n0:   use_flash_attn_triton ........................... False
x3003c0s37b0n0:   use_flash_attn_v1 ............................... False
x3003c0s37b0n0:   use_flash_attn_v2 ............................... False
x3003c0s37b0n0:   use_one_sent_docs ............................... False
x3003c0s37b0n0:   use_pin_memory .................................. False
x3003c0s37b0n0:   use_ring_exchange_p2p ........................... False
x3003c0s37b0n0:   use_rotary_position_embeddings .................. True
x3003c0s37b0n0:   use_tutel ....................................... False
x3003c0s37b0n0:   valid_data_path ................................. None
x3003c0s37b0n0:   variable_seq_lengths ............................ False
x3003c0s37b0n0:   virtual_pipeline_model_parallel_size ............ None
x3003c0s37b0n0:   vision_backbone_type ............................ vit
x3003c0s37b0n0:   vision_pretraining .............................. False
x3003c0s37b0n0:   vision_pretraining_type ......................... classify
x3003c0s37b0n0:   vocab_extra_ids ................................. 0
x3003c0s37b0n0:   vocab_file ...................................... /home/am6429/dl-io/datasets/gpt2-vocab.json
x3003c0s37b0n0:   vocab_size ...................................... None
x3003c0s37b0n0:   weight_decay .................................... 0.1
x3003c0s37b0n0:   weight_decay_incr_style ......................... constant
x3003c0s37b0n0:   world_size ...................................... 16
x3003c0s37b0n0:   zero_allgather_bucket_size ...................... 0.0
x3003c0s37b0n0:   zero_contigious_gradients ....................... False
x3003c0s37b0n0:   zero_reduce_bucket_size ......................... 0.0
x3003c0s37b0n0:   zero_reduce_scatter ............................. False
x3003c0s37b0n0:   zero_stage ...................................... 3
x3003c0s37b0n0: -------------------- end of arguments ---------------------
x3003c0s37b0n0: setting number of micro-batches to constant 2
x3003c0s37b0n0: > building GPT2BPETokenizer tokenizer ...
x3003c0s37b0n0: [2024-03-28 13:22:36,980] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b0n0: [2024-03-28 13:22:36,980] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b0n0:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
x3003c0s37b0n0: > initializing torch distributed ...
x3003c0s37b0n0: [2024-03-28 13:22:36,982] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b0n0: [2024-03-28 13:22:36,982] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b1n0:       meet the required dependencies to JIT install the op.
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: JIT compiled ops requires ninja
x3003c0s7b1n0: ninja .................. [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: op name ................ installed .. compatible
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b0n0:       meet the required dependencies to JIT install the op.
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: JIT compiled ops requires ninja
x3003c0s7b0n0: ninja .................. [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: op name ................ installed .. compatible
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b0n0:       meet the required dependencies to JIT install the op.
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: JIT compiled ops requires ninja
x3003c0s7b0n0: ninja .................. [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: op name ................ installed .. compatible
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b1n0:       meet the required dependencies to JIT install the op.
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: JIT compiled ops requires ninja
x3003c0s7b1n0: ninja .................. [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: op name ................ installed .. compatible
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b1n0:       meet the required dependencies to JIT install the op.
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: JIT compiled ops requires ninja
x3003c0s7b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: ninja .................. [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: op name ................ installed .. compatible
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b0n0:       meet the required dependencies to JIT install the op.
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: JIT compiled ops requires ninja
x3003c0s7b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: ninja .................. [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: op name ................ installed .. compatible
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b0n0:       meet the required dependencies to JIT install the op.
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: JIT compiled ops requires ninja
x3003c0s7b0n0: ninja .................. [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: op name ................ installed .. compatible
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b1n0: DeepSpeed general environment info:
x3003c0s7b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b1n0: torch version .................... 2.0.1+cu118
x3003c0s7b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b1n0: torch cuda version ............... 11.8
x3003c0s7b1n0: torch hip version ................ None
x3003c0s7b1n0: nvcc version ..................... 11.8
x3003c0s7b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b1n0:       meet the required dependencies to JIT install the op.
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: JIT compiled ops requires ninja
x3003c0s7b1n0: ninja .................. [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: op name ................ installed .. compatible
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [2024-03-28 13:22:37,392] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b1n0: DeepSpeed general environment info:
x3003c0s7b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b1n0: torch version .................... 2.0.1+cu118
x3003c0s7b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b1n0: torch cuda version ............... 11.8
x3003c0s7b1n0: torch hip version ................ None
x3003c0s7b1n0: nvcc version ..................... 11.8
x3003c0s7b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: DeepSpeed general environment info:
x3003c0s7b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b0n0: torch version .................... 2.0.1+cu118
x3003c0s7b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b0n0: torch cuda version ............... 11.8
x3003c0s7b0n0: torch hip version ................ None
x3003c0s7b0n0: nvcc version ..................... 11.8
x3003c0s7b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: DeepSpeed general environment info:
x3003c0s7b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b0n0: torch version .................... 2.0.1+cu118
x3003c0s7b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b0n0: torch cuda version ............... 11.8
x3003c0s7b0n0: torch hip version ................ None
x3003c0s7b0n0: nvcc version ..................... 11.8
x3003c0s7b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: DeepSpeed general environment info:
x3003c0s7b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b1n0: torch version .................... 2.0.1+cu118
x3003c0s7b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b1n0: torch cuda version ............... 11.8
x3003c0s7b1n0: torch hip version ................ None
x3003c0s7b1n0: nvcc version ..................... 11.8
x3003c0s7b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b0n0: DeepSpeed general environment info:
x3003c0s7b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b0n0: torch version .................... 2.0.1+cu118
x3003c0s7b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b0n0: torch cuda version ............... 11.8
x3003c0s7b0n0: torch hip version ................ None
x3003c0s7b0n0: nvcc version ..................... 11.8
x3003c0s7b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b1n0: DeepSpeed general environment info:
x3003c0s7b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b1n0: torch version .................... 2.0.1+cu118
x3003c0s7b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b1n0: torch cuda version ............... 11.8
x3003c0s7b1n0: torch hip version ................ None
x3003c0s7b1n0: nvcc version ..................... 11.8
x3003c0s7b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b0n0: DeepSpeed general environment info:
x3003c0s7b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b0n0: torch version .................... 2.0.1+cu118
x3003c0s7b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b0n0: torch cuda version ............... 11.8
x3003c0s7b0n0: torch hip version ................ None
x3003c0s7b0n0: nvcc version ..................... 11.8
x3003c0s7b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b1n0: [2024-03-28 13:22:37,499] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b0n0: [2024-03-28 13:22:37,503] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b0n0: [2024-03-28 13:22:37,508] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b1n0: [2024-03-28 13:22:37,521] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b0n0: [2024-03-28 13:22:37,534] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b1n0: [2024-03-28 13:22:37,540] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b0n0: [2024-03-28 13:22:37,541] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b0n0: > initialized tensor model parallel with size 1
x3003c0s37b0n0: > initialized pipeline model parallel with size 1
x3003c0s37b0n0: > setting random seeds to 1234 ...
x3003c0s37b0n0: [2024-03-28 13:22:38,939] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
x3003c0s37b0n0: > compiling dataset index builder ...
x3003c0s37b0n0: make: Entering directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3003c0s37b0n0: make: Nothing to be done for 'default'.
x3003c0s37b0n0: make: Leaving directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3003c0s37b0n0: >>> done with dataset index builder. Compilation time: 0.085 seconds
x3003c0s37b0n0: > compiling and loading fused kernels ...
x3003c0s37b0n0: ninja: no work to do.
x3003c0s37b0n0: ninja: no work to do.
x3003c0s37b0n0: ninja: no work to do.
x3003c0s37b0n0: >>> done with compiling and loading fused kernels. Compilation time: 4.102 seconds
x3003c0s37b1n0: <<<<<<<<<<< 4
x3003c0s37b1n0: <<<<<<<<<<< 7
x3003c0s37b1n0: <<<<<<<<<<< 5
x3003c0s37b1n0: <<<<<<<<<<< 6
x3003c0s7b0n0: <<<<<<<<<<< 8
x3003c0s7b1n0: <<<<<<<<<<< 14
x3003c0s37b0n0: <<<<<<<<<<< 2
x3003c0s7b1n0: <<<<<<<<<<< 13
x3003c0s7b0n0: <<<<<<<<<<< 11
x3003c0s7b0n0: <<<<<<<<<<< 10
x3003c0s7b1n0: <<<<<<<<<<< 15
x3003c0s7b0n0: <<<<<<<<<<< 9
x3003c0s7b1n0: <<<<<<<<<<< 12
x3003c0s37b0n0: <<<<<<<<<<< 1
x3003c0s37b0n0: <<<<<<<<<<< 3
x3003c0s37b0n0: initialize_megatron took 6.995548963546753
x3003c0s37b0n0: <<<<<<<<<<< 0
x3003c0s37b0n0: time to initialize megatron (seconds): 8.610
x3003c0s37b0n0: [after megatron is initialized] datetime: 2024-03-28 13:22:43 
x3003c0s37b0n0: get_accelerator and all_reduce  took 0.0008246898651123047
x3003c0s37b0n0: building GPT model ...
x3003c0s37b0n0: [2024-03-28 13:22:43,999] [INFO] [utils.py:800:see_memory_usage] Before Building Model
x3003c0s37b0n0: [2024-03-28 13:22:44,000] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 5.68 GB         CA 0.0 GB         Max_CA 6 GB 
x3003c0s37b0n0: [2024-03-28 13:22:44,000] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 19.85 GB, percent = 3.9%
x3003c0s37b0n0: [2024-03-28 13:22:57,133] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 563, num_elems = 55.14B
x3003c0s37b0n0: [2024-03-28 13:22:57,214] [INFO] [utils.py:800:see_memory_usage] After Building Model
x3003c0s37b0n0: [2024-03-28 13:22:57,215] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 7.22 GB         CA 32.64 GB         Max_CA 38 GB 
x3003c0s37b0n0: [2024-03-28 13:22:57,215] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 20.31 GB, percent = 4.0%
x3003c0s37b0n0:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 55141736448
x3003c0s37b1n0: ninja: no work to do.
x3003c0s37b1n0: Time to load cpu_adam op: 2.3422396183013916 seconds
x3003c0s37b1n0: Time to load cpu_adam op: 2.338087797164917 seconds
x3003c0s37b1n0: Time to load cpu_adam op: 2.348485231399536 seconds
x3003c0s37b1n0: Time to load cpu_adam op: 2.395462989807129 seconds
x3003c0s7b0n0: ninja: no work to do.
x3003c0s7b0n0: Time to load cpu_adam op: 2.4852893352508545 seconds
x3003c0s37b0n0: Time to load cpu_adam op: 2.4684481620788574 seconds
x3003c0s7b1n0: Time to load cpu_adam op: 2.4667716026306152 seconds
x3003c0s37b0n0: Time to load cpu_adam op: 2.469054698944092 seconds
x3003c0s7b1n0: Time to load cpu_adam op: 2.449556827545166 seconds
x3003c0s7b0n0: Time to load cpu_adam op: 2.4951589107513428 seconds
x3003c0s7b1n0: Time to load cpu_adam op: 2.497924327850342 seconds
x3003c0s37b0n0: ninja: no work to do.
x3003c0s37b0n0: Time to load cpu_adam op: 2.5261991024017334 seconds
x3003c0s7b0n0: Time to load cpu_adam op: 2.6646177768707275 seconds
x3003c0s7b0n0: Time to load cpu_adam op: 2.655855417251587 seconds
x3003c0s7b1n0: Time to load cpu_adam op: 2.6650278568267822 seconds
x3003c0s37b0n0: Time to load cpu_adam op: 2.667881488800049 seconds
x3003c0s37b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b0n0: > learning rate decay style: cosine
x3003c0s37b0n0: DeepSpeed is enabled.
x3003c0s37b0n0: [2024-03-28 13:23:01,794] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.3+8074cd62, git-hash=8074cd62, git-branch=hybrid_opt_offload
x3003c0s37b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b0n0: [2024-03-28 13:23:01,868] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After args sanity test
x3003c0s37b0n0: [2024-03-28 13:23:01,869] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 32.64 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 13:23:01,869] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.7 GB, percent = 5.5%
x3003c0s37b0n0: [2024-03-28 13:23:01,928] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure distributed model
x3003c0s37b0n0: [2024-03-28 13:23:01,929] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 32.64 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 13:23:01,929] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.71 GB, percent = 5.5%
x3003c0s37b0n0: [2024-03-28 13:23:01,996] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After configure distributed model
x3003c0s37b0n0: [2024-03-28 13:23:01,997] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 32.64 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 13:23:01,997] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.71 GB, percent = 5.5%
x3003c0s37b0n0: [2024-03-28 13:23:01,997] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
x3003c0s37b0n0: [2024-03-28 13:23:02,053] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After setting model parameters
x3003c0s37b0n0: [2024-03-28 13:23:02,054] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 32.64 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 13:23:02,054] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.71 GB, percent = 5.5%
x3003c0s37b0n0: [2024-03-28 13:23:02,111] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure optimizer
x3003c0s37b0n0: [2024-03-28 13:23:02,112] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 32.64 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 13:23:02,112] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.71 GB, percent = 5.5%
x3003c0s37b0n0: [2024-03-28 13:23:02,112] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
x3003c0s37b0n0: [2024-03-28 13:23:02,112] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
x3003c0s37b0n0: [2024-03-28 13:23:02,144] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
x3003c0s37b0n0: [2024-03-28 13:23:02,144] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
x3003c0s37b0n0: [2024-03-28 13:23:02,144] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
x3003c0s37b0n0: [2024-03-28 13:23:02,144] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
x3003c0s37b0n0: [2024-03-28 13:23:02,200] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
x3003c0s37b0n0: [2024-03-28 13:23:02,201] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 32.64 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 13:23:02,201] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.71 GB, percent = 5.5%
x3003c0s37b0n0: [2024-03-28 13:23:02,203] [INFO] [stage3.py:137:__init__] Reduce bucket size 500,000,000
x3003c0s37b0n0: [2024-03-28 13:23:02,203] [INFO] [stage3.py:138:__init__] Prefetch bucket size 50,000,000
x3003c0s37b0n0: [2024-03-28 13:23:02,260] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
x3003c0s37b0n0: [2024-03-28 13:23:02,261] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 32.64 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 13:23:02,261] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.71 GB, percent = 5.5%
x3003c0s37b0n0: Parameter Offload: Total persistent parameters: 1318912 in 161 params
x3003c0s37b0n0: [2024-03-28 13:23:02,352] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
x3003c0s37b0n0: [2024-03-28 13:23:02,352] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 32.64 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 13:23:02,352] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.71 GB, percent = 5.5%
x3003c0s37b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b0n0: [2024-03-28 13:23:02,413] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
x3003c0s37b0n0: [2024-03-28 13:23:02,414] [INFO] [utils.py:801:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 32.64 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 13:23:02,414] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.71 GB, percent = 5.5%
x3003c0s7b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s7b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s7b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s7b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s7b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s7b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s7b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s7b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b1n0: [2024-03-28 13:23:02,588] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b0n0: [2024-03-28 13:23:02,588] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b1n0: [2024-03-28 13:23:02,588] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b1n0: [2024-03-28 13:23:02,588] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b0n0: [2024-03-28 13:23:02,588] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b1n0: [2024-03-28 13:23:02,588] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b1n0: [2024-03-28 13:23:02,588] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b1n0: [2024-03-28 13:23:02,588] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b1n0: [2024-03-28 13:23:02,588] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b1n0: [2024-03-28 13:23:02,588] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b0n0: [2024-03-28 13:23:02,588] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b0n0: [2024-03-28 13:23:02,588] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b0n0: [2024-03-28 13:23:02,588] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b0n0: [2024-03-28 13:23:02,588] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b0n0: [2024-03-28 13:23:02,588] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b0n0: [2024-03-28 13:23:02,588] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b0n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b1n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b0n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b1n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b0n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b0n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b1n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b0n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b1n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b0n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b0n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b0n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b1n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b1n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b1n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b1n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b0n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,589] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,590] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,591] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,592] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,593] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,594] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,595] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b1n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b1n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b0n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b0n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b1n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b0n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b0n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b1n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b1n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b0n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b0n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b0n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b0n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b1n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b1n0: [2024-03-28 13:23:02,596] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b0n0: [2024-03-28 13:23:04,895] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 33
x3003c0s37b0n0: [2024-03-28 13:23:04,895] [INFO] [utils.py:801:see_memory_usage] MA 6.42 GB         Max_MA 6.45 GB         CA 6.42 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 13:23:04,895] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 46.99 GB, percent = 9.3%
x3003c0s37b0n0: [2024-03-28 13:23:04,958] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
x3003c0s37b0n0: [2024-03-28 13:23:04,959] [INFO] [utils.py:801:see_memory_usage] MA 6.42 GB         Max_MA 6.42 GB         CA 6.42 GB         Max_CA 6 GB 
x3003c0s37b0n0: [2024-03-28 13:23:04,959] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 47.0 GB, percent = 9.3%
x3003c0s37b0n0: [2024-03-28 13:23:24,543] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
x3003c0s37b0n0: [2024-03-28 13:23:24,544] [INFO] [utils.py:801:see_memory_usage] MA 6.42 GB         Max_MA 6.42 GB         CA 6.42 GB         Max_CA 6 GB 
x3003c0s37b0n0: [2024-03-28 13:23:24,544] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 215.39 GB, percent = 42.8%
x3003c0s37b0n0: [2024-03-28 13:23:26,380] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
x3003c0s37b0n0: [2024-03-28 13:23:26,381] [INFO] [utils.py:801:see_memory_usage] MA 6.42 GB         Max_MA 6.42 GB         CA 6.42 GB         Max_CA 6 GB 
x3003c0s37b0n0: [2024-03-28 13:23:26,381] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 230.76 GB, percent = 45.9%
x3003c0s37b0n0: [2024-03-28 13:23:32,960] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | init_optimizer_state: 6563.37
x3003c0s37b0n0: [2024-03-28 13:23:33,067] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
x3003c0s37b0n0: [2024-03-28 13:23:33,068] [INFO] [utils.py:801:see_memory_usage] MA 6.42 GB         Max_MA 6.42 GB         CA 6.42 GB         Max_CA 6 GB 
x3003c0s37b0n0: [2024-03-28 13:23:33,068] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 293.51 GB, percent = 58.3%
x3003c0s37b0n0: [2024-03-28 13:23:33,331] [INFO] [stage3.py:520:_setup_for_real_optimizer] optimizer state initialized
x3003c0s37b0n0: [2024-03-28 13:23:50,011] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
x3003c0s37b0n0: [2024-03-28 13:23:50,011] [INFO] [utils.py:801:see_memory_usage] MA 7.35 GB         Max_MA 8.89 GB         CA 20.98 GB         Max_CA 21 GB 
x3003c0s37b0n0: [2024-03-28 13:23:50,012] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 326.43 GB, percent = 64.9%
x3003c0s37b0n0: [2024-03-28 13:23:50,012] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
x3003c0s37b0n0: [2024-03-28 13:23:50,079] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure LR scheduler
x3003c0s37b0n0: [2024-03-28 13:23:50,080] [INFO] [utils.py:801:see_memory_usage] MA 7.35 GB         Max_MA 7.35 GB         CA 20.98 GB         Max_CA 21 GB 
x3003c0s37b0n0: [2024-03-28 13:23:50,080] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 326.43 GB, percent = 64.9%
x3003c0s37b0n0: [2024-03-28 13:23:50,080] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
x3003c0s37b0n0: [2024-03-28 13:23:50,080] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7f0206b502b0>
x3003c0s37b0n0: [2024-03-28 13:23:50,080] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 13:23:50,145] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before rewriting optimizer step
x3003c0s37b0n0: [2024-03-28 13:23:50,146] [INFO] [utils.py:801:see_memory_usage] MA 7.35 GB         Max_MA 7.35 GB         CA 20.98 GB         Max_CA 21 GB 
x3003c0s37b0n0: [2024-03-28 13:23:50,146] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 326.44 GB, percent = 64.9%
x3003c0s37b0n0: [2024-03-28 13:23:50,214] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure checkpointing
x3003c0s37b0n0: [2024-03-28 13:23:50,214] [INFO] [utils.py:801:see_memory_usage] MA 7.35 GB         Max_MA 7.35 GB         CA 20.98 GB         Max_CA 21 GB 
x3003c0s37b0n0: [2024-03-28 13:23:50,214] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 326.42 GB, percent = 64.9%
x3003c0s37b0n0: [2024-03-28 13:23:50,214] [INFO] [config.py:998:print] DeepSpeedEngine configuration:
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   activation_checkpointing_config  {
x3003c0s37b0n0:     "partition_activations": false, 
x3003c0s37b0n0:     "contiguous_memory_optimization": false, 
x3003c0s37b0n0:     "cpu_checkpointing": false, 
x3003c0s37b0n0:     "number_checkpoints": null, 
x3003c0s37b0n0:     "synchronize_checkpoint_boundary": false, 
x3003c0s37b0n0:     "profile": false
x3003c0s37b0n0: }
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   amp_enabled .................. False
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   amp_params ................... False
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   autotuning_config ............ {
x3003c0s37b0n0:     "enabled": false, 
x3003c0s37b0n0:     "start_step": null, 
x3003c0s37b0n0:     "end_step": null, 
x3003c0s37b0n0:     "metric_path": null, 
x3003c0s37b0n0:     "arg_mappings": null, 
x3003c0s37b0n0:     "metric": "throughput", 
x3003c0s37b0n0:     "model_info": null, 
x3003c0s37b0n0:     "results_dir": "autotuning_results", 
x3003c0s37b0n0:     "exps_dir": "autotuning_exps", 
x3003c0s37b0n0:     "overwrite": true, 
x3003c0s37b0n0:     "fast": true, 
x3003c0s37b0n0:     "start_profile_step": 3, 
x3003c0s37b0n0:     "end_profile_step": 5, 
x3003c0s37b0n0:     "tuner_type": "gridsearch", 
x3003c0s37b0n0:     "tuner_early_stopping": 5, 
x3003c0s37b0n0:     "tuner_num_trials": 50, 
x3003c0s37b0n0:     "model_info_path": null, 
x3003c0s37b0n0:     "mp_size": 1, 
x3003c0s37b0n0:     "max_train_batch_size": null, 
x3003c0s37b0n0:     "min_train_batch_size": 1, 
x3003c0s37b0n0:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
x3003c0s37b0n0:     "min_train_micro_batch_size_per_gpu": 1, 
x3003c0s37b0n0:     "num_tuning_micro_batch_sizes": 3
x3003c0s37b0n0: }
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   bfloat16_enabled ............. True
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   bfloat16_immediate_grad_update  False
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   checkpoint_parallel_write_pipeline  False
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   checkpoint_tag_validation_enabled  True
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   checkpoint_tag_validation_fail  False
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f0206b50c10>
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   communication_data_type ...... None
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   curriculum_enabled_legacy .... False
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   curriculum_params_legacy ..... False
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   data_efficiency_enabled ...... False
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   dataloader_drop_last ......... False
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   datastates_config ............ {
x3003c0s37b0n0:     "enabled": null, 
x3003c0s37b0n0:     "config": {
x3003c0s37b0n0:     }
x3003c0s37b0n0: }
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   disable_allgather ............ False
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   dump_state ................... False
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   dynamic_loss_scale_args ...... None
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   eigenvalue_enabled ........... False
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   eigenvalue_gas_boundary_resolution  1
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   eigenvalue_layer_name ........ bert.encoder.layer
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   eigenvalue_layer_num ......... 0
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   eigenvalue_max_iter .......... 100
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   eigenvalue_stability ......... 1e-06
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   eigenvalue_tol ............... 0.01
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   eigenvalue_verbose ........... False
x3003c0s37b0n0: [2024-03-28 13:23:50,215] [INFO] [config.py:1002:print]   elasticity_enabled ........... False
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   flops_profiler_config ........ {
x3003c0s37b0n0:     "enabled": false, 
x3003c0s37b0n0:     "recompute_fwd_factor": 0.0, 
x3003c0s37b0n0:     "profile_step": 1, 
x3003c0s37b0n0:     "module_depth": -1, 
x3003c0s37b0n0:     "top_modules": 1, 
x3003c0s37b0n0:     "detailed": true, 
x3003c0s37b0n0:     "output_file": null
x3003c0s37b0n0: }
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   fp16_auto_cast ............... None
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   fp16_enabled ................. False
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   fp16_master_weights_and_gradients  False
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   global_rank .................. 0
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   grad_accum_dtype ............. bf16
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   gradient_accumulation_steps .. 2
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   gradient_clipping ............ 0.0
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   gradient_predivide_factor .... 1.0
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   graph_harvesting ............. False
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   initial_dynamic_scale ........ 1
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   load_universal_checkpoint .... False
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   loss_scale ................... 1.0
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   memory_breakdown ............. True
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   mics_hierarchial_params_gather  False
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   mics_shard_size .............. -1
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   nebula_config ................ {
x3003c0s37b0n0:     "enabled": false, 
x3003c0s37b0n0:     "persistent_storage_path": null, 
x3003c0s37b0n0:     "persistent_time_interval": 100, 
x3003c0s37b0n0:     "num_of_version_in_retention": 2, 
x3003c0s37b0n0:     "enable_nebula_load": true, 
x3003c0s37b0n0:     "load_path": null
x3003c0s37b0n0: }
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   optimizer_legacy_fusion ...... False
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   optimizer_name ............... None
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   optimizer_params ............. None
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   pld_enabled .................. False
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   pld_params ................... False
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   prescale_gradients ........... False
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   scheduler_name ............... None
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   scheduler_params ............. None
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   seq_parallel_communication_data_type  torch.float32
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   sparse_attention ............. None
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   sparse_gradients_enabled ..... False
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   steps_per_print .............. 1
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   train_batch_size ............. 128
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   train_micro_batch_size_per_gpu  4
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   use_data_before_expert_parallel_  False
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   use_node_local_storage ....... False
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   wall_clock_breakdown ......... True
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   weight_quantization_config ... None
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   world_size ................... 16
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   zero_allow_untested_optimizer  False
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0, prefetch_optimizer=True, part_grads_async=True, prefetch_optimizer_gap=5) sub_group_size=100000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   zero_enabled ................. True
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   zero_force_ds_cpu_optimizer .. True
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:1002:print]   zero_optimization_stage ...... 3
x3003c0s37b0n0: [2024-03-28 13:23:50,216] [INFO] [config.py:988:print_user_config]   json = {
x3003c0s37b0n0:     "train_batch_size": 128, 
x3003c0s37b0n0:     "train_micro_batch_size_per_gpu": 4, 
x3003c0s37b0n0:     "steps_per_print": 1, 
x3003c0s37b0n0:     "zero_optimization": {
x3003c0s37b0n0:         "stage": 3, 
x3003c0s37b0n0:         "offload_optimizer": {
x3003c0s37b0n0:             "device": "cpu", 
x3003c0s37b0n0:             "ratio": 1, 
x3003c0s37b0n0:             "pin_memory": true, 
x3003c0s37b0n0:             "prefetch_optimizer": 1, 
x3003c0s37b0n0:             "part_grads_async": 1, 
x3003c0s37b0n0:             "prefetch_optimizer_gap": 5
x3003c0s37b0n0:         }, 
x3003c0s37b0n0:         "sub_group_size": 1.000000e+08
x3003c0s37b0n0:     }, 
x3003c0s37b0n0:     "bf16": {
x3003c0s37b0n0:         "enabled": true
x3003c0s37b0n0:     }, 
x3003c0s37b0n0:     "data_types": {
x3003c0s37b0n0:         "grad_accum_dtype": "bf16"
x3003c0s37b0n0:     }, 
x3003c0s37b0n0:     "wall_clock_breakdown": true, 
x3003c0s37b0n0:     "memory_breakdown": true, 
x3003c0s37b0n0:     "flops_profiler": {
x3003c0s37b0n0:         "enabled": false
x3003c0s37b0n0:     }
x3003c0s37b0n0: }
x3003c0s7b0n0: <TIMER:model-and-optimizer-setup,67.47916293144226>
x3003c0s7b0n0: <TIMER:model-and-optimizer-setup,67.47928619384766>
x3003c0s7b0n0: <TIMER:model-and-optimizer-setup,67.47956037521362>
x3003c0s7b0n0: <TIMER:model-and-optimizer-setup,67.47956395149231>
x3003c0s37b1n0: <TIMER:model-and-optimizer-setup,67.47952389717102>
x3003c0s37b1n0: <TIMER:model-and-optimizer-setup,67.47953271865845>
x3003c0s37b1n0: <TIMER:model-and-optimizer-setup,67.47960591316223>
x3003c0s37b1n0: <TIMER:model-and-optimizer-setup,67.47987294197083>
x3003c0s37b0n0: <TIMER:model-and-optimizer-setup,67.48022413253784>
x3003c0s37b0n0: <TIMER:model-and-optimizer-setup,67.48023891448975>
x3003c0s37b0n0: <TIMER:model-and-optimizer-setup,67.48029518127441>
x3003c0s37b0n0: <TIMER:model-and-optimizer-setup,67.48036646842957>
x3003c0s7b1n0: <TIMER:model-and-optimizer-setup,67.48032140731812>
x3003c0s7b1n0: <TIMER:model-and-optimizer-setup,67.48041486740112>
x3003c0s7b1n0: <TIMER:model-and-optimizer-setup,67.48049068450928>
x3003c0s7b1n0: <TIMER:model-and-optimizer-setup,67.48056769371033>
x3003c0s37b0n0: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-28 13:23:51 
x3003c0s37b0n0: > building train, validation, and test datasets ...
x3003c0s37b0n0:  > datasets target sizes (minimum size):
x3003c0s37b0n0:     train:      1280
x3003c0s37b0n0:     validation: 0
x3003c0s37b0n0:     test:       0
x3003c0s37b0n0: > building train, validation, and test datasets for GPT ...
x3003c0s37b0n0: Single data path provided for train, valid & test
x3003c0s37b0n0:  > building dataset index ...
x3003c0s37b0n0:     reading sizes...
x3003c0s37b0n0:     reading pointers...
x3003c0s37b0n0:     reading document index...
x3003c0s37b0n0:     creating numpy buffer of mmap...
x3003c0s37b0n0:     creating memory view of numpy buffer...
x3003c0s37b0n0:  > finished creating indexed dataset in 0.002813 seconds
x3003c0s37b0n0:     number of documents: 79000
x3003c0s37b0n0:  > dataset split:
x3003c0s37b0n0:     train:
x3003c0s37b0n0:      document indices in [0, 74971) total of 74971 documents
x3003c0s37b0n0:     validation:
x3003c0s37b0n0:      document indices in [74971, 78921) total of 3950 documents
x3003c0s37b0n0:     test:
x3003c0s37b0n0:      document indices in [78921, 79000) total of 79 documents
x3003c0s37b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/fb5d930c458e43cb8d38d58b121f5a8e_doc_idx.npy
x3003c0s37b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/fb5d930c458e43cb8d38d58b121f5a8e_sample_idx.npy
x3003c0s37b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/fb5d930c458e43cb8d38d58b121f5a8e_shuffle_idx.npy
x3003c0s37b0n0:     loaded indexed file in 0.007 seconds
x3003c0s37b0n0:     total number of samples: 108448
x3003c0s37b0n0:     total number of epochs: 1
x3003c0s37b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_doc_idx.npy
x3003c0s37b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_sample_idx.npy
x3003c0s37b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_shuffle_idx.npy
x3003c0s37b0n0:     loaded indexed file in 0.003 seconds
x3003c0s37b0n0:     total number of samples: 5792
x3003c0s37b0n0:     total number of epochs: 1
x3003c0s37b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_doc_idx.npy
x3003c0s37b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_sample_idx.npy
x3003c0s37b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_shuffle_idx.npy
x3003c0s37b0n0:     loaded indexed file in 0.033 seconds
x3003c0s37b0n0:     total number of samples: 185
x3003c0s37b0n0:     total number of epochs: 1
x3003c0s37b0n0: > finished creating GPT datasets ...
x3003c0s7b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5143156051635742>
x3003c0s37b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5147359371185303>
x3003c0s7b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5164213180541992>
x3003c0s37b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5229923725128174>
x3003c0s37b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5263876914978027>
x3003c0s37b1n0: <TIMER:train/valid/test-data-iterators-setup,0.535017728805542>
x3003c0s37b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5420894622802734>
x3003c0s7b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5426008701324463>
x3003c0s37b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5444512367248535>
x3003c0s37b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5487687587738037>
x3003c0s7b0n0: <TIMER:train/valid/test-data-iterators-setup,0.555464506149292>
x3003c0s7b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5620107650756836>
x3003c0s37b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5623147487640381>
x3003c0s7b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5701918601989746>
x3003c0s7b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5761258602142334>
x3003c0s7b1n0: <TIMER:train/valid/test-data-iterators-setup,0.6564991474151611>
x3003c0s37b0n0: [after dataloaders are built] datetime: 2024-03-28 13:23:52 
x3003c0s37b0n0: done with setup ...
x3003c0s37b0n0: training ...
x3003c0s7b1n0: (min, max) time across ranks (ms):
x3003c0s7b1n0:     model-and-optimizer-setup ......................: (67479.16, 67480.57)
x3003c0s7b1n0:     train/valid/test-data-iterators-setup ..........: (514.32, 656.50)
x3003c0s37b0n0: [before training begins] datetime: 2024-03-28 13:23:52 
x3003c0s37b0n0: [before the start of training step] datetime: 2024-03-28 13:23:52 
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 13:23:52,299] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 13:23:52,300] [INFO] [utils.py:801:see_memory_usage] MA 7.36 GB         Max_MA 7.36 GB         CA 8.02 GB         Max_CA 21 GB 
x3003c0s37b0n0: [2024-03-28 13:23:52,300] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 328.61 GB, percent = 65.3%
x3003c0s37b0n0: [2024-03-28 13:23:52,421] [INFO] [checkpointing.py:539:forward] Activation Checkpointing Information
x3003c0s37b0n0: [2024-03-28 13:23:52,421] [INFO] [checkpointing.py:540:forward] ----Partition Activations False, CPU CHECKPOINTING False
x3003c0s37b0n0: [2024-03-28 13:23:52,421] [INFO] [checkpointing.py:541:forward] ----contiguous Memory Checkpointing False with 80 total layers
x3003c0s37b0n0: [2024-03-28 13:23:52,421] [INFO] [checkpointing.py:543:forward] ----Synchronization False
x3003c0s37b0n0: [2024-03-28 13:23:52,421] [INFO] [checkpointing.py:544:forward] ----Profiling time in checkpointing False
x3003c0s37b0n0: [2024-03-28 13:24:10,503] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 13:24:10,504] [INFO] [utils.py:801:see_memory_usage] MA 21.15 GB         Max_MA 24.22 GB         CA 28.51 GB         Max_CA 29 GB 
x3003c0s37b0n0: [2024-03-28 13:24:10,504] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 328.88 GB, percent = 65.4%
x3003c0s37b0n0: [2024-03-28 13:24:10,807] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 13:24:10,808] [INFO] [utils.py:801:see_memory_usage] MA 21.15 GB         Max_MA 21.15 GB         CA 21.43 GB         Max_CA 29 GB 
x3003c0s37b0n0: [2024-03-28 13:24:10,808] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 328.9 GB, percent = 65.4%
x3003c0s37b0n0: [2024-03-28 13:25:02,227] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 13:25:02,227] [INFO] [utils.py:801:see_memory_usage] MA 10.9 GB         Max_MA 26.14 GB         CA 11.55 GB         Max_CA 35 GB 
x3003c0s37b0n0: [2024-03-28 13:25:02,227] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 328.98 GB, percent = 65.4%
x3003c0s37b0n0: [2024-03-28 13:25:02,317] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 13:25:02,318] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 10.91 GB         CA 11.55 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:25:02,318] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 329.0 GB, percent = 65.4%
x3003c0s37b0n0: [2024-03-28 13:25:15,452] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 13:25:15,452] [INFO] [utils.py:801:see_memory_usage] MA 24.71 GB         Max_MA 27.7 GB         CA 31.42 GB         Max_CA 31 GB 
x3003c0s37b0n0: [2024-03-28 13:25:15,453] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 328.98 GB, percent = 65.4%
x3003c0s37b0n0: [2024-03-28 13:25:15,540] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 13:25:15,541] [INFO] [utils.py:801:see_memory_usage] MA 24.71 GB         Max_MA 24.71 GB         CA 25.13 GB         Max_CA 31 GB 
x3003c0s37b0n0: [2024-03-28 13:25:15,541] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 328.98 GB, percent = 65.4%
x3003c0s37b0n0: [2024-03-28 13:25:49,359] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 13:25:49,359] [INFO] [utils.py:801:see_memory_usage] MA 12.09 GB         Max_MA 28.79 GB         CA 13.54 GB         Max_CA 37 GB 
x3003c0s37b0n0: [2024-03-28 13:25:49,359] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 329.01 GB, percent = 65.4%
x3003c0s37b0n0: [2024-03-28 13:25:49,437] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 13:25:49,438] [INFO] [utils.py:801:see_memory_usage] MA 12.09 GB         Max_MA 12.09 GB         CA 13.54 GB         Max_CA 14 GB 
x3003c0s37b0n0: [2024-03-28 13:25:49,438] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 329.12 GB, percent = 65.4%
x3003c0s37b0n0: [2024-03-28 13:25:56,008] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.479421854019165
x3003c0s37b0n0: [2024-03-28 13:25:56,040] [INFO] [stage3.py:2251:step] Full outer step loop took 6.512143850326538
x3003c0s7b1n0: [2024-03-28 13:25:56,123] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.594387769699097
x3003c0s7b1n0: [2024-03-28 13:25:56,135] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.6061766147613525
x3003c0s7b1n0: [2024-03-28 13:25:56,139] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.610178709030151
x3003c0s7b1n0: [2024-03-28 13:25:56,141] [INFO] [stage3.py:2251:step] Full outer step loop took 6.6123833656311035
x3003c0s7b1n0: [2024-03-28 13:25:56,141] [INFO] [stage3.py:2251:step] Full outer step loop took 6.612759113311768
x3003c0s7b1n0: [2024-03-28 13:25:56,144] [INFO] [stage3.py:2251:step] Full outer step loop took 6.615047454833984
x3003c0s7b1n0: [2024-03-28 13:25:56,154] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.6258063316345215
x3003c0s7b1n0: [2024-03-28 13:25:56,159] [INFO] [stage3.py:2251:step] Full outer step loop took 6.630185842514038
x3003c0s7b0n0: [2024-03-28 13:25:56,175] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.645641565322876
x3003c0s37b0n0: [2024-03-28 13:25:56,388] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.859358310699463
x3003c0s37b0n0: [2024-03-28 13:25:56,398] [INFO] [stage3.py:2251:step] Full outer step loop took 6.869277715682983
x3003c0s7b0n0: [2024-03-28 13:25:56,410] [INFO] [stage3.py:2251:step] Full outer step loop took 6.878198862075806
x3003c0s37b0n0: [2024-03-28 13:25:56,440] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.9118812084198
x3003c0s37b0n0: [2024-03-28 13:25:56,445] [INFO] [stage3.py:2251:step] Full outer step loop took 6.9168701171875
x3003c0s37b0n0: [2024-03-28 13:25:56,458] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.929118394851685
x3003c0s37b0n0: [2024-03-28 13:25:56,462] [INFO] [stage3.py:2251:step] Full outer step loop took 6.934005975723267
x3003c0s7b0n0: [2024-03-28 13:25:56,717] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.1837639808654785
x3003c0s7b0n0: [2024-03-28 13:25:56,754] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.2255027294158936
x3003c0s7b0n0: [2024-03-28 13:25:56,769] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.240320682525635
x3003c0s7b0n0: [2024-03-28 13:25:56,776] [INFO] [stage3.py:2251:step] Full outer step loop took 7.247467041015625
x3003c0s7b0n0: [2024-03-28 13:25:56,776] [INFO] [stage3.py:2251:step] Full outer step loop took 7.247334718704224
x3003c0s7b0n0: [2024-03-28 13:25:56,778] [INFO] [stage3.py:2251:step] Full outer step loop took 7.249316453933716
x3003c0s37b1n0: [2024-03-28 13:25:56,910] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.381173372268677
x3003c0s37b1n0: [2024-03-28 13:25:56,914] [INFO] [stage3.py:2251:step] Full outer step loop took 7.386052370071411
x3003c0s37b1n0: [2024-03-28 13:25:56,918] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.389745235443115
x3003c0s37b1n0: [2024-03-28 13:25:56,923] [INFO] [stage3.py:2251:step] Full outer step loop took 7.394667625427246
x3003c0s37b1n0: [2024-03-28 13:25:57,076] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.548032283782959
x3003c0s37b1n0: [2024-03-28 13:25:57,081] [INFO] [stage3.py:2251:step] Full outer step loop took 7.552904844284058
x3003c0s37b1n0: [2024-03-28 13:25:57,098] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.569437503814697
x3003c0s37b1n0: [2024-03-28 13:25:57,102] [INFO] [stage3.py:2251:step] Full outer step loop took 7.573789358139038
x3003c0s37b1n0: [2024-03-28 13:25:57,126] [INFO] [stage3.py:2277:step] End to end step took 7.598111867904663
x3003c0s37b1n0: [2024-03-28 13:25:57,126] [INFO] [stage3.py:2277:step] End to end step took 7.5980963706970215
x3003c0s7b1n0: [2024-03-28 13:25:57,127] [INFO] [stage3.py:2277:step] End to end step took 7.597970962524414
x3003c0s7b1n0: [2024-03-28 13:25:57,127] [INFO] [stage3.py:2277:step] End to end step took 7.59825587272644
x3003c0s37b0n0: [2024-03-28 13:25:57,127] [INFO] [stage3.py:2277:step] End to end step took 7.598151445388794
x3003c0s37b0n0: [2024-03-28 13:25:57,127] [INFO] [stage3.py:2277:step] End to end step took 7.5983593463897705
x3003c0s37b0n0: [2024-03-28 13:25:57,127] [INFO] [stage3.py:2277:step] End to end step took 7.598649501800537
x3003c0s37b0n0: [2024-03-28 13:25:57,127] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6869.83
x3003c0s37b1n0: [2024-03-28 13:25:57,127] [INFO] [stage3.py:2277:step] End to end step took 7.5987913608551025
x3003c0s37b1n0: [2024-03-28 13:25:57,127] [INFO] [stage3.py:2277:step] End to end step took 7.598759412765503
x3003c0s37b0n0: [2024-03-28 13:25:57,127] [WARNING] [stage3.py:2267:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3003c0s37b0n0: [2024-03-28 13:25:57,127] [INFO] [stage3.py:2277:step] End to end step took 7.599097490310669
x3003c0s7b1n0: [2024-03-28 13:25:57,127] [INFO] [stage3.py:2277:step] End to end step took 7.598527193069458
x3003c0s7b1n0: [2024-03-28 13:25:57,127] [INFO] [stage3.py:2277:step] End to end step took 7.5987818241119385
x3003c0s37b0n0: [2024-03-28 13:25:57,128] [INFO] [logging.py:96:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 13:25:57,128] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 31505.11 | bwd_microstep: 84947.80 | bwd_inner_microstep: 84579.72 | bwd_allreduce_microstep: 367.90 | step_microstep: 7689.67
x3003c0s37b0n0: [2024-03-28 13:25:57,128] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 31505.12 | bwd: 84947.79 | bwd_inner: 84579.72 | bwd_allreduce: 367.92 | step: 7689.67
x3003c0s7b0n0: [2024-03-28 13:25:57,133] [INFO] [stage3.py:2277:step] End to end step took 7.604469299316406
x3003c0s7b0n0: [2024-03-28 13:25:57,133] [INFO] [stage3.py:2277:step] End to end step took 7.604595899581909
x3003c0s7b0n0: [2024-03-28 13:25:57,133] [INFO] [stage3.py:2277:step] End to end step took 7.6045753955841064
x3003c0s7b0n0: [2024-03-28 13:25:57,133] [INFO] [stage3.py:2277:step] End to end step took 7.604669570922852
x3003c0s37b0n0: [2024-03-28 13:25:57,244] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 13:25:57,244] [INFO] [utils.py:801:see_memory_usage] MA 10.62 GB         Max_MA 13.28 GB         CA 11.28 GB         Max_CA 18 GB 
x3003c0s37b0n0: [2024-03-28 13:25:57,244] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.05 GB, percent = 82.1%
x3003c0s37b0n0: <TIMER:interval-time,125.15787672996521><TIMER:interval-time,125.15788149833679><TIMER:interval-time,125.15788292884827><TIMER:interval-time,125.1578803062439>
x3003c0s37b0n0: 
x3003c0s37b0n0: 
x3003c0s37b0n0: 
x3003c0s37b1n0: <TIMER:interval-time,125.15794229507446><TIMER:interval-time,125.15795016288757>
x3003c0s37b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,125.15794706344604>
x3003c0s7b0n0: <TIMER:interval-time,125.15787982940674><TIMER:interval-time,125.15785527229309>
x3003c0s7b0n0: 
x3003c0s7b0n0: <TIMER:interval-time,125.1578733921051>
x3003c0s7b0n0: <TIMER:interval-time,125.15791320800781>
x3003c0s7b1n0: <TIMER:interval-time,125.15800547599792><TIMER:interval-time,125.15799736976624><TIMER:interval-time,125.15800619125366>
x3003c0s7b1n0: 
x3003c0s7b1n0: 
x3003c0s7b1n0: <TIMER:interval-time,125.15800499916077>
x3003c0s37b1n0: <TIMER:interval-time,125.15804886817932>
x3003c0s37b0n0: [Rank 0] (after 1 iterations) memory (MB) | allocated: 10878.7685546875 | max allocated: 10878.76953125 | reserved: 11546.0 | max reserved: 11546.0
x3003c0s7b1n0:  elapsed_time 125.158005 | consumed samples:          128 | consumed tokens:       262144 | elapsed time per iteration (ms): 125158.0 | learning rate: 3.000E-04 | global batch size:   128 | lm loss: 1.246674E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.023 | TFLOPs: 70.60 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 13:25:57,416] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 13:25:57,416] [INFO] [utils.py:801:see_memory_usage] MA 10.63 GB         Max_MA 10.63 GB         CA 11.28 GB         Max_CA 11 GB 
x3003c0s37b0n0: [2024-03-28 13:25:57,416] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.09 GB, percent = 82.1%
x3003c0s37b0n0: [2024-03-28 13:26:11,642] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 13:26:11,642] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 28.96 GB         CA 32.87 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 13:26:11,643] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.11 GB, percent = 82.1%
x3003c0s37b0n0: [2024-03-28 13:26:11,726] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 13:26:11,727] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 25.84 GB         CA 26.54 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 13:26:11,727] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.11 GB, percent = 82.1%
x3003c0s37b0n0: [2024-03-28 13:26:45,288] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 13:26:45,289] [INFO] [utils.py:801:see_memory_usage] MA 12.16 GB         Max_MA 29.72 GB         CA 12.81 GB         Max_CA 37 GB 
x3003c0s37b0n0: [2024-03-28 13:26:45,289] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.14 GB, percent = 82.1%
x3003c0s37b0n0: [2024-03-28 13:26:45,384] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 13:26:45,384] [INFO] [utils.py:801:see_memory_usage] MA 12.16 GB         Max_MA 12.16 GB         CA 12.81 GB         Max_CA 13 GB 
x3003c0s37b0n0: [2024-03-28 13:26:45,384] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.15 GB, percent = 82.1%
x3003c0s37b0n0: [2024-03-28 13:26:58,335] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 13:26:58,335] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 28.96 GB         CA 32.87 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 13:26:58,335] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.13 GB, percent = 82.1%
x3003c0s37b0n0: [2024-03-28 13:26:58,426] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 13:26:58,427] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 25.84 GB         CA 26.54 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 13:26:58,427] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.13 GB, percent = 82.1%
x3003c0s37b0n0: [2024-03-28 13:27:33,168] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 13:27:33,169] [INFO] [utils.py:801:see_memory_usage] MA 13.35 GB         Max_MA 29.91 GB         CA 14.79 GB         Max_CA 37 GB 
x3003c0s37b0n0: [2024-03-28 13:27:33,169] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.1 GB, percent = 82.1%
x3003c0s37b0n0: [2024-03-28 13:27:33,248] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 13:27:33,249] [INFO] [utils.py:801:see_memory_usage] MA 13.35 GB         Max_MA 13.35 GB         CA 14.79 GB         Max_CA 15 GB 
x3003c0s37b0n0: [2024-03-28 13:27:33,249] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.1 GB, percent = 82.1%
x3003c0s7b1n0: [2024-03-28 13:27:37,779] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.500434398651123
x3003c0s7b1n0: [2024-03-28 13:27:37,786] [INFO] [stage3.py:2251:step] Full outer step loop took 4.506712436676025
x3003c0s7b1n0: [2024-03-28 13:27:37,844] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.564990997314453
x3003c0s7b1n0: [2024-03-28 13:27:37,849] [INFO] [stage3.py:2251:step] Full outer step loop took 4.570146799087524
x3003c0s7b1n0: [2024-03-28 13:27:37,882] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.602778673171997
x3003c0s7b1n0: [2024-03-28 13:27:37,886] [INFO] [stage3.py:2251:step] Full outer step loop took 4.6076295375823975
x3003c0s37b0n0: [2024-03-28 13:27:37,939] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.660237073898315
x3003c0s37b0n0: [2024-03-28 13:27:37,947] [INFO] [stage3.py:2251:step] Full outer step loop took 4.6677727699279785
x3003c0s7b1n0: [2024-03-28 13:27:37,998] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.71874737739563
x3003c0s7b1n0: [2024-03-28 13:27:38,002] [INFO] [stage3.py:2251:step] Full outer step loop took 4.723597049713135
x3003c0s7b0n0: [2024-03-28 13:27:38,019] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.737458944320679
x3003c0s7b0n0: [2024-03-28 13:27:38,027] [INFO] [stage3.py:2251:step] Full outer step loop took 4.745832681655884
x3003c0s7b0n0: [2024-03-28 13:27:38,040] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.7580726146698
x3003c0s7b0n0: [2024-03-28 13:27:38,045] [INFO] [stage3.py:2251:step] Full outer step loop took 4.763410329818726
x3003c0s37b0n0: [2024-03-28 13:27:38,052] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.773200035095215
x3003c0s7b0n0: [2024-03-28 13:27:38,058] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.776567697525024
x3003c0s37b0n0: [2024-03-28 13:27:38,058] [INFO] [stage3.py:2251:step] Full outer step loop took 4.779397249221802
x3003c0s7b0n0: [2024-03-28 13:27:38,061] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.779937982559204
x3003c0s7b0n0: [2024-03-28 13:27:38,063] [INFO] [stage3.py:2251:step] Full outer step loop took 4.781180143356323
x3003c0s7b0n0: [2024-03-28 13:27:38,066] [INFO] [stage3.py:2251:step] Full outer step loop took 4.784810543060303
x3003c0s37b0n0: [2024-03-28 13:27:38,080] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.801092624664307
x3003c0s37b0n0: [2024-03-28 13:27:38,088] [INFO] [stage3.py:2251:step] Full outer step loop took 4.8090410232543945
x3003c0s37b0n0: [2024-03-28 13:27:38,088] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.809125661849976
x3003c0s37b0n0: [2024-03-28 13:27:38,093] [INFO] [stage3.py:2251:step] Full outer step loop took 4.813992261886597
x3003c0s37b1n0: [2024-03-28 13:27:38,106] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.826782941818237
x3003c0s37b1n0: [2024-03-28 13:27:38,116] [INFO] [stage3.py:2251:step] Full outer step loop took 4.8367884159088135
x3003c0s37b1n0: [2024-03-28 13:27:38,286] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.007189750671387
x3003c0s37b1n0: [2024-03-28 13:27:38,300] [INFO] [stage3.py:2251:step] Full outer step loop took 5.021385192871094
x3003c0s37b1n0: [2024-03-28 13:27:38,328] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.049238443374634
x3003c0s37b1n0: [2024-03-28 13:27:38,329] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.050238847732544
x3003c0s37b1n0: [2024-03-28 13:27:38,333] [INFO] [stage3.py:2251:step] Full outer step loop took 5.054220676422119
x3003c0s37b1n0: [2024-03-28 13:27:38,334] [INFO] [stage3.py:2251:step] Full outer step loop took 5.055101633071899
x3003c0s37b1n0: [2024-03-28 13:27:38,357] [INFO] [stage3.py:2277:step] End to end step took 5.078648567199707
x3003c0s7b0n0: [2024-03-28 13:27:38,357] [INFO] [stage3.py:2277:step] End to end step took 5.076048135757446
x3003c0s7b0n0: [2024-03-28 13:27:38,357] [INFO] [stage3.py:2277:step] End to end step took 5.076061964035034
x3003c0s37b1n0: [2024-03-28 13:27:38,357] [INFO] [stage3.py:2277:step] End to end step took 5.078685760498047
x3003c0s7b0n0: [2024-03-28 13:27:38,358] [INFO] [stage3.py:2277:step] End to end step took 5.076125383377075
x3003c0s7b1n0: [2024-03-28 13:27:38,358] [INFO] [stage3.py:2277:step] End to end step took 5.078749895095825
x3003c0s37b0n0: [2024-03-28 13:27:38,357] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4779.77
x3003c0s37b1n0: [2024-03-28 13:27:38,358] [INFO] [stage3.py:2277:step] End to end step took 5.07884955406189
x3003c0s37b0n0: [2024-03-28 13:27:38,358] [INFO] [stage3.py:2277:step] End to end step took 5.078961372375488
x3003c0s37b0n0: [2024-03-28 13:27:38,358] [WARNING] [stage3.py:2267:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3003c0s37b0n0: [2024-03-28 13:27:38,358] [INFO] [stage3.py:2277:step] End to end step took 5.079149484634399
x3003c0s7b0n0: [2024-03-28 13:27:38,358] [INFO] [stage3.py:2277:step] End to end step took 5.076425075531006
x3003c0s37b1n0: [2024-03-28 13:27:38,358] [INFO] [stage3.py:2277:step] End to end step took 5.079380512237549
x3003c0s37b0n0: [2024-03-28 13:27:38,358] [INFO] [stage3.py:2277:step] End to end step took 5.079482793807983
x3003c0s37b0n0: [2024-03-28 13:27:38,358] [INFO] [logging.py:96:log_dist] [Rank 0] step=2, skipped=0, lr=[0.0002918585038060976], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 13:27:38,358] [INFO] [stage3.py:2277:step] End to end step took 5.079561233520508
x3003c0s7b1n0: [2024-03-28 13:27:38,358] [INFO] [stage3.py:2277:step] End to end step took 5.079197645187378
x3003c0s7b1n0: [2024-03-28 13:27:38,358] [INFO] [stage3.py:2277:step] End to end step took 5.079294681549072
x3003c0s7b1n0: [2024-03-28 13:27:38,358] [INFO] [stage3.py:2277:step] End to end step took 5.079347372055054
x3003c0s37b0n0: [2024-03-28 13:27:38,359] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 27116.87 | bwd_microstep: 68036.47 | bwd_inner_microstep: 67644.64 | bwd_allreduce_microstep: 391.69 | step_microstep: 5109.30
x3003c0s37b0n0: [2024-03-28 13:27:38,359] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 27116.86 | bwd: 68036.46 | bwd_inner: 67644.62 | bwd_allreduce: 391.72 | step: 5109.30
x3003c0s37b0n0: [2024-03-28 13:27:38,471] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 13:27:38,472] [INFO] [utils.py:801:see_memory_usage] MA 10.62 GB         Max_MA 13.58 GB         CA 11.28 GB         Max_CA 18 GB 
x3003c0s37b0n0: [2024-03-28 13:27:38,472] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.09 GB, percent = 82.1%
x3003c0s37b0n0: <TIMER:interval-time,101.22697687149048><TIMER:interval-time,101.22697830200195><TIMER:interval-time,101.226979970932>
x3003c0s37b0n0: 
x3003c0s37b0n0: <TIMER:interval-time,101.22697854042053>
x3003c0s37b0n0: 
x3003c0s37b1n0: <TIMER:interval-time,101.22700953483582>
x3003c0s37b1n0: <TIMER:interval-time,101.22702598571777><TIMER:interval-time,101.22702836990356>
x3003c0s37b1n0: 
x3003c0s7b1n0: <TIMER:interval-time,101.2270028591156><TIMER:interval-time,101.22700095176697><TIMER:interval-time,101.22700071334839>
x3003c0s7b1n0: 
x3003c0s7b1n0: 
x3003c0s7b1n0: <TIMER:interval-time,101.22701096534729>
x3003c0s37b1n0: <TIMER:interval-time,101.22711515426636>
x3003c0s7b0n0: <TIMER:interval-time,101.22701573371887><TIMER:interval-time,101.22701835632324><TIMER:interval-time,101.22700142860413>
x3003c0s7b0n0: 
x3003c0s7b0n0: <TIMER:interval-time,101.22702240943909>
x3003c0s7b0n0: 
x3003c0s7b1n0:  elapsed_time 101.227001 | consumed samples:          256 | consumed tokens:       524288 | elapsed time per iteration (ms): 101227.0 | learning rate: 2.919E-04 | global batch size:   128 | lm loss: 1.082585E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.264 | TFLOPs: 87.29 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 13:27:38,617] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 13:27:38,618] [INFO] [utils.py:801:see_memory_usage] MA 10.63 GB         Max_MA 10.63 GB         CA 11.28 GB         Max_CA 11 GB 
x3003c0s37b0n0: [2024-03-28 13:27:38,618] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.11 GB, percent = 82.1%
x3003c0s37b0n0: [2024-03-28 13:27:53,140] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 13:27:53,141] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 28.96 GB         CA 32.87 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 13:27:53,141] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.1 GB, percent = 82.1%
x3003c0s37b0n0: [2024-03-28 13:27:53,228] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 13:27:53,228] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 25.84 GB         CA 26.54 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 13:27:53,229] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.1 GB, percent = 82.1%
x3003c0s37b0n0: [2024-03-28 13:28:25,855] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 13:28:25,856] [INFO] [utils.py:801:see_memory_usage] MA 12.16 GB         Max_MA 29.72 GB         CA 12.81 GB         Max_CA 37 GB 
x3003c0s37b0n0: [2024-03-28 13:28:25,856] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.13 GB, percent = 82.1%
x3003c0s37b0n0: [2024-03-28 13:28:25,947] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 13:28:25,947] [INFO] [utils.py:801:see_memory_usage] MA 12.16 GB         Max_MA 12.16 GB         CA 12.81 GB         Max_CA 13 GB 
x3003c0s37b0n0: [2024-03-28 13:28:25,947] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.13 GB, percent = 82.1%
x3003c0s37b0n0: [2024-03-28 13:28:39,271] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 13:28:39,272] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 28.96 GB         CA 32.87 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 13:28:39,272] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.12 GB, percent = 82.1%
x3003c0s37b0n0: [2024-03-28 13:28:39,360] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 13:28:39,360] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 25.84 GB         CA 26.54 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 13:28:39,360] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.11 GB, percent = 82.1%
x3003c0s37b0n0: [2024-03-28 13:29:13,279] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 13:29:13,280] [INFO] [utils.py:801:see_memory_usage] MA 13.35 GB         Max_MA 29.91 GB         CA 14.79 GB         Max_CA 37 GB 
x3003c0s37b0n0: [2024-03-28 13:29:13,280] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.08 GB, percent = 82.1%
x3003c0s37b0n0: [2024-03-28 13:29:13,358] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 13:29:13,358] [INFO] [utils.py:801:see_memory_usage] MA 13.35 GB         Max_MA 13.35 GB         CA 14.79 GB         Max_CA 15 GB 
x3003c0s37b0n0: [2024-03-28 13:29:13,358] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.08 GB, percent = 82.1%
x3003c0s7b0n0: [2024-03-28 13:29:18,100] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.7122087478637695
x3003c0s7b0n0: [2024-03-28 13:29:18,106] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.71795654296875
x3003c0s7b0n0: [2024-03-28 13:29:18,108] [INFO] [stage3.py:2251:step] Full outer step loop took 4.720726728439331
x3003c0s7b0n0: [2024-03-28 13:29:18,115] [INFO] [stage3.py:2251:step] Full outer step loop took 4.727904796600342
x3003c0s7b0n0: [2024-03-28 13:29:18,119] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.731489896774292
x3003c0s7b0n0: [2024-03-28 13:29:18,124] [INFO] [stage3.py:2251:step] Full outer step loop took 4.736335039138794
x3003c0s7b0n0: [2024-03-28 13:29:18,144] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.756193161010742
x3003c0s7b0n0: [2024-03-28 13:29:18,148] [INFO] [stage3.py:2251:step] Full outer step loop took 4.760538816452026
x3003c0s7b1n0: [2024-03-28 13:29:18,169] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.781050682067871
x3003c0s7b1n0: [2024-03-28 13:29:18,182] [INFO] [stage3.py:2251:step] Full outer step loop took 4.7944090366363525
x3003c0s7b1n0: [2024-03-28 13:29:18,192] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.804870367050171
x3003c0s7b1n0: [2024-03-28 13:29:18,198] [INFO] [stage3.py:2251:step] Full outer step loop took 4.809938907623291
x3003c0s37b0n0: [2024-03-28 13:29:18,202] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.815070629119873
x3003c0s7b1n0: [2024-03-28 13:29:18,210] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.82292914390564
x3003c0s7b1n0: [2024-03-28 13:29:18,215] [INFO] [stage3.py:2251:step] Full outer step loop took 4.827794790267944
x3003c0s7b1n0: [2024-03-28 13:29:18,229] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.84186863899231
x3003c0s37b0n0: [2024-03-28 13:29:18,230] [INFO] [stage3.py:2251:step] Full outer step loop took 4.842931747436523
x3003c0s7b1n0: [2024-03-28 13:29:18,234] [INFO] [stage3.py:2251:step] Full outer step loop took 4.846688508987427
x3003c0s37b0n0: [2024-03-28 13:29:18,254] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.867003440856934
x3003c0s37b0n0: [2024-03-28 13:29:18,261] [INFO] [stage3.py:2251:step] Full outer step loop took 4.8732898235321045
x3003c0s37b0n0: [2024-03-28 13:29:18,268] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.8801305294036865
x3003c0s37b0n0: [2024-03-28 13:29:18,277] [INFO] [stage3.py:2251:step] Full outer step loop took 4.890070199966431
x3003c0s37b0n0: [2024-03-28 13:29:18,278] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.891064882278442
x3003c0s37b0n0: [2024-03-28 13:29:18,283] [INFO] [stage3.py:2251:step] Full outer step loop took 4.895913124084473
x3003c0s37b1n0: [2024-03-28 13:29:18,399] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.010975360870361
x3003c0s37b1n0: [2024-03-28 13:29:18,413] [INFO] [stage3.py:2251:step] Full outer step loop took 5.025141000747681
x3003c0s37b1n0: [2024-03-28 13:29:18,490] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.102853298187256
x3003c0s37b1n0: [2024-03-28 13:29:18,495] [INFO] [stage3.py:2251:step] Full outer step loop took 5.107671022415161
x3003c0s37b1n0: [2024-03-28 13:29:18,520] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.1330084800720215
x3003c0s37b1n0: [2024-03-28 13:29:18,525] [INFO] [stage3.py:2251:step] Full outer step loop took 5.1378607749938965
x3003c0s37b1n0: [2024-03-28 13:29:18,585] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.197511434555054
x3003c0s37b1n0: [2024-03-28 13:29:18,590] [INFO] [stage3.py:2251:step] Full outer step loop took 5.2030463218688965
x3003c0s7b1n0: [2024-03-28 13:29:18,614] [INFO] [stage3.py:2277:step] End to end step took 5.226085186004639
x3003c0s7b1n0: [2024-03-28 13:29:18,614] [INFO] [stage3.py:2277:step] End to end step took 5.226128101348877
x3003c0s37b0n0: [2024-03-28 13:29:18,613] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4890.37
x3003c0s37b1n0: [2024-03-28 13:29:18,614] [INFO] [stage3.py:2277:step] End to end step took 5.2261669635772705
x3003c0s7b0n0: [2024-03-28 13:29:18,614] [INFO] [stage3.py:2277:step] End to end step took 5.226239204406738
x3003c0s37b0n0: [2024-03-28 13:29:18,614] [WARNING] [stage3.py:2267:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3003c0s37b0n0: [2024-03-28 13:29:18,614] [INFO] [stage3.py:2277:step] End to end step took 5.226491689682007
x3003c0s7b0n0: [2024-03-28 13:29:18,614] [INFO] [stage3.py:2277:step] End to end step took 5.226381063461304
x3003c0s37b0n0: [2024-03-28 13:29:18,614] [INFO] [stage3.py:2277:step] End to end step took 5.226501226425171
x3003c0s37b0n0: [2024-03-28 13:29:18,614] [INFO] [stage3.py:2277:step] End to end step took 5.226567268371582
x3003c0s7b1n0: [2024-03-28 13:29:18,614] [INFO] [stage3.py:2277:step] End to end step took 5.226510047912598
x3003c0s37b1n0: [2024-03-28 13:29:18,614] [INFO] [stage3.py:2277:step] End to end step took 5.226689100265503
x3003c0s37b0n0: [2024-03-28 13:29:18,614] [INFO] [logging.py:96:log_dist] [Rank 0] step=3, skipped=0, lr=[0.00026841599982106197], mom=[(0.9, 0.95)]
x3003c0s37b1n0: [2024-03-28 13:29:18,614] [INFO] [stage3.py:2277:step] End to end step took 5.22675633430481
x3003c0s7b0n0: [2024-03-28 13:29:18,614] [INFO] [stage3.py:2277:step] End to end step took 5.226788759231567
x3003c0s7b0n0: [2024-03-28 13:29:18,614] [INFO] [stage3.py:2277:step] End to end step took 5.2268664836883545
x3003c0s37b1n0: [2024-03-28 13:29:18,614] [INFO] [stage3.py:2277:step] End to end step took 5.226871967315674
x3003c0s37b0n0: [2024-03-28 13:29:18,614] [INFO] [stage3.py:2277:step] End to end step took 5.226813793182373
x3003c0s7b1n0: [2024-03-28 13:29:18,614] [INFO] [stage3.py:2277:step] End to end step took 5.22691011428833
x3003c0s37b0n0: [2024-03-28 13:29:18,615] [INFO] [timer.py:260:stop] epoch=0/micro_step=3/global_step=3, RunningAvgSamplesPerSec=2.430464220774924, CurrSamplesPerSec=2.430464220774924, MemAllocated=10.62GB, MaxMemAllocated=13.58GB
x3003c0s37b0n0: [2024-03-28 13:29:18,615] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 27779.87 | bwd_microstep: 66275.30 | bwd_inner_microstep: 65888.75 | bwd_allreduce_microstep: 386.40 | step_microstep: 5256.11
x3003c0s37b0n0: [2024-03-28 13:29:18,615] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 27779.85 | bwd: 66275.28 | bwd_inner: 65888.74 | bwd_allreduce: 386.42 | step: 5256.12
x3003c0s37b0n0: [2024-03-28 13:29:18,730] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 13:29:18,730] [INFO] [utils.py:801:see_memory_usage] MA 10.62 GB         Max_MA 13.58 GB         CA 11.28 GB         Max_CA 18 GB 
x3003c0s37b0n0: [2024-03-28 13:29:18,731] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.07 GB, percent = 82.1%
x3003c0s37b0n0: <TIMER:interval-time,100.25814461708069><TIMER:interval-time,100.25815105438232><TIMER:interval-time,100.25815010070801><TIMER:interval-time,100.25815343856812>
x3003c0s37b0n0: 
x3003c0s37b0n0: 
x3003c0s37b0n0: 
x3003c0s37b1n0: <TIMER:interval-time,100.25818586349487><TIMER:interval-time,100.25819730758667>
x3003c0s37b1n0: <TIMER:interval-time,100.25819873809814>
x3003c0s37b1n0: 
x3003c0s7b1n0: <TIMER:interval-time,100.25819897651672><TIMER:interval-time,100.25820231437683>
x3003c0s7b1n0: 
x3003c0s7b1n0: <TIMER:interval-time,100.25820875167847><TIMER:interval-time,100.25821089744568>
x3003c0s7b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,100.2583155632019>
x3003c0s7b0n0: <TIMER:interval-time,100.25817847251892><TIMER:interval-time,100.25818347930908>
x3003c0s7b0n0: <TIMER:interval-time,100.25818634033203>
x3003c0s7b0n0: <TIMER:interval-time,100.25817608833313>
x3003c0s7b0n0: 
x3003c0s7b1n0:  elapsed_time 100.258211 | consumed samples:          384 | consumed tokens:       786432 | elapsed time per iteration (ms): 100258.2 | learning rate: 2.684E-04 | global batch size:   128 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.277 | TFLOPs: 88.14 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 13:29:18,871] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 13:29:18,871] [INFO] [utils.py:801:see_memory_usage] MA 10.63 GB         Max_MA 10.63 GB         CA 11.28 GB         Max_CA 11 GB 
x3003c0s37b0n0: [2024-03-28 13:29:18,871] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.11 GB, percent = 82.1%
x3003c0s37b0n0: [2024-03-28 13:29:31,849] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 13:29:31,849] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 28.96 GB         CA 32.87 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 13:29:31,849] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.1 GB, percent = 82.1%
x3003c0s37b0n0: [2024-03-28 13:29:31,935] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 13:29:31,936] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 25.84 GB         CA 26.54 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 13:29:31,936] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.1 GB, percent = 82.1%
x3003c0s37b0n0: [2024-03-28 13:30:05,008] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 13:30:05,008] [INFO] [utils.py:801:see_memory_usage] MA 12.16 GB         Max_MA 29.72 GB         CA 12.81 GB         Max_CA 37 GB 
x3003c0s37b0n0: [2024-03-28 13:30:05,009] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.13 GB, percent = 82.1%
x3003c0s37b0n0: [2024-03-28 13:30:05,096] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 13:30:05,097] [INFO] [utils.py:801:see_memory_usage] MA 12.16 GB         Max_MA 12.16 GB         CA 12.81 GB         Max_CA 13 GB 
x3003c0s37b0n0: [2024-03-28 13:30:05,097] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.13 GB, percent = 82.1%
x3003c0s37b0n0: [2024-03-28 13:30:18,140] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 13:30:18,140] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 28.96 GB         CA 32.87 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 13:30:18,140] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.12 GB, percent = 82.1%
x3003c0s37b0n0: [2024-03-28 13:30:18,227] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 13:30:18,227] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 25.84 GB         CA 26.54 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 13:30:18,227] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.12 GB, percent = 82.1%
x3003c0s37b0n0: [2024-03-28 13:30:52,875] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 13:30:52,876] [INFO] [utils.py:801:see_memory_usage] MA 13.35 GB         Max_MA 29.91 GB         CA 14.79 GB         Max_CA 37 GB 
x3003c0s37b0n0: [2024-03-28 13:30:52,876] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.12 GB, percent = 82.1%
x3003c0s37b0n0: [2024-03-28 13:30:52,953] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 13:30:52,954] [INFO] [utils.py:801:see_memory_usage] MA 13.35 GB         Max_MA 13.35 GB         CA 14.79 GB         Max_CA 15 GB 
x3003c0s37b0n0: [2024-03-28 13:30:52,954] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.12 GB, percent = 82.1%
x3003c0s37b0n0: [2024-03-28 13:30:57,571] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.587193489074707
x3003c0s7b0n0: [2024-03-28 13:30:57,574] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.590329885482788
x3003c0s37b0n0: [2024-03-28 13:30:57,579] [INFO] [stage3.py:2251:step] Full outer step loop took 4.594894886016846
x3003c0s7b0n0: [2024-03-28 13:30:57,584] [INFO] [stage3.py:2251:step] Full outer step loop took 4.599637985229492
x3003c0s7b0n0: [2024-03-28 13:30:57,599] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.614287376403809
x3003c0s37b0n0: [2024-03-28 13:30:57,622] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.637503385543823
x3003c0s37b0n0: [2024-03-28 13:30:57,627] [INFO] [stage3.py:2251:step] Full outer step loop took 4.643283128738403
x3003c0s7b0n0: [2024-03-28 13:30:57,632] [INFO] [stage3.py:2251:step] Full outer step loop took 4.64746880531311
x3003c0s7b0n0: [2024-03-28 13:30:57,690] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.704802751541138
x3003c0s7b0n0: [2024-03-28 13:30:57,697] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.712981939315796
x3003c0s7b0n0: [2024-03-28 13:30:57,699] [INFO] [stage3.py:2251:step] Full outer step loop took 4.714690208435059
x3003c0s7b0n0: [2024-03-28 13:30:57,701] [INFO] [stage3.py:2251:step] Full outer step loop took 4.717327356338501
x3003c0s7b1n0: [2024-03-28 13:30:57,711] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.726684808731079
x3003c0s7b1n0: [2024-03-28 13:30:57,718] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.73411226272583
x3003c0s7b1n0: [2024-03-28 13:30:57,723] [INFO] [stage3.py:2251:step] Full outer step loop took 4.73896598815918
x3003c0s7b1n0: [2024-03-28 13:30:57,723] [INFO] [stage3.py:2251:step] Full outer step loop took 4.739152908325195
x3003c0s7b1n0: [2024-03-28 13:30:57,739] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.754714250564575
x3003c0s37b1n0: [2024-03-28 13:30:57,739] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.7546610832214355
x3003c0s7b1n0: [2024-03-28 13:30:57,744] [INFO] [stage3.py:2251:step] Full outer step loop took 4.759574890136719
x3003c0s7b1n0: [2024-03-28 13:30:57,758] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.773816108703613
x3003c0s37b1n0: [2024-03-28 13:30:57,759] [INFO] [stage3.py:2251:step] Full outer step loop took 4.774475812911987
x3003c0s7b1n0: [2024-03-28 13:30:57,762] [INFO] [stage3.py:2251:step] Full outer step loop took 4.7781500816345215
x3003c0s37b0n0: [2024-03-28 13:30:57,865] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.880638360977173
x3003c0s37b0n0: [2024-03-28 13:30:57,870] [INFO] [stage3.py:2251:step] Full outer step loop took 4.885528802871704
x3003c0s37b0n0: [2024-03-28 13:30:57,891] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.906514883041382
x3003c0s37b0n0: [2024-03-28 13:30:57,895] [INFO] [stage3.py:2251:step] Full outer step loop took 4.911357879638672
x3003c0s37b1n0: [2024-03-28 13:30:58,036] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.052206993103027
x3003c0s37b1n0: [2024-03-28 13:30:58,047] [INFO] [stage3.py:2251:step] Full outer step loop took 5.062855005264282
x3003c0s37b1n0: [2024-03-28 13:30:58,078] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.093687057495117
x3003c0s37b1n0: [2024-03-28 13:30:58,078] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.0938780307769775
x3003c0s37b1n0: [2024-03-28 13:30:58,083] [INFO] [stage3.py:2251:step] Full outer step loop took 5.0985426902771
x3003c0s37b1n0: [2024-03-28 13:30:58,083] [INFO] [stage3.py:2251:step] Full outer step loop took 5.098708629608154
x3003c0s37b1n0: [2024-03-28 13:30:58,110] [INFO] [stage3.py:2277:step] End to end step took 5.1258931159973145
x3003c0s37b0n0: [2024-03-28 13:30:58,110] [INFO] [stage3.py:2277:step] End to end step took 5.125936031341553
x3003c0s37b1n0: [2024-03-28 13:30:58,110] [INFO] [stage3.py:2277:step] End to end step took 5.126049041748047
x3003c0s7b1n0: [2024-03-28 13:30:58,110] [INFO] [stage3.py:2277:step] End to end step took 5.125972270965576
x3003c0s7b1n0: [2024-03-28 13:30:58,110] [INFO] [stage3.py:2277:step] End to end step took 5.126044750213623
x3003c0s7b0n0: [2024-03-28 13:30:58,110] [INFO] [stage3.py:2277:step] End to end step took 5.126091241836548
x3003c0s37b0n0: [2024-03-28 13:30:58,110] [INFO] [stage3.py:2277:step] End to end step took 5.1262452602386475
x3003c0s37b1n0: [2024-03-28 13:30:58,110] [INFO] [stage3.py:2277:step] End to end step took 5.126319408416748
x3003c0s7b0n0: [2024-03-28 13:30:58,110] [INFO] [stage3.py:2277:step] End to end step took 5.126296520233154
x3003c0s7b1n0: [2024-03-28 13:30:58,111] [INFO] [stage3.py:2277:step] End to end step took 5.126372337341309
x3003c0s37b0n0: [2024-03-28 13:30:58,110] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4643.46
x3003c0s37b1n0: [2024-03-28 13:30:58,111] [INFO] [stage3.py:2277:step] End to end step took 5.126601696014404
x3003c0s7b1n0: [2024-03-28 13:30:58,111] [INFO] [stage3.py:2277:step] End to end step took 5.126561641693115
x3003c0s37b0n0: [2024-03-28 13:30:58,111] [WARNING] [stage3.py:2267:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3003c0s37b0n0: [2024-03-28 13:30:58,111] [INFO] [stage3.py:2277:step] End to end step took 5.1268792152404785
x3003c0s7b0n0: [2024-03-28 13:30:58,111] [INFO] [stage3.py:2277:step] End to end step took 5.1267571449279785
x3003c0s7b0n0: [2024-03-28 13:30:58,111] [INFO] [stage3.py:2277:step] End to end step took 5.126796722412109
x3003c0s37b0n0: [2024-03-28 13:30:58,111] [INFO] [stage3.py:2277:step] End to end step took 5.126812934875488
x3003c0s37b0n0: [2024-03-28 13:30:58,111] [INFO] [logging.py:96:log_dist] [Rank 0] step=4, skipped=0, lr=[0.00023249999999999996], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 13:30:58,112] [INFO] [timer.py:260:stop] epoch=0/micro_step=4/global_step=4, RunningAvgSamplesPerSec=2.4224548945729225, CurrSamplesPerSec=2.414498182684732, MemAllocated=10.62GB, MaxMemAllocated=13.58GB
x3003c0s37b0n0: [2024-03-28 13:30:58,112] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 25955.75 | bwd_microstep: 67453.47 | bwd_inner_microstep: 67064.62 | bwd_allreduce_microstep: 388.72 | step_microstep: 5157.35
x3003c0s37b0n0: [2024-03-28 13:30:58,112] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 25955.74 | bwd: 67453.46 | bwd_inner: 67064.61 | bwd_allreduce: 388.74 | step: 5157.35
x3003c0s37b0n0: [2024-03-28 13:30:58,223] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 13:30:58,223] [INFO] [utils.py:801:see_memory_usage] MA 10.62 GB         Max_MA 13.58 GB         CA 11.28 GB         Max_CA 18 GB 
x3003c0s37b0n0: [2024-03-28 13:30:58,223] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.12 GB, percent = 82.1%
x3003c0s37b0n0: <TIMER:interval-time,99.49198842048645><TIMER:interval-time,99.49198985099792><TIMER:interval-time,99.49198484420776><TIMER:interval-time,99.4919912815094>
x3003c0s37b0n0: 
x3003c0s37b0n0: 
x3003c0s37b0n0: 
x3003c0s37b1n0: <TIMER:interval-time,99.49206471443176><TIMER:interval-time,99.49205088615417><TIMER:interval-time,99.49206805229187>
x3003c0s37b1n0: 
x3003c0s37b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,99.49207425117493>
x3003c0s7b1n0: <TIMER:interval-time,99.4920494556427><TIMER:interval-time,99.49204754829407><TIMER:interval-time,99.49204897880554>
x3003c0s7b1n0: 
x3003c0s7b1n0: 
x3003c0s7b1n0: <TIMER:interval-time,99.49205446243286>
x3003c0s7b0n0: <TIMER:interval-time,99.49203824996948><TIMER:interval-time,99.49203824996948><TIMER:interval-time,99.49204087257385>
x3003c0s7b0n0: 
x3003c0s7b0n0: 
x3003c0s7b0n0: <TIMER:interval-time,99.49213981628418>
x3003c0s7b1n0:  elapsed_time 99.492054 | consumed samples:          512 | consumed tokens:      1048576 | elapsed time per iteration (ms): 99492.1 | learning rate: 2.325E-04 | global batch size:   128 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.287 | TFLOPs: 88.82 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 13:30:58,334] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 13:30:58,335] [INFO] [utils.py:801:see_memory_usage] MA 10.63 GB         Max_MA 10.63 GB         CA 11.28 GB         Max_CA 11 GB 
x3003c0s37b0n0: [2024-03-28 13:30:58,335] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.11 GB, percent = 82.1%
x3003c0s37b0n0: [2024-03-28 13:31:13,136] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 13:31:13,136] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 28.96 GB         CA 32.87 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 13:31:13,136] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.1 GB, percent = 82.1%
x3003c0s37b0n0: [2024-03-28 13:31:13,222] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 13:31:13,223] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 25.84 GB         CA 26.54 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 13:31:13,223] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.1 GB, percent = 82.1%

[2024-03-28 13:00:45,416] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-28 13:00:48,494] [INFO] [runner.py:463:main] Using IP address of 10.140.57.42 for node x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-28 13:00:48,496] [INFO] [runner.py:557:main] deepspeed_env file = ./.deepspeed_env
[2024-03-28 13:00:48,496] [INFO] [multinode_runner.py:80:get_cmd] Running on the following workers: x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov,x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov,x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov,x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-28 13:00:48,496] [INFO] [runner.py:568:main] cmd = pdsh -S -f 1024 -w x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov,x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov,x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov,x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov export PYTHONUSERBASE=/home/am6429/.local/polaris/conda/2023-10-04; export PYTHONPATH=/home/am6429/dl-io/Megatron-DeepSpeed; export PATH=/home/am6429/.conda/envs/dspeed_env/bin:/soft/datascience/conda/2023-10-04/mconda3/condabin:/soft/compilers/cudatoolkit/cuda-11.8.0/bin:/soft/buildtools/cmake/cmake-3.23.2/cmake-3.23.2-linux-x86_64/bin:/opt/cray/pe/gcc/11.2.0/bin:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/include:/opt/cray/pe/pals/1.2.11/bin:/opt/cray/pe/craype/2.7.20/bin:/opt/cray/libfabric/1.15.2.0/bin:/home/am6429/.conda/envs/dspeed_env/bin:/opt/cray/pe/perftools/23.03.0/bin:/opt/cray/pe/papi/7.0.0.1/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/am6429/.local/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/am6429/veloc-build/include:/home/am6429/veloc-build/bin:/soft/datascience/conda/2023-01-10/mconda3/include:/opt/cray/pe/bin:/soft/datascience/conda/2023-01-10/mconda3/include; export LD_LIBRARY_PATH=/usr/lib64/:/soft/compilers/cudatoolkit/cuda-11.8.0/lib64:/opt/cray/pe/gcc/11.2.0/snos/lib64:/soft/compilers/cudatoolkit/cuda-11.8.0/extras/CUPTI/lib64:/soft/libraries/trt/TensorRT-8.5.3.1.Linux.x86_64-gnu.cuda-11.8.cudnn8.6/lib:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/lib:/soft/libraries/cudnn/cudnn-11-linux-x64-v8.6.0.163/lib:/opt/cray/libfabric/1.15.2.0/lib64:/usr/lib64:/opt/cray/pe/papi/7.0.0.1/lib64:/home/am6429/veloc-build/lib:/home/am6429/veloc-build/lib64:/home/am6429/nvcomp/lib:/soft/datascience/conda/2023-01-10/mconda3/lib:/soft/datascience/conda/2023-01-10/mconda3/lib/; export http_proxy=http://proxy.alcf.anl.gov:3128; export https_proxy=http://proxy.alcf.anl.gov:3128; export CC=gcc; export CXX=g++; export IBV_FORK_SAFE=1; export CFLAGS=-I/soft/datascience/conda/2023-01-10/mconda3/include/; export LDFLAGS=-L/soft/datascience/conda/2023-01-10/mconda3/lib/; export CUDA_DEVICE_MAX_CONNECTIONS=1; export TORCHSNAPSHOT_PER_RANK_MEMORY_BUDGET_BYTES=34359738368; export _DEFAULT_MAX_PER_RANK_IO_CONCURRENCY=1; export _MAX_PER_RANK_IO_CONCURRENCY=1; export NSYS_REPORT_DIR=/home/am6429/dl-io/dl-io-outputs/act-output-70B//rep-70B-tp1-dp16-l80-h8192-a64-sl2048-gbs64-mbs4-ratio1-subg100000000-prefetch0-flush_async0-opt_gaps5-%n;  cd /home/am6429/dl-io/Megatron-DeepSpeed; /home/am6429/.conda/envs/dspeed_env/bin/python -u -m deepspeed.launcher.launch --world_info=eyJ4MzAwM2MwczM3YjBuMC5oc24uY20ucG9sYXJpcy5hbGNmLmFubC5nb3YiOiBbMCwgMSwgMiwgM10sICJ4MzAwM2MwczM3YjFuMC5oc24uY20ucG9sYXJpcy5hbGNmLmFubC5nb3YiOiBbMCwgMSwgMiwgM10sICJ4MzAwM2MwczdiMG4wLmhzbi5jbS5wb2xhcmlzLmFsY2YuYW5sLmdvdiI6IFswLCAxLCAyLCAzXSwgIngzMDAzYzBzN2IxbjAuaHNuLmNtLnBvbGFyaXMuYWxjZi5hbmwuZ292IjogWzAsIDEsIDIsIDNdfQ== --node_rank=%n --master_addr=10.140.57.42 --master_port=29700 /home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py --tensor-model-parallel-size 1 --num-layers 80 --hidden-size 8192 --num-attention-heads 64 --micro-batch-size 4 --global-batch-size 64 --seq-length 2048 --max-position-embeddings 2048 --train-iters 10 --save /local/scratch/llama2/zero3-tp1}_dp16 --data-path /home/am6429/dl-io/datasets/meg-gpt2_text_document --vocab-file /home/am6429/dl-io/datasets/gpt2-vocab.json --merge-file /home/am6429/dl-io/datasets/gpt2-merges.txt --data-impl mmap --tokenizer-type GPTSentencePieceTokenizer --tokenizer-model /home/am6429/dl-io/datasets/tokenizer.model --split 949,50,1 --distributed-backend nccl --lr 3e-4 --lr-decay-style cosine --min-lr 3e-5 --weight-decay 0.1 --clip-grad 1 --lr-warmup-iters 1 --optimizer adam --adam-beta1 0.9 --adam-beta2 0.95 --log-interval 1 --save-interval 1000 --eval-interval 1000 --eval-iters 0 --bf16 --no-query-key-layer-scaling --attention-dropout 0 --hidden-dropout 0 --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization rmsnorm --disable-bias-linear --num-key-value-heads 4 --deepspeed --exit-interval 20 --deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json --zero-stage=3 --no-pipeline-parallel --cpu-optimizer --checkpoint-activations --deepspeed-activation-checkpointing
x3003c0s37b1n0: [2024-03-28 13:00:50,244] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b0n0: [2024-03-28 13:00:50,513] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b0n0: [2024-03-28 13:00:50,525] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b1n0: [2024-03-28 13:00:50,528] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b1n0: [2024-03-28 13:00:52,058] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3003c0s37b1n0: [2024-03-28 13:00:52,058] [INFO] [launch.py:151:main] nnodes=4, num_local_procs=4, node_rank=1
x3003c0s37b1n0: [2024-03-28 13:00:52,058] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [8, 9, 10, 11], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [12, 13, 14, 15]})
x3003c0s37b1n0: [2024-03-28 13:00:52,058] [INFO] [launch.py:163:main] dist_world_size=16
x3003c0s37b1n0: [2024-03-28 13:00:52,058] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3003c0s37b1n0: [2024-03-28 13:00:52,059] [INFO] [launch.py:253:main] process 35645 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b1n0: [2024-03-28 13:00:52,059] [INFO] [launch.py:253:main] process 35646 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b1n0: [2024-03-28 13:00:52,060] [INFO] [launch.py:253:main] process 35647 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b1n0: [2024-03-28 13:00:52,060] [INFO] [launch.py:253:main] process 35648 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b0n0: [2024-03-28 13:00:52,347] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3003c0s37b0n0: [2024-03-28 13:00:52,347] [INFO] [launch.py:151:main] nnodes=4, num_local_procs=4, node_rank=0
x3003c0s37b0n0: [2024-03-28 13:00:52,347] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [8, 9, 10, 11], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [12, 13, 14, 15]})
x3003c0s37b0n0: [2024-03-28 13:00:52,347] [INFO] [launch.py:163:main] dist_world_size=16
x3003c0s37b0n0: [2024-03-28 13:00:52,347] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3003c0s37b0n0: [2024-03-28 13:00:52,348] [INFO] [launch.py:253:main] process 42457 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b0n0: [2024-03-28 13:00:52,349] [INFO] [launch.py:253:main] process 42458 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b0n0: [2024-03-28 13:00:52,349] [INFO] [launch.py:253:main] process 42459 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b0n0: [2024-03-28 13:00:52,350] [INFO] [launch.py:253:main] process 42460 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b1n0: [2024-03-28 13:00:52,878] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3003c0s7b1n0: [2024-03-28 13:00:52,878] [INFO] [launch.py:151:main] nnodes=4, num_local_procs=4, node_rank=3
x3003c0s7b1n0: [2024-03-28 13:00:52,878] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [8, 9, 10, 11], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [12, 13, 14, 15]})
x3003c0s7b1n0: [2024-03-28 13:00:52,878] [INFO] [launch.py:163:main] dist_world_size=16
x3003c0s7b1n0: [2024-03-28 13:00:52,878] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3003c0s7b1n0: [2024-03-28 13:00:52,879] [INFO] [launch.py:253:main] process 55092 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b1n0: [2024-03-28 13:00:52,880] [INFO] [launch.py:253:main] process 55093 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b1n0: [2024-03-28 13:00:52,880] [INFO] [launch.py:253:main] process 55094 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b1n0: [2024-03-28 13:00:52,881] [INFO] [launch.py:253:main] process 55095 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b0n0: [2024-03-28 13:00:52,906] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3003c0s7b0n0: [2024-03-28 13:00:52,906] [INFO] [launch.py:151:main] nnodes=4, num_local_procs=4, node_rank=2
x3003c0s7b0n0: [2024-03-28 13:00:52,906] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [8, 9, 10, 11], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [12, 13, 14, 15]})
x3003c0s7b0n0: [2024-03-28 13:00:52,906] [INFO] [launch.py:163:main] dist_world_size=16
x3003c0s7b0n0: [2024-03-28 13:00:52,906] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3003c0s7b0n0: [2024-03-28 13:00:52,907] [INFO] [launch.py:253:main] process 47314 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b0n0: [2024-03-28 13:00:52,907] [INFO] [launch.py:253:main] process 47315 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b0n0: [2024-03-28 13:00:52,908] [INFO] [launch.py:253:main] process 47316 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b0n0: [2024-03-28 13:00:52,908] [INFO] [launch.py:253:main] process 47317 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b1n0: [2024-03-28 13:00:53,498] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b1n0: [2024-03-28 13:00:53,534] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b1n0: [2024-03-28 13:00:53,553] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b1n0: [2024-03-28 13:00:53,597] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b0n0: [2024-03-28 13:00:54,093] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b0n0: [2024-03-28 13:00:54,120] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b0n0: [2024-03-28 13:00:54,131] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b0n0: [2024-03-28 13:00:54,137] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b0n0: [2024-03-28 13:00:54,662] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b1n0: [2024-03-28 13:00:54,664] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b1n0: [2024-03-28 13:00:54,667] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b1n0: [2024-03-28 13:00:54,681] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b0n0: [2024-03-28 13:00:54,694] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b1n0: [2024-03-28 13:00:54,698] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b0n0: [2024-03-28 13:00:54,709] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b0n0: [2024-03-28 13:00:54,711] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b1n0:       meet the required dependencies to JIT install the op.
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: JIT compiled ops requires ninja
x3003c0s37b1n0: ninja .................. [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: op name ................ installed .. compatible
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b1n0:       meet the required dependencies to JIT install the op.
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: JIT compiled ops requires ninja
x3003c0s37b1n0: ninja .................. [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: op name ................ installed .. compatible
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: DeepSpeed general environment info:
x3003c0s37b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b1n0: torch version .................... 2.0.1+cu118
x3003c0s37b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b1n0: torch cuda version ............... 11.8
x3003c0s37b1n0: torch hip version ................ None
x3003c0s37b1n0: nvcc version ..................... 11.8
x3003c0s37b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b1n0:       meet the required dependencies to JIT install the op.
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: JIT compiled ops requires ninja
x3003c0s37b1n0: ninja .................. [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: op name ................ installed .. compatible
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b1n0: [2024-03-28 13:00:55,619] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b1n0:       meet the required dependencies to JIT install the op.
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: JIT compiled ops requires ninja
x3003c0s37b1n0: ninja .................. [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: op name ................ installed .. compatible
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: DeepSpeed general environment info:
x3003c0s37b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b1n0: torch version .................... 2.0.1+cu118
x3003c0s37b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b1n0: torch cuda version ............... 11.8
x3003c0s37b1n0: torch hip version ................ None
x3003c0s37b1n0: nvcc version ..................... 11.8
x3003c0s37b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [2024-03-28 13:00:55,705] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: DeepSpeed general environment info:
x3003c0s37b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b1n0: torch version .................... 2.0.1+cu118
x3003c0s37b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b1n0: torch cuda version ............... 11.8
x3003c0s37b1n0: torch hip version ................ None
x3003c0s37b1n0: nvcc version ..................... 11.8
x3003c0s37b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: DeepSpeed general environment info:
x3003c0s37b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b1n0: torch version .................... 2.0.1+cu118
x3003c0s37b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b1n0: torch cuda version ............... 11.8
x3003c0s37b1n0: torch hip version ................ None
x3003c0s37b1n0: nvcc version ..................... 11.8
x3003c0s37b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b1n0: [2024-03-28 13:00:55,769] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b1n0: [2024-03-28 13:00:55,777] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b0n0:       meet the required dependencies to JIT install the op.
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: JIT compiled ops requires ninja
x3003c0s37b0n0: ninja .................. [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: op name ................ installed .. compatible
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b0n0:       meet the required dependencies to JIT install the op.
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: JIT compiled ops requires ninja
x3003c0s37b0n0: ninja .................. [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: op name ................ installed .. compatible
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b0n0:       meet the required dependencies to JIT install the op.
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: JIT compiled ops requires ninja
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b0n0:       meet the required dependencies to JIT install the op.
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: JIT compiled ops requires ninja
x3003c0s37b0n0: ninja .................. [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: op name ................ installed .. compatible
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b0n0: ninja .................. [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: op name ................ installed .. compatible
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: DeepSpeed general environment info:
x3003c0s37b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b0n0: torch version .................... 2.0.1+cu118
x3003c0s37b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b0n0: torch cuda version ............... 11.8
x3003c0s37b0n0: torch hip version ................ None
x3003c0s37b0n0: nvcc version ..................... 11.8
x3003c0s37b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [2024-03-28 13:00:57,023] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: DeepSpeed general environment info:
x3003c0s37b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b0n0: torch version .................... 2.0.1+cu118
x3003c0s37b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b0n0: torch cuda version ............... 11.8
x3003c0s37b0n0: torch hip version ................ None
x3003c0s37b0n0: nvcc version ..................... 11.8
x3003c0s37b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: DeepSpeed general environment info:
x3003c0s37b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b0n0: torch version .................... 2.0.1+cu118
x3003c0s37b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b0n0: torch cuda version ............... 11.8
x3003c0s37b0n0: torch hip version ................ None
x3003c0s37b0n0: nvcc version ..................... 11.8
x3003c0s37b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b0n0: DeepSpeed general environment info:
x3003c0s37b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b0n0: torch version .................... 2.0.1+cu118
x3003c0s37b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b0n0: torch cuda version ............... 11.8
x3003c0s37b0n0: torch hip version ................ None
x3003c0s37b0n0: nvcc version ..................... 11.8
x3003c0s37b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b0n0: INFO: overriding default arguments for tokenizer_type:GPTSentencePieceTokenizer                    with tokenizer_type:GPT2BPETokenizer
x3003c0s37b0n0: using world size: 16, data-parallel-size: 16, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
x3003c0s37b0n0: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
x3003c0s37b0n0: using torch.bfloat16 for parameters ...
x3003c0s37b0n0: ------------------------ arguments ------------------------
x3003c0s37b0n0:   accumulate_allreduce_grads_in_fp32 .............. True
x3003c0s37b0n0:   adam_beta1 ...................................... 0.9
x3003c0s37b0n0:   adam_beta2 ...................................... 0.95
x3003c0s37b0n0:   adam_eps ........................................ 1e-08
x3003c0s37b0n0:   add_bias_linear ................................. False
x3003c0s37b0n0:   add_position_embedding .......................... False
x3003c0s37b0n0:   adlr_autoresume ................................. False
x3003c0s37b0n0:   adlr_autoresume_interval ........................ 1000
x3003c0s37b0n0:   aml_data_download_path .......................... None
x3003c0s37b0n0:   apply_layernorm_1p .............................. False
x3003c0s37b0n0:   apply_query_key_layer_scaling ................... False
x3003c0s37b0n0:   apply_residual_connection_post_layernorm ........ False
x3003c0s37b0n0:   async_tensor_model_parallel_allreduce ........... False
x3003c0s37b0n0:   attention_dropout ............................... 0.0
x3003c0s37b0n0:   attention_softmax_in_fp32 ....................... False
x3003c0s37b0n0:   barrier_with_L1_time ............................ True
x3003c0s37b0n0:   bert_binary_head ................................ True
x3003c0s37b0n0:   bert_embedder_type .............................. megatron
x3003c0s37b0n0:   bert_load ....................................... None
x3003c0s37b0n0:   bf16 ............................................ True
x3003c0s37b0n0:   bias_dropout_fusion ............................. True
x3003c0s37b0n0:   bias_gelu_fusion ................................ False
x3003c0s37b0n0:   biencoder_projection_dim ........................ 0
x3003c0s37b0n0:   biencoder_shared_query_context_model ............ False
x3003c0s37b0n0:   block_data_path ................................. None
x3003c0s37b0n0:   checkpoint_activations .......................... True
x3003c0s37b0n0:   checkpoint_in_cpu ............................... False
x3003c0s37b0n0:   checkpoint_num_layers ........................... 1
x3003c0s37b0n0:   classes_fraction ................................ 1.0
x3003c0s37b0n0:   clip_grad ....................................... 1.0
x3003c0s37b0n0:   compression_training ............................ False
x3003c0s37b0n0:   consumed_train_samples .......................... 0
x3003c0s37b0n0:   consumed_train_tokens ........................... 0
x3003c0s37b0n0:   consumed_valid_samples .......................... 0
x3003c0s37b0n0:   contigious_checkpointing ........................ False
x3003c0s37b0n0:   cpu_optimizer ................................... True
x3003c0s37b0n0:   cpu_torch_adam .................................. False
x3003c0s37b0n0:   create_moe_param_group .......................... False
x3003c0s37b0n0:   curriculum_learning_legacy ...................... False
x3003c0s37b0n0:   data_cache_path ................................. None
x3003c0s37b0n0:   data_efficiency_curriculum_learning ............. False
x3003c0s37b0n0:   data_impl ....................................... mmap
x3003c0s37b0n0:   data_parallel_random_init ....................... False
x3003c0s37b0n0:   data_parallel_size .............................. 16
x3003c0s37b0n0:   data_path ....................................... ['/home/am6429/dl-io/datasets/meg-gpt2_text_document']
x3003c0s37b0n0:   data_per_class_fraction ......................... 1.0
x3003c0s37b0n0:   data_sharding ................................... True
x3003c0s37b0n0:   dataloader_type ................................. single
x3003c0s37b0n0:   DDP_impl ........................................ local
x3003c0s37b0n0:   decoder_num_layers .............................. None
x3003c0s37b0n0:   decoder_seq_length .............................. None
x3003c0s37b0n0:   deepscale ....................................... False
x3003c0s37b0n0:   deepscale_config ................................ None
x3003c0s37b0n0:   deepspeed ....................................... True
x3003c0s37b0n0:   deepspeed_activation_checkpointing .............. True
x3003c0s37b0n0:   deepspeed_config ................................ /home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json
x3003c0s37b0n0:   dino_bottleneck_size ............................ 256
x3003c0s37b0n0:   dino_freeze_last_layer .......................... 1
x3003c0s37b0n0:   dino_head_hidden_size ........................... 2048
x3003c0s37b0n0:   dino_local_crops_number ......................... 10
x3003c0s37b0n0:   dino_local_img_size ............................. 96
x3003c0s37b0n0:   dino_norm_last_layer ............................ False
x3003c0s37b0n0:   dino_teacher_temp ............................... 0.07
x3003c0s37b0n0:   dino_warmup_teacher_temp ........................ 0.04
x3003c0s37b0n0:   dino_warmup_teacher_temp_epochs ................. 30
x3003c0s37b0n0:   distribute_checkpointed_activations ............. False
x3003c0s37b0n0:   distribute_saved_activations .................... False
x3003c0s37b0n0:   distributed_backend ............................. nccl
x3003c0s37b0n0:   distributed_timeout_minutes ..................... 10
x3003c0s37b0n0:   ds_inference .................................... False
x3003c0s37b0n0:   ds_pipeline_enabled ............................. False
x3003c0s37b0n0:   ds_sequence_parallel_size ....................... 1
x3003c0s37b0n0:   embedding_path .................................. None
x3003c0s37b0n0:   embedding_weights_in_fp32 ....................... False
x3003c0s37b0n0:   empty_unused_memory_level ....................... 0
x3003c0s37b0n0:   enable_expert_tensor_parallelism ................ False
x3003c0s37b0n0:   encoder_num_layers .............................. 80
x3003c0s37b0n0:   encoder_seq_length .............................. 2048
x3003c0s37b0n0:   end_weight_decay ................................ 0.1
x3003c0s37b0n0:   eod_mask_loss ................................... False
x3003c0s37b0n0:   eval_interval ................................... 1000
x3003c0s37b0n0:   eval_iters ...................................... 0
x3003c0s37b0n0:   evidence_data_path .............................. None
x3003c0s37b0n0:   exit_duration_in_mins ........................... None
x3003c0s37b0n0:   exit_interval ................................... 20
x3003c0s37b0n0:   exit_on_missing_checkpoint ...................... False
x3003c0s37b0n0:   exit_signal_handler ............................. False
x3003c0s37b0n0:   expert_interval ................................. 2
x3003c0s37b0n0:   ffn_hidden_size ................................. 21824
x3003c0s37b0n0:   finetune ........................................ False
x3003c0s37b0n0:   force_ds_sequence_parallel ...................... False
x3003c0s37b0n0:   fp16 ............................................ False
x3003c0s37b0n0:   fp16_lm_cross_entropy ........................... False
x3003c0s37b0n0:   fp32_residual_connection ........................ False
x3003c0s37b0n0:   fp8_amax_compute_algo ........................... most_recent
x3003c0s37b0n0:   fp8_amax_history_len ............................ 1
x3003c0s37b0n0:   fp8_e4m3 ........................................ False
x3003c0s37b0n0:   fp8_hybrid ...................................... False
x3003c0s37b0n0:   fp8_interval .................................... 1
x3003c0s37b0n0:   fp8_margin ...................................... 0
x3003c0s37b0n0:   fp8_wgrad ....................................... True
x3003c0s37b0n0:   global_batch_size ............................... 64
x3003c0s37b0n0:   gradient_accumulation_fusion .................... True
x3003c0s37b0n0:   head_lr_mult .................................... 1.0
x3003c0s37b0n0:   hidden_dropout .................................. 0.0
x3003c0s37b0n0:   hidden_size ..................................... 8192
x3003c0s37b0n0:   hidden_size_teacher ............................. None
x3003c0s37b0n0:   hysteresis ...................................... 2
x3003c0s37b0n0:   ict_head_size ................................... None
x3003c0s37b0n0:   ict_load ........................................ None
x3003c0s37b0n0:   img_h ........................................... 224
x3003c0s37b0n0:   img_w ........................................... 224
x3003c0s37b0n0:   indexer_batch_size .............................. 128
x3003c0s37b0n0:   indexer_log_interval ............................ 1000
x3003c0s37b0n0:   inference ....................................... False
x3003c0s37b0n0:   inference_batch_times_seqlen_threshold .......... 512
x3003c0s37b0n0:   init_method_std ................................. 0.02
x3003c0s37b0n0:   init_method_xavier_uniform ...................... False
x3003c0s37b0n0:   initial_loss_scale .............................. 4294967296
x3003c0s37b0n0:   iter_per_epoch .................................. 1250
x3003c0s37b0n0:   kd .............................................. False
x3003c0s37b0n0:   kd_alpha_ce ..................................... 1
x3003c0s37b0n0:   kd_beta_ce ...................................... 1
x3003c0s37b0n0:   kd_temp ......................................... 1.0
x3003c0s37b0n0:   kv_channels ..................................... 128
x3003c0s37b0n0:   layernorm_epsilon ............................... 1e-05
x3003c0s37b0n0:   lazy_mpu_init ................................... None
x3003c0s37b0n0:   load ............................................ None
x3003c0s37b0n0:   load_teacher .................................... None
x3003c0s37b0n0:   local_rank ...................................... 0
x3003c0s37b0n0:   log_batch_size_to_tensorboard ................... False
x3003c0s37b0n0:   log_interval .................................... 1
x3003c0s37b0n0:   log_learning_rate_to_tensorboard ................ True
x3003c0s37b0n0:   log_loss_scale_to_tensorboard ................... True
x3003c0s37b0n0:   log_memory_to_tensorboard ....................... False
x3003c0s37b0n0:   log_num_zeros_in_grad ........................... False
x3003c0s37b0n0:   log_optimizer_states_to_tensorboard ............. False
x3003c0s37b0n0:   log_params_norm ................................. False
x3003c0s37b0n0:   log_timers_to_tensorboard ....................... False
x3003c0s37b0n0:   log_validation_ppl_to_tensorboard ............... False
x3003c0s37b0n0:   log_world_size_to_tensorboard ................... False
x3003c0s37b0n0:   loss_scale ...................................... None
x3003c0s37b0n0:   loss_scale_window ............................... 1000
x3003c0s37b0n0:   lr .............................................. 0.0003
x3003c0s37b0n0:   lr_decay_iters .................................. None
x3003c0s37b0n0:   lr_decay_samples ................................ None
x3003c0s37b0n0:   lr_decay_style .................................. cosine
x3003c0s37b0n0:   lr_decay_tokens ................................. None
x3003c0s37b0n0:   lr_warmup_fraction .............................. None
x3003c0s37b0n0:   lr_warmup_iters ................................. 1
x3003c0s37b0n0:   lr_warmup_samples ............................... 0
x3003c0s37b0n0:   lr_warmup_tokens ................................ None
x3003c0s37b0n0:   make_vocab_size_divisible_by .................... 128
x3003c0s37b0n0:   mask_factor ..................................... 1.0
x3003c0s37b0n0:   mask_prob ....................................... 0.15
x3003c0s37b0n0:   mask_type ....................................... random
x3003c0s37b0n0:   masked_softmax_fusion ........................... True
x3003c0s37b0n0:   max_position_embeddings ......................... 2048
x3003c0s37b0n0:   max_tokens_to_oom ............................... 12000
x3003c0s37b0n0:   memory_centric_tiled_linear ..................... False
x3003c0s37b0n0:   merge_file ...................................... /home/am6429/dl-io/datasets/gpt2-merges.txt
x3003c0s37b0n0:   micro_batch_size ................................ 4
x3003c0s37b0n0:   min_loss_scale .................................. 1.0
x3003c0s37b0n0:   min_lr .......................................... 3e-05
x3003c0s37b0n0:   mlp_type ........................................ standard
x3003c0s37b0n0:   mmap_warmup ..................................... False
x3003c0s37b0n0:   moe_eval_capacity_factor ........................ 1.0
x3003c0s37b0n0:   moe_expert_parallel_size ........................ 1
x3003c0s37b0n0:   moe_loss_coeff .................................. 0.1
x3003c0s37b0n0:   moe_min_capacity ................................ 4
x3003c0s37b0n0:   moe_token_dropping .............................. True
x3003c0s37b0n0:   moe_train_capacity_factor ....................... 1.0
x3003c0s37b0n0:   mos ............................................. False
x3003c0s37b0n0:   no_load_lr_state ................................ False
x3003c0s37b0n0:   no_load_optim ................................... None
x3003c0s37b0n0:   no_load_rng ..................................... None
x3003c0s37b0n0:   no_persist_layer_norm ........................... False
x3003c0s37b0n0:   no_pipeline_parallel ............................ True
x3003c0s37b0n0:   no_save_optim ................................... None
x3003c0s37b0n0:   no_save_rng ..................................... None
x3003c0s37b0n0:   normalization ................................... rmsnorm
x3003c0s37b0n0:   num_attention_heads ............................. 64
x3003c0s37b0n0:   num_attention_heads_teacher ..................... None
x3003c0s37b0n0:   num_channels .................................... 3
x3003c0s37b0n0:   num_classes ..................................... 1000
x3003c0s37b0n0:   num_experts ..................................... [1]
x3003c0s37b0n0:   num_experts_switch .............................. None
x3003c0s37b0n0:   num_experts_teacher ............................. [1]
x3003c0s37b0n0:   num_key_value_heads ............................. 4
x3003c0s37b0n0:   num_layers ...................................... 80
x3003c0s37b0n0:   num_layers_per_virtual_pipeline_stage ........... None
x3003c0s37b0n0:   num_layers_teacher .............................. None
x3003c0s37b0n0:   num_workers ..................................... 2
x3003c0s37b0n0:   onnx_safe ....................................... None
x3003c0s37b0n0:   openai_gelu ..................................... False
x3003c0s37b0n0:   optimizer ....................................... adam
x3003c0s37b0n0:   output_bert_embeddings .......................... False
x3003c0s37b0n0:   overlap_p2p_comm ................................ False
x3003c0s37b0n0:   override_opt_param_scheduler .................... False
x3003c0s37b0n0:   params_dtype .................................... torch.bfloat16
x3003c0s37b0n0:   partition_activations ........................... False
x3003c0s37b0n0:   patch_dim ....................................... 16
x3003c0s37b0n0:   perform_initialization .......................... True
x3003c0s37b0n0:   pipeline_model_parallel_size .................... 1
x3003c0s37b0n0:   pipeline_model_parallel_split_rank .............. None
x3003c0s37b0n0:   profile_backward ................................ False
x3003c0s37b0n0:   query_in_block_prob ............................. 0.1
x3003c0s37b0n0:   rampup_batch_size ............................... None
x3003c0s37b0n0:   random_ltd ...................................... False
x3003c0s37b0n0:   rank ............................................ 0
x3003c0s37b0n0:   recompute_granularity ........................... None
x3003c0s37b0n0:   recompute_method ................................ None
x3003c0s37b0n0:   recompute_num_layers ............................ 1
x3003c0s37b0n0:   remote_device ................................... none
x3003c0s37b0n0:   reset_attention_mask ............................ False
x3003c0s37b0n0:   reset_iteration ................................. False
x3003c0s37b0n0:   reset_position_ids .............................. False
x3003c0s37b0n0:   retriever_report_topk_accuracies ................ []
x3003c0s37b0n0:   retriever_score_scaling ......................... False
x3003c0s37b0n0:   retriever_seq_length ............................ 256
x3003c0s37b0n0:   retro_add_retriever ............................. False
x3003c0s37b0n0:   retro_cyclic_train_iters ........................ None
x3003c0s37b0n0:   retro_encoder_attention_dropout ................. 0.1
x3003c0s37b0n0:   retro_encoder_hidden_dropout .................... 0.1
x3003c0s37b0n0:   retro_encoder_layers ............................ 2
x3003c0s37b0n0:   retro_num_neighbors ............................. 2
x3003c0s37b0n0:   retro_num_retrieved_chunks ...................... 2
x3003c0s37b0n0:   retro_return_doc_ids ............................ False
x3003c0s37b0n0:   retro_workdir ................................... None
x3003c0s37b0n0:   return_data_index ............................... False
x3003c0s37b0n0:   rotary_percent .................................. 1.0
x3003c0s37b0n0:   sample_rate ..................................... 1.0
x3003c0s37b0n0:   save ............................................ /local/scratch/llama2/zero3-tp1}_dp16
x3003c0s37b0n0:   save_interval ................................... 1000
x3003c0s37b0n0:   scatter_gather_tensors_in_pipeline .............. True
x3003c0s37b0n0:   scattered_embeddings ............................ False
x3003c0s37b0n0:   seed ............................................ 1234
x3003c0s37b0n0:   seq_length ...................................... 2048
x3003c0s37b0n0:   sequence_parallel ............................... False
x3003c0s37b0n0:   sgd_momentum .................................... 0.9
x3003c0s37b0n0:   short_seq_prob .................................. 0.1
x3003c0s37b0n0:   skip_train ...................................... False
x3003c0s37b0n0:   split ........................................... 949,50,1
x3003c0s37b0n0:   split_transformers .............................. False
x3003c0s37b0n0:   squared_relu .................................... False
x3003c0s37b0n0:   standalone_embedding_stage ...................... False
x3003c0s37b0n0:   start_weight_decay .............................. 0.1
x3003c0s37b0n0:   swiglu .......................................... True
x3003c0s37b0n0:   swin_backbone_type .............................. tiny
x3003c0s37b0n0:   synchronize_each_layer .......................... False
x3003c0s37b0n0:   tensor_model_parallel_size ...................... 1
x3003c0s37b0n0:   tensorboard_dir ................................. None
x3003c0s37b0n0:   tensorboard_log_interval ........................ 1
x3003c0s37b0n0:   tensorboard_queue_size .......................... 1000
x3003c0s37b0n0:   test_data_path .................................. None
x3003c0s37b0n0:   tile_factor ..................................... 1
x3003c0s37b0n0:   timing_log_level ................................ 0
x3003c0s37b0n0:   timing_log_option ............................... minmax
x3003c0s37b0n0:   titles_data_path ................................ None
x3003c0s37b0n0:   tokenizer_model ................................. /home/am6429/dl-io/datasets/tokenizer.model
x3003c0s37b0n0:   tokenizer_type .................................. GPT2BPETokenizer
x3003c0s37b0n0:   topk ............................................ 1
x3003c0s37b0n0:   train_data_exact_num_epochs ..................... None
x3003c0s37b0n0:   train_data_path ................................. None
x3003c0s37b0n0:   train_desc_path ................................. None
x3003c0s37b0n0:   train_doc_idx_path .............................. None
x3003c0s37b0n0:   train_idx_path .................................. None
x3003c0s37b0n0:   train_iters ..................................... 10
x3003c0s37b0n0:   train_sample_idx_path ........................... None
x3003c0s37b0n0:   train_samples ................................... None
x3003c0s37b0n0:   train_shuffle_idx_path .......................... None
x3003c0s37b0n0:   train_tokens .................................... None
x3003c0s37b0n0:   transformer_impl ................................ local
x3003c0s37b0n0:   transformer_pipeline_model_parallel_size ........ 1
x3003c0s37b0n0:   untie_embeddings_and_output_weights ............. True
x3003c0s37b0n0:   use_checkpoint_args ............................. False
x3003c0s37b0n0:   use_checkpoint_opt_param_scheduler .............. False
x3003c0s37b0n0:   use_contiguous_buffers_in_local_ddp ............. True
x3003c0s37b0n0:   use_cpu_initialization .......................... None
x3003c0s37b0n0:   use_dataset_only ................................ False
x3003c0s37b0n0:   use_distributed_optimizer ....................... False
x3003c0s37b0n0:   use_flash_attn .................................. False
x3003c0s37b0n0:   use_flash_attn_triton ........................... False
x3003c0s37b0n0:   use_flash_attn_v1 ............................... False
x3003c0s37b0n0:   use_flash_attn_v2 ............................... False
x3003c0s37b0n0:   use_one_sent_docs ............................... False
x3003c0s37b0n0:   use_pin_memory .................................. False
x3003c0s37b0n0:   use_ring_exchange_p2p ........................... False
x3003c0s37b0n0:   use_rotary_position_embeddings .................. True
x3003c0s37b0n0:   use_tutel ....................................... False
x3003c0s37b0n0:   valid_data_path ................................. None
x3003c0s37b0n0:   variable_seq_lengths ............................ False
x3003c0s37b0n0:   virtual_pipeline_model_parallel_size ............ None
x3003c0s37b0n0:   vision_backbone_type ............................ vit
x3003c0s37b0n0:   vision_pretraining .............................. False
x3003c0s37b0n0:   vision_pretraining_type ......................... classify
x3003c0s37b0n0:   vocab_extra_ids ................................. 0
x3003c0s37b0n0:   vocab_file ...................................... /home/am6429/dl-io/datasets/gpt2-vocab.json
x3003c0s37b0n0:   vocab_size ...................................... None
x3003c0s37b0n0:   weight_decay .................................... 0.1
x3003c0s37b0n0:   weight_decay_incr_style ......................... constant
x3003c0s37b0n0:   world_size ...................................... 16
x3003c0s37b0n0:   zero_allgather_bucket_size ...................... 0.0
x3003c0s37b0n0:   zero_contigious_gradients ....................... False
x3003c0s37b0n0:   zero_reduce_bucket_size ......................... 0.0
x3003c0s37b0n0:   zero_reduce_scatter ............................. False
x3003c0s37b0n0:   zero_stage ...................................... 3
x3003c0s37b0n0: -------------------- end of arguments ---------------------
x3003c0s37b0n0: setting number of micro-batches to constant 1
x3003c0s37b0n0: > building GPT2BPETokenizer tokenizer ...
x3003c0s37b0n0: [2024-03-28 13:00:57,153] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b0n0: [2024-03-28 13:00:57,165] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b0n0:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
x3003c0s37b0n0: > initializing torch distributed ...
x3003c0s37b0n0: [2024-03-28 13:00:57,167] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b0n0: [2024-03-28 13:00:57,167] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b0n0:       meet the required dependencies to JIT install the op.
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: JIT compiled ops requires ninja
x3003c0s7b0n0: ninja .................. [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: op name ................ installed .. compatible
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b1n0:       meet the required dependencies to JIT install the op.
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: JIT compiled ops requires ninja
x3003c0s7b1n0: ninja .................. [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: op name ................ installed .. compatible
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b1n0:       meet the required dependencies to JIT install the op.
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: JIT compiled ops requires ninja
x3003c0s7b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: ninja .................. [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: op name ................ installed .. compatible
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b1n0:       meet the required dependencies to JIT install the op.
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: JIT compiled ops requires ninja
x3003c0s7b1n0: ninja .................. [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: op name ................ installed .. compatible
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b0n0: DeepSpeed general environment info:
x3003c0s7b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b0n0: torch version .................... 2.0.1+cu118
x3003c0s7b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b0n0: torch cuda version ............... 11.8
x3003c0s7b0n0: torch hip version ................ None
x3003c0s7b0n0: nvcc version ..................... 11.8
x3003c0s7b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b0n0:       meet the required dependencies to JIT install the op.
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: JIT compiled ops requires ninja
x3003c0s7b0n0: ninja .................. [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: op name ................ installed .. compatible
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b0n0:       meet the required dependencies to JIT install the op.
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: JIT compiled ops requires ninja
x3003c0s7b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: ninja .................. [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: op name ................ installed .. compatible
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b1n0:       meet the required dependencies to JIT install the op.
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: JIT compiled ops requires ninja
x3003c0s7b1n0: ninja .................. [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: op name ................ installed .. compatible
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b0n0:       meet the required dependencies to JIT install the op.
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: JIT compiled ops requires ninja
x3003c0s7b0n0: ninja .................. [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: op name ................ installed .. compatible
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b1n0: DeepSpeed general environment info:
x3003c0s7b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b1n0: torch version .................... 2.0.1+cu118
x3003c0s7b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b1n0: torch cuda version ............... 11.8
x3003c0s7b1n0: torch hip version ................ None
x3003c0s7b1n0: nvcc version ..................... 11.8
x3003c0s7b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b0n0: [2024-03-28 13:00:57,537] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [2024-03-28 13:00:57,565] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: DeepSpeed general environment info:
x3003c0s7b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b1n0: torch version .................... 2.0.1+cu118
x3003c0s7b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b1n0: torch cuda version ............... 11.8
x3003c0s7b1n0: torch hip version ................ None
x3003c0s7b1n0: nvcc version ..................... 11.8
x3003c0s7b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b1n0: DeepSpeed general environment info:
x3003c0s7b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b1n0: torch version .................... 2.0.1+cu118
x3003c0s7b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b1n0: torch cuda version ............... 11.8
x3003c0s7b1n0: torch hip version ................ None
x3003c0s7b1n0: nvcc version ..................... 11.8
x3003c0s7b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: DeepSpeed general environment info:
x3003c0s7b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b0n0: torch version .................... 2.0.1+cu118
x3003c0s7b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b0n0: torch cuda version ............... 11.8
x3003c0s7b0n0: torch hip version ................ None
x3003c0s7b0n0: nvcc version ..................... 11.8
x3003c0s7b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: DeepSpeed general environment info:
x3003c0s7b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b0n0: torch version .................... 2.0.1+cu118
x3003c0s7b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b0n0: torch cuda version ............... 11.8
x3003c0s7b0n0: torch hip version ................ None
x3003c0s7b0n0: nvcc version ..................... 11.8
x3003c0s7b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b1n0: DeepSpeed general environment info:
x3003c0s7b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b1n0: torch version .................... 2.0.1+cu118
x3003c0s7b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b1n0: torch cuda version ............... 11.8
x3003c0s7b1n0: torch hip version ................ None
x3003c0s7b1n0: nvcc version ..................... 11.8
x3003c0s7b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: DeepSpeed general environment info:
x3003c0s7b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b0n0: torch version .................... 2.0.1+cu118
x3003c0s7b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b0n0: torch cuda version ............... 11.8
x3003c0s7b0n0: torch hip version ................ None
x3003c0s7b0n0: nvcc version ..................... 11.8
x3003c0s7b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b1n0: [2024-03-28 13:00:57,653] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b1n0: [2024-03-28 13:00:57,681] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b0n0: [2024-03-28 13:00:57,685] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b1n0: [2024-03-28 13:00:57,687] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b0n0: [2024-03-28 13:00:57,689] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b0n0: [2024-03-28 13:00:57,702] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b0n0: > initialized tensor model parallel with size 1
x3003c0s37b0n0: > initialized pipeline model parallel with size 1
x3003c0s37b0n0: > setting random seeds to 1234 ...
x3003c0s37b0n0: [2024-03-28 13:00:59,132] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
x3003c0s37b0n0: > compiling dataset index builder ...
x3003c0s37b0n0: make: Entering directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3003c0s37b0n0: make: Nothing to be done for 'default'.
x3003c0s37b0n0: make: Leaving directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3003c0s37b0n0: >>> done with dataset index builder. Compilation time: 0.085 seconds
x3003c0s37b0n0: > compiling and loading fused kernels ...
x3003c0s37b0n0: ninja: no work to do.
x3003c0s37b0n0: ninja: no work to do.
x3003c0s37b0n0: ninja: no work to do.
x3003c0s37b0n0: >>> done with compiling and loading fused kernels. Compilation time: 3.886 seconds
x3003c0s37b1n0: <<<<<<<<<<< 4
x3003c0s37b1n0: <<<<<<<<<<< 7
x3003c0s7b0n0: <<<<<<<<<<< 10
x3003c0s37b1n0: <<<<<<<<<<< 6
x3003c0s37b1n0: <<<<<<<<<<< 5
x3003c0s37b0n0: initialize_megatron took 6.772826671600342
x3003c0s37b0n0: <<<<<<<<<<< 0
x3003c0s7b1n0: <<<<<<<<<<< 12
x3003c0s7b0n0: <<<<<<<<<<< 8
x3003c0s7b0n0: <<<<<<<<<<< 9
x3003c0s7b0n0: <<<<<<<<<<< 11
x3003c0s37b0n0: <<<<<<<<<<< 3
x3003c0s37b0n0: <<<<<<<<<<< 2
x3003c0s37b0n0: <<<<<<<<<<< 1
x3003c0s7b1n0: <<<<<<<<<<< 13
x3003c0s7b1n0: <<<<<<<<<<< 14
x3003c0s7b1n0: <<<<<<<<<<< 15
x3003c0s37b0n0: time to initialize megatron (seconds): 8.518
x3003c0s37b0n0: [after megatron is initialized] datetime: 2024-03-28 13:01:03 
x3003c0s37b0n0: get_accelerator and all_reduce  took 0.013252735137939453
x3003c0s37b0n0: building GPT model ...
x3003c0s37b0n0: [2024-03-28 13:01:03,977] [INFO] [utils.py:800:see_memory_usage] Before Building Model
x3003c0s37b0n0: [2024-03-28 13:01:03,977] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 5.68 GB         CA 0.0 GB         Max_CA 6 GB 
x3003c0s37b0n0: [2024-03-28 13:01:03,977] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 19.86 GB, percent = 3.9%
x3003c0s37b0n0: [2024-03-28 13:01:16,594] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 563, num_elems = 55.14B
x3003c0s37b0n0: [2024-03-28 13:01:16,662] [INFO] [utils.py:800:see_memory_usage] After Building Model
x3003c0s37b0n0: [2024-03-28 13:01:16,662] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 7.23 GB         CA 35.28 GB         Max_CA 38 GB 
x3003c0s37b0n0: [2024-03-28 13:01:16,662] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 20.31 GB, percent = 4.0%
x3003c0s37b0n0:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 55141736448
x3003c0s37b1n0: ninja: no work to do.
x3003c0s37b1n0: Time to load cpu_adam op: 2.3332347869873047 seconds
x3003c0s37b1n0: Time to load cpu_adam op: 2.348433017730713 seconds
x3003c0s37b1n0: Time to load cpu_adam op: 2.3731603622436523 seconds
x3003c0s37b1n0: Time to load cpu_adam op: 2.3783416748046875 seconds
x3003c0s7b1n0: ninja: no work to do.
x3003c0s7b1n0: Time to load cpu_adam op: 2.4588961601257324 seconds
x3003c0s37b0n0: ninja: no work to do.
x3003c0s37b0n0: Time to load cpu_adam op: 2.565596580505371 seconds
x3003c0s7b1n0: Time to load cpu_adam op: 2.634443521499634 seconds
x3003c0s7b1n0: Time to load cpu_adam op: 2.6632797718048096 seconds
x3003c0s37b0n0: Time to load cpu_adam op: 2.666877269744873 seconds
x3003c0s37b0n0: Time to load cpu_adam op: 2.658689260482788 seconds
x3003c0s37b0n0: Time to load cpu_adam op: 2.665757656097412 seconds
x3003c0s7b0n0: ninja: no work to do.
x3003c0s7b0n0: Time to load cpu_adam op: 2.4517180919647217 seconds
x3003c0s7b0n0: ninja: no work to do.
x3003c0s7b0n0: Time to load cpu_adam op: 2.560852289199829 seconds
x3003c0s7b0n0: ninja: no work to do.
x3003c0s7b0n0: Time to load cpu_adam op: 2.4487335681915283 seconds
x3003c0s7b1n0: Time to load cpu_adam op: 2.435004472732544 seconds
x3003c0s7b0n0: ninja: no work to do.
x3003c0s7b0n0: Time to load cpu_adam op: 2.4394025802612305 seconds
x3003c0s37b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s7b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s7b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s7b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b0n0: > learning rate decay style: cosine
x3003c0s37b0n0: DeepSpeed is enabled.
x3003c0s37b0n0: [2024-03-28 13:01:22,073] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.3+8074cd62, git-hash=8074cd62, git-branch=hybrid_opt_offload
x3003c0s37b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b0n0: [2024-03-28 13:01:22,141] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After args sanity test
x3003c0s37b0n0: [2024-03-28 13:01:22,142] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 6.46 GB         CA 35.28 GB         Max_CA 35 GB 
x3003c0s37b0n0: [2024-03-28 13:01:22,142] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.74 GB, percent = 5.5%
x3003c0s37b0n0: [2024-03-28 13:01:22,195] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure distributed model
x3003c0s37b0n0: [2024-03-28 13:01:22,195] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 6.46 GB         CA 35.28 GB         Max_CA 35 GB 
x3003c0s37b0n0: [2024-03-28 13:01:22,195] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.74 GB, percent = 5.5%
x3003c0s37b0n0: [2024-03-28 13:01:22,257] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After configure distributed model
x3003c0s37b0n0: [2024-03-28 13:01:22,257] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 6.46 GB         CA 35.28 GB         Max_CA 35 GB 
x3003c0s37b0n0: [2024-03-28 13:01:22,257] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.74 GB, percent = 5.5%
x3003c0s37b0n0: [2024-03-28 13:01:22,258] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
x3003c0s37b0n0: [2024-03-28 13:01:22,310] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After setting model parameters
x3003c0s37b0n0: [2024-03-28 13:01:22,310] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 6.46 GB         CA 35.28 GB         Max_CA 35 GB 
x3003c0s37b0n0: [2024-03-28 13:01:22,310] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.74 GB, percent = 5.5%
x3003c0s7b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b0n0: [2024-03-28 13:01:22,363] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure optimizer
x3003c0s37b0n0: [2024-03-28 13:01:22,363] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 6.46 GB         CA 35.28 GB         Max_CA 35 GB 
x3003c0s37b0n0: [2024-03-28 13:01:22,364] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.74 GB, percent = 5.5%
x3003c0s37b0n0: [2024-03-28 13:01:22,364] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
x3003c0s37b0n0: [2024-03-28 13:01:22,364] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
x3003c0s37b0n0: [2024-03-28 13:01:22,396] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
x3003c0s37b0n0: [2024-03-28 13:01:22,396] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
x3003c0s37b0n0: [2024-03-28 13:01:22,396] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
x3003c0s37b0n0: [2024-03-28 13:01:22,396] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
x3003c0s37b0n0: [2024-03-28 13:01:22,447] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
x3003c0s37b0n0: [2024-03-28 13:01:22,448] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 6.46 GB         CA 35.28 GB         Max_CA 35 GB 
x3003c0s37b0n0: [2024-03-28 13:01:22,448] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.74 GB, percent = 5.5%
x3003c0s37b0n0: [2024-03-28 13:01:22,450] [INFO] [stage3.py:137:__init__] Reduce bucket size 500,000,000
x3003c0s37b0n0: [2024-03-28 13:01:22,450] [INFO] [stage3.py:138:__init__] Prefetch bucket size 50,000,000
x3003c0s7b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b0n0: [2024-03-28 13:01:22,503] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
x3003c0s37b0n0: [2024-03-28 13:01:22,503] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 6.46 GB         CA 35.28 GB         Max_CA 35 GB 
x3003c0s37b0n0: [2024-03-28 13:01:22,503] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.74 GB, percent = 5.5%
x3003c0s37b0n0: Parameter Offload: Total persistent parameters: 1318912 in 161 params
x3003c0s7b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s7b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b0n0: [2024-03-28 13:01:22,588] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
x3003c0s37b0n0: [2024-03-28 13:01:22,589] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 6.46 GB         CA 35.28 GB         Max_CA 35 GB 
x3003c0s37b0n0: [2024-03-28 13:01:22,589] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.75 GB, percent = 5.5%
x3003c0s37b0n0: [2024-03-28 13:01:22,645] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
x3003c0s37b0n0: [2024-03-28 13:01:22,645] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 6.46 GB         CA 35.28 GB         Max_CA 35 GB 
x3003c0s37b0n0: [2024-03-28 13:01:22,645] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.75 GB, percent = 5.5%
x3003c0s7b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b0n0: [2024-03-28 13:01:22,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b0n0: [2024-03-28 13:01:22,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b0n0: [2024-03-28 13:01:22,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b0n0: [2024-03-28 13:01:22,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b0n0: [2024-03-28 13:01:22,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b0n0: [2024-03-28 13:01:22,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b0n0: [2024-03-28 13:01:22,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b0n0: [2024-03-28 13:01:22,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b1n0: [2024-03-28 13:01:22,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b1n0: [2024-03-28 13:01:22,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b1n0: [2024-03-28 13:01:22,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b1n0: [2024-03-28 13:01:22,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b1n0: [2024-03-28 13:01:22,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b1n0: [2024-03-28 13:01:22,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b1n0: [2024-03-28 13:01:22,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b1n0: [2024-03-28 13:01:22,755] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b0n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b0n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b0n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b0n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b0n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b0n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b0n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b0n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b1n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b1n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b1n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b1n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b1n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b1n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b1n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b1n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b0n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,756] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,757] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,758] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,759] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,760] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,761] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:01:22,762] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b0n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b0n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b0n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b1n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b1n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b1n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b0n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b1n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b1n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b1n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b0n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b1n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b0n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b1n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b0n0: [2024-03-28 13:01:22,763] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b0n0: [2024-03-28 13:01:25,339] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 33
x3003c0s37b0n0: [2024-03-28 13:01:25,339] [INFO] [utils.py:801:see_memory_usage] MA 6.42 GB         Max_MA 6.46 GB         CA 6.42 GB         Max_CA 35 GB 
x3003c0s37b0n0: [2024-03-28 13:01:25,340] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 47.02 GB, percent = 9.3%
x3003c0s37b0n0: [2024-03-28 13:01:25,398] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
x3003c0s37b0n0: [2024-03-28 13:01:25,398] [INFO] [utils.py:801:see_memory_usage] MA 6.42 GB         Max_MA 6.42 GB         CA 6.42 GB         Max_CA 6 GB 
x3003c0s37b0n0: [2024-03-28 13:01:25,398] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 47.02 GB, percent = 9.3%
x3003c0s37b0n0: [2024-03-28 13:01:27,592] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
x3003c0s37b0n0: [2024-03-28 13:01:27,593] [INFO] [utils.py:801:see_memory_usage] MA 6.42 GB         Max_MA 6.42 GB         CA 6.42 GB         Max_CA 6 GB 
x3003c0s37b0n0: [2024-03-28 13:01:27,593] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 49.66 GB, percent = 9.9%
x3003c0s37b0n0: [2024-03-28 13:01:30,107] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
x3003c0s37b0n0: [2024-03-28 13:01:30,107] [INFO] [utils.py:801:see_memory_usage] MA 6.42 GB         Max_MA 6.42 GB         CA 6.42 GB         Max_CA 6 GB 
x3003c0s37b0n0: [2024-03-28 13:01:30,107] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 83.01 GB, percent = 16.5%
x3003c0s37b0n0: [2024-03-28 13:01:36,341] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | init_optimizer_state: 6219.38
x3003c0s37b0n0: [2024-03-28 13:01:36,423] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
x3003c0s37b0n0: [2024-03-28 13:01:36,423] [INFO] [utils.py:801:see_memory_usage] MA 6.42 GB         Max_MA 6.42 GB         CA 6.42 GB         Max_CA 6 GB 
x3003c0s37b0n0: [2024-03-28 13:01:36,423] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 146.53 GB, percent = 29.1%
x3003c0s37b0n0: [2024-03-28 13:01:37,205] [INFO] [stage3.py:520:_setup_for_real_optimizer] optimizer state initialized
x3003c0s37b0n0: [2024-03-28 13:01:51,869] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
x3003c0s37b0n0: [2024-03-28 13:01:51,870] [INFO] [utils.py:801:see_memory_usage] MA 7.35 GB         Max_MA 8.89 GB         CA 20.98 GB         Max_CA 21 GB 
x3003c0s37b0n0: [2024-03-28 13:01:51,870] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 178.52 GB, percent = 35.5%
x3003c0s37b0n0: [2024-03-28 13:01:51,870] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
x3003c0s37b0n0: [2024-03-28 13:01:51,934] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure LR scheduler
x3003c0s37b0n0: [2024-03-28 13:01:51,934] [INFO] [utils.py:801:see_memory_usage] MA 7.35 GB         Max_MA 7.35 GB         CA 20.98 GB         Max_CA 21 GB 
x3003c0s37b0n0: [2024-03-28 13:01:51,934] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 178.51 GB, percent = 35.5%
x3003c0s37b0n0: [2024-03-28 13:01:51,935] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
x3003c0s37b0n0: [2024-03-28 13:01:51,935] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7fc697d3c250>
x3003c0s37b0n0: [2024-03-28 13:01:51,935] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 13:01:51,998] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before rewriting optimizer step
x3003c0s37b0n0: [2024-03-28 13:01:51,998] [INFO] [utils.py:801:see_memory_usage] MA 7.35 GB         Max_MA 7.35 GB         CA 20.98 GB         Max_CA 21 GB 
x3003c0s37b0n0: [2024-03-28 13:01:51,998] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 178.52 GB, percent = 35.5%
x3003c0s37b0n0: [2024-03-28 13:01:52,062] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure checkpointing
x3003c0s37b0n0: [2024-03-28 13:01:52,063] [INFO] [utils.py:801:see_memory_usage] MA 7.35 GB         Max_MA 7.35 GB         CA 20.98 GB         Max_CA 21 GB 
x3003c0s37b0n0: [2024-03-28 13:01:52,063] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 178.53 GB, percent = 35.5%
x3003c0s37b0n0: [2024-03-28 13:01:52,063] [INFO] [config.py:998:print] DeepSpeedEngine configuration:
x3003c0s37b0n0: [2024-03-28 13:01:52,063] [INFO] [config.py:1002:print]   activation_checkpointing_config  {
x3003c0s37b0n0:     "partition_activations": false, 
x3003c0s37b0n0:     "contiguous_memory_optimization": false, 
x3003c0s37b0n0:     "cpu_checkpointing": false, 
x3003c0s37b0n0:     "number_checkpoints": null, 
x3003c0s37b0n0:     "synchronize_checkpoint_boundary": false, 
x3003c0s37b0n0:     "profile": false
x3003c0s37b0n0: }
x3003c0s37b0n0: [2024-03-28 13:01:52,063] [INFO] [config.py:1002:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
x3003c0s37b0n0: [2024-03-28 13:01:52,063] [INFO] [config.py:1002:print]   amp_enabled .................. False
x3003c0s37b0n0: [2024-03-28 13:01:52,063] [INFO] [config.py:1002:print]   amp_params ................... False
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   autotuning_config ............ {
x3003c0s37b0n0:     "enabled": false, 
x3003c0s37b0n0:     "start_step": null, 
x3003c0s37b0n0:     "end_step": null, 
x3003c0s37b0n0:     "metric_path": null, 
x3003c0s37b0n0:     "arg_mappings": null, 
x3003c0s37b0n0:     "metric": "throughput", 
x3003c0s37b0n0:     "model_info": null, 
x3003c0s37b0n0:     "results_dir": "autotuning_results", 
x3003c0s37b0n0:     "exps_dir": "autotuning_exps", 
x3003c0s37b0n0:     "overwrite": true, 
x3003c0s37b0n0:     "fast": true, 
x3003c0s37b0n0:     "start_profile_step": 3, 
x3003c0s37b0n0:     "end_profile_step": 5, 
x3003c0s37b0n0:     "tuner_type": "gridsearch", 
x3003c0s37b0n0:     "tuner_early_stopping": 5, 
x3003c0s37b0n0:     "tuner_num_trials": 50, 
x3003c0s37b0n0:     "model_info_path": null, 
x3003c0s37b0n0:     "mp_size": 1, 
x3003c0s37b0n0:     "max_train_batch_size": null, 
x3003c0s37b0n0:     "min_train_batch_size": 1, 
x3003c0s37b0n0:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
x3003c0s37b0n0:     "min_train_micro_batch_size_per_gpu": 1, 
x3003c0s37b0n0:     "num_tuning_micro_batch_sizes": 3
x3003c0s37b0n0: }
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   bfloat16_enabled ............. True
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   bfloat16_immediate_grad_update  False
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   checkpoint_parallel_write_pipeline  False
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   checkpoint_tag_validation_enabled  True
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   checkpoint_tag_validation_fail  False
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc697d3cbb0>
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   communication_data_type ...... None
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   curriculum_enabled_legacy .... False
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   curriculum_params_legacy ..... False
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   data_efficiency_enabled ...... False
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   dataloader_drop_last ......... False
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   datastates_config ............ {
x3003c0s37b0n0:     "enabled": null, 
x3003c0s37b0n0:     "config": {
x3003c0s37b0n0:     }
x3003c0s37b0n0: }
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   disable_allgather ............ False
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   dump_state ................... False
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   dynamic_loss_scale_args ...... None
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   eigenvalue_enabled ........... False
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   eigenvalue_gas_boundary_resolution  1
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   eigenvalue_layer_name ........ bert.encoder.layer
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   eigenvalue_layer_num ......... 0
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   eigenvalue_max_iter .......... 100
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   eigenvalue_stability ......... 1e-06
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   eigenvalue_tol ............... 0.01
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   eigenvalue_verbose ........... False
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   elasticity_enabled ........... False
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   flops_profiler_config ........ {
x3003c0s37b0n0:     "enabled": false, 
x3003c0s37b0n0:     "recompute_fwd_factor": 0.0, 
x3003c0s37b0n0:     "profile_step": 1, 
x3003c0s37b0n0:     "module_depth": -1, 
x3003c0s37b0n0:     "top_modules": 1, 
x3003c0s37b0n0:     "detailed": true, 
x3003c0s37b0n0:     "output_file": null
x3003c0s37b0n0: }
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   fp16_auto_cast ............... None
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   fp16_enabled ................. False
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   fp16_master_weights_and_gradients  False
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   global_rank .................. 0
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   grad_accum_dtype ............. bf16
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   gradient_accumulation_steps .. 1
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   gradient_clipping ............ 0.0
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   gradient_predivide_factor .... 1.0
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   graph_harvesting ............. False
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   initial_dynamic_scale ........ 1
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   load_universal_checkpoint .... False
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   loss_scale ................... 1.0
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   memory_breakdown ............. True
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   mics_hierarchial_params_gather  False
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   mics_shard_size .............. -1
x3003c0s37b0n0: [2024-03-28 13:01:52,064] [INFO] [config.py:1002:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
x3003c0s37b0n0: [2024-03-28 13:01:52,065] [INFO] [config.py:1002:print]   nebula_config ................ {
x3003c0s37b0n0:     "enabled": false, 
x3003c0s37b0n0:     "persistent_storage_path": null, 
x3003c0s37b0n0:     "persistent_time_interval": 100, 
x3003c0s37b0n0:     "num_of_version_in_retention": 2, 
x3003c0s37b0n0:     "enable_nebula_load": true, 
x3003c0s37b0n0:     "load_path": null
x3003c0s37b0n0: }
x3003c0s37b0n0: [2024-03-28 13:01:52,065] [INFO] [config.py:1002:print]   optimizer_legacy_fusion ...... False
x3003c0s37b0n0: [2024-03-28 13:01:52,065] [INFO] [config.py:1002:print]   optimizer_name ............... None
x3003c0s37b0n0: [2024-03-28 13:01:52,065] [INFO] [config.py:1002:print]   optimizer_params ............. None
x3003c0s37b0n0: [2024-03-28 13:01:52,065] [INFO] [config.py:1002:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
x3003c0s37b0n0: [2024-03-28 13:01:52,065] [INFO] [config.py:1002:print]   pld_enabled .................. False
x3003c0s37b0n0: [2024-03-28 13:01:52,065] [INFO] [config.py:1002:print]   pld_params ................... False
x3003c0s37b0n0: [2024-03-28 13:01:52,065] [INFO] [config.py:1002:print]   prescale_gradients ........... False
x3003c0s37b0n0: [2024-03-28 13:01:52,065] [INFO] [config.py:1002:print]   scheduler_name ............... None
x3003c0s37b0n0: [2024-03-28 13:01:52,065] [INFO] [config.py:1002:print]   scheduler_params ............. None
x3003c0s37b0n0: [2024-03-28 13:01:52,065] [INFO] [config.py:1002:print]   seq_parallel_communication_data_type  torch.float32
x3003c0s37b0n0: [2024-03-28 13:01:52,065] [INFO] [config.py:1002:print]   sparse_attention ............. None
x3003c0s37b0n0: [2024-03-28 13:01:52,065] [INFO] [config.py:1002:print]   sparse_gradients_enabled ..... False
x3003c0s37b0n0: [2024-03-28 13:01:52,065] [INFO] [config.py:1002:print]   steps_per_print .............. 1
x3003c0s37b0n0: [2024-03-28 13:01:52,065] [INFO] [config.py:1002:print]   train_batch_size ............. 64
x3003c0s37b0n0: [2024-03-28 13:01:52,065] [INFO] [config.py:1002:print]   train_micro_batch_size_per_gpu  4
x3003c0s37b0n0: [2024-03-28 13:01:52,065] [INFO] [config.py:1002:print]   use_data_before_expert_parallel_  False
x3003c0s37b0n0: [2024-03-28 13:01:52,065] [INFO] [config.py:1002:print]   use_node_local_storage ....... False
x3003c0s37b0n0: [2024-03-28 13:01:52,065] [INFO] [config.py:1002:print]   wall_clock_breakdown ......... True
x3003c0s37b0n0: [2024-03-28 13:01:52,065] [INFO] [config.py:1002:print]   weight_quantization_config ... None
x3003c0s37b0n0: [2024-03-28 13:01:52,065] [INFO] [config.py:1002:print]   world_size ................... 16
x3003c0s37b0n0: [2024-03-28 13:01:52,065] [INFO] [config.py:1002:print]   zero_allow_untested_optimizer  False
x3003c0s37b0n0: [2024-03-28 13:01:52,065] [INFO] [config.py:1002:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0, prefetch_optimizer=False, part_grads_async=False, prefetch_optimizer_gap=5) sub_group_size=100000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
x3003c0s37b0n0: [2024-03-28 13:01:52,065] [INFO] [config.py:1002:print]   zero_enabled ................. True
x3003c0s37b0n0: [2024-03-28 13:01:52,065] [INFO] [config.py:1002:print]   zero_force_ds_cpu_optimizer .. True
x3003c0s37b0n0: [2024-03-28 13:01:52,065] [INFO] [config.py:1002:print]   zero_optimization_stage ...... 3
x3003c0s37b0n0: [2024-03-28 13:01:52,065] [INFO] [config.py:988:print_user_config]   json = {
x3003c0s37b0n0:     "train_batch_size": 64, 
x3003c0s37b0n0:     "train_micro_batch_size_per_gpu": 4, 
x3003c0s37b0n0:     "steps_per_print": 1, 
x3003c0s37b0n0:     "zero_optimization": {
x3003c0s37b0n0:         "stage": 3, 
x3003c0s37b0n0:         "offload_optimizer": {
x3003c0s37b0n0:             "device": "cpu", 
x3003c0s37b0n0:             "ratio": 1, 
x3003c0s37b0n0:             "pin_memory": true, 
x3003c0s37b0n0:             "prefetch_optimizer": 0, 
x3003c0s37b0n0:             "part_grads_async": 0, 
x3003c0s37b0n0:             "prefetch_optimizer_gap": 5
x3003c0s37b0n0:         }, 
x3003c0s37b0n0:         "sub_group_size": 1.000000e+08
x3003c0s37b0n0:     }, 
x3003c0s37b0n0:     "bf16": {
x3003c0s37b0n0:         "enabled": true
x3003c0s37b0n0:     }, 
x3003c0s37b0n0:     "data_types": {
x3003c0s37b0n0:         "grad_accum_dtype": "bf16"
x3003c0s37b0n0:     }, 
x3003c0s37b0n0:     "wall_clock_breakdown": true, 
x3003c0s37b0n0:     "memory_breakdown": true, 
x3003c0s37b0n0:     "flops_profiler": {
x3003c0s37b0n0:         "enabled": false
x3003c0s37b0n0:     }
x3003c0s37b0n0: }
x3003c0s7b0n0: <TIMER:model-and-optimizer-setup,49.17115116119385>
x3003c0s7b0n0: <TIMER:model-and-optimizer-setup,49.17152285575867>
x3003c0s7b0n0: <TIMER:model-and-optimizer-setup,49.17183065414429>
x3003c0s7b1n0: <TIMER:model-and-optimizer-setup,49.17189264297485>
x3003c0s37b0n0: <TIMER:model-and-optimizer-setup,49.17208385467529>
x3003c0s37b1n0: <TIMER:model-and-optimizer-setup,49.1720175743103>
x3003c0s7b1n0: <TIMER:model-and-optimizer-setup,49.17203402519226>
x3003c0s7b1n0: <TIMER:model-and-optimizer-setup,49.172136545181274>
x3003c0s37b0n0: <TIMER:model-and-optimizer-setup,49.17220139503479>
x3003c0s7b0n0: <TIMER:model-and-optimizer-setup,49.17223310470581>
x3003c0s37b1n0: <TIMER:model-and-optimizer-setup,49.172226667404175>
x3003c0s7b1n0: <TIMER:model-and-optimizer-setup,49.172260761260986>
x3003c0s37b0n0: <TIMER:model-and-optimizer-setup,49.17233061790466>
x3003c0s37b0n0: <TIMER:model-and-optimizer-setup,49.17238211631775>
x3003c0s37b1n0: <TIMER:model-and-optimizer-setup,49.172322034835815>
x3003c0s37b1n0: <TIMER:model-and-optimizer-setup,49.172356367111206>
x3003c0s37b0n0: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-28 13:01:53 
x3003c0s37b0n0: > building train, validation, and test datasets ...
x3003c0s37b0n0:  > datasets target sizes (minimum size):
x3003c0s37b0n0:     train:      640
x3003c0s37b0n0:     validation: 0
x3003c0s37b0n0:     test:       0
x3003c0s37b0n0: > building train, validation, and test datasets for GPT ...
x3003c0s37b0n0: Single data path provided for train, valid & test
x3003c0s37b0n0:  > building dataset index ...
x3003c0s37b0n0:     reading sizes...
x3003c0s37b0n0:     reading pointers...
x3003c0s37b0n0:     reading document index...
x3003c0s37b0n0:     creating numpy buffer of mmap...
x3003c0s37b0n0:     creating memory view of numpy buffer...
x3003c0s37b0n0:  > finished creating indexed dataset in 0.001928 seconds
x3003c0s37b0n0:     number of documents: 79000
x3003c0s37b0n0:  > dataset split:
x3003c0s37b0n0:     train:
x3003c0s37b0n0:      document indices in [0, 74971) total of 74971 documents
x3003c0s37b0n0:     validation:
x3003c0s37b0n0:      document indices in [74971, 78921) total of 3950 documents
x3003c0s37b0n0:     test:
x3003c0s37b0n0:      document indices in [78921, 79000) total of 79 documents
x3003c0s37b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/d5d1daec41eb416469c3827ed48205ed_doc_idx.npy
x3003c0s37b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/d5d1daec41eb416469c3827ed48205ed_sample_idx.npy
x3003c0s37b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/d5d1daec41eb416469c3827ed48205ed_shuffle_idx.npy
x3003c0s37b0n0:     loaded indexed file in 0.003 seconds
x3003c0s37b0n0:     total number of samples: 108448
x3003c0s37b0n0:     total number of epochs: 1
x3003c0s37b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_doc_idx.npy
x3003c0s37b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_sample_idx.npy
x3003c0s37b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_shuffle_idx.npy
x3003c0s37b0n0:     loaded indexed file in 0.004 seconds
x3003c0s37b0n0:     total number of samples: 5792
x3003c0s37b0n0:     total number of epochs: 1
x3003c0s37b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_doc_idx.npy
x3003c0s37b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_sample_idx.npy
x3003c0s37b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_shuffle_idx.npy
x3003c0s37b0n0:     loaded indexed file in 0.042 seconds
x3003c0s37b0n0:     total number of samples: 185
x3003c0s37b0n0:     total number of epochs: 1
x3003c0s37b0n0: > finished creating GPT datasets ...
x3003c0s37b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5197288990020752>
x3003c0s37b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5247116088867188>
x3003c0s7b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5271518230438232>
x3003c0s7b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5308480262756348>
x3003c0s37b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5323021411895752>
x3003c0s37b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5387864112854004>
x3003c0s37b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5511705875396729>
x3003c0s7b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5513949394226074>
x3003c0s7b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5630366802215576>
x3003c0s7b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5663292407989502>
x3003c0s7b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5690689086914062>
x3003c0s37b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5711843967437744>
x3003c0s7b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5722744464874268>
x3003c0s7b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5756680965423584>
x3003c0s37b1n0: <TIMER:train/valid/test-data-iterators-setup,0.575946569442749>
x3003c0s37b0n0: <TIMER:train/valid/test-data-iterators-setup,0.6225206851959229>
x3003c0s37b0n0: [after dataloaders are built] datetime: 2024-03-28 13:01:53 
x3003c0s37b0n0: done with setup ...
x3003c0s7b1n0: (min, max) time across ranks (ms):
x3003c0s7b1n0:     model-and-optimizer-setup ......................: (49171.15, 49172.38)
x3003c0s7b1n0:     train/valid/test-data-iterators-setup ..........: (519.73, 622.52)
x3003c0s37b0n0: training ...
x3003c0s37b0n0: [before training begins] datetime: 2024-03-28 13:01:53 
x3003c0s37b0n0: [before the start of training step] datetime: 2024-03-28 13:01:53 
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 13:01:53,911] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 13:01:53,912] [INFO] [utils.py:801:see_memory_usage] MA 7.36 GB         Max_MA 7.36 GB         CA 8.02 GB         Max_CA 21 GB 
x3003c0s37b0n0: [2024-03-28 13:01:53,912] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 181.44 GB, percent = 36.1%
x3003c0s37b0n0: [2024-03-28 13:01:54,055] [INFO] [checkpointing.py:539:forward] Activation Checkpointing Information
x3003c0s37b0n0: [2024-03-28 13:01:54,055] [INFO] [checkpointing.py:540:forward] ----Partition Activations False, CPU CHECKPOINTING False
x3003c0s37b0n0: [2024-03-28 13:01:54,055] [INFO] [checkpointing.py:541:forward] ----contiguous Memory Checkpointing False with 80 total layers
x3003c0s37b0n0: [2024-03-28 13:01:54,055] [INFO] [checkpointing.py:543:forward] ----Synchronization False
x3003c0s37b0n0: [2024-03-28 13:01:54,055] [INFO] [checkpointing.py:544:forward] ----Profiling time in checkpointing False
x3003c0s37b0n0: [2024-03-28 13:02:10,304] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 13:02:10,305] [INFO] [utils.py:801:see_memory_usage] MA 21.15 GB         Max_MA 24.22 GB         CA 28.51 GB         Max_CA 29 GB 
x3003c0s37b0n0: [2024-03-28 13:02:10,305] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 181.8 GB, percent = 36.1%
x3003c0s37b0n0: [2024-03-28 13:02:10,598] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 13:02:10,599] [INFO] [utils.py:801:see_memory_usage] MA 21.15 GB         Max_MA 21.15 GB         CA 21.43 GB         Max_CA 29 GB 
x3003c0s37b0n0: [2024-03-28 13:02:10,599] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 181.79 GB, percent = 36.1%
x3003c0s37b0n0: [2024-03-28 13:03:03,654] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 13:03:03,654] [INFO] [utils.py:801:see_memory_usage] MA 10.9 GB         Max_MA 26.34 GB         CA 11.56 GB         Max_CA 35 GB 
x3003c0s37b0n0: [2024-03-28 13:03:03,655] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 185.52 GB, percent = 36.9%
x3003c0s37b0n0: [2024-03-28 13:03:03,739] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 13:03:03,739] [INFO] [utils.py:801:see_memory_usage] MA 10.9 GB         Max_MA 10.9 GB         CA 11.56 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:03:03,740] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 185.56 GB, percent = 36.9%
x3003c0s37b1n0: [2024-03-28 13:03:12,993] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.184437274932861
x3003c0s37b1n0: [2024-03-28 13:03:12,994] [INFO] [stage3.py:2251:step] Full outer step loop took 9.184677600860596
x3003c0s7b1n0: [2024-03-28 13:03:13,006] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.196341753005981
x3003c0s7b1n0: [2024-03-28 13:03:13,007] [INFO] [stage3.py:2251:step] Full outer step loop took 9.198008298873901
x3003c0s37b0n0: [2024-03-28 13:03:13,056] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.246577739715576
x3003c0s37b0n0: [2024-03-28 13:03:13,056] [INFO] [stage3.py:2251:step] Full outer step loop took 9.24697756767273
x3003c0s37b1n0: [2024-03-28 13:03:13,129] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.319798707962036
x3003c0s37b1n0: [2024-03-28 13:03:13,130] [INFO] [stage3.py:2251:step] Full outer step loop took 9.321178197860718
x3003c0s7b0n0: [2024-03-28 13:03:13,135] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.326291561126709
x3003c0s7b0n0: [2024-03-28 13:03:13,136] [INFO] [stage3.py:2251:step] Full outer step loop took 9.326625347137451
x3003c0s37b1n0: [2024-03-28 13:03:13,149] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.339430809020996
x3003c0s37b1n0: [2024-03-28 13:03:13,149] [INFO] [stage3.py:2251:step] Full outer step loop took 9.339665412902832
x3003c0s7b0n0: [2024-03-28 13:03:13,210] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.400873899459839
x3003c0s7b0n0: [2024-03-28 13:03:13,211] [INFO] [stage3.py:2251:step] Full outer step loop took 9.401928663253784
x3003c0s37b0n0: [2024-03-28 13:03:13,221] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.412196636199951
x3003c0s37b0n0: [2024-03-28 13:03:13,222] [INFO] [stage3.py:2251:step] Full outer step loop took 9.413041114807129
x3003c0s7b0n0: [2024-03-28 13:03:13,228] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.418745517730713
x3003c0s7b0n0: [2024-03-28 13:03:13,228] [INFO] [stage3.py:2251:step] Full outer step loop took 9.419243574142456
x3003c0s7b0n0: [2024-03-28 13:03:13,237] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.427877187728882
x3003c0s7b0n0: [2024-03-28 13:03:13,237] [INFO] [stage3.py:2251:step] Full outer step loop took 9.428250312805176
x3003c0s7b1n0: [2024-03-28 13:03:13,260] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.450745820999146
x3003c0s7b1n0: [2024-03-28 13:03:13,260] [INFO] [stage3.py:2251:step] Full outer step loop took 9.450966358184814
x3003c0s37b0n0: [2024-03-28 13:03:13,265] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.456430912017822
x3003c0s37b0n0: [2024-03-28 13:03:13,266] [INFO] [stage3.py:2251:step] Full outer step loop took 9.456724643707275
x3003c0s37b1n0: [2024-03-28 13:03:13,285] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.476353406906128
x3003c0s37b1n0: [2024-03-28 13:03:13,286] [INFO] [stage3.py:2251:step] Full outer step loop took 9.476752519607544
x3003c0s7b1n0: [2024-03-28 13:03:13,294] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.484991312026978
x3003c0s7b1n0: [2024-03-28 13:03:13,294] [INFO] [stage3.py:2251:step] Full outer step loop took 9.485280513763428
x3003c0s37b0n0: [2024-03-28 13:03:13,298] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.488907814025879
x3003c0s37b0n0: [2024-03-28 13:03:13,298] [INFO] [stage3.py:2251:step] Full outer step loop took 9.489305973052979
x3003c0s7b1n0: [2024-03-28 13:03:13,328] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.518668413162231
x3003c0s7b1n0: [2024-03-28 13:03:13,328] [INFO] [stage3.py:2251:step] Full outer step loop took 9.51904821395874
x3003c0s7b0n0: [2024-03-28 13:03:13,364] [INFO] [stage3.py:2277:step] End to end step took 9.554924488067627
x3003c0s37b0n0: [2024-03-28 13:03:13,364] [INFO] [stage3.py:2277:step] End to end step took 9.55497407913208
x3003c0s7b0n0: [2024-03-28 13:03:13,364] [INFO] [stage3.py:2277:step] End to end step took 9.55502700805664
x3003c0s7b0n0: [2024-03-28 13:03:13,364] [INFO] [stage3.py:2277:step] End to end step took 9.55506157875061
x3003c0s7b1n0: [2024-03-28 13:03:13,364] [INFO] [stage3.py:2277:step] End to end step took 9.555108547210693
x3003c0s37b1n0: [2024-03-28 13:03:13,364] [INFO] [stage3.py:2277:step] End to end step took 9.555399894714355
x3003c0s37b0n0: [2024-03-28 13:03:13,364] [INFO] [stage3.py:2277:step] End to end step took 9.555391311645508
x3003c0s7b1n0: [2024-03-28 13:03:13,365] [INFO] [stage3.py:2277:step] End to end step took 9.555548429489136
x3003c0s37b0n0: [2024-03-28 13:03:13,364] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 9247.23
x3003c0s37b0n0: [2024-03-28 13:03:13,365] [INFO] [stage3.py:2277:step] End to end step took 9.555630445480347
x3003c0s7b1n0: [2024-03-28 13:03:13,365] [INFO] [stage3.py:2277:step] End to end step took 9.55554747581482
x3003c0s37b1n0: [2024-03-28 13:03:13,365] [INFO] [stage3.py:2277:step] End to end step took 9.555534839630127
x3003c0s37b1n0: [2024-03-28 13:03:13,365] [INFO] [stage3.py:2277:step] End to end step took 9.555643320083618
x3003c0s37b1n0: [2024-03-28 13:03:13,365] [INFO] [stage3.py:2277:step] End to end step took 9.555752038955688
x3003c0s7b0n0: [2024-03-28 13:03:13,365] [INFO] [stage3.py:2277:step] End to end step took 9.55568528175354
x3003c0s7b1n0: [2024-03-28 13:03:13,365] [INFO] [stage3.py:2277:step] End to end step took 9.55578351020813
x3003c0s37b0n0: [2024-03-28 13:03:13,365] [WARNING] [stage3.py:2267:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3003c0s37b0n0: [2024-03-28 13:03:13,365] [INFO] [stage3.py:2277:step] End to end step took 9.555975437164307
x3003c0s37b0n0: [2024-03-28 13:03:13,365] [INFO] [logging.py:96:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 13:03:13,366] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 16546.04 | bwd_microstep: 52829.52 | bwd_inner_microstep: 52662.14 | bwd_allreduce_microstep: 167.26 | step_microstep: 9625.83
x3003c0s37b0n0: [2024-03-28 13:03:13,366] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 16546.04 | bwd: 52829.52 | bwd_inner: 52662.15 | bwd_allreduce: 167.27 | step: 9625.81
x3003c0s37b0n0: [2024-03-28 13:03:13,459] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 13:03:13,460] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 10.9 GB         CA 10.02 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:03:13,460] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.87 GB, percent = 67.5%
x3003c0s37b0n0: <TIMER:interval-time,79.74269151687622><TIMER:interval-time,79.7426929473877><TIMER:interval-time,79.74268341064453>
x3003c0s37b0n0: 
x3003c0s37b0n0: 
x3003c0s7b0n0: <TIMER:interval-time,79.74272203445435><TIMER:interval-time,79.74271845817566><TIMER:interval-time,79.74271965026855>
x3003c0s7b0n0: 
x3003c0s7b0n0: 
x3003c0s7b1n0: <TIMER:interval-time,79.74272680282593><TIMER:interval-time,79.74273562431335><TIMER:interval-time,79.74272918701172><TIMER:interval-time,79.74273204803467>
x3003c0s7b1n0: 
x3003c0s7b1n0: 
x3003c0s7b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,79.74272847175598><TIMER:interval-time,79.7427306175232><TIMER:interval-time,79.74273204803467>
x3003c0s37b1n0: <TIMER:interval-time,79.74273419380188>
x3003c0s37b1n0: 
x3003c0s37b1n0: 
x3003c0s37b0n0: <TIMER:interval-time,79.74279499053955>
x3003c0s7b0n0: <TIMER:interval-time,79.74283790588379>
x3003c0s37b0n0: [Rank 0] (after 1 iterations) memory (MB) | allocated: 9594.7685546875 | max allocated: 9594.76904296875 | reserved: 10262.0 | max reserved: 10262.0
x3003c0s7b1n0:  elapsed_time 79.742732 | consumed samples:           64 | consumed tokens:       131072 | elapsed time per iteration (ms): 79742.7 | learning rate: 3.000E-04 | global batch size:    64 | lm loss: 1.246584E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.803 | TFLOPs: 55.41 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 13:03:13,597] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 13:03:13,597] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 9.37 GB         CA 10.02 GB         Max_CA 10 GB 
x3003c0s37b0n0: [2024-03-28 13:03:13,597] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.4 GB, percent = 67.4%
x3003c0s37b0n0: [2024-03-28 13:03:26,197] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 13:03:26,198] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 27.7 GB         CA 31.61 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:03:26,198] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.36 GB, percent = 67.4%
x3003c0s37b0n0: [2024-03-28 13:03:26,287] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 13:03:26,288] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 24.58 GB         CA 25.29 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:03:26,288] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.36 GB, percent = 67.4%
x3003c0s37b0n0: [2024-03-28 13:04:00,766] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 13:04:00,767] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 28.66 GB         CA 11.56 GB         Max_CA 36 GB 
x3003c0s37b0n0: [2024-03-28 13:04:00,767] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.36 GB, percent = 67.4%
x3003c0s37b0n0: [2024-03-28 13:04:00,845] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 13:04:00,845] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 10.91 GB         CA 11.56 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:04:00,845] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.36 GB, percent = 67.4%
x3003c0s37b0n0: [2024-03-28 13:04:07,469] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.593458414077759
x3003c0s37b0n0: [2024-03-28 13:04:07,469] [INFO] [stage3.py:2251:step] Full outer step loop took 6.593980073928833
x3003c0s37b1n0: [2024-03-28 13:04:07,620] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.7445759773254395
x3003c0s37b1n0: [2024-03-28 13:04:07,620] [INFO] [stage3.py:2251:step] Full outer step loop took 6.744808197021484
x3003c0s37b0n0: [2024-03-28 13:04:07,688] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.81258225440979
x3003c0s37b0n0: [2024-03-28 13:04:07,688] [INFO] [stage3.py:2251:step] Full outer step loop took 6.812836647033691
x3003c0s7b0n0: [2024-03-28 13:04:07,776] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.900695562362671
x3003c0s7b0n0: [2024-03-28 13:04:07,777] [INFO] [stage3.py:2251:step] Full outer step loop took 6.901423931121826
x3003c0s7b1n0: [2024-03-28 13:04:07,825] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.949781179428101
x3003c0s7b1n0: [2024-03-28 13:04:07,825] [INFO] [stage3.py:2251:step] Full outer step loop took 6.949942588806152
x3003c0s37b1n0: [2024-03-28 13:04:07,828] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.952486515045166
x3003c0s37b1n0: [2024-03-28 13:04:07,829] [INFO] [stage3.py:2251:step] Full outer step loop took 6.953239917755127
x3003c0s7b1n0: [2024-03-28 13:04:07,870] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.9941935539245605
x3003c0s7b1n0: [2024-03-28 13:04:07,870] [INFO] [stage3.py:2251:step] Full outer step loop took 6.994444847106934
x3003c0s7b1n0: [2024-03-28 13:04:07,879] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.0032947063446045
x3003c0s7b1n0: [2024-03-28 13:04:07,879] [INFO] [stage3.py:2251:step] Full outer step loop took 7.00372838973999
x3003c0s37b0n0: [2024-03-28 13:04:07,935] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.0595409870147705
x3003c0s37b0n0: [2024-03-28 13:04:07,935] [INFO] [stage3.py:2251:step] Full outer step loop took 7.059808969497681
x3003c0s37b1n0: [2024-03-28 13:04:07,961] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.086113214492798
x3003c0s37b1n0: [2024-03-28 13:04:07,962] [INFO] [stage3.py:2251:step] Full outer step loop took 7.086296319961548
x3003c0s7b0n0: [2024-03-28 13:04:07,961] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.086108684539795
x3003c0s7b0n0: [2024-03-28 13:04:07,962] [INFO] [stage3.py:2251:step] Full outer step loop took 7.086515188217163
x3003c0s37b0n0: [2024-03-28 13:04:07,982] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.1068291664123535
x3003c0s37b0n0: [2024-03-28 13:04:07,982] [INFO] [stage3.py:2251:step] Full outer step loop took 7.106993198394775
x3003c0s7b0n0: [2024-03-28 13:04:07,985] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.082415580749512
x3003c0s7b0n0: [2024-03-28 13:04:07,985] [INFO] [stage3.py:2251:step] Full outer step loop took 7.0826005935668945
x3003c0s37b1n0: [2024-03-28 13:04:07,988] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.112600564956665
x3003c0s37b1n0: [2024-03-28 13:04:07,988] [INFO] [stage3.py:2251:step] Full outer step loop took 7.112760782241821
x3003c0s7b1n0: [2024-03-28 13:04:08,007] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.131659269332886
x3003c0s7b1n0: [2024-03-28 13:04:08,007] [INFO] [stage3.py:2251:step] Full outer step loop took 7.131810903549194
x3003c0s7b0n0: [2024-03-28 13:04:08,013] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.137824535369873
x3003c0s7b0n0: [2024-03-28 13:04:08,013] [INFO] [stage3.py:2251:step] Full outer step loop took 7.137987852096558
x3003c0s37b1n0: [2024-03-28 13:04:08,043] [INFO] [stage3.py:2277:step] End to end step took 7.1672749519348145
x3003c0s37b0n0: [2024-03-28 13:04:08,043] [INFO] [stage3.py:2277:step] End to end step took 7.167382001876831
x3003c0s7b1n0: [2024-03-28 13:04:08,043] [INFO] [stage3.py:2277:step] End to end step took 7.167464256286621
x3003c0s7b0n0: [2024-03-28 13:04:08,043] [INFO] [stage3.py:2277:step] End to end step took 7.140624284744263
x3003c0s7b0n0: [2024-03-28 13:04:08,043] [INFO] [stage3.py:2277:step] End to end step took 7.167487859725952
x3003c0s37b1n0: [2024-03-28 13:04:08,043] [INFO] [stage3.py:2277:step] End to end step took 7.167617321014404
x3003c0s7b1n0: [2024-03-28 13:04:08,043] [INFO] [stage3.py:2277:step] End to end step took 7.167762994766235
x3003c0s7b0n0: [2024-03-28 13:04:08,043] [INFO] [stage3.py:2277:step] End to end step took 7.167784929275513
x3003c0s37b0n0: [2024-03-28 13:04:08,043] [INFO] [stage3.py:2277:step] End to end step took 7.167665719985962
x3003c0s37b0n0: [2024-03-28 13:04:08,043] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6594.44
x3003c0s37b0n0: [2024-03-28 13:04:08,043] [INFO] [stage3.py:2277:step] End to end step took 7.167982578277588
x3003c0s37b1n0: [2024-03-28 13:04:08,043] [INFO] [stage3.py:2277:step] End to end step took 7.167969703674316
x3003c0s7b1n0: [2024-03-28 13:04:08,043] [INFO] [stage3.py:2277:step] End to end step took 7.168025970458984
x3003c0s7b0n0: [2024-03-28 13:04:08,043] [INFO] [stage3.py:2277:step] End to end step took 7.16792893409729
x3003c0s37b0n0: [2024-03-28 13:04:08,043] [INFO] [stage3.py:2277:step] End to end step took 7.1681225299835205
x3003c0s7b1n0: [2024-03-28 13:04:08,043] [INFO] [stage3.py:2277:step] End to end step took 7.168110609054565
x3003c0s37b1n0: [2024-03-28 13:04:08,043] [INFO] [stage3.py:2277:step] End to end step took 7.1680052280426025
x3003c0s37b0n0: [2024-03-28 13:04:08,044] [INFO] [logging.py:96:log_dist] [Rank 0] step=2, skipped=0, lr=[0.0002918585038060976], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 13:04:08,044] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 12571.77 | bwd_microstep: 34341.51 | bwd_inner_microstep: 34202.83 | bwd_allreduce_microstep: 138.50 | step_microstep: 7198.58
x3003c0s37b0n0: [2024-03-28 13:04:08,044] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 12571.76 | bwd: 34341.55 | bwd_inner: 34202.82 | bwd_allreduce: 138.56 | step: 7198.58
x3003c0s37b0n0: [2024-03-28 13:04:08,174] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 13:04:08,174] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 10.91 GB         CA 10.02 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:04:08,174] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.24 GB, percent = 67.4%
x3003c0s37b0n0: <TIMER:interval-time,54.71411466598511><TIMER:interval-time,54.71411848068237>
x3003c0s37b0n0: 
x3003c0s37b0n0: <TIMER:interval-time,54.71412539482117>
x3003c0s37b0n0: <TIMER:interval-time,54.7141637802124>
x3003c0s7b0n0: <TIMER:interval-time,54.714149475097656>
x3003c0s7b0n0: <TIMER:interval-time,54.714152812957764><TIMER:interval-time,54.71415400505066><TIMER:interval-time,54.71415567398071>
x3003c0s7b0n0: 
x3003c0s7b0n0: 
x3003c0s7b1n0: <TIMER:interval-time,54.71415615081787><TIMER:interval-time,54.71416258811951><TIMER:interval-time,54.71416258811951>
x3003c0s37b1n0: <TIMER:interval-time,54.71414351463318><TIMER:interval-time,54.714139223098755><TIMER:interval-time,54.71414542198181><TIMER:interval-time,54.714147329330444>
x3003c0s7b1n0: 
x3003c0s7b1n0: <TIMER:interval-time,54.71416640281677>
x3003c0s7b1n0: 
x3003c0s37b1n0: 
x3003c0s37b1n0: 
x3003c0s37b1n0: 
x3003c0s7b1n0:  elapsed_time 54.714163 | consumed samples:          128 | consumed tokens:       262144 | elapsed time per iteration (ms): 54714.2 | learning rate: 2.919E-04 | global batch size:    64 | lm loss: 1.246764E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.170 | TFLOPs: 80.75 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 13:04:08,317] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 13:04:08,317] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 9.37 GB         CA 10.02 GB         Max_CA 10 GB 
x3003c0s37b0n0: [2024-03-28 13:04:08,318] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.28 GB, percent = 67.4%
x3003c0s37b0n0: [2024-03-28 13:04:21,015] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 13:04:21,016] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 27.7 GB         CA 31.61 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:04:21,016] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.25 GB, percent = 67.4%
x3003c0s37b0n0: [2024-03-28 13:04:21,104] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 13:04:21,104] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 24.58 GB         CA 25.29 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:04:21,105] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.24 GB, percent = 67.4%
x3003c0s37b0n0: [2024-03-28 13:04:56,526] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 13:04:56,527] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 28.66 GB         CA 11.56 GB         Max_CA 36 GB 
x3003c0s37b0n0: [2024-03-28 13:04:56,527] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.61 GB, percent = 67.5%
x3003c0s37b0n0: [2024-03-28 13:04:56,639] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 13:04:56,639] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 10.91 GB         CA 11.56 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:04:56,639] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.61 GB, percent = 67.5%
x3003c0s37b1n0: [2024-03-28 13:05:03,274] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.604887247085571
x3003c0s37b1n0: [2024-03-28 13:05:03,274] [INFO] [stage3.py:2251:step] Full outer step loop took 6.605090856552124
x3003c0s37b0n0: [2024-03-28 13:05:03,524] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.837987422943115
x3003c0s37b0n0: [2024-03-28 13:05:03,524] [INFO] [stage3.py:2251:step] Full outer step loop took 6.838200807571411
x3003c0s37b1n0: [2024-03-28 13:05:03,618] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.948815822601318
x3003c0s37b1n0: [2024-03-28 13:05:03,618] [INFO] [stage3.py:2251:step] Full outer step loop took 6.949080944061279
x3003c0s37b1n0: [2024-03-28 13:05:03,627] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.957044839859009
x3003c0s37b1n0: [2024-03-28 13:05:03,627] [INFO] [stage3.py:2251:step] Full outer step loop took 6.9578118324279785
x3003c0s37b1n0: [2024-03-28 13:05:03,647] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.977435827255249
x3003c0s37b1n0: [2024-03-28 13:05:03,647] [INFO] [stage3.py:2251:step] Full outer step loop took 6.9775896072387695
x3003c0s7b1n0: [2024-03-28 13:05:03,657] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.98739218711853
x3003c0s7b1n0: [2024-03-28 13:05:03,657] [INFO] [stage3.py:2251:step] Full outer step loop took 6.9877028465271
x3003c0s37b0n0: [2024-03-28 13:05:03,682] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.012588977813721
x3003c0s37b0n0: [2024-03-28 13:05:03,683] [INFO] [stage3.py:2251:step] Full outer step loop took 7.013243913650513
x3003c0s7b1n0: [2024-03-28 13:05:03,708] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.038878679275513
x3003c0s7b1n0: [2024-03-28 13:05:03,708] [INFO] [stage3.py:2251:step] Full outer step loop took 7.039092302322388
x3003c0s37b0n0: [2024-03-28 13:05:03,720] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.050758123397827
x3003c0s37b0n0: [2024-03-28 13:05:03,720] [INFO] [stage3.py:2251:step] Full outer step loop took 7.050980806350708
x3003c0s7b0n0: [2024-03-28 13:05:03,722] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.052524089813232
x3003c0s7b0n0: [2024-03-28 13:05:03,722] [INFO] [stage3.py:2251:step] Full outer step loop took 7.053004503250122
x3003c0s37b0n0: [2024-03-28 13:05:03,755] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.085562229156494
x3003c0s37b0n0: [2024-03-28 13:05:03,755] [INFO] [stage3.py:2251:step] Full outer step loop took 7.085724830627441
x3003c0s7b0n0: [2024-03-28 13:05:03,768] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.098933219909668
x3003c0s7b0n0: [2024-03-28 13:05:03,769] [INFO] [stage3.py:2251:step] Full outer step loop took 7.099966049194336
x3003c0s7b0n0: [2024-03-28 13:05:03,799] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.129590034484863
x3003c0s7b0n0: [2024-03-28 13:05:03,799] [INFO] [stage3.py:2251:step] Full outer step loop took 7.130033254623413
x3003c0s7b0n0: [2024-03-28 13:05:03,845] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.17557168006897
x3003c0s7b0n0: [2024-03-28 13:05:03,845] [INFO] [stage3.py:2251:step] Full outer step loop took 7.175735235214233
x3003c0s7b1n0: [2024-03-28 13:05:03,886] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.216557264328003
x3003c0s7b1n0: [2024-03-28 13:05:03,886] [INFO] [stage3.py:2251:step] Full outer step loop took 7.216737747192383
x3003c0s7b1n0: [2024-03-28 13:05:03,914] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.244891405105591
x3003c0s7b1n0: [2024-03-28 13:05:03,914] [INFO] [stage3.py:2251:step] Full outer step loop took 7.245049476623535
x3003c0s7b1n0: [2024-03-28 13:05:03,937] [INFO] [stage3.py:2277:step] End to end step took 7.267464637756348
x3003c0s7b0n0: [2024-03-28 13:05:03,937] [INFO] [stage3.py:2277:step] End to end step took 7.2676472663879395
x3003c0s37b0n0: [2024-03-28 13:05:03,937] [INFO] [stage3.py:2277:step] End to end step took 7.267722845077515
x3003c0s37b0n0: [2024-03-28 13:05:03,937] [INFO] [stage3.py:2277:step] End to end step took 7.267792224884033
x3003c0s37b1n0: [2024-03-28 13:05:03,937] [INFO] [stage3.py:2277:step] End to end step took 7.2676801681518555
x3003c0s37b1n0: [2024-03-28 13:05:03,937] [INFO] [stage3.py:2277:step] End to end step took 7.2677202224731445
x3003c0s7b1n0: [2024-03-28 13:05:03,937] [INFO] [stage3.py:2277:step] End to end step took 7.267829418182373
x3003c0s7b0n0: [2024-03-28 13:05:03,937] [INFO] [stage3.py:2277:step] End to end step took 7.267889022827148
x3003c0s7b0n0: [2024-03-28 13:05:03,938] [INFO] [stage3.py:2277:step] End to end step took 7.268199682235718
x3003c0s7b1n0: [2024-03-28 13:05:03,938] [INFO] [stage3.py:2277:step] End to end step took 7.268226146697998
x3003c0s7b0n0: [2024-03-28 13:05:03,938] [INFO] [stage3.py:2277:step] End to end step took 7.268332004547119
x3003c0s37b0n0: [2024-03-28 13:05:03,938] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6838.40
x3003c0s7b1n0: [2024-03-28 13:05:03,938] [INFO] [stage3.py:2277:step] End to end step took 7.2684550285339355
x3003c0s37b0n0: [2024-03-28 13:05:03,938] [INFO] [stage3.py:2277:step] End to end step took 7.268403053283691
x3003c0s37b0n0: [2024-03-28 13:05:03,938] [INFO] [stage3.py:2277:step] End to end step took 7.252270221710205
x3003c0s37b1n0: [2024-03-28 13:05:03,938] [INFO] [stage3.py:2277:step] End to end step took 7.268478870391846
x3003c0s37b1n0: [2024-03-28 13:05:03,938] [INFO] [stage3.py:2277:step] End to end step took 7.268519878387451
x3003c0s37b0n0: [2024-03-28 13:05:03,938] [INFO] [logging.py:96:log_dist] [Rank 0] step=3, skipped=0, lr=[0.00026841599982106197], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 13:05:03,939] [INFO] [timer.py:260:stop] epoch=0/micro_step=3/global_step=3, RunningAvgSamplesPerSec=1.1506773250693951, CurrSamplesPerSec=1.1506773250693951, MemAllocated=9.37GB, MaxMemAllocated=10.91GB
x3003c0s37b0n0: [2024-03-28 13:05:03,939] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 12675.06 | bwd_microstep: 35274.59 | bwd_inner_microstep: 35133.99 | bwd_allreduce_microstep: 140.53 | step_microstep: 7299.11
x3003c0s37b0n0: [2024-03-28 13:05:03,939] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 12675.05 | bwd: 35274.59 | bwd_inner: 35133.98 | bwd_allreduce: 140.54 | step: 7299.11
x3003c0s37b0n0: [2024-03-28 13:05:04,077] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 13:05:04,077] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 10.91 GB         CA 10.02 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:05:04,077] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.67 GB, percent = 67.5%
x3003c0s37b0n0: <TIMER:interval-time,55.902747631073><TIMER:interval-time,55.90275812149048><TIMER:interval-time,55.90275764465332>
x3003c0s37b0n0: 
x3003c0s37b0n0: 
x3003c0s37b0n0: <TIMER:interval-time,55.902758836746216>
x3003c0s37b1n0: <TIMER:interval-time,55.90277862548828><TIMER:interval-time,55.902777671813965><TIMER:interval-time,55.9027795791626>
x3003c0s37b1n0: 
x3003c0s37b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,55.902788162231445>
x3003c0s7b1n0: <TIMER:interval-time,55.902771949768066><TIMER:interval-time,55.902772665023804>
x3003c0s7b1n0: 
x3003c0s7b1n0: <TIMER:interval-time,55.90277051925659>
x3003c0s7b1n0: <TIMER:interval-time,55.902873039245605>
x3003c0s7b0n0: <TIMER:interval-time,55.90275430679321><TIMER:interval-time,55.90275835990906><TIMER:interval-time,55.902756214141846>
x3003c0s7b0n0: 
x3003c0s7b0n0: 
x3003c0s7b0n0: <TIMER:interval-time,55.90276002883911>
x3003c0s7b1n0:  elapsed_time 55.902772 | consumed samples:          192 | consumed tokens:       393216 | elapsed time per iteration (ms): 55902.8 | learning rate: 2.684E-04 | global batch size:    64 | lm loss: 4.076758E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.145 | TFLOPs: 79.04 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 13:05:04,223] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 13:05:04,223] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 9.37 GB         CA 10.02 GB         Max_CA 10 GB 
x3003c0s37b0n0: [2024-03-28 13:05:04,224] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.64 GB, percent = 67.5%
x3003c0s37b0n0: [2024-03-28 13:05:16,329] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 13:05:16,330] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 27.7 GB         CA 31.61 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:05:16,330] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.61 GB, percent = 67.5%
x3003c0s37b0n0: [2024-03-28 13:05:16,420] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 13:05:16,421] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 24.58 GB         CA 25.29 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:05:16,421] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.61 GB, percent = 67.5%
x3003c0s37b0n0: [2024-03-28 13:05:50,783] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 13:05:50,784] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 28.66 GB         CA 11.56 GB         Max_CA 36 GB 
x3003c0s37b0n0: [2024-03-28 13:05:50,784] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.01 GB, percent = 67.6%
x3003c0s37b0n0: [2024-03-28 13:05:50,865] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 13:05:50,865] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 10.91 GB         CA 11.56 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:05:50,865] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.01 GB, percent = 67.6%
x3003c0s37b0n0: [2024-03-28 13:05:57,735] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.839985609054565
x3003c0s37b0n0: [2024-03-28 13:05:57,736] [INFO] [stage3.py:2251:step] Full outer step loop took 6.840606927871704
x3003c0s37b0n0: [2024-03-28 13:05:57,762] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.866678953170776
x3003c0s37b0n0: [2024-03-28 13:05:57,762] [INFO] [stage3.py:2251:step] Full outer step loop took 6.866857290267944
x3003c0s37b1n0: [2024-03-28 13:05:57,804] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.908334493637085
x3003c0s37b1n0: [2024-03-28 13:05:57,804] [INFO] [stage3.py:2251:step] Full outer step loop took 6.908783912658691
x3003c0s37b0n0: [2024-03-28 13:05:57,924] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.029089689254761
x3003c0s37b0n0: [2024-03-28 13:05:57,925] [INFO] [stage3.py:2251:step] Full outer step loop took 7.029571533203125
x3003c0s37b0n0: [2024-03-28 13:05:57,946] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.050573825836182
x3003c0s37b0n0: [2024-03-28 13:05:57,946] [INFO] [stage3.py:2251:step] Full outer step loop took 7.050841569900513
x3003c0s37b1n0: [2024-03-28 13:05:57,950] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.054996490478516
x3003c0s37b1n0: [2024-03-28 13:05:57,950] [INFO] [stage3.py:2251:step] Full outer step loop took 7.055169343948364
x3003c0s37b1n0: [2024-03-28 13:05:57,953] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.057632923126221
x3003c0s37b1n0: [2024-03-28 13:05:57,953] [INFO] [stage3.py:2251:step] Full outer step loop took 7.057859420776367
x3003c0s37b1n0: [2024-03-28 13:05:57,956] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.06044340133667
x3003c0s37b1n0: [2024-03-28 13:05:57,956] [INFO] [stage3.py:2251:step] Full outer step loop took 7.060608148574829
x3003c0s7b0n0: [2024-03-28 13:05:58,043] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.14736533164978
x3003c0s7b0n0: [2024-03-28 13:05:58,044] [INFO] [stage3.py:2251:step] Full outer step loop took 7.149194240570068
x3003c0s7b1n0: [2024-03-28 13:05:58,050] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.155258417129517
x3003c0s7b1n0: [2024-03-28 13:05:58,051] [INFO] [stage3.py:2251:step] Full outer step loop took 7.155431032180786
x3003c0s7b1n0: [2024-03-28 13:05:58,052] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.156809091567993
x3003c0s7b1n0: [2024-03-28 13:05:58,052] [INFO] [stage3.py:2251:step] Full outer step loop took 7.1569788455963135
x3003c0s7b0n0: [2024-03-28 13:05:58,066] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.170913219451904
x3003c0s7b0n0: [2024-03-28 13:05:58,066] [INFO] [stage3.py:2251:step] Full outer step loop took 7.171306133270264
x3003c0s7b0n0: [2024-03-28 13:05:58,076] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.181262969970703
x3003c0s7b0n0: [2024-03-28 13:05:58,077] [INFO] [stage3.py:2251:step] Full outer step loop took 7.181529521942139
x3003c0s7b0n0: [2024-03-28 13:05:58,116] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.2210612297058105
x3003c0s7b0n0: [2024-03-28 13:05:58,116] [INFO] [stage3.py:2251:step] Full outer step loop took 7.221239805221558
x3003c0s7b1n0: [2024-03-28 13:05:58,124] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.229262113571167
x3003c0s7b1n0: [2024-03-28 13:05:58,125] [INFO] [stage3.py:2251:step] Full outer step loop took 7.229432582855225
x3003c0s7b1n0: [2024-03-28 13:05:58,128] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.233005046844482
x3003c0s7b1n0: [2024-03-28 13:05:58,128] [INFO] [stage3.py:2251:step] Full outer step loop took 7.2331626415252686
x3003c0s37b0n0: [2024-03-28 13:05:58,150] [INFO] [stage3.py:2277:step] End to end step took 7.255309581756592
x3003c0s7b0n0: [2024-03-28 13:05:58,150] [INFO] [stage3.py:2277:step] End to end step took 7.255272388458252
x3003c0s37b1n0: [2024-03-28 13:05:58,150] [INFO] [stage3.py:2277:step] End to end step took 7.255242347717285
x3003c0s7b1n0: [2024-03-28 13:05:58,150] [INFO] [stage3.py:2277:step] End to end step took 7.255324602127075
x3003c0s7b1n0: [2024-03-28 13:05:58,150] [INFO] [stage3.py:2277:step] End to end step took 7.255313873291016
x3003c0s37b1n0: [2024-03-28 13:05:58,150] [INFO] [stage3.py:2277:step] End to end step took 7.255322456359863
x3003c0s37b0n0: [2024-03-28 13:05:58,151] [INFO] [stage3.py:2277:step] End to end step took 7.2554545402526855
x3003c0s7b0n0: [2024-03-28 13:05:58,151] [INFO] [stage3.py:2277:step] End to end step took 7.255495071411133
x3003c0s7b0n0: [2024-03-28 13:05:58,151] [INFO] [stage3.py:2277:step] End to end step took 7.255483388900757
x3003c0s7b0n0: [2024-03-28 13:05:58,151] [INFO] [stage3.py:2277:step] End to end step took 7.255636692047119
x3003c0s7b1n0: [2024-03-28 13:05:58,151] [INFO] [stage3.py:2277:step] End to end step took 7.2557532787323
x3003c0s37b0n0: [2024-03-28 13:05:58,151] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6840.82
x3003c0s7b1n0: [2024-03-28 13:05:58,151] [INFO] [stage3.py:2277:step] End to end step took 7.255975961685181
x3003c0s37b0n0: [2024-03-28 13:05:58,151] [INFO] [stage3.py:2277:step] End to end step took 7.255925893783569
x3003c0s37b1n0: [2024-03-28 13:05:58,151] [INFO] [stage3.py:2277:step] End to end step took 7.255990028381348
x3003c0s37b0n0: [2024-03-28 13:05:58,151] [INFO] [stage3.py:2277:step] End to end step took 7.255861759185791
x3003c0s37b1n0: [2024-03-28 13:05:58,151] [INFO] [stage3.py:2277:step] End to end step took 7.25607442855835
x3003c0s37b0n0: [2024-03-28 13:05:58,152] [INFO] [logging.py:96:log_dist] [Rank 0] step=4, skipped=0, lr=[0.00023249999999999996], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 13:05:58,152] [INFO] [timer.py:260:stop] epoch=0/micro_step=4/global_step=4, RunningAvgSamplesPerSec=1.1684567022800472, CurrSamplesPerSec=1.1867941284387022, MemAllocated=9.37GB, MaxMemAllocated=10.91GB
x3003c0s37b0n0: [2024-03-28 13:05:58,152] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 12074.49 | bwd_microstep: 34168.22 | bwd_inner_microstep: 33997.62 | bwd_allreduce_microstep: 170.51 | step_microstep: 7286.60
x3003c0s37b0n0: [2024-03-28 13:05:58,152] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 12074.48 | bwd: 34168.22 | bwd_inner: 33997.62 | bwd_allreduce: 170.53 | step: 7286.61
x3003c0s37b0n0: [2024-03-28 13:05:58,281] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 13:05:58,281] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 10.91 GB         CA 10.02 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:05:58,281] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.06 GB, percent = 67.6%
x3003c0s37b0n0: <TIMER:interval-time,54.20323991775513><TIMER:interval-time,54.20324158668518><TIMER:interval-time,54.20324206352234><TIMER:interval-time,54.20324206352234>
x3003c0s37b0n0: 
x3003c0s37b0n0: 
x3003c0s37b0n0: 
x3003c0s7b0n0: <TIMER:interval-time,54.20322799682617><TIMER:interval-time,54.20323467254639>
x3003c0s7b0n0: 
x3003c0s7b0n0: <TIMER:interval-time,54.2032356262207>
x3003c0s7b0n0: <TIMER:interval-time,54.203264236450195>
x3003c0s37b1n0: <TIMER:interval-time,54.20323991775513><TIMER:interval-time,54.20324373245239>
x3003c0s37b1n0: <TIMER:interval-time,54.20324230194092>
x3003c0s37b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,54.203248262405396>
x3003c0s7b1n0: <TIMER:interval-time,54.20323061943054><TIMER:interval-time,54.2032196521759><TIMER:interval-time,54.20323467254639>
x3003c0s7b1n0: <TIMER:interval-time,54.20323467254639>
x3003c0s7b1n0: 
x3003c0s7b1n0: 
x3003c0s7b1n0:  elapsed_time 54.203235 | consumed samples:          256 | consumed tokens:       524288 | elapsed time per iteration (ms): 54203.2 | learning rate: 2.325E-04 | global batch size:    64 | lm loss: 2.462902E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.181 | TFLOPs: 81.51 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 13:05:58,440] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 13:05:58,441] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 9.37 GB         CA 10.02 GB         Max_CA 10 GB 
x3003c0s37b0n0: [2024-03-28 13:05:58,441] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.07 GB, percent = 67.6%
x3003c0s37b0n0: [2024-03-28 13:06:10,517] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 13:06:10,518] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 27.7 GB         CA 31.61 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:06:10,518] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.03 GB, percent = 67.6%
x3003c0s37b0n0: [2024-03-28 13:06:10,601] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 13:06:10,602] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 24.58 GB         CA 25.29 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:06:10,602] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.85 GB, percent = 67.5%
x3003c0s37b0n0: [2024-03-28 13:06:44,945] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 13:06:44,946] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 28.66 GB         CA 11.56 GB         Max_CA 36 GB 
x3003c0s37b0n0: [2024-03-28 13:06:44,946] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.08 GB, percent = 67.6%
x3003c0s37b0n0: [2024-03-28 13:06:45,026] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 13:06:45,027] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 10.91 GB         CA 11.56 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:06:45,027] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.08 GB, percent = 67.6%
x3003c0s37b0n0: [2024-03-28 13:06:51,634] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.57597541809082
x3003c0s37b0n0: [2024-03-28 13:06:51,634] [INFO] [stage3.py:2251:step] Full outer step loop took 6.576178550720215
x3003c0s37b1n0: [2024-03-28 13:06:51,858] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.800630569458008
x3003c0s37b1n0: [2024-03-28 13:06:51,858] [INFO] [stage3.py:2251:step] Full outer step loop took 6.800879955291748
x3003c0s7b0n0: [2024-03-28 13:06:51,881] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.823302984237671
x3003c0s7b0n0: [2024-03-28 13:06:51,881] [INFO] [stage3.py:2251:step] Full outer step loop took 6.823641538619995
x3003c0s37b1n0: [2024-03-28 13:06:51,963] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.905064105987549
x3003c0s37b1n0: [2024-03-28 13:06:51,963] [INFO] [stage3.py:2251:step] Full outer step loop took 6.9052393436431885
x3003c0s37b1n0: [2024-03-28 13:06:51,978] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.92028546333313
x3003c0s37b1n0: [2024-03-28 13:06:51,978] [INFO] [stage3.py:2251:step] Full outer step loop took 6.920508623123169
x3003c0s37b1n0: [2024-03-28 13:06:52,010] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.95276141166687
x3003c0s37b1n0: [2024-03-28 13:06:52,010] [INFO] [stage3.py:2251:step] Full outer step loop took 6.952924728393555
x3003c0s37b0n0: [2024-03-28 13:06:52,050] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.992126226425171
x3003c0s37b0n0: [2024-03-28 13:06:52,050] [INFO] [stage3.py:2251:step] Full outer step loop took 6.992392063140869
x3003c0s7b0n0: [2024-03-28 13:06:52,052] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.9947121143341064
x3003c0s7b0n0: [2024-03-28 13:06:52,052] [INFO] [stage3.py:2251:step] Full outer step loop took 6.9948890209198
x3003c0s7b0n0: [2024-03-28 13:06:52,069] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.011128664016724
x3003c0s7b0n0: [2024-03-28 13:06:52,069] [INFO] [stage3.py:2251:step] Full outer step loop took 7.0113136768341064
x3003c0s37b0n0: [2024-03-28 13:06:52,101] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.043893337249756
x3003c0s37b0n0: [2024-03-28 13:06:52,102] [INFO] [stage3.py:2251:step] Full outer step loop took 7.044338226318359
x3003c0s37b0n0: [2024-03-28 13:06:52,112] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.054645538330078
x3003c0s37b0n0: [2024-03-28 13:06:52,112] [INFO] [stage3.py:2251:step] Full outer step loop took 7.054811954498291
x3003c0s7b1n0: [2024-03-28 13:06:52,125] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.067843914031982
x3003c0s7b1n0: [2024-03-28 13:06:52,126] [INFO] [stage3.py:2251:step] Full outer step loop took 7.068029165267944
x3003c0s7b1n0: [2024-03-28 13:06:52,165] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.107518434524536
x3003c0s7b1n0: [2024-03-28 13:06:52,165] [INFO] [stage3.py:2251:step] Full outer step loop took 7.107721567153931
x3003c0s7b0n0: [2024-03-28 13:06:52,166] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.108901739120483
x3003c0s7b0n0: [2024-03-28 13:06:52,167] [INFO] [stage3.py:2251:step] Full outer step loop took 7.109140872955322
x3003c0s7b1n0: [2024-03-28 13:06:52,167] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.1096720695495605
x3003c0s7b1n0: [2024-03-28 13:06:52,167] [INFO] [stage3.py:2251:step] Full outer step loop took 7.110012054443359
x3003c0s7b1n0: [2024-03-28 13:06:52,214] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.1562628746032715
x3003c0s7b1n0: [2024-03-28 13:06:52,214] [INFO] [stage3.py:2251:step] Full outer step loop took 7.156433343887329
x3003c0s7b0n0: [2024-03-28 13:06:52,236] [INFO] [stage3.py:2277:step] End to end step took 7.178706169128418
x3003c0s7b1n0: [2024-03-28 13:06:52,236] [INFO] [stage3.py:2277:step] End to end step took 7.17875862121582
x3003c0s37b0n0: [2024-03-28 13:06:52,236] [INFO] [stage3.py:2277:step] End to end step took 7.178849697113037
x3003c0s37b1n0: [2024-03-28 13:06:52,237] [INFO] [stage3.py:2277:step] End to end step took 7.1790711879730225
x3003c0s7b0n0: [2024-03-28 13:06:52,237] [INFO] [stage3.py:2277:step] End to end step took 7.179247617721558
x3003c0s7b1n0: [2024-03-28 13:06:52,237] [INFO] [stage3.py:2277:step] End to end step took 7.179328203201294
x3003c0s37b1n0: [2024-03-28 13:06:52,237] [INFO] [stage3.py:2277:step] End to end step took 7.179330348968506
x3003c0s37b0n0: [2024-03-28 13:06:52,237] [INFO] [stage3.py:2277:step] End to end step took 7.1793129444122314
x3003c0s37b0n0: [2024-03-28 13:06:52,237] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6576.35
x3003c0s7b0n0: [2024-03-28 13:06:52,237] [INFO] [stage3.py:2277:step] End to end step took 7.1794092655181885
x3003c0s7b0n0: [2024-03-28 13:06:52,237] [INFO] [stage3.py:2277:step] End to end step took 7.179410219192505
x3003c0s37b1n0: [2024-03-28 13:06:52,237] [INFO] [stage3.py:2277:step] End to end step took 7.179400682449341
x3003c0s7b1n0: [2024-03-28 13:06:52,237] [INFO] [stage3.py:2277:step] End to end step took 7.179458856582642
x3003c0s7b1n0: [2024-03-28 13:06:52,237] [INFO] [stage3.py:2277:step] End to end step took 7.179519414901733
x3003c0s37b1n0: [2024-03-28 13:06:52,237] [INFO] [stage3.py:2277:step] End to end step took 7.179445266723633
x3003c0s37b0n0: [2024-03-28 13:06:52,237] [INFO] [stage3.py:2277:step] End to end step took 7.179654359817505
x3003c0s37b0n0: [2024-03-28 13:06:52,237] [INFO] [stage3.py:2277:step] End to end step took 7.179650545120239
x3003c0s37b0n0: [2024-03-28 13:06:52,238] [INFO] [logging.py:96:log_dist] [Rank 0] step=5, skipped=0, lr=[0.0001884425039850356], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 13:06:52,238] [INFO] [timer.py:260:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=1.1754526853923093, CurrSamplesPerSec=1.1896990229016995, MemAllocated=9.37GB, MaxMemAllocated=10.91GB
x3003c0s37b0n0: [2024-03-28 13:06:52,238] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 12049.92 | bwd_microstep: 34137.40 | bwd_inner_microstep: 33976.45 | bwd_allreduce_microstep: 160.88 | step_microstep: 7210.69
x3003c0s37b0n0: [2024-03-28 13:06:52,238] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 12049.91 | bwd: 34137.40 | bwd_inner: 33976.44 | bwd_allreduce: 160.90 | step: 7210.69
x3003c0s37b0n0: [2024-03-28 13:06:52,368] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 13:06:52,369] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 10.91 GB         CA 10.02 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:06:52,369] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.04 GB, percent = 67.6%
x3003c0s7b0n0: <TIMER:interval-time,54.08738374710083><TIMER:interval-time,54.087385416030884><TIMER:interval-time,54.087382793426514><TIMER:interval-time,54.08739137649536>
x3003c0s7b0n0: 
x3003c0s7b0n0: 
x3003c0s7b0n0: 
x3003c0s37b0n0: <TIMER:interval-time,54.08733320236206><TIMER:interval-time,54.087334394454956><TIMER:interval-time,54.087334394454956><TIMER:interval-time,54.08733415603638>
x3003c0s37b0n0: 
x3003c0s37b0n0: 
x3003c0s37b0n0: 
x3003c0s7b1n0: <TIMER:interval-time,54.08732271194458>
x3003c0s7b1n0: <TIMER:interval-time,54.087326526641846>
x3003c0s37b1n0: <TIMER:interval-time,54.08735799789429><TIMER:interval-time,54.0873601436615><TIMER:interval-time,54.08736062049866>
x3003c0s37b1n0: 
x3003c0s37b1n0: 
x3003c0s7b1n0: <TIMER:interval-time,54.08743405342102>
x3003c0s7b1n0: <TIMER:interval-time,54.08744025230408>
x3003c0s37b1n0: <TIMER:interval-time,54.08747124671936>
x3003c0s7b1n0:  elapsed_time 54.087440 | consumed samples:          320 | consumed tokens:       655360 | elapsed time per iteration (ms): 54087.4 | learning rate: 1.884E-04 | global batch size:    64 | lm loss: 1.722123E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.183 | TFLOPs: 81.69 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 13:06:52,500] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 13:06:52,500] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 9.37 GB         CA 10.02 GB         Max_CA 10 GB 
x3003c0s37b0n0: [2024-03-28 13:06:52,500] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.05 GB, percent = 67.6%
x3003c0s37b0n0: [2024-03-28 13:07:04,631] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 13:07:04,632] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 27.7 GB         CA 31.61 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:07:04,632] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.01 GB, percent = 67.6%
x3003c0s37b0n0: [2024-03-28 13:07:04,715] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 13:07:04,715] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 24.58 GB         CA 25.29 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:07:04,715] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.01 GB, percent = 67.6%
x3003c0s37b0n0: [2024-03-28 13:07:39,260] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 13:07:39,260] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 28.66 GB         CA 11.56 GB         Max_CA 36 GB 
x3003c0s37b0n0: [2024-03-28 13:07:39,260] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.87 GB, percent = 67.5%
x3003c0s37b0n0: [2024-03-28 13:07:39,335] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 13:07:39,336] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 10.91 GB         CA 11.56 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:07:39,336] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.87 GB, percent = 67.5%
x3003c0s7b1n0: [2024-03-28 13:07:46,123] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.756105422973633
x3003c0s7b1n0: [2024-03-28 13:07:46,123] [INFO] [stage3.py:2251:step] Full outer step loop took 6.756649017333984
x3003c0s37b1n0: [2024-03-28 13:07:46,232] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.866212844848633
x3003c0s37b1n0: [2024-03-28 13:07:46,233] [INFO] [stage3.py:2251:step] Full outer step loop took 6.866863012313843
x3003c0s37b0n0: [2024-03-28 13:07:46,250] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.883469104766846
x3003c0s37b0n0: [2024-03-28 13:07:46,250] [INFO] [stage3.py:2251:step] Full outer step loop took 6.8836987018585205
x3003c0s7b1n0: [2024-03-28 13:07:46,277] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.910356521606445
x3003c0s7b1n0: [2024-03-28 13:07:46,278] [INFO] [stage3.py:2251:step] Full outer step loop took 6.911217451095581
x3003c0s7b0n0: [2024-03-28 13:07:46,299] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.93250584602356
x3003c0s7b0n0: [2024-03-28 13:07:46,299] [INFO] [stage3.py:2251:step] Full outer step loop took 6.932671308517456
x3003c0s7b1n0: [2024-03-28 13:07:46,315] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.948355436325073
x3003c0s7b1n0: [2024-03-28 13:07:46,315] [INFO] [stage3.py:2251:step] Full outer step loop took 6.948541164398193
x3003c0s37b1n0: [2024-03-28 13:07:46,343] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.976508855819702
x3003c0s37b1n0: [2024-03-28 13:07:46,343] [INFO] [stage3.py:2251:step] Full outer step loop took 6.976915121078491
x3003c0s7b1n0: [2024-03-28 13:07:46,354] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.987432241439819
x3003c0s7b1n0: [2024-03-28 13:07:46,354] [INFO] [stage3.py:2251:step] Full outer step loop took 6.987584829330444
x3003c0s37b0n0: [2024-03-28 13:07:46,376] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.010074138641357
x3003c0s37b0n0: [2024-03-28 13:07:46,377] [INFO] [stage3.py:2251:step] Full outer step loop took 7.010470628738403
x3003c0s37b0n0: [2024-03-28 13:07:46,384] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.017492055892944
x3003c0s37b0n0: [2024-03-28 13:07:46,384] [INFO] [stage3.py:2251:step] Full outer step loop took 7.017838954925537
x3003c0s37b0n0: [2024-03-28 13:07:46,390] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.02390456199646
x3003c0s37b0n0: [2024-03-28 13:07:46,390] [INFO] [stage3.py:2251:step] Full outer step loop took 7.0240561962127686
x3003c0s37b1n0: [2024-03-28 13:07:46,391] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.02510929107666
x3003c0s37b1n0: [2024-03-28 13:07:46,391] [INFO] [stage3.py:2251:step] Full outer step loop took 7.025257349014282
x3003c0s37b1n0: [2024-03-28 13:07:46,419] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.0532331466674805
x3003c0s37b1n0: [2024-03-28 13:07:46,420] [INFO] [stage3.py:2251:step] Full outer step loop took 7.053396224975586
x3003c0s7b0n0: [2024-03-28 13:07:46,513] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.146220922470093
x3003c0s7b0n0: [2024-03-28 13:07:46,513] [INFO] [stage3.py:2251:step] Full outer step loop took 7.146451234817505
x3003c0s7b0n0: [2024-03-28 13:07:46,581] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.215051174163818
x3003c0s7b0n0: [2024-03-28 13:07:46,582] [INFO] [stage3.py:2251:step] Full outer step loop took 7.215234279632568
x3003c0s7b0n0: [2024-03-28 13:07:46,618] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.251265287399292
x3003c0s7b0n0: [2024-03-28 13:07:46,618] [INFO] [stage3.py:2251:step] Full outer step loop took 7.251472473144531
x3003c0s7b1n0: [2024-03-28 13:07:46,645] [INFO] [stage3.py:2277:step] End to end step took 7.2781829833984375
x3003c0s7b0n0: [2024-03-28 13:07:46,645] [INFO] [stage3.py:2277:step] End to end step took 7.278502464294434
x3003c0s7b0n0: [2024-03-28 13:07:46,645] [INFO] [stage3.py:2277:step] End to end step took 7.278500080108643
x3003c0s37b1n0: [2024-03-28 13:07:46,645] [INFO] [stage3.py:2277:step] End to end step took 7.278483867645264
x3003c0s37b0n0: [2024-03-28 13:07:46,645] [INFO] [stage3.py:2277:step] End to end step took 7.278552770614624
x3003c0s37b0n0: [2024-03-28 13:07:46,645] [INFO] [stage3.py:2277:step] End to end step took 7.2785797119140625
x3003c0s7b1n0: [2024-03-28 13:07:46,645] [INFO] [stage3.py:2277:step] End to end step took 7.278705358505249
x3003c0s37b1n0: [2024-03-28 13:07:46,645] [INFO] [stage3.py:2277:step] End to end step took 7.278715372085571
x3003c0s37b0n0: [2024-03-28 13:07:46,645] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6883.88
x3003c0s37b0n0: [2024-03-28 13:07:46,645] [INFO] [stage3.py:2277:step] End to end step took 7.279080867767334
x3003c0s7b1n0: [2024-03-28 13:07:46,645] [INFO] [stage3.py:2277:step] End to end step took 7.278899431228638
x3003c0s37b1n0: [2024-03-28 13:07:46,645] [INFO] [stage3.py:2277:step] End to end step took 7.278960704803467
x3003c0s7b1n0: [2024-03-28 13:07:46,645] [INFO] [stage3.py:2277:step] End to end step took 7.279044151306152
x3003c0s37b1n0: [2024-03-28 13:07:46,645] [INFO] [stage3.py:2277:step] End to end step took 7.279102087020874
x3003c0s7b0n0: [2024-03-28 13:07:46,645] [INFO] [stage3.py:2277:step] End to end step took 7.279072284698486
x3003c0s7b0n0: [2024-03-28 13:07:46,645] [INFO] [stage3.py:2277:step] End to end step took 7.279094696044922
x3003c0s37b0n0: [2024-03-28 13:07:46,645] [INFO] [stage3.py:2277:step] End to end step took 7.279204368591309
x3003c0s37b0n0: [2024-03-28 13:07:46,646] [INFO] [logging.py:96:log_dist] [Rank 0] step=6, skipped=0, lr=[0.0001415574960149644], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 13:07:46,646] [INFO] [timer.py:260:stop] epoch=0/micro_step=6/global_step=6, RunningAvgSamplesPerSec=1.1770906735224895, CurrSamplesPerSec=1.1820321432495027, MemAllocated=9.37GB, MaxMemAllocated=10.91GB
x3003c0s37b0n0: [2024-03-28 13:07:46,646] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 12108.42 | bwd_microstep: 34344.27 | bwd_inner_microstep: 34199.39 | bwd_allreduce_microstep: 144.81 | step_microstep: 7309.92
x3003c0s37b0n0: [2024-03-28 13:07:46,646] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 12108.41 | bwd: 34344.27 | bwd_inner: 34199.38 | bwd_allreduce: 144.82 | step: 7309.92
x3003c0s37b0n0: [2024-03-28 13:07:46,778] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 13:07:46,779] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 10.91 GB         CA 10.02 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:07:46,779] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.8 GB, percent = 67.5%
x3003c0s37b0n0: <TIMER:interval-time,54.40941548347473>
x3003c0s37b0n0: <TIMER:interval-time,54.409427642822266>
x3003c0s37b0n0: <TIMER:interval-time,54.40946435928345>
x3003c0s7b0n0: <TIMER:interval-time,54.40946626663208><TIMER:interval-time,54.40946888923645><TIMER:interval-time,54.40947341918945>
x3003c0s7b0n0: <TIMER:interval-time,54.40947365760803>
x3003c0s7b0n0: 
x3003c0s7b0n0: 
x3003c0s7b1n0: <TIMER:interval-time,54.40944838523865><TIMER:interval-time,54.409443855285645>
x3003c0s7b1n0: 
x3003c0s7b1n0: <TIMER:interval-time,54.40945649147034><TIMER:interval-time,54.40945816040039>
x3003c0s7b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,54.4093759059906><TIMER:interval-time,54.40937542915344><TIMER:interval-time,54.40937662124634>
x3003c0s37b1n0: 
x3003c0s37b1n0: 
x3003c0s37b0n0: <TIMER:interval-time,54.40953993797302>
x3003c0s37b1n0: <TIMER:interval-time,54.409494400024414>
x3003c0s7b1n0:  elapsed_time 54.409444 | consumed samples:          384 | consumed tokens:       786432 | elapsed time per iteration (ms): 54409.4 | learning rate: 1.416E-04 | global batch size:    64 | lm loss: 1.545124E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.176 | TFLOPs: 81.20 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 13:07:46,917] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 13:07:46,918] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 9.37 GB         CA 10.02 GB         Max_CA 10 GB 
x3003c0s37b0n0: [2024-03-28 13:07:46,918] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.81 GB, percent = 67.5%
x3003c0s37b0n0: [2024-03-28 13:07:59,515] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 13:07:59,516] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 27.7 GB         CA 31.61 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:07:59,516] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.78 GB, percent = 67.5%
x3003c0s37b0n0: [2024-03-28 13:07:59,600] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 13:07:59,600] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 24.58 GB         CA 25.29 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:07:59,600] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 339.78 GB, percent = 67.5%
x3003c0s37b0n0: [2024-03-28 13:08:34,357] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 13:08:34,357] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 28.66 GB         CA 11.56 GB         Max_CA 36 GB 
x3003c0s37b0n0: [2024-03-28 13:08:34,357] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.22 GB, percent = 67.6%
x3003c0s37b0n0: [2024-03-28 13:08:34,433] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 13:08:34,434] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 10.91 GB         CA 11.56 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:08:34,434] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.22 GB, percent = 67.6%
x3003c0s37b0n0: [2024-03-28 13:08:41,186] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.7219507694244385
x3003c0s37b0n0: [2024-03-28 13:08:41,186] [INFO] [stage3.py:2251:step] Full outer step loop took 6.722151517868042
x3003c0s7b1n0: [2024-03-28 13:08:41,316] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.851349830627441
x3003c0s7b1n0: [2024-03-28 13:08:41,316] [INFO] [stage3.py:2251:step] Full outer step loop took 6.852165460586548
x3003c0s37b1n0: [2024-03-28 13:08:41,365] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.901113748550415
x3003c0s37b1n0: [2024-03-28 13:08:41,366] [INFO] [stage3.py:2251:step] Full outer step loop took 6.901302814483643
x3003c0s37b1n0: [2024-03-28 13:08:41,409] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.945152521133423
x3003c0s37b1n0: [2024-03-28 13:08:41,410] [INFO] [stage3.py:2251:step] Full outer step loop took 6.945363283157349
x3003c0s37b1n0: [2024-03-28 13:08:41,422] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.956403732299805
x3003c0s37b1n0: [2024-03-28 13:08:41,425] [INFO] [stage3.py:2251:step] Full outer step loop took 6.960469961166382
x3003c0s37b0n0: [2024-03-28 13:08:41,460] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.995367050170898
x3003c0s37b0n0: [2024-03-28 13:08:41,460] [INFO] [stage3.py:2251:step] Full outer step loop took 6.995625972747803
x3003c0s37b0n0: [2024-03-28 13:08:41,468] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.003507137298584
x3003c0s37b0n0: [2024-03-28 13:08:41,468] [INFO] [stage3.py:2251:step] Full outer step loop took 7.003671407699585
x3003c0s37b0n0: [2024-03-28 13:08:41,491] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.027031183242798
x3003c0s37b0n0: [2024-03-28 13:08:41,492] [INFO] [stage3.py:2251:step] Full outer step loop took 7.0273072719573975
x3003c0s37b1n0: [2024-03-28 13:08:41,496] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.031344652175903
x3003c0s37b1n0: [2024-03-28 13:08:41,496] [INFO] [stage3.py:2251:step] Full outer step loop took 7.031489133834839
x3003c0s7b1n0: [2024-03-28 13:08:41,558] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.093971252441406
x3003c0s7b1n0: [2024-03-28 13:08:41,562] [INFO] [stage3.py:2251:step] Full outer step loop took 7.097222089767456
x3003c0s7b1n0: [2024-03-28 13:08:41,594] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.129801273345947
x3003c0s7b1n0: [2024-03-28 13:08:41,594] [INFO] [stage3.py:2251:step] Full outer step loop took 7.1299824714660645
x3003c0s7b1n0: [2024-03-28 13:08:41,599] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.135120153427124
x3003c0s7b1n0: [2024-03-28 13:08:41,600] [INFO] [stage3.py:2251:step] Full outer step loop took 7.1352763175964355
x3003c0s7b0n0: [2024-03-28 13:08:41,784] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.320035219192505
x3003c0s7b0n0: [2024-03-28 13:08:41,785] [INFO] [stage3.py:2251:step] Full outer step loop took 7.320233345031738
x3003c0s7b0n0: [2024-03-28 13:08:41,794] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.3298211097717285
x3003c0s7b0n0: [2024-03-28 13:08:41,794] [INFO] [stage3.py:2251:step] Full outer step loop took 7.33000922203064
x3003c0s7b0n0: [2024-03-28 13:08:41,820] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.355660676956177
x3003c0s7b0n0: [2024-03-28 13:08:41,820] [INFO] [stage3.py:2251:step] Full outer step loop took 7.3558433055877686
x3003c0s7b0n0: [2024-03-28 13:08:41,822] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.357872724533081
x3003c0s7b0n0: [2024-03-28 13:08:41,822] [INFO] [stage3.py:2251:step] Full outer step loop took 7.358011245727539
x3003c0s7b0n0: [2024-03-28 13:08:41,845] [INFO] [stage3.py:2277:step] End to end step took 7.3808252811431885
x3003c0s37b1n0: [2024-03-28 13:08:41,845] [INFO] [stage3.py:2277:step] End to end step took 7.3808839321136475
x3003c0s7b1n0: [2024-03-28 13:08:41,845] [INFO] [stage3.py:2277:step] End to end step took 7.380899667739868
x3003c0s37b0n0: [2024-03-28 13:08:41,845] [INFO] [stage3.py:2277:step] End to end step took 7.381005048751831
x3003c0s37b0n0: [2024-03-28 13:08:41,845] [INFO] [stage3.py:2277:step] End to end step took 7.381067991256714
x3003c0s7b0n0: [2024-03-28 13:08:41,845] [INFO] [stage3.py:2277:step] End to end step took 7.381060600280762
x3003c0s7b0n0: [2024-03-28 13:08:41,845] [INFO] [stage3.py:2277:step] End to end step took 7.381056547164917
x3003c0s7b1n0: [2024-03-28 13:08:41,845] [INFO] [stage3.py:2277:step] End to end step took 7.381120920181274
x3003c0s7b0n0: [2024-03-28 13:08:41,846] [INFO] [stage3.py:2277:step] End to end step took 7.381220817565918
x3003c0s7b1n0: [2024-03-28 13:08:41,845] [INFO] [stage3.py:2277:step] End to end step took 7.381256103515625
x3003c0s37b0n0: [2024-03-28 13:08:41,846] [INFO] [stage3.py:2277:step] End to end step took 7.3812477588653564
x3003c0s7b1n0: [2024-03-28 13:08:41,846] [INFO] [stage3.py:2277:step] End to end step took 7.381546258926392
x3003c0s37b1n0: [2024-03-28 13:08:41,846] [INFO] [stage3.py:2277:step] End to end step took 7.3816211223602295
x3003c0s37b0n0: [2024-03-28 13:08:41,846] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6722.30
x3003c0s37b0n0: [2024-03-28 13:08:41,846] [INFO] [stage3.py:2277:step] End to end step took 7.381882190704346
x3003c0s37b1n0: [2024-03-28 13:08:41,846] [INFO] [stage3.py:2277:step] End to end step took 7.3817338943481445
x3003c0s37b1n0: [2024-03-28 13:08:41,846] [INFO] [stage3.py:2277:step] End to end step took 7.3817360401153564
x3003c0s37b0n0: [2024-03-28 13:08:41,847] [INFO] [logging.py:96:log_dist] [Rank 0] step=7, skipped=0, lr=[9.750000000000001e-05], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 13:08:41,847] [INFO] [timer.py:260:stop] epoch=0/micro_step=7/global_step=7, RunningAvgSamplesPerSec=1.1746896194004484, CurrSamplesPerSec=1.1651825645491205, MemAllocated=9.37GB, MaxMemAllocated=10.91GB
x3003c0s37b0n0: [2024-03-28 13:08:41,847] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 12574.50 | bwd_microstep: 34586.02 | bwd_inner_microstep: 34442.17 | bwd_allreduce_microstep: 143.78 | step_microstep: 7413.07
x3003c0s37b0n0: [2024-03-28 13:08:41,847] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 12574.49 | bwd: 34586.02 | bwd_inner: 34442.16 | bwd_allreduce: 143.80 | step: 7413.07
x3003c0s37b0n0: [2024-03-28 13:08:41,976] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 13:08:41,976] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 10.91 GB         CA 10.02 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:08:41,977] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.18 GB, percent = 67.6%
x3003c0s37b0n0: <TIMER:interval-time,55.197367429733276><TIMER:interval-time,55.1973602771759><TIMER:interval-time,55.197370290756226><TIMER:interval-time,55.19736957550049>
x3003c0s37b0n0: 
x3003c0s37b0n0: 
x3003c0s37b0n0: 
x3003c0s7b0n0: <TIMER:interval-time,55.19737720489502><TIMER:interval-time,55.1973774433136><TIMER:interval-time,55.1973774433136>
x3003c0s7b0n0: 
x3003c0s7b0n0: 
x3003c0s37b1n0: <TIMER:interval-time,55.197511196136475>
x3003c0s37b1n0: <TIMER:interval-time,55.19752383232117>
x3003c0s37b1n0: <TIMER:interval-time,55.197529315948486>
x3003c0s7b0n0: <TIMER:interval-time,55.19746398925781>
x3003c0s7b1n0: <TIMER:interval-time,55.19738054275513><TIMER:interval-time,55.19737768173218><TIMER:interval-time,55.19738245010376><TIMER:interval-time,55.197381019592285>
x3003c0s7b1n0: 
x3003c0s7b1n0: 
x3003c0s7b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,55.19762468338013>
x3003c0s7b1n0:  elapsed_time 55.197382 | consumed samples:          448 | consumed tokens:       917504 | elapsed time per iteration (ms): 55197.4 | learning rate: 9.750E-05 | global batch size:    64 | lm loss: 1.501596E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.159 | TFLOPs: 80.05 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 13:08:42,131] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 13:08:42,132] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 9.37 GB         CA 10.02 GB         Max_CA 10 GB 
x3003c0s37b0n0: [2024-03-28 13:08:42,132] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.19 GB, percent = 67.6%
x3003c0s37b0n0: [2024-03-28 13:08:54,991] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 13:08:54,991] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 27.7 GB         CA 31.61 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:08:54,992] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.15 GB, percent = 67.6%
x3003c0s37b0n0: [2024-03-28 13:08:55,080] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 13:08:55,081] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 24.58 GB         CA 25.29 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:08:55,081] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.15 GB, percent = 67.6%
x3003c0s37b0n0: [2024-03-28 13:09:29,616] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 13:09:29,617] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 28.66 GB         CA 11.56 GB         Max_CA 36 GB 
x3003c0s37b0n0: [2024-03-28 13:09:29,617] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.38 GB, percent = 67.6%
x3003c0s37b0n0: [2024-03-28 13:09:29,691] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 13:09:29,692] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 10.91 GB         CA 11.56 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:09:29,692] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.38 GB, percent = 67.6%
x3003c0s37b0n0: [2024-03-28 13:09:36,297] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.573200702667236
x3003c0s37b0n0: [2024-03-28 13:09:36,298] [INFO] [stage3.py:2251:step] Full outer step loop took 6.573721885681152
x3003c0s37b0n0: [2024-03-28 13:09:36,602] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.877358913421631
x3003c0s37b0n0: [2024-03-28 13:09:36,603] [INFO] [stage3.py:2251:step] Full outer step loop took 6.877641201019287
x3003c0s7b1n0: [2024-03-28 13:09:36,700] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.9760401248931885
x3003c0s7b1n0: [2024-03-28 13:09:36,701] [INFO] [stage3.py:2251:step] Full outer step loop took 6.976691246032715
x3003c0s37b0n0: [2024-03-28 13:09:36,714] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.989919185638428
x3003c0s37b0n0: [2024-03-28 13:09:36,714] [INFO] [stage3.py:2251:step] Full outer step loop took 6.9902472496032715
x3003c0s37b1n0: [2024-03-28 13:09:36,723] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.998607158660889
x3003c0s37b1n0: [2024-03-28 13:09:36,724] [INFO] [stage3.py:2251:step] Full outer step loop took 6.999561071395874
x3003c0s37b0n0: [2024-03-28 13:09:36,759] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.034400463104248
x3003c0s37b0n0: [2024-03-28 13:09:36,759] [INFO] [stage3.py:2251:step] Full outer step loop took 7.034666538238525
x3003c0s37b1n0: [2024-03-28 13:09:36,812] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.087897777557373
x3003c0s37b1n0: [2024-03-28 13:09:36,812] [INFO] [stage3.py:2251:step] Full outer step loop took 7.088101387023926
x3003c0s7b0n0: [2024-03-28 13:09:36,817] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.093016147613525
x3003c0s7b0n0: [2024-03-28 13:09:36,817] [INFO] [stage3.py:2251:step] Full outer step loop took 7.093233346939087
x3003c0s7b0n0: [2024-03-28 13:09:36,830] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.105286359786987
x3003c0s7b0n0: [2024-03-28 13:09:36,830] [INFO] [stage3.py:2251:step] Full outer step loop took 7.10607647895813
x3003c0s7b1n0: [2024-03-28 13:09:36,833] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.108360290527344
x3003c0s7b1n0: [2024-03-28 13:09:36,833] [INFO] [stage3.py:2251:step] Full outer step loop took 7.108556270599365
x3003c0s7b1n0: [2024-03-28 13:09:36,835] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.110398054122925
x3003c0s7b1n0: [2024-03-28 13:09:36,835] [INFO] [stage3.py:2251:step] Full outer step loop took 7.110585927963257
x3003c0s7b1n0: [2024-03-28 13:09:36,837] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.1126556396484375
x3003c0s7b1n0: [2024-03-28 13:09:36,837] [INFO] [stage3.py:2251:step] Full outer step loop took 7.112805604934692
x3003c0s37b1n0: [2024-03-28 13:09:36,856] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.131399631500244
x3003c0s37b1n0: [2024-03-28 13:09:36,856] [INFO] [stage3.py:2251:step] Full outer step loop took 7.132117509841919
x3003c0s37b1n0: [2024-03-28 13:09:36,865] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.140397787094116
x3003c0s37b1n0: [2024-03-28 13:09:36,865] [INFO] [stage3.py:2251:step] Full outer step loop took 7.140563726425171
x3003c0s7b0n0: [2024-03-28 13:09:36,884] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.159745931625366
x3003c0s7b0n0: [2024-03-28 13:09:36,884] [INFO] [stage3.py:2251:step] Full outer step loop took 7.159938812255859
x3003c0s7b0n0: [2024-03-28 13:09:36,911] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.187054634094238
x3003c0s7b0n0: [2024-03-28 13:09:36,911] [INFO] [stage3.py:2251:step] Full outer step loop took 7.187206029891968
x3003c0s7b0n0: [2024-03-28 13:09:36,934] [INFO] [stage3.py:2277:step] End to end step took 7.209942102432251
x3003c0s37b1n0: [2024-03-28 13:09:36,934] [INFO] [stage3.py:2277:step] End to end step took 7.2099244594573975
x3003c0s37b0n0: [2024-03-28 13:09:36,934] [INFO] [stage3.py:2277:step] End to end step took 7.209982872009277
x3003c0s37b1n0: [2024-03-28 13:09:36,934] [INFO] [stage3.py:2277:step] End to end step took 7.2100160121917725
x3003c0s7b1n0: [2024-03-28 13:09:36,934] [INFO] [stage3.py:2277:step] End to end step took 7.209980487823486
x3003c0s7b1n0: [2024-03-28 13:09:36,934] [INFO] [stage3.py:2277:step] End to end step took 7.210005044937134
x3003c0s7b0n0: [2024-03-28 13:09:36,934] [INFO] [stage3.py:2277:step] End to end step took 7.210036516189575
x3003c0s7b1n0: [2024-03-28 13:09:36,934] [INFO] [stage3.py:2277:step] End to end step took 7.210068464279175
x3003c0s37b0n0: [2024-03-28 13:09:36,934] [INFO] [stage3.py:2277:step] End to end step took 7.210259199142456
x3003c0s37b1n0: [2024-03-28 13:09:36,934] [INFO] [stage3.py:2277:step] End to end step took 7.210322380065918
x3003c0s37b0n0: [2024-03-28 13:09:36,934] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6573.95
x3003c0s7b0n0: [2024-03-28 13:09:36,935] [INFO] [stage3.py:2277:step] End to end step took 7.210494041442871
x3003c0s37b0n0: [2024-03-28 13:09:36,935] [INFO] [stage3.py:2277:step] End to end step took 7.209868907928467
x3003c0s37b0n0: [2024-03-28 13:09:36,935] [INFO] [stage3.py:2277:step] End to end step took 7.210644245147705
x3003c0s7b0n0: [2024-03-28 13:09:36,935] [INFO] [stage3.py:2277:step] End to end step took 7.210601091384888
x3003c0s7b1n0: [2024-03-28 13:09:36,935] [INFO] [stage3.py:2277:step] End to end step took 7.210643529891968
x3003c0s37b1n0: [2024-03-28 13:09:36,935] [INFO] [stage3.py:2277:step] End to end step took 7.210733413696289
x3003c0s37b0n0: [2024-03-28 13:09:36,935] [INFO] [logging.py:96:log_dist] [Rank 0] step=8, skipped=0, lr=[6.158400017893797e-05], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 13:09:36,936] [INFO] [timer.py:260:stop] epoch=0/micro_step=8/global_step=8, RunningAvgSamplesPerSec=1.173544546092997, CurrSamplesPerSec=1.1678525032485527, MemAllocated=9.37GB, MaxMemAllocated=10.91GB
x3003c0s37b0n0: [2024-03-28 13:09:36,936] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 12826.51 | bwd_microstep: 34346.74 | bwd_inner_microstep: 34192.87 | bwd_allreduce_microstep: 153.80 | step_microstep: 7243.57
x3003c0s37b0n0: [2024-03-28 13:09:36,936] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 12826.50 | bwd: 34346.74 | bwd_inner: 34192.86 | bwd_allreduce: 153.81 | step: 7243.57
x3003c0s37b0n0: [2024-03-28 13:09:37,070] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 13:09:37,071] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 10.91 GB         CA 10.02 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:09:37,071] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.41 GB, percent = 67.6%
x3003c0s37b0n0: <TIMER:interval-time,55.0935697555542><TIMER:interval-time,55.09357285499573><TIMER:interval-time,55.093573331832886>
x3003c0s37b0n0: 
x3003c0s37b0n0: 
x3003c0s7b0n0: <TIMER:interval-time,55.093581438064575><TIMER:interval-time,55.093585729599>
x3003c0s7b0n0: 
x3003c0s7b0n0: <TIMER:interval-time,55.093591928482056><TIMER:interval-time,55.093589782714844>
x3003c0s7b0n0: 
x3003c0s37b1n0: <TIMER:interval-time,55.09360671043396>
x3003c0s37b1n0: <TIMER:interval-time,55.09360861778259><TIMER:interval-time,55.09360933303833>
x3003c0s37b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,55.09361457824707>
x3003c0s7b1n0: <TIMER:interval-time,55.09338021278381>
x3003c0s7b1n0: <TIMER:interval-time,55.09338355064392>
x3003c0s7b1n0: <TIMER:interval-time,55.09338688850403><TIMER:interval-time,55.09338688850403>
x3003c0s7b1n0: 
x3003c0s37b0n0: <TIMER:interval-time,55.09367918968201>
x3003c0s7b1n0:  elapsed_time 55.093387 | consumed samples:          512 | consumed tokens:      1048576 | elapsed time per iteration (ms): 55093.4 | learning rate: 6.158E-05 | global batch size:    64 | lm loss: 1.323451E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.162 | TFLOPs: 80.20 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 13:09:37,198] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 13:09:37,198] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 9.37 GB         CA 10.02 GB         Max_CA 10 GB 
x3003c0s37b0n0: [2024-03-28 13:09:37,198] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.42 GB, percent = 67.6%
x3003c0s37b0n0: [2024-03-28 13:09:49,422] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 13:09:49,422] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 27.7 GB         CA 31.61 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:09:49,423] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.39 GB, percent = 67.6%
x3003c0s37b0n0: [2024-03-28 13:09:49,512] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 13:09:49,513] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 24.58 GB         CA 25.29 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:09:49,513] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.39 GB, percent = 67.6%
x3003c0s37b0n0: [2024-03-28 13:10:24,059] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 13:10:24,060] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 28.66 GB         CA 11.56 GB         Max_CA 36 GB 
x3003c0s37b0n0: [2024-03-28 13:10:24,060] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.11 GB, percent = 67.6%
x3003c0s37b0n0: [2024-03-28 13:10:24,154] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 13:10:24,155] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 10.91 GB         CA 11.56 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:10:24,155] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.11 GB, percent = 67.6%
x3003c0s37b1n0: [2024-03-28 13:10:31,128] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.942880153656006
x3003c0s37b1n0: [2024-03-28 13:10:31,128] [INFO] [stage3.py:2251:step] Full outer step loop took 6.943135023117065
x3003c0s37b1n0: [2024-03-28 13:10:31,150] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.9648566246032715
x3003c0s37b1n0: [2024-03-28 13:10:31,150] [INFO] [stage3.py:2251:step] Full outer step loop took 6.965151071548462
x3003c0s37b0n0: [2024-03-28 13:10:31,153] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.967464447021484
x3003c0s37b0n0: [2024-03-28 13:10:31,153] [INFO] [stage3.py:2251:step] Full outer step loop took 6.9677019119262695
x3003c0s37b1n0: [2024-03-28 13:10:31,172] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.986929178237915
x3003c0s37b1n0: [2024-03-28 13:10:31,172] [INFO] [stage3.py:2251:step] Full outer step loop took 6.987176179885864
x3003c0s37b1n0: [2024-03-28 13:10:31,182] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.9961559772491455
x3003c0s37b1n0: [2024-03-28 13:10:31,182] [INFO] [stage3.py:2251:step] Full outer step loop took 6.996339321136475
x3003c0s7b0n0: [2024-03-28 13:10:31,193] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.9945738315582275
x3003c0s7b0n0: [2024-03-28 13:10:31,198] [INFO] [stage3.py:2251:step] Full outer step loop took 6.999253273010254
x3003c0s7b1n0: [2024-03-28 13:10:31,228] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.042459011077881
x3003c0s7b1n0: [2024-03-28 13:10:31,228] [INFO] [stage3.py:2251:step] Full outer step loop took 7.042688608169556
x3003c0s7b0n0: [2024-03-28 13:10:31,238] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.052933931350708
x3003c0s7b0n0: [2024-03-28 13:10:31,239] [INFO] [stage3.py:2251:step] Full outer step loop took 7.0532896518707275
x3003c0s37b0n0: [2024-03-28 13:10:31,248] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.0630714893341064
x3003c0s37b0n0: [2024-03-28 13:10:31,249] [INFO] [stage3.py:2251:step] Full outer step loop took 7.0632781982421875
x3003c0s7b1n0: [2024-03-28 13:10:31,262] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.0766682624816895
x3003c0s7b1n0: [2024-03-28 13:10:31,262] [INFO] [stage3.py:2251:step] Full outer step loop took 7.076985120773315
x3003c0s37b0n0: [2024-03-28 13:10:31,264] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.078999042510986
x3003c0s37b0n0: [2024-03-28 13:10:31,265] [INFO] [stage3.py:2251:step] Full outer step loop took 7.079171180725098
x3003c0s7b0n0: [2024-03-28 13:10:31,271] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.08549427986145
x3003c0s7b0n0: [2024-03-28 13:10:31,271] [INFO] [stage3.py:2251:step] Full outer step loop took 7.085663557052612
x3003c0s7b1n0: [2024-03-28 13:10:31,275] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.0897157192230225
x3003c0s7b1n0: [2024-03-28 13:10:31,275] [INFO] [stage3.py:2251:step] Full outer step loop took 7.089931964874268
x3003c0s37b0n0: [2024-03-28 13:10:31,291] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.1055779457092285
x3003c0s37b0n0: [2024-03-28 13:10:31,291] [INFO] [stage3.py:2251:step] Full outer step loop took 7.105855941772461
x3003c0s7b0n0: [2024-03-28 13:10:31,295] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.109989881515503
x3003c0s7b0n0: [2024-03-28 13:10:31,296] [INFO] [stage3.py:2251:step] Full outer step loop took 7.110256910324097
x3003c0s7b1n0: [2024-03-28 13:10:31,377] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.191858530044556
x3003c0s7b1n0: [2024-03-28 13:10:31,377] [INFO] [stage3.py:2251:step] Full outer step loop took 7.192012310028076
x3003c0s7b1n0: [2024-03-28 13:10:31,400] [INFO] [stage3.py:2277:step] End to end step took 7.214574337005615
x3003c0s7b0n0: [2024-03-28 13:10:31,400] [INFO] [stage3.py:2277:step] End to end step took 7.214580774307251
x3003c0s37b1n0: [2024-03-28 13:10:31,400] [INFO] [stage3.py:2277:step] End to end step took 7.214566230773926
x3003c0s37b1n0: [2024-03-28 13:10:31,400] [INFO] [stage3.py:2277:step] End to end step took 7.214674472808838
x3003c0s37b0n0: [2024-03-28 13:10:31,400] [INFO] [stage3.py:2277:step] End to end step took 7.21463680267334
x3003c0s7b0n0: [2024-03-28 13:10:31,400] [INFO] [stage3.py:2277:step] End to end step took 7.214866399765015
x3003c0s37b0n0: [2024-03-28 13:10:31,400] [INFO] [stage3.py:2277:step] End to end step took 7.214965105056763
x3003c0s7b1n0: [2024-03-28 13:10:31,400] [INFO] [stage3.py:2277:step] End to end step took 7.215089321136475
x3003c0s7b0n0: [2024-03-28 13:10:31,400] [INFO] [stage3.py:2277:step] End to end step took 7.215083360671997
x3003c0s37b0n0: [2024-03-28 13:10:31,400] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6968.72
x3003c0s37b0n0: [2024-03-28 13:10:31,400] [INFO] [stage3.py:2277:step] End to end step took 7.215038299560547
x3003c0s7b1n0: [2024-03-28 13:10:31,400] [INFO] [stage3.py:2277:step] End to end step took 7.215118885040283
x3003c0s37b1n0: [2024-03-28 13:10:31,401] [INFO] [stage3.py:2277:step] End to end step took 7.215209722518921
x3003c0s37b0n0: [2024-03-28 13:10:31,401] [INFO] [stage3.py:2277:step] End to end step took 7.215349197387695
x3003c0s7b1n0: [2024-03-28 13:10:31,401] [INFO] [stage3.py:2277:step] End to end step took 7.215263843536377
x3003c0s37b1n0: [2024-03-28 13:10:31,401] [INFO] [stage3.py:2277:step] End to end step took 7.215262413024902
x3003c0s7b0n0: [2024-03-28 13:10:31,401] [INFO] [stage3.py:2277:step] End to end step took 7.201924085617065
x3003c0s37b0n0: [2024-03-28 13:10:31,401] [INFO] [logging.py:96:log_dist] [Rank 0] step=9, skipped=0, lr=[3.814149619390238e-05], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 13:10:31,401] [INFO] [timer.py:260:stop] epoch=0/micro_step=9/global_step=9, RunningAvgSamplesPerSec=1.1745730325560853, CurrSamplesPerSec=1.180782008511821, MemAllocated=9.37GB, MaxMemAllocated=10.91GB
x3003c0s37b0n0: [2024-03-28 13:10:31,402] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 12192.27 | bwd_microstep: 34332.23 | bwd_inner_microstep: 34175.96 | bwd_allreduce_microstep: 156.19 | step_microstep: 7246.70
x3003c0s37b0n0: [2024-03-28 13:10:31,402] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 12192.26 | bwd: 34332.23 | bwd_inner: 34175.96 | bwd_allreduce: 156.21 | step: 7246.70
x3003c0s37b0n0: [2024-03-28 13:10:31,531] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 13:10:31,532] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 10.91 GB         CA 10.02 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:10:31,532] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.15 GB, percent = 67.6%
x3003c0s37b0n0: <TIMER:interval-time,54.46097421646118><TIMER:interval-time,54.46099591255188><TIMER:interval-time,54.46099591255188>
x3003c0s7b0n0: <TIMER:interval-time,54.46108913421631><TIMER:interval-time,54.46108865737915>
x3003c0s7b0n0: <TIMER:interval-time,54.46108865737915><TIMER:interval-time,54.461092472076416>
x3003c0s7b0n0: 
x3003c0s7b0n0: 
x3003c0s37b0n0: 
x3003c0s37b0n0: 
x3003c0s37b0n0: <TIMER:interval-time,54.46100640296936>
x3003c0s7b1n0: <TIMER:interval-time,54.46104669570923><TIMER:interval-time,54.46104550361633><TIMER:interval-time,54.46104979515076><TIMER:interval-time,54.46105122566223>
x3003c0s7b1n0: 
x3003c0s7b1n0: 
x3003c0s7b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,54.461105823516846><TIMER:interval-time,54.46110677719116>
x3003c0s37b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,54.461108446121216>
x3003c0s37b1n0: <TIMER:interval-time,54.46111249923706>
x3003c0s7b1n0:  elapsed_time 54.461050 | consumed samples:          576 | consumed tokens:      1179648 | elapsed time per iteration (ms): 54461.0 | learning rate: 3.814E-05 | global batch size:    64 | lm loss: 1.256594E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.175 | TFLOPs: 81.13 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 13:10:31,664] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 13:10:31,664] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 9.37 GB         CA 10.02 GB         Max_CA 10 GB 
x3003c0s37b0n0: [2024-03-28 13:10:31,665] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.17 GB, percent = 67.6%
x3003c0s37b0n0: [2024-03-28 13:10:45,472] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 13:10:45,473] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 27.7 GB         CA 31.61 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:10:45,473] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.14 GB, percent = 67.6%
x3003c0s37b0n0: [2024-03-28 13:10:45,555] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 13:10:45,556] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 24.58 GB         CA 25.29 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:10:45,556] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.14 GB, percent = 67.6%
x3003c0s37b0n0: [2024-03-28 13:11:20,114] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 13:11:20,115] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 28.66 GB         CA 11.56 GB         Max_CA 36 GB 
x3003c0s37b0n0: [2024-03-28 13:11:20,115] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.24 GB, percent = 67.6%
x3003c0s37b0n0: [2024-03-28 13:11:20,192] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 13:11:20,192] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 10.91 GB         CA 11.56 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:11:20,193] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.24 GB, percent = 67.6%
x3003c0s37b0n0: [2024-03-28 13:11:26,741] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.503001928329468
x3003c0s37b0n0: [2024-03-28 13:11:26,742] [INFO] [stage3.py:2251:step] Full outer step loop took 6.5040788650512695
x3003c0s7b0n0: [2024-03-28 13:11:26,941] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.718230724334717
x3003c0s7b0n0: [2024-03-28 13:11:26,941] [INFO] [stage3.py:2251:step] Full outer step loop took 6.718524217605591
x3003c0s37b1n0: [2024-03-28 13:11:27,110] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.887588977813721
x3003c0s37b1n0: [2024-03-28 13:11:27,110] [INFO] [stage3.py:2251:step] Full outer step loop took 6.887875080108643
x3003c0s37b1n0: [2024-03-28 13:11:27,122] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.899175405502319
x3003c0s37b1n0: [2024-03-28 13:11:27,122] [INFO] [stage3.py:2251:step] Full outer step loop took 6.899357557296753
x3003c0s37b0n0: [2024-03-28 13:11:27,133] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.910131216049194
x3003c0s37b0n0: [2024-03-28 13:11:27,133] [INFO] [stage3.py:2251:step] Full outer step loop took 6.910337209701538
x3003c0s37b0n0: [2024-03-28 13:11:27,189] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.96704888343811
x3003c0s37b0n0: [2024-03-28 13:11:27,190] [INFO] [stage3.py:2251:step] Full outer step loop took 6.967992067337036
x3003c0s37b0n0: [2024-03-28 13:11:27,232] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.009962558746338
x3003c0s37b0n0: [2024-03-28 13:11:27,233] [INFO] [stage3.py:2251:step] Full outer step loop took 7.010131597518921
x3003c0s7b0n0: [2024-03-28 13:11:27,258] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.0359766483306885
x3003c0s7b0n0: [2024-03-28 13:11:27,259] [INFO] [stage3.py:2251:step] Full outer step loop took 7.03623366355896
x3003c0s37b1n0: [2024-03-28 13:11:27,295] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.072587013244629
x3003c0s37b1n0: [2024-03-28 13:11:27,295] [INFO] [stage3.py:2251:step] Full outer step loop took 7.072747230529785
x3003c0s37b1n0: [2024-03-28 13:11:27,295] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.07292103767395
x3003c0s37b1n0: [2024-03-28 13:11:27,295] [INFO] [stage3.py:2251:step] Full outer step loop took 7.073067665100098
x3003c0s7b1n0: [2024-03-28 13:11:27,296] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.072927236557007
x3003c0s7b0n0: [2024-03-28 13:11:27,303] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.080486297607422
x3003c0s7b0n0: [2024-03-28 13:11:27,304] [INFO] [stage3.py:2251:step] Full outer step loop took 7.081882476806641
x3003c0s7b1n0: [2024-03-28 13:11:27,300] [INFO] [stage3.py:2251:step] Full outer step loop took 7.077395677566528
x3003c0s7b1n0: [2024-03-28 13:11:27,365] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.142162799835205
x3003c0s7b1n0: [2024-03-28 13:11:27,365] [INFO] [stage3.py:2251:step] Full outer step loop took 7.142385721206665
x3003c0s7b0n0: [2024-03-28 13:11:27,375] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.152249336242676
x3003c0s7b0n0: [2024-03-28 13:11:27,375] [INFO] [stage3.py:2251:step] Full outer step loop took 7.152408838272095
x3003c0s7b1n0: [2024-03-28 13:11:27,451] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.228180408477783
x3003c0s7b1n0: [2024-03-28 13:11:27,451] [INFO] [stage3.py:2251:step] Full outer step loop took 7.228974103927612
x3003c0s7b1n0: [2024-03-28 13:11:27,459] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.236933708190918
x3003c0s7b1n0: [2024-03-28 13:11:27,459] [INFO] [stage3.py:2251:step] Full outer step loop took 7.237090110778809
x3003c0s7b1n0: [2024-03-28 13:11:27,482] [INFO] [stage3.py:2277:step] End to end step took 7.259358882904053
x3003c0s7b1n0: [2024-03-28 13:11:27,482] [INFO] [stage3.py:2277:step] End to end step took 7.259361505508423
x3003c0s7b0n0: [2024-03-28 13:11:27,482] [INFO] [stage3.py:2277:step] End to end step took 7.25938081741333
x3003c0s37b1n0: [2024-03-28 13:11:27,482] [INFO] [stage3.py:2277:step] End to end step took 7.259512662887573
x3003c0s7b0n0: [2024-03-28 13:11:27,482] [INFO] [stage3.py:2277:step] End to end step took 7.2597129344940186
x3003c0s37b0n0: [2024-03-28 13:11:27,482] [INFO] [stage3.py:2277:step] End to end step took 7.25987434387207
x3003c0s7b0n0: [2024-03-28 13:11:27,482] [INFO] [stage3.py:2277:step] End to end step took 7.25986123085022
x3003c0s37b0n0: [2024-03-28 13:11:27,482] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6505.04
x3003c0s7b1n0: [2024-03-28 13:11:27,482] [INFO] [stage3.py:2277:step] End to end step took 7.260102272033691
x3003c0s7b1n0: [2024-03-28 13:11:27,482] [INFO] [stage3.py:2277:step] End to end step took 7.260076999664307
x3003c0s7b0n0: [2024-03-28 13:11:27,482] [INFO] [stage3.py:2277:step] End to end step took 7.260067939758301
x3003c0s37b1n0: [2024-03-28 13:11:27,482] [INFO] [stage3.py:2277:step] End to end step took 7.260053634643555
x3003c0s37b0n0: [2024-03-28 13:11:27,482] [INFO] [stage3.py:2277:step] End to end step took 7.260063648223877
x3003c0s37b0n0: [2024-03-28 13:11:27,483] [INFO] [stage3.py:2277:step] End to end step took 7.245212554931641
x3003c0s37b1n0: [2024-03-28 13:11:27,483] [INFO] [stage3.py:2277:step] End to end step took 7.260153770446777
x3003c0s37b1n0: [2024-03-28 13:11:27,483] [INFO] [stage3.py:2277:step] End to end step took 7.260182857513428
x3003c0s37b0n0: [2024-03-28 13:11:27,483] [INFO] [stage3.py:2277:step] End to end step took 7.260235548019409
x3003c0s37b0n0: [2024-03-28 13:11:27,483] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[3e-05], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 13:11:27,483] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=1.1710020977003275, CurrSamplesPerSec=1.1466008407660044, MemAllocated=9.37GB, MaxMemAllocated=10.91GB
x3003c0s37b0n0: [2024-03-28 13:11:27,484] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 13783.80 | bwd_microstep: 34390.18 | bwd_inner_microstep: 34249.42 | bwd_allreduce_microstep: 140.69 | step_microstep: 7290.75
x3003c0s37b0n0: [2024-03-28 13:11:27,484] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 13783.79 | bwd: 34390.18 | bwd_inner: 34249.41 | bwd_allreduce: 140.71 | step: 7290.75
x3003c0s37b0n0: [2024-03-28 13:11:27,619] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 13:11:27,619] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 10.91 GB         CA 10.02 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:11:27,619] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 340.27 GB, percent = 67.6%
x3003c0s37b0n0: <TIMER:interval-time,56.08704400062561><TIMER:interval-time,56.08704328536987><TIMER:interval-time,56.0870418548584><TIMER:interval-time,56.08704328536987>
x3003c0s37b0n0: 
x3003c0s37b0n0: 
x3003c0s37b0n0: 
x3003c0s7b0n0: <TIMER:interval-time,56.08701252937317><TIMER:interval-time,56.08701419830322><TIMER:interval-time,56.087010622024536>
x3003c0s7b0n0: 
x3003c0s7b0n0: 
x3003c0s7b0n0: <TIMER:interval-time,56.087018966674805>
x3003c0s7b1n0: <TIMER:interval-time,56.087071657180786><TIMER:interval-time,56.08707308769226>
x3003c0s7b1n0: <TIMER:interval-time,56.087074279785156>
x3003c0s7b1n0: <TIMER:interval-time,56.08707547187805>
x3003c0s7b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,56.08665752410889><TIMER:interval-time,56.08665990829468><TIMER:interval-time,56.08666253089905><TIMER:interval-time,56.08666181564331>
x3003c0s37b1n0: 
x3003c0s37b1n0: 
x3003c0s37b1n0: 
x3003c0s7b1n0:  elapsed_time 56.087073 | consumed samples:          640 | consumed tokens:      1310720 | elapsed time per iteration (ms): 56087.1 | learning rate: 3.000E-05 | global batch size:    64 | lm loss: 1.172162E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.141 | TFLOPs: 78.78 |
x3003c0s37b1n0: <<<only_train:573.9127714633942>>><<<only_train:573.9127547740936>>>
x3003c0s37b1n0: 
x3003c0s37b1n0: <<<only_train:573.9127328395844>>>
x3003c0s37b1n0: <<<only_train:573.9127817153931>>>
x3003c0s37b0n0: <<<only_train:573.9126770496368>>>
x3003c0s37b0n0: <<<only_train:573.912525177002>>>
x3003c0s37b0n0: <<<only_train:573.9124772548676>>>
x3003c0s7b1n0: <<<only_train:573.912663936615>>>
x3003c0s7b1n0: <<<only_train:573.9127020835876>>>
x3003c0s7b0n0: <<<only_train:573.9128983020782>>>
x3003c0s7b0n0: <<<only_train:573.9129428863525>>>
x3003c0s7b0n0: <<<only_train:573.9127578735352>>>
x3003c0s7b0n0: <<<only_train:573.9127831459045>>>
x3003c0s7b1n0: <<<only_train:573.9128341674805>>>
x3003c0s7b1n0: <<<only_train:573.9129076004028>>>
x3003c0s37b0n0: <<<only_train:573.9129128456116>>>
x3003c0s37b0n0: [after training ends] datetime: 2024-03-28 13:11:27 
x3003c0s37b0n0: <<<full_time:573.9132041931152>>><<<full_time:573.9130301475525>>>
x3003c0s7b0n0: <<<full_time:573.9133017063141>>><<<full_time:573.9130687713623>>><<<full_time:573.9132714271545>>>
x3003c0s7b0n0: 
x3003c0s7b0n0: 
x3003c0s7b0n0: <<<full_time:573.9131021499634>>>
x3003c0s37b0n0: 
x3003c0s37b0n0: <<<full_time:573.9129855632782>>>
x3003c0s7b1n0: <<<full_time:573.9130504131317>>><<<full_time:573.9130544662476>>>
x3003c0s7b1n0: 
x3003c0s7b1n0: <<<full_time:573.9131736755371>>>
x3003c0s7b1n0: <<<full_time:573.9132325649261>>>
x3003c0s37b0n0: <<<full_time:573.9132099151611>>>
x3003c0s37b1n0: <<<full_time:573.9132947921753>>><<<full_time:573.9133365154266>>><<<full_time:573.91335272789>>><<<full_time:573.9133327007294>>>
x3003c0s37b1n0: 
x3003c0s37b1n0: 
x3003c0s37b1n0: 
x3003c0s7b0n0: [2024-03-28 13:11:31,642] [INFO] [launch.py:348:main] Process 47316 exits successfully.
x3003c0s7b1n0: [2024-03-28 13:11:31,646] [INFO] [launch.py:348:main] Process 55093 exits successfully.
x3003c0s37b1n0: [2024-03-28 13:11:31,832] [INFO] [launch.py:348:main] Process 35648 exits successfully.
x3003c0s37b0n0: [2024-03-28 13:11:32,074] [INFO] [launch.py:348:main] Process 42460 exits successfully.
x3003c0s37b0n0: [2024-03-28 13:11:34,077] [INFO] [launch.py:348:main] Process 42458 exits successfully.
x3003c0s37b0n0: [2024-03-28 13:11:34,077] [INFO] [launch.py:348:main] Process 42459 exits successfully.
x3003c0s37b0n0: [2024-03-28 13:11:34,077] [INFO] [launch.py:348:main] Process 42457 exits successfully.
x3003c0s7b0n0: [2024-03-28 13:11:34,646] [INFO] [launch.py:348:main] Process 47314 exits successfully.
x3003c0s7b0n0: [2024-03-28 13:11:34,646] [INFO] [launch.py:348:main] Process 47317 exits successfully.
x3003c0s7b0n0: [2024-03-28 13:11:34,646] [INFO] [launch.py:348:main] Process 47315 exits successfully.
x3003c0s7b1n0: [2024-03-28 13:11:34,649] [INFO] [launch.py:348:main] Process 55095 exits successfully.
x3003c0s7b1n0: [2024-03-28 13:11:34,649] [INFO] [launch.py:348:main] Process 55094 exits successfully.
x3003c0s7b1n0: [2024-03-28 13:11:34,649] [INFO] [launch.py:348:main] Process 55092 exits successfully.
x3003c0s37b1n0: [2024-03-28 13:11:34,835] [INFO] [launch.py:348:main] Process 35646 exits successfully.
x3003c0s37b1n0: [2024-03-28 13:11:34,835] [INFO] [launch.py:348:main] Process 35647 exits successfully.
x3003c0s37b1n0: [2024-03-28 13:11:34,835] [INFO] [launch.py:348:main] Process 35645 exits successfully.

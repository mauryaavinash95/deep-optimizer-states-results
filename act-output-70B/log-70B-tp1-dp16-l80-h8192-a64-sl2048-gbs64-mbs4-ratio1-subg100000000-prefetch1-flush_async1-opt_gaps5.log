[2024-03-28 12:49:06,384] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-28 12:49:09,498] [INFO] [runner.py:463:main] Using IP address of 10.140.57.42 for node x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-28 12:49:09,500] [INFO] [runner.py:557:main] deepspeed_env file = ./.deepspeed_env
[2024-03-28 12:49:09,501] [INFO] [multinode_runner.py:80:get_cmd] Running on the following workers: x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov,x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov,x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov,x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-28 12:49:09,501] [INFO] [runner.py:568:main] cmd = pdsh -S -f 1024 -w x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov,x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov,x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov,x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov export PYTHONUSERBASE=/home/am6429/.local/polaris/conda/2023-10-04; export PYTHONPATH=/home/am6429/dl-io/Megatron-DeepSpeed; export PATH=/home/am6429/.conda/envs/dspeed_env/bin:/soft/datascience/conda/2023-10-04/mconda3/condabin:/soft/compilers/cudatoolkit/cuda-11.8.0/bin:/soft/buildtools/cmake/cmake-3.23.2/cmake-3.23.2-linux-x86_64/bin:/opt/cray/pe/gcc/11.2.0/bin:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/include:/opt/cray/pe/pals/1.2.11/bin:/opt/cray/pe/craype/2.7.20/bin:/opt/cray/libfabric/1.15.2.0/bin:/home/am6429/.conda/envs/dspeed_env/bin:/opt/cray/pe/perftools/23.03.0/bin:/opt/cray/pe/papi/7.0.0.1/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/am6429/.local/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/am6429/veloc-build/include:/home/am6429/veloc-build/bin:/soft/datascience/conda/2023-01-10/mconda3/include:/opt/cray/pe/bin:/soft/datascience/conda/2023-01-10/mconda3/include; export LD_LIBRARY_PATH=/usr/lib64/:/soft/compilers/cudatoolkit/cuda-11.8.0/lib64:/opt/cray/pe/gcc/11.2.0/snos/lib64:/soft/compilers/cudatoolkit/cuda-11.8.0/extras/CUPTI/lib64:/soft/libraries/trt/TensorRT-8.5.3.1.Linux.x86_64-gnu.cuda-11.8.cudnn8.6/lib:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/lib:/soft/libraries/cudnn/cudnn-11-linux-x64-v8.6.0.163/lib:/opt/cray/libfabric/1.15.2.0/lib64:/usr/lib64:/opt/cray/pe/papi/7.0.0.1/lib64:/home/am6429/veloc-build/lib:/home/am6429/veloc-build/lib64:/home/am6429/nvcomp/lib:/soft/datascience/conda/2023-01-10/mconda3/lib:/soft/datascience/conda/2023-01-10/mconda3/lib/; export http_proxy=http://proxy.alcf.anl.gov:3128; export https_proxy=http://proxy.alcf.anl.gov:3128; export CC=gcc; export CXX=g++; export IBV_FORK_SAFE=1; export CFLAGS=-I/soft/datascience/conda/2023-01-10/mconda3/include/; export LDFLAGS=-L/soft/datascience/conda/2023-01-10/mconda3/lib/; export CUDA_DEVICE_MAX_CONNECTIONS=1; export TORCHSNAPSHOT_PER_RANK_MEMORY_BUDGET_BYTES=34359738368; export _DEFAULT_MAX_PER_RANK_IO_CONCURRENCY=1; export _MAX_PER_RANK_IO_CONCURRENCY=1; export NSYS_REPORT_DIR=/home/am6429/dl-io/dl-io-outputs/act-output-70B//rep-70B-tp1-dp16-l80-h8192-a64-sl2048-gbs64-mbs4-ratio1-subg100000000-prefetch1-flush_async1-opt_gaps5-%n;  cd /home/am6429/dl-io/Megatron-DeepSpeed; /home/am6429/.conda/envs/dspeed_env/bin/python -u -m deepspeed.launcher.launch --world_info=eyJ4MzAwM2MwczM3YjBuMC5oc24uY20ucG9sYXJpcy5hbGNmLmFubC5nb3YiOiBbMCwgMSwgMiwgM10sICJ4MzAwM2MwczM3YjFuMC5oc24uY20ucG9sYXJpcy5hbGNmLmFubC5nb3YiOiBbMCwgMSwgMiwgM10sICJ4MzAwM2MwczdiMG4wLmhzbi5jbS5wb2xhcmlzLmFsY2YuYW5sLmdvdiI6IFswLCAxLCAyLCAzXSwgIngzMDAzYzBzN2IxbjAuaHNuLmNtLnBvbGFyaXMuYWxjZi5hbmwuZ292IjogWzAsIDEsIDIsIDNdfQ== --node_rank=%n --master_addr=10.140.57.42 --master_port=29700 /home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py --tensor-model-parallel-size 1 --num-layers 80 --hidden-size 8192 --num-attention-heads 64 --micro-batch-size 4 --global-batch-size 64 --seq-length 2048 --max-position-embeddings 2048 --train-iters 10 --save /local/scratch/llama2/zero3-tp1}_dp16 --data-path /home/am6429/dl-io/datasets/meg-gpt2_text_document --vocab-file /home/am6429/dl-io/datasets/gpt2-vocab.json --merge-file /home/am6429/dl-io/datasets/gpt2-merges.txt --data-impl mmap --tokenizer-type GPTSentencePieceTokenizer --tokenizer-model /home/am6429/dl-io/datasets/tokenizer.model --split 949,50,1 --distributed-backend nccl --lr 3e-4 --lr-decay-style cosine --min-lr 3e-5 --weight-decay 0.1 --clip-grad 1 --lr-warmup-iters 1 --optimizer adam --adam-beta1 0.9 --adam-beta2 0.95 --log-interval 1 --save-interval 1000 --eval-interval 1000 --eval-iters 0 --bf16 --no-query-key-layer-scaling --attention-dropout 0 --hidden-dropout 0 --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization rmsnorm --disable-bias-linear --num-key-value-heads 4 --deepspeed --exit-interval 20 --deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json --zero-stage=3 --no-pipeline-parallel --cpu-optimizer --checkpoint-activations --deepspeed-activation-checkpointing
x3003c0s37b1n0: [2024-03-28 12:49:11,232] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b1n0: [2024-03-28 12:49:11,503] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b0n0: [2024-03-28 12:49:11,505] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b0n0: [2024-03-28 12:49:11,527] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b1n0: [2024-03-28 12:49:13,004] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3003c0s37b1n0: [2024-03-28 12:49:13,004] [INFO] [launch.py:151:main] nnodes=4, num_local_procs=4, node_rank=1
x3003c0s37b1n0: [2024-03-28 12:49:13,004] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [8, 9, 10, 11], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [12, 13, 14, 15]})
x3003c0s37b1n0: [2024-03-28 12:49:13,004] [INFO] [launch.py:163:main] dist_world_size=16
x3003c0s37b1n0: [2024-03-28 12:49:13,004] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3003c0s37b1n0: [2024-03-28 12:49:13,005] [INFO] [launch.py:253:main] process 32402 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b1n0: [2024-03-28 12:49:13,005] [INFO] [launch.py:253:main] process 32403 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b1n0: [2024-03-28 12:49:13,006] [INFO] [launch.py:253:main] process 32404 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b1n0: [2024-03-28 12:49:13,006] [INFO] [launch.py:253:main] process 32405 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b0n0: [2024-03-28 12:49:13,345] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3003c0s37b0n0: [2024-03-28 12:49:13,346] [INFO] [launch.py:151:main] nnodes=4, num_local_procs=4, node_rank=0
x3003c0s37b0n0: [2024-03-28 12:49:13,346] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [8, 9, 10, 11], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [12, 13, 14, 15]})
x3003c0s37b0n0: [2024-03-28 12:49:13,346] [INFO] [launch.py:163:main] dist_world_size=16
x3003c0s37b0n0: [2024-03-28 12:49:13,346] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3003c0s37b0n0: [2024-03-28 12:49:13,346] [INFO] [launch.py:253:main] process 38675 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b0n0: [2024-03-28 12:49:13,347] [INFO] [launch.py:253:main] process 38676 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b0n0: [2024-03-28 12:49:13,347] [INFO] [launch.py:253:main] process 38677 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b0n0: [2024-03-28 12:49:13,348] [INFO] [launch.py:253:main] process 38678 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b0n0: [2024-03-28 12:49:13,816] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3003c0s7b0n0: [2024-03-28 12:49:13,816] [INFO] [launch.py:151:main] nnodes=4, num_local_procs=4, node_rank=2
x3003c0s7b0n0: [2024-03-28 12:49:13,816] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [8, 9, 10, 11], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [12, 13, 14, 15]})
x3003c0s7b0n0: [2024-03-28 12:49:13,816] [INFO] [launch.py:163:main] dist_world_size=16
x3003c0s7b0n0: [2024-03-28 12:49:13,816] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3003c0s7b0n0: [2024-03-28 12:49:13,817] [INFO] [launch.py:253:main] process 44073 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b0n0: [2024-03-28 12:49:13,818] [INFO] [launch.py:253:main] process 44074 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b0n0: [2024-03-28 12:49:13,818] [INFO] [launch.py:253:main] process 44075 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b0n0: [2024-03-28 12:49:13,819] [INFO] [launch.py:253:main] process 44076 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b1n0: [2024-03-28 12:49:13,821] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3003c0s7b1n0: [2024-03-28 12:49:13,821] [INFO] [launch.py:151:main] nnodes=4, num_local_procs=4, node_rank=3
x3003c0s7b1n0: [2024-03-28 12:49:13,821] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [8, 9, 10, 11], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [12, 13, 14, 15]})
x3003c0s7b1n0: [2024-03-28 12:49:13,821] [INFO] [launch.py:163:main] dist_world_size=16
x3003c0s7b1n0: [2024-03-28 12:49:13,821] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3003c0s7b1n0: [2024-03-28 12:49:13,822] [INFO] [launch.py:253:main] process 51859 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b1n0: [2024-03-28 12:49:13,822] [INFO] [launch.py:253:main] process 51860 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b1n0: [2024-03-28 12:49:13,823] [INFO] [launch.py:253:main] process 51861 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b1n0: [2024-03-28 12:49:13,824] [INFO] [launch.py:253:main] process 51862 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b1n0: [2024-03-28 12:49:14,502] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b1n0: [2024-03-28 12:49:14,506] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b1n0: [2024-03-28 12:49:14,508] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b1n0: [2024-03-28 12:49:14,517] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b0n0: [2024-03-28 12:49:15,068] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b0n0: [2024-03-28 12:49:15,149] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b0n0: [2024-03-28 12:49:15,186] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b0n0: [2024-03-28 12:49:15,190] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b0n0: [2024-03-28 12:49:15,583] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b1n0: [2024-03-28 12:49:15,615] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b0n0: [2024-03-28 12:49:15,637] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b1n0: [2024-03-28 12:49:15,639] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b0n0: [2024-03-28 12:49:15,640] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b0n0: [2024-03-28 12:49:15,654] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b1n0: [2024-03-28 12:49:15,657] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b1n0: [2024-03-28 12:49:15,668] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b1n0:       meet the required dependencies to JIT install the op.
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: JIT compiled ops requires ninja
x3003c0s37b1n0: ninja .................. [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: op name ................ installed .. compatible
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b1n0:       meet the required dependencies to JIT install the op.
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: JIT compiled ops requires ninja
x3003c0s37b1n0: ninja .................. [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: op name ................ installed .. compatible
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b1n0:       meet the required dependencies to JIT install the op.
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: JIT compiled ops requires ninja
x3003c0s37b1n0: ninja .................. [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: op name ................ installed .. compatible
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b1n0:       meet the required dependencies to JIT install the op.
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: JIT compiled ops requires ninja
x3003c0s37b1n0: ninja .................. [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: op name ................ installed .. compatible
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: DeepSpeed general environment info:
x3003c0s37b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b1n0: torch version .................... 2.0.1+cu118
x3003c0s37b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b1n0: torch cuda version ............... 11.8
x3003c0s37b1n0: torch hip version ................ None
x3003c0s37b1n0: nvcc version ..................... 11.8
x3003c0s37b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [2024-03-28 12:49:16,701] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: DeepSpeed general environment info:
x3003c0s37b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b1n0: torch version .................... 2.0.1+cu118
x3003c0s37b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b1n0: torch cuda version ............... 11.8
x3003c0s37b1n0: torch hip version ................ None
x3003c0s37b1n0: nvcc version ..................... 11.8
x3003c0s37b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: DeepSpeed general environment info:
x3003c0s37b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b1n0: torch version .................... 2.0.1+cu118
x3003c0s37b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b1n0: torch cuda version ............... 11.8
x3003c0s37b1n0: torch hip version ................ None
x3003c0s37b1n0: nvcc version ..................... 11.8
x3003c0s37b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b1n0: DeepSpeed general environment info:
x3003c0s37b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b1n0: torch version .................... 2.0.1+cu118
x3003c0s37b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b1n0: torch cuda version ............... 11.8
x3003c0s37b1n0: torch hip version ................ None
x3003c0s37b1n0: nvcc version ..................... 11.8
x3003c0s37b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b1n0: [2024-03-28 12:49:16,783] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b1n0: [2024-03-28 12:49:16,788] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b1n0: [2024-03-28 12:49:16,798] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b0n0:       meet the required dependencies to JIT install the op.
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: JIT compiled ops requires ninja
x3003c0s37b0n0: ninja .................. [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: op name ................ installed .. compatible
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: DeepSpeed general environment info:
x3003c0s37b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b0n0: torch version .................... 2.0.1+cu118
x3003c0s37b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b0n0: torch cuda version ............... 11.8
x3003c0s37b0n0: torch hip version ................ None
x3003c0s37b0n0: nvcc version ..................... 11.8
x3003c0s37b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b0n0: [2024-03-28 12:49:17,773] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b0n0:       meet the required dependencies to JIT install the op.
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: JIT compiled ops requires ninja
x3003c0s37b0n0: ninja .................. [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: op name ................ installed .. compatible
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b0n0:       meet the required dependencies to JIT install the op.
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: JIT compiled ops requires ninja
x3003c0s37b0n0: ninja .................. [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: op name ................ installed .. compatible
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b0n0:       meet the required dependencies to JIT install the op.
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: JIT compiled ops requires ninja
x3003c0s37b0n0: ninja .................. [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: op name ................ installed .. compatible
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: DeepSpeed general environment info:
x3003c0s37b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b0n0: torch version .................... 2.0.1+cu118
x3003c0s37b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b0n0: torch cuda version ............... 11.8
x3003c0s37b0n0: torch hip version ................ None
x3003c0s37b0n0: nvcc version ..................... 11.8
x3003c0s37b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [2024-03-28 12:49:18,009] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: DeepSpeed general environment info:
x3003c0s37b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b0n0: torch version .................... 2.0.1+cu118
x3003c0s37b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b0n0: torch cuda version ............... 11.8
x3003c0s37b0n0: torch hip version ................ None
x3003c0s37b0n0: nvcc version ..................... 11.8
x3003c0s37b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: INFO: overriding default arguments for tokenizer_type:GPTSentencePieceTokenizer                    with tokenizer_type:GPT2BPETokenizer
x3003c0s37b0n0: using world size: 16, data-parallel-size: 16, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
x3003c0s37b0n0: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
x3003c0s37b0n0: using torch.bfloat16 for parameters ...
x3003c0s37b0n0: ------------------------ arguments ------------------------
x3003c0s37b0n0:   accumulate_allreduce_grads_in_fp32 .............. True
x3003c0s37b0n0:   adam_beta1 ...................................... 0.9
x3003c0s37b0n0:   adam_beta2 ...................................... 0.95
x3003c0s37b0n0:   adam_eps ........................................ 1e-08
x3003c0s37b0n0:   add_bias_linear ................................. False
x3003c0s37b0n0:   add_position_embedding .......................... False
x3003c0s37b0n0:   adlr_autoresume ................................. False
x3003c0s37b0n0:   adlr_autoresume_interval ........................ 1000
x3003c0s37b0n0:   aml_data_download_path .......................... None
x3003c0s37b0n0:   apply_layernorm_1p .............................. False
x3003c0s37b0n0:   apply_query_key_layer_scaling ................... False
x3003c0s37b0n0:   apply_residual_connection_post_layernorm ........ False
x3003c0s37b0n0:   async_tensor_model_parallel_allreduce ........... False
x3003c0s37b0n0:   attention_dropout ............................... 0.0
x3003c0s37b0n0:   attention_softmax_in_fp32 ....................... False
x3003c0s37b0n0:   barrier_with_L1_time ............................ True
x3003c0s37b0n0:   bert_binary_head ................................ True
x3003c0s37b0n0:   bert_embedder_type .............................. megatron
x3003c0s37b0n0:   bert_load ....................................... None
x3003c0s37b0n0:   bf16 ............................................ True
x3003c0s37b0n0:   bias_dropout_fusion ............................. True
x3003c0s37b0n0:   bias_gelu_fusion ................................ False
x3003c0s37b0n0:   biencoder_projection_dim ........................ 0
x3003c0s37b0n0:   biencoder_shared_query_context_model ............ False
x3003c0s37b0n0:   block_data_path ................................. None
x3003c0s37b0n0:   checkpoint_activations .......................... True
x3003c0s37b0n0:   checkpoint_in_cpu ............................... False
x3003c0s37b0n0:   checkpoint_num_layers ........................... 1
x3003c0s37b0n0:   classes_fraction ................................ 1.0
x3003c0s37b0n0:   clip_grad ....................................... 1.0
x3003c0s37b0n0:   compression_training ............................ False
x3003c0s37b0n0:   consumed_train_samples .......................... 0
x3003c0s37b0n0:   consumed_train_tokens ........................... 0
x3003c0s37b0n0:   consumed_valid_samples .......................... 0
x3003c0s37b0n0:   contigious_checkpointing ........................ False
x3003c0s37b0n0:   cpu_optimizer ................................... True
x3003c0s37b0n0:   cpu_torch_adam .................................. False
x3003c0s37b0n0:   create_moe_param_group .......................... False
x3003c0s37b0n0:   curriculum_learning_legacy ...................... False
x3003c0s37b0n0:   data_cache_path ................................. None
x3003c0s37b0n0:   data_efficiency_curriculum_learning ............. False
x3003c0s37b0n0:   data_impl ....................................... mmap
x3003c0s37b0n0:   data_parallel_random_init ....................... False
x3003c0s37b0n0:   data_parallel_size .............................. 16
x3003c0s37b0n0:   data_path ....................................... ['/home/am6429/dl-io/datasets/meg-gpt2_text_document']
x3003c0s37b0n0:   data_per_class_fraction ......................... 1.0
x3003c0s37b0n0:   data_sharding ................................... True
x3003c0s37b0n0:   dataloader_type ................................. single
x3003c0s37b0n0:   DDP_impl ........................................ local
x3003c0s37b0n0:   decoder_num_layers .............................. None
x3003c0s37b0n0:   decoder_seq_length .............................. None
x3003c0s37b0n0:   deepscale ....................................... False
x3003c0s37b0n0:   deepscale_config ................................ None
x3003c0s37b0n0:   deepspeed ....................................... True
x3003c0s37b0n0:   deepspeed_activation_checkpointing .............. True
x3003c0s37b0n0:   deepspeed_config ................................ /home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json
x3003c0s37b0n0:   dino_bottleneck_size ............................ 256
x3003c0s37b0n0:   dino_freeze_last_layer .......................... 1
x3003c0s37b0n0:   dino_head_hidden_size ........................... 2048
x3003c0s37b0n0:   dino_local_crops_number ......................... 10
x3003c0s37b0n0:   dino_local_img_size ............................. 96
x3003c0s37b0n0:   dino_norm_last_layer ............................ False
x3003c0s37b0n0:   dino_teacher_temp ............................... 0.07
x3003c0s37b0n0:   dino_warmup_teacher_temp ........................ 0.04
x3003c0s37b0n0:   dino_warmup_teacher_temp_epochs ................. 30
x3003c0s37b0n0:   distribute_checkpointed_activations ............. False
x3003c0s37b0n0:   distribute_saved_activations .................... False
x3003c0s37b0n0:   distributed_backend ............................. nccl
x3003c0s37b0n0:   distributed_timeout_minutes ..................... 10
x3003c0s37b0n0:   ds_inference .................................... False
x3003c0s37b0n0:   ds_pipeline_enabled ............................. False
x3003c0s37b0n0:   ds_sequence_parallel_size ....................... 1
x3003c0s37b0n0:   embedding_path .................................. None
x3003c0s37b0n0:   embedding_weights_in_fp32 ....................... False
x3003c0s37b0n0:   empty_unused_memory_level ....................... 0
x3003c0s37b0n0:   enable_expert_tensor_parallelism ................ False
x3003c0s37b0n0:   encoder_num_layers .............................. 80
x3003c0s37b0n0:   encoder_seq_length .............................. 2048
x3003c0s37b0n0:   end_weight_decay ................................ 0.1
x3003c0s37b0n0:   eod_mask_loss ................................... False
x3003c0s37b0n0:   eval_interval ................................... 1000
x3003c0s37b0n0:   eval_iters ...................................... 0
x3003c0s37b0n0:   evidence_data_path .............................. None
x3003c0s37b0n0:   exit_duration_in_mins ........................... None
x3003c0s37b0n0:   exit_interval ................................... 20
x3003c0s37b0n0:   exit_on_missing_checkpoint ...................... False
x3003c0s37b0n0:   exit_signal_handler ............................. False
x3003c0s37b0n0:   expert_interval ................................. 2
x3003c0s37b0n0:   ffn_hidden_size ................................. 21824
x3003c0s37b0n0:   finetune ........................................ False
x3003c0s37b0n0:   force_ds_sequence_parallel ...................... False
x3003c0s37b0n0:   fp16 ............................................ False
x3003c0s37b0n0:   fp16_lm_cross_entropy ........................... False
x3003c0s37b0n0:   fp32_residual_connection ........................ False
x3003c0s37b0n0:   fp8_amax_compute_algo ........................... most_recent
x3003c0s37b0n0:   fp8_amax_history_len ............................ 1
x3003c0s37b0n0:   fp8_e4m3 ........................................ False
x3003c0s37b0n0:   fp8_hybrid ...................................... False
x3003c0s37b0n0:   fp8_interval .................................... 1
x3003c0s37b0n0:   fp8_margin ...................................... 0
x3003c0s37b0n0:   fp8_wgrad ....................................... True
x3003c0s37b0n0:   global_batch_size ............................... 64
x3003c0s37b0n0:   gradient_accumulation_fusion .................... True
x3003c0s37b0n0:   head_lr_mult .................................... 1.0
x3003c0s37b0n0:   hidden_dropout .................................. 0.0
x3003c0s37b0n0:   hidden_size ..................................... 8192
x3003c0s37b0n0:   hidden_size_teacher ............................. None
x3003c0s37b0n0:   hysteresis ...................................... 2
x3003c0s37b0n0:   ict_head_size ................................... None
x3003c0s37b0n0:   ict_load ........................................ None
x3003c0s37b0n0:   img_h ........................................... 224
x3003c0s37b0n0:   img_w ........................................... 224
x3003c0s37b0n0:   indexer_batch_size .............................. 128
x3003c0s37b0n0:   indexer_log_interval ............................ 1000
x3003c0s37b0n0:   inference ....................................... False
x3003c0s37b0n0:   inference_batch_times_seqlen_threshold .......... 512
x3003c0s37b0n0:   init_method_std ................................. 0.02
x3003c0s37b0n0:   init_method_xavier_uniform ...................... False
x3003c0s37b0n0:   initial_loss_scale .............................. 4294967296
x3003c0s37b0n0:   iter_per_epoch .................................. 1250
x3003c0s37b0n0:   kd .............................................. False
x3003c0s37b0n0:   kd_alpha_ce ..................................... 1
x3003c0s37b0n0:   kd_beta_ce ...................................... 1
x3003c0s37b0n0:   kd_temp ......................................... 1.0
x3003c0s37b0n0:   kv_channels ..................................... 128
x3003c0s37b0n0:   layernorm_epsilon ............................... 1e-05
x3003c0s37b0n0:   lazy_mpu_init ................................... None
x3003c0s37b0n0:   load ............................................ None
x3003c0s37b0n0:   load_teacher .................................... None
x3003c0s37b0n0:   local_rank ...................................... 0
x3003c0s37b0n0:   log_batch_size_to_tensorboard ................... False
x3003c0s37b0n0:   log_interval .................................... 1
x3003c0s37b0n0:   log_learning_rate_to_tensorboard ................ True
x3003c0s37b0n0:   log_loss_scale_to_tensorboard ................... True
x3003c0s37b0n0:   log_memory_to_tensorboard ....................... False
x3003c0s37b0n0:   log_num_zeros_in_grad ........................... False
x3003c0s37b0n0:   log_optimizer_states_to_tensorboard ............. False
x3003c0s37b0n0:   log_params_norm ................................. False
x3003c0s37b0n0:   log_timers_to_tensorboard ....................... False
x3003c0s37b0n0:   log_validation_ppl_to_tensorboard ............... False
x3003c0s37b0n0:   log_world_size_to_tensorboard ................... False
x3003c0s37b0n0:   loss_scale ...................................... None
x3003c0s37b0n0:   loss_scale_window ............................... 1000
x3003c0s37b0n0:   lr .............................................. 0.0003
x3003c0s37b0n0:   lr_decay_iters .................................. None
x3003c0s37b0n0:   lr_decay_samples ................................ None
x3003c0s37b0n0:   lr_decay_style .................................. cosine
x3003c0s37b0n0:   lr_decay_tokens ................................. None
x3003c0s37b0n0:   lr_warmup_fraction .............................. None
x3003c0s37b0n0:   lr_warmup_iters ................................. 1
x3003c0s37b0n0:   lr_warmup_samples ............................... 0
x3003c0s37b0n0:   lr_warmup_tokens ................................ None
x3003c0s37b0n0:   make_vocab_size_divisible_by .................... 128
x3003c0s37b0n0:   mask_factor ..................................... 1.0
x3003c0s37b0n0:   mask_prob ....................................... 0.15
x3003c0s37b0n0:   mask_type ....................................... random
x3003c0s37b0n0:   masked_softmax_fusion ........................... True
x3003c0s37b0n0:   max_position_embeddings ......................... 2048
x3003c0s37b0n0:   max_tokens_to_oom ............................... 12000
x3003c0s37b0n0:   memory_centric_tiled_linear ..................... False
x3003c0s37b0n0:   merge_file ...................................... /home/am6429/dl-io/datasets/gpt2-merges.txt
x3003c0s37b0n0:   micro_batch_size ................................ 4
x3003c0s37b0n0:   min_loss_scale .................................. 1.0
x3003c0s37b0n0:   min_lr .......................................... 3e-05
x3003c0s37b0n0:   mlp_type ........................................ standard
x3003c0s37b0n0:   mmap_warmup ..................................... False
x3003c0s37b0n0:   moe_eval_capacity_factor ........................ 1.0
x3003c0s37b0n0:   moe_expert_parallel_size ........................ 1
x3003c0s37b0n0:   moe_loss_coeff .................................. 0.1
x3003c0s37b0n0:   moe_min_capacity ................................ 4
x3003c0s37b0n0:   moe_token_dropping .............................. True
x3003c0s37b0n0:   moe_train_capacity_factor ....................... 1.0
x3003c0s37b0n0:   mos ............................................. False
x3003c0s37b0n0:   no_load_lr_state ................................ False
x3003c0s37b0n0:   no_load_optim ................................... None
x3003c0s37b0n0:   no_load_rng ..................................... None
x3003c0s37b0n0:   no_persist_layer_norm ........................... False
x3003c0s37b0n0:   no_pipeline_parallel ............................ True
x3003c0s37b0n0:   no_save_optim ................................... None
x3003c0s37b0n0:   no_save_rng ..................................... None
x3003c0s37b0n0:   normalization ................................... rmsnorm
x3003c0s37b0n0:   num_attention_heads ............................. 64
x3003c0s37b0n0:   num_attention_heads_teacher ..................... None
x3003c0s37b0n0:   num_channels .................................... 3
x3003c0s37b0n0:   num_classes ..................................... 1000
x3003c0s37b0n0:   num_experts ..................................... [1]
x3003c0s37b0n0:   num_experts_switch .............................. None
x3003c0s37b0n0:   num_experts_teacher ............................. [1]
x3003c0s37b0n0:   num_key_value_heads ............................. 4
x3003c0s37b0n0:   num_layers ...................................... 80
x3003c0s37b0n0:   num_layers_per_virtual_pipeline_stage ........... None
x3003c0s37b0n0:   num_layers_teacher .............................. None
x3003c0s37b0n0:   num_workers ..................................... 2
x3003c0s37b0n0:   onnx_safe ....................................... None
x3003c0s37b0n0:   openai_gelu ..................................... False
x3003c0s37b0n0:   optimizer ....................................... adam
x3003c0s37b0n0:   output_bert_embeddings .......................... False
x3003c0s37b0n0:   overlap_p2p_comm ................................ False
x3003c0s37b0n0:   override_opt_param_scheduler .................... False
x3003c0s37b0n0:   params_dtype .................................... torch.bfloat16
x3003c0s37b0n0:   partition_activations ........................... False
x3003c0s37b0n0:   patch_dim ....................................... 16
x3003c0s37b0n0:   perform_initialization .......................... True
x3003c0s37b0n0:   pipeline_model_parallel_size .................... 1
x3003c0s37b0n0:   pipeline_model_parallel_split_rank .............. None
x3003c0s37b0n0:   profile_backward ................................ False
x3003c0s37b0n0:   query_in_block_prob ............................. 0.1
x3003c0s37b0n0:   rampup_batch_size ............................... None
x3003c0s37b0n0:   random_ltd ...................................... False
x3003c0s37b0n0:   rank ............................................ 0
x3003c0s37b0n0:   recompute_granularity ........................... None
x3003c0s37b0n0:   recompute_method ................................ None
x3003c0s37b0n0:   recompute_num_layers ............................ 1
x3003c0s37b0n0:   remote_device ................................... none
x3003c0s37b0n0:   reset_attention_mask ............................ False
x3003c0s37b0n0:   reset_iteration ................................. False
x3003c0s37b0n0:   reset_position_ids .............................. False
x3003c0s37b0n0:   retriever_report_topk_accuracies ................ []
x3003c0s37b0n0:   retriever_score_scaling ......................... False
x3003c0s37b0n0:   retriever_seq_length ............................ 256
x3003c0s37b0n0:   retro_add_retriever ............................. False
x3003c0s37b0n0:   retro_cyclic_train_iters ........................ None
x3003c0s37b0n0:   retro_encoder_attention_dropout ................. 0.1
x3003c0s37b0n0:   retro_encoder_hidden_dropout .................... 0.1
x3003c0s37b0n0:   retro_encoder_layers ............................ 2
x3003c0s37b0n0:   retro_num_neighbors ............................. 2
x3003c0s37b0n0:   retro_num_retrieved_chunks ...................... 2
x3003c0s37b0n0:   retro_return_doc_ids ............................ False
x3003c0s37b0n0:   retro_workdir ................................... None
x3003c0s37b0n0:   return_data_index ............................... False
x3003c0s37b0n0:   rotary_percent .................................. 1.0
x3003c0s37b0n0:   sample_rate ..................................... 1.0
x3003c0s37b0n0:   save ............................................ /local/scratch/llama2/zero3-tp1}_dp16
x3003c0s37b0n0:   save_interval ................................... 1000
x3003c0s37b0n0:   scatter_gather_tensors_in_pipeline .............. True
x3003c0s37b0n0:   scattered_embeddings ............................ False
x3003c0s37b0n0:   seed ............................................ 1234
x3003c0s37b0n0:   seq_length ...................................... 2048
x3003c0s37b0n0:   sequence_parallel ............................... False
x3003c0s37b0n0:   sgd_momentum .................................... 0.9
x3003c0s37b0n0:   short_seq_prob .................................. 0.1
x3003c0s37b0n0:   skip_train ...................................... False
x3003c0s37b0n0:   split ........................................... 949,50,1
x3003c0s37b0n0:   split_transformers .............................. False
x3003c0s37b0n0:   squared_relu .................................... False
x3003c0s37b0n0:   standalone_embedding_stage ...................... False
x3003c0s37b0n0:   start_weight_decay .............................. 0.1
x3003c0s37b0n0:   swiglu .......................................... True
x3003c0s37b0n0:   swin_backbone_type .............................. tiny
x3003c0s37b0n0:   synchronize_each_layer .......................... False
x3003c0s37b0n0:   tensor_model_parallel_size ...................... 1
x3003c0s37b0n0:   tensorboard_dir ................................. None
x3003c0s37b0n0:   tensorboard_log_interval ........................ 1
x3003c0s37b0n0:   tensorboard_queue_size .......................... 1000
x3003c0s37b0n0:   test_data_path .................................. None
x3003c0s37b0n0:   tile_factor ..................................... 1
x3003c0s37b0n0:   timing_log_level ................................ 0
x3003c0s37b0n0:   timing_log_option ............................... minmax
x3003c0s37b0n0:   titles_data_path ................................ None
x3003c0s37b0n0:   tokenizer_model ................................. /home/am6429/dl-io/datasets/tokenizer.model
x3003c0s37b0n0:   tokenizer_type .................................. GPT2BPETokenizer
x3003c0s37b0n0:   topk ............................................ 1
x3003c0s37b0n0:   train_data_exact_num_epochs ..................... None
x3003c0s37b0n0:   train_data_path ................................. None
x3003c0s37b0n0:   train_desc_path ................................. None
x3003c0s37b0n0:   train_doc_idx_path .............................. None
x3003c0s37b0n0:   train_idx_path .................................. None
x3003c0s37b0n0:   train_iters ..................................... 10
x3003c0s37b0n0:   train_sample_idx_path ........................... None
x3003c0s37b0n0:   train_samples ................................... None
x3003c0s37b0n0:   train_shuffle_idx_path .......................... None
x3003c0s37b0n0:   train_tokens .................................... None
x3003c0s37b0n0:   transformer_impl ................................ local
x3003c0s37b0n0:   transformer_pipeline_model_parallel_size ........ 1
x3003c0s37b0n0:   untie_embeddings_and_output_weights ............. True
x3003c0s37b0n0:   use_checkpoint_args ............................. False
x3003c0s37b0n0:   use_checkpoint_opt_param_scheduler .............. False
x3003c0s37b0n0:   use_contiguous_buffers_in_local_ddp ............. True
x3003c0s37b0n0:   use_cpu_initialization .......................... None
x3003c0s37b0n0:   use_dataset_only ................................ False
x3003c0s37b0n0:   use_distributed_optimizer ....................... False
x3003c0s37b0n0:   use_flash_attn .................................. False
x3003c0s37b0n0:   use_flash_attn_triton ........................... False
x3003c0s37b0n0:   use_flash_attn_v1 ............................... False
x3003c0s37b0n0:   use_flash_attn_v2 ............................... False
x3003c0s37b0n0:   use_one_sent_docs ............................... False
x3003c0s37b0n0:   use_pin_memory .................................. False
x3003c0s37b0n0:   use_ring_exchange_p2p ........................... False
x3003c0s37b0n0:   use_rotary_position_embeddings .................. True
x3003c0s37b0n0:   use_tutel ....................................... False
x3003c0s37b0n0: ragged_device_ops  valid_data_path ................................. None
x3003c0s37b0n0:   variable_seq_lengths ............................ False 
x3003c0s37b0n0: ......  virtual_pipeline_model_parallel_size ............ None 
x3003c0s37b0n0: [93m[NO][0m  vision_backbone_type ............................ vit 
x3003c0s37b0n0: .......  vision_pretraining .............................. False 
x3003c0s37b0n0: [92m[OKAY][0m  vision_pretraining_type ......................... classify
x3003c0s37b0n0: 
x3003c0s37b0n0:   vocab_extra_ids ................................. 0
x3003c0s37b0n0:   vocab_file ...................................... /home/am6429/dl-io/datasets/gpt2-vocab.json
x3003c0s37b0n0:   vocab_size ...................................... None
x3003c0s37b0n0:   weight_decay .................................... 0.1
x3003c0s37b0n0:   weight_decay_incr_style ......................... constant
x3003c0s37b0n0:   world_size ...................................... 16
x3003c0s37b0n0:   zero_allgather_bucket_size ...................... 0.0
x3003c0s37b0n0:   zero_contigious_gradients ....................... False
x3003c0s37b0n0:   zero_reduce_bucket_size ......................... 0.0
x3003c0s37b0n0:   zero_reduce_scatter ............................. False
x3003c0s37b0n0:   zero_stage ...................................... 3
x3003c0s37b0n0: -------------------- end of arguments ---------------------
x3003c0s37b0n0: setting number of micro-batches to constant 1
x3003c0s37b0n0: > building GPT2BPETokenizer tokenizer ...
x3003c0s37b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: DeepSpeed general environment info:
x3003c0s37b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b0n0: torch version .................... 2.0.1+cu118
x3003c0s37b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b0n0: torch cuda version ............... 11.8
x3003c0s37b0n0: torch hip version ................ None
x3003c0s37b0n0: nvcc version ..................... 11.8
x3003c0s37b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b0n0:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
x3003c0s37b0n0: > initializing torch distributed ...
x3003c0s37b0n0: [2024-03-28 12:49:18,088] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b0n0: [2024-03-28 12:49:18,088] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b0n0:       meet the required dependencies to JIT install the op.
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: JIT compiled ops requires ninja
x3003c0s7b0n0: ninja .................. [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: op name ................ installed .. compatible
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b0n0: [2024-03-28 12:49:18,124] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b1n0:       meet the required dependencies to JIT install the op.
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: JIT compiled ops requires ninja
x3003c0s7b1n0: ninja .................. [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: op name ................ installed .. compatible
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: DeepSpeed general environment info:
x3003c0s7b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b0n0: torch version .................... 2.0.1+cu118
x3003c0s7b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b0n0: torch cuda version ............... 11.8
x3003c0s7b0n0: torch hip version ................ None
x3003c0s7b0n0: nvcc version ..................... 11.8
x3003c0s7b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [2024-03-28 12:49:18,302] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b1n0:       meet the required dependencies to JIT install the op.
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: JIT compiled ops requires ninja
x3003c0s7b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: ninja .................. [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: op name ................ installed .. compatible
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: DeepSpeed general environment info:
x3003c0s7b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b1n0: torch version .................... 2.0.1+cu118
x3003c0s7b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b1n0: torch cuda version ............... 11.8
x3003c0s7b1n0: torch hip version ................ None
x3003c0s7b1n0: nvcc version ..................... 11.8
x3003c0s7b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b0n0:       meet the required dependencies to JIT install the op.
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: JIT compiled ops requires ninja
x3003c0s7b0n0: ninja .................. [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: op name ................ installed .. compatible
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b0n0:       meet the required dependencies to JIT install the op.
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: JIT compiled ops requires ninja
x3003c0s7b0n0: ninja .................. [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: op name ................ installed .. compatible
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b0n0:       meet the required dependencies to JIT install the op.
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: JIT compiled ops requires ninja
x3003c0s7b0n0: ninja .................. [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: op name ................ installed .. compatible
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b1n0:       meet the required dependencies to JIT install the op.
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: JIT compiled ops requires ninja
x3003c0s7b1n0: ninja .................. [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: op name ................ installed .. compatible
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [2024-03-28 12:49:18,438] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b1n0:       meet the required dependencies to JIT install the op.
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: JIT compiled ops requires ninja
x3003c0s7b1n0: ninja .................. [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: op name ................ installed .. compatible
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: DeepSpeed general environment info:
x3003c0s7b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b1n0: torch version .................... 2.0.1+cu118
x3003c0s7b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b1n0: torch cuda version ............... 11.8
x3003c0s7b1n0: torch hip version ................ None
x3003c0s7b1n0: nvcc version ..................... 11.8
x3003c0s7b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: DeepSpeed general environment info:
x3003c0s7b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b0n0: torch version .................... 2.0.1+cu118
x3003c0s7b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b0n0: torch cuda version ............... 11.8
x3003c0s7b0n0: torch hip version ................ None
x3003c0s7b0n0: nvcc version ..................... 11.8
x3003c0s7b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: DeepSpeed general environment info:
x3003c0s7b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b0n0: torch version .................... 2.0.1+cu118
x3003c0s7b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b0n0: torch cuda version ............... 11.8
x3003c0s7b0n0: torch hip version ................ None
x3003c0s7b0n0: nvcc version ..................... 11.8
x3003c0s7b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b1n0: DeepSpeed general environment info:
x3003c0s7b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b1n0: torch version .................... 2.0.1+cu118
x3003c0s7b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b1n0: torch cuda version ............... 11.8
x3003c0s7b1n0: torch hip version ................ None
x3003c0s7b1n0: nvcc version ..................... 11.8
x3003c0s7b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b0n0: DeepSpeed general environment info:
x3003c0s7b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b0n0: torch version .................... 2.0.1+cu118
x3003c0s7b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b0n0: torch cuda version ............... 11.8
x3003c0s7b0n0: torch hip version ................ None
x3003c0s7b0n0: nvcc version ..................... 11.8
x3003c0s7b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [2024-03-28 12:49:18,566] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: DeepSpeed general environment info:
x3003c0s7b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b1n0: torch version .................... 2.0.1+cu118
x3003c0s7b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b1n0: torch cuda version ............... 11.8
x3003c0s7b1n0: torch hip version ................ None
x3003c0s7b1n0: nvcc version ..................... 11.8
x3003c0s7b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b0n0: [2024-03-28 12:49:18,599] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b0n0: [2024-03-28 12:49:18,612] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b1n0: [2024-03-28 12:49:18,620] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b0n0: [2024-03-28 12:49:18,622] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b1n0: [2024-03-28 12:49:18,651] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b0n0: > initialized tensor model parallel with size 1
x3003c0s37b0n0: > initialized pipeline model parallel with size 1
x3003c0s37b0n0: > setting random seeds to 1234 ...
x3003c0s37b0n0: [2024-03-28 12:49:19,963] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
x3003c0s37b0n0: > compiling dataset index builder ...
x3003c0s37b0n0: make: Entering directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3003c0s37b0n0: make: Nothing to be done for 'default'.
x3003c0s37b0n0: make: Leaving directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3003c0s37b0n0: >>> done with dataset index builder. Compilation time: 0.083 seconds
x3003c0s37b0n0: > compiling and loading fused kernels ...
x3003c0s37b0n0: ninja: no work to do.
x3003c0s37b0n0: ninja: no work to do.
x3003c0s37b0n0: ninja: no work to do.
x3003c0s37b0n0: >>> done with compiling and loading fused kernels. Compilation time: 3.769 seconds
x3003c0s37b1n0: <<<<<<<<<<< 4
x3003c0s37b1n0: <<<<<<<<<<< 7
x3003c0s7b1n0: <<<<<<<<<<< 12
x3003c0s37b1n0: <<<<<<<<<<< 6
x3003c0s37b0n0: initialize_megatron took 6.557299852371216
x3003c0s37b0n0: <<<<<<<<<<< 0
x3003c0s37b1n0: <<<<<<<<<<< 5
x3003c0s7b0n0: <<<<<<<<<<< 10
x3003c0s7b1n0: <<<<<<<<<<< 15
x3003c0s37b0n0: <<<<<<<<<<< 1
x3003c0s7b1n0: <<<<<<<<<<< 14
x3003c0s7b1n0: <<<<<<<<<<< 13
x3003c0s37b0n0: <<<<<<<<<<< 2
x3003c0s37b0n0: <<<<<<<<<<< 3
x3003c0s7b0n0: <<<<<<<<<<< 9
x3003c0s37b0n0: time to initialize megatron (seconds): 8.150
x3003c0s7b0n0: <<<<<<<<<<< 8
x3003c0s7b0n0: <<<<<<<<<<< 11
x3003c0s37b0n0: [after megatron is initialized] datetime: 2024-03-28 12:49:24 
x3003c0s37b0n0: get_accelerator and all_reduce  took 0.019037246704101562
x3003c0s37b0n0: building GPT model ...
x3003c0s37b0n0: [2024-03-28 12:49:24,689] [INFO] [utils.py:800:see_memory_usage] Before Building Model
x3003c0s37b0n0: [2024-03-28 12:49:24,690] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 5.68 GB         CA 0.0 GB         Max_CA 6 GB 
x3003c0s37b0n0: [2024-03-28 12:49:24,690] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 19.8 GB, percent = 3.9%
x3003c0s37b0n0: [2024-03-28 12:49:37,101] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 563, num_elems = 55.14B
x3003c0s37b0n0: [2024-03-28 12:49:37,172] [INFO] [utils.py:800:see_memory_usage] After Building Model
x3003c0s37b0n0: [2024-03-28 12:49:37,172] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 7.23 GB         CA 36.4 GB         Max_CA 38 GB 
x3003c0s37b0n0: [2024-03-28 12:49:37,172] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 20.29 GB, percent = 4.0%
x3003c0s37b0n0:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 55141736448
x3003c0s37b1n0: ninja: no work to do.
x3003c0s37b1n0: Time to load cpu_adam op: 2.2837510108947754 seconds
x3003c0s37b1n0: Time to load cpu_adam op: 2.3240175247192383 seconds
x3003c0s37b1n0: Time to load cpu_adam op: 2.331315517425537 seconds
x3003c0s37b0n0: ninja: no work to do.
x3003c0s37b0n0: Time to load cpu_adam op: 2.4834160804748535 seconds
x3003c0s37b0n0: Time to load cpu_adam op: 2.5378940105438232 seconds
x3003c0s37b0n0: Time to load cpu_adam op: 2.576603651046753 seconds
x3003c0s37b0n0: Time to load cpu_adam op: 2.5734846591949463 seconds
x3003c0s7b0n0: ninja: no work to do.
x3003c0s7b0n0: Time to load cpu_adam op: 2.46502423286438 seconds
x3003c0s7b0n0: Time to load cpu_adam op: 2.488898277282715 seconds
x3003c0s7b1n0: ninja: no work to do.
x3003c0s7b1n0: Time to load cpu_adam op: 2.46222186088562 seconds
x3003c0s7b1n0: Time to load cpu_adam op: 2.543740749359131 seconds
x3003c0s7b1n0: ninja: no work to do.
x3003c0s7b1n0: Time to load cpu_adam op: 2.497447967529297 seconds
x3003c0s7b0n0: ninja: no work to do.
x3003c0s7b0n0: Time to load cpu_adam op: 2.4463024139404297 seconds
x3003c0s7b0n0: Time to load cpu_adam op: 2.494598865509033 seconds
x3003c0s7b1n0: Time to load cpu_adam op: 2.501034736633301 seconds
x3003c0s37b1n0: ninja: no work to do.
x3003c0s37b1n0: Time to load cpu_adam op: 2.358081340789795 seconds
x3003c0s37b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b0n0: > learning rate decay style: cosine
x3003c0s37b0n0: DeepSpeed is enabled.
x3003c0s37b0n0: [2024-03-28 12:49:42,564] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.3+8074cd62, git-hash=8074cd62, git-branch=hybrid_opt_offload
x3003c0s7b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s7b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s7b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s7b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b0n0: [2024-03-28 12:49:42,641] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After args sanity test
x3003c0s37b0n0: [2024-03-28 12:49:42,642] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 6.46 GB         CA 36.4 GB         Max_CA 36 GB 
x3003c0s37b0n0: [2024-03-28 12:49:42,642] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.67 GB, percent = 5.5%
x3003c0s37b0n0: [2024-03-28 12:49:42,704] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure distributed model
x3003c0s37b0n0: [2024-03-28 12:49:42,704] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 6.46 GB         CA 36.4 GB         Max_CA 36 GB 
x3003c0s37b0n0: [2024-03-28 12:49:42,704] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.68 GB, percent = 5.5%
x3003c0s7b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b0n0: [2024-03-28 12:49:42,771] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After configure distributed model
x3003c0s37b0n0: [2024-03-28 12:49:42,772] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 6.46 GB         CA 36.4 GB         Max_CA 36 GB 
x3003c0s37b0n0: [2024-03-28 12:49:42,772] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.68 GB, percent = 5.5%
x3003c0s37b0n0: [2024-03-28 12:49:42,772] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
x3003c0s37b0n0: [2024-03-28 12:49:42,828] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After setting model parameters
x3003c0s37b0n0: [2024-03-28 12:49:42,829] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 6.46 GB         CA 36.4 GB         Max_CA 36 GB 
x3003c0s37b0n0: [2024-03-28 12:49:42,829] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.68 GB, percent = 5.5%
x3003c0s37b0n0: [2024-03-28 12:49:42,887] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure optimizer
x3003c0s37b0n0: [2024-03-28 12:49:42,887] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 6.46 GB         CA 36.4 GB         Max_CA 36 GB 
x3003c0s37b0n0: [2024-03-28 12:49:42,887] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.68 GB, percent = 5.5%
x3003c0s37b0n0: [2024-03-28 12:49:42,888] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
x3003c0s37b0n0: [2024-03-28 12:49:42,888] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
x3003c0s37b0n0: [2024-03-28 12:49:42,919] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
x3003c0s37b0n0: [2024-03-28 12:49:42,919] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
x3003c0s37b0n0: [2024-03-28 12:49:42,919] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
x3003c0s37b0n0: [2024-03-28 12:49:42,919] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
x3003c0s37b0n0: [2024-03-28 12:49:42,975] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
x3003c0s37b0n0: [2024-03-28 12:49:42,976] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 6.46 GB         CA 36.4 GB         Max_CA 36 GB 
x3003c0s37b0n0: [2024-03-28 12:49:42,976] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.68 GB, percent = 5.5%
x3003c0s37b0n0: [2024-03-28 12:49:42,978] [INFO] [stage3.py:137:__init__] Reduce bucket size 500,000,000
x3003c0s37b0n0: [2024-03-28 12:49:42,978] [INFO] [stage3.py:138:__init__] Prefetch bucket size 50,000,000
x3003c0s37b0n0: [2024-03-28 12:49:43,035] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
x3003c0s37b0n0: [2024-03-28 12:49:43,036] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 6.46 GB         CA 36.4 GB         Max_CA 36 GB 
x3003c0s37b0n0: [2024-03-28 12:49:43,036] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.68 GB, percent = 5.5%
x3003c0s37b0n0: Parameter Offload: Total persistent parameters: 1318912 in 161 params
x3003c0s7b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s7b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b0n0: [2024-03-28 12:49:43,127] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
x3003c0s37b0n0: [2024-03-28 12:49:43,127] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 6.46 GB         CA 36.4 GB         Max_CA 36 GB 
x3003c0s37b0n0: [2024-03-28 12:49:43,128] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.68 GB, percent = 5.5%
x3003c0s7b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b0n0: [2024-03-28 12:49:43,189] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
x3003c0s37b0n0: [2024-03-28 12:49:43,190] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 6.46 GB         CA 36.4 GB         Max_CA 36 GB 
x3003c0s37b0n0: [2024-03-28 12:49:43,190] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.68 GB, percent = 5.5%
x3003c0s37b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b0n0: [2024-03-28 12:49:43,282] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b0n0: [2024-03-28 12:49:43,282] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b0n0: [2024-03-28 12:49:43,282] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b0n0: [2024-03-28 12:49:43,282] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b0n0: [2024-03-28 12:49:43,282] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b0n0: [2024-03-28 12:49:43,282] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b0n0: [2024-03-28 12:49:43,282] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b0n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b1n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b1n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b1n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b1n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b1n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b1n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b1n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b1n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b0n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b0n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b0n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b0n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b0n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b0n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b0n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b0n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b1n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b1n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b1n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b1n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b1n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b1n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b1n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b1n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b0n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,283] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,284] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,285] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,286] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,287] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,288] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,289] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b0n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b1n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b1n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b0n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b0n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b1n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b1n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b0n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b0n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b0n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b0n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b0n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b0n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b0n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b1n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b0n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b1n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b1n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b1n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b1n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b1n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b1n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b1n0: [2024-03-28 12:49:43,290] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b0n0: [2024-03-28 12:49:47,603] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 33
x3003c0s37b0n0: [2024-03-28 12:49:47,604] [INFO] [utils.py:801:see_memory_usage] MA 6.42 GB         Max_MA 6.46 GB         CA 6.42 GB         Max_CA 36 GB 
x3003c0s37b0n0: [2024-03-28 12:49:47,604] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 36.25 GB, percent = 7.2%
x3003c0s37b0n0: [2024-03-28 12:49:47,753] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
x3003c0s37b0n0: [2024-03-28 12:49:47,754] [INFO] [utils.py:801:see_memory_usage] MA 6.42 GB         Max_MA 6.42 GB         CA 6.42 GB         Max_CA 6 GB 
x3003c0s37b0n0: [2024-03-28 12:49:47,754] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 37.61 GB, percent = 7.5%
x3003c0s37b0n0: [2024-03-28 12:50:07,308] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
x3003c0s37b0n0: [2024-03-28 12:50:07,309] [INFO] [utils.py:801:see_memory_usage] MA 6.42 GB         Max_MA 6.42 GB         CA 6.42 GB         Max_CA 6 GB 
x3003c0s37b0n0: [2024-03-28 12:50:07,309] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 227.25 GB, percent = 45.2%
x3003c0s37b0n0: [2024-03-28 12:50:07,432] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
x3003c0s37b0n0: [2024-03-28 12:50:07,432] [INFO] [utils.py:801:see_memory_usage] MA 6.42 GB         Max_MA 6.42 GB         CA 6.42 GB         Max_CA 6 GB 
x3003c0s37b0n0: [2024-03-28 12:50:07,433] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 230.94 GB, percent = 45.9%
x3003c0s37b0n0: [2024-03-28 12:50:14,043] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | init_optimizer_state: 6597.34
x3003c0s37b0n0: [2024-03-28 12:50:14,175] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
x3003c0s37b0n0: [2024-03-28 12:50:14,176] [INFO] [utils.py:801:see_memory_usage] MA 6.42 GB         Max_MA 6.42 GB         CA 6.42 GB         Max_CA 6 GB 
x3003c0s37b0n0: [2024-03-28 12:50:14,176] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 293.76 GB, percent = 58.4%
x3003c0s37b0n0: [2024-03-28 12:50:14,334] [INFO] [stage3.py:520:_setup_for_real_optimizer] optimizer state initialized
x3003c0s37b0n0: [2024-03-28 12:50:29,328] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
x3003c0s37b0n0: [2024-03-28 12:50:29,329] [INFO] [utils.py:801:see_memory_usage] MA 7.35 GB         Max_MA 8.89 GB         CA 20.98 GB         Max_CA 21 GB 
x3003c0s37b0n0: [2024-03-28 12:50:29,329] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 326.48 GB, percent = 64.9%
x3003c0s37b0n0: [2024-03-28 12:50:29,329] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
x3003c0s37b0n0: [2024-03-28 12:50:29,395] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure LR scheduler
x3003c0s37b0n0: [2024-03-28 12:50:29,396] [INFO] [utils.py:801:see_memory_usage] MA 7.35 GB         Max_MA 7.35 GB         CA 20.98 GB         Max_CA 21 GB 
x3003c0s37b0n0: [2024-03-28 12:50:29,396] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 326.48 GB, percent = 64.9%
x3003c0s37b0n0: [2024-03-28 12:50:29,396] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
x3003c0s37b0n0: [2024-03-28 12:50:29,396] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7fa8a21802b0>
x3003c0s37b0n0: [2024-03-28 12:50:29,396] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 12:50:29,462] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before rewriting optimizer step
x3003c0s37b0n0: [2024-03-28 12:50:29,463] [INFO] [utils.py:801:see_memory_usage] MA 7.35 GB         Max_MA 7.35 GB         CA 20.98 GB         Max_CA 21 GB 
x3003c0s37b0n0: [2024-03-28 12:50:29,463] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 326.48 GB, percent = 64.9%
x3003c0s37b0n0: [2024-03-28 12:50:29,530] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure checkpointing
x3003c0s37b0n0: [2024-03-28 12:50:29,530] [INFO] [utils.py:801:see_memory_usage] MA 7.35 GB         Max_MA 7.35 GB         CA 20.98 GB         Max_CA 21 GB 
x3003c0s37b0n0: [2024-03-28 12:50:29,531] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 326.47 GB, percent = 64.9%
x3003c0s37b0n0: [2024-03-28 12:50:29,531] [INFO] [config.py:998:print] DeepSpeedEngine configuration:
x3003c0s37b0n0: [2024-03-28 12:50:29,531] [INFO] [config.py:1002:print]   activation_checkpointing_config  {
x3003c0s37b0n0:     "partition_activations": false, 
x3003c0s37b0n0:     "contiguous_memory_optimization": false, 
x3003c0s37b0n0:     "cpu_checkpointing": false, 
x3003c0s37b0n0:     "number_checkpoints": null, 
x3003c0s37b0n0:     "synchronize_checkpoint_boundary": false, 
x3003c0s37b0n0:     "profile": false
x3003c0s37b0n0: }
x3003c0s37b0n0: [2024-03-28 12:50:29,531] [INFO] [config.py:1002:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
x3003c0s37b0n0: [2024-03-28 12:50:29,531] [INFO] [config.py:1002:print]   amp_enabled .................. False
x3003c0s37b0n0: [2024-03-28 12:50:29,531] [INFO] [config.py:1002:print]   amp_params ................... False
x3003c0s37b0n0: [2024-03-28 12:50:29,531] [INFO] [config.py:1002:print]   autotuning_config ............ {
x3003c0s37b0n0:     "enabled": false, 
x3003c0s37b0n0:     "start_step": null, 
x3003c0s37b0n0:     "end_step": null, 
x3003c0s37b0n0:     "metric_path": null, 
x3003c0s37b0n0:     "arg_mappings": null, 
x3003c0s37b0n0:     "metric": "throughput", 
x3003c0s37b0n0:     "model_info": null, 
x3003c0s37b0n0:     "results_dir": "autotuning_results", 
x3003c0s37b0n0:     "exps_dir": "autotuning_exps", 
x3003c0s37b0n0:     "overwrite": true, 
x3003c0s37b0n0:     "fast": true, 
x3003c0s37b0n0:     "start_profile_step": 3, 
x3003c0s37b0n0:     "end_profile_step": 5, 
x3003c0s37b0n0:     "tuner_type": "gridsearch", 
x3003c0s37b0n0:     "tuner_early_stopping": 5, 
x3003c0s37b0n0:     "tuner_num_trials": 50, 
x3003c0s37b0n0:     "model_info_path": null, 
x3003c0s37b0n0:     "mp_size": 1, 
x3003c0s37b0n0:     "max_train_batch_size": null, 
x3003c0s37b0n0:     "min_train_batch_size": 1, 
x3003c0s37b0n0:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
x3003c0s37b0n0:     "min_train_micro_batch_size_per_gpu": 1, 
x3003c0s37b0n0:     "num_tuning_micro_batch_sizes": 3
x3003c0s37b0n0: }
x3003c0s37b0n0: [2024-03-28 12:50:29,531] [INFO] [config.py:1002:print]   bfloat16_enabled ............. True
x3003c0s37b0n0: [2024-03-28 12:50:29,531] [INFO] [config.py:1002:print]   bfloat16_immediate_grad_update  False
x3003c0s37b0n0: [2024-03-28 12:50:29,531] [INFO] [config.py:1002:print]   checkpoint_parallel_write_pipeline  False
x3003c0s37b0n0: [2024-03-28 12:50:29,531] [INFO] [config.py:1002:print]   checkpoint_tag_validation_enabled  True
x3003c0s37b0n0: [2024-03-28 12:50:29,531] [INFO] [config.py:1002:print]   checkpoint_tag_validation_fail  False
x3003c0s37b0n0: [2024-03-28 12:50:29,531] [INFO] [config.py:1002:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fa8a2180520>
x3003c0s37b0n0: [2024-03-28 12:50:29,531] [INFO] [config.py:1002:print]   communication_data_type ...... None
x3003c0s37b0n0: [2024-03-28 12:50:29,531] [INFO] [config.py:1002:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
x3003c0s37b0n0: [2024-03-28 12:50:29,531] [INFO] [config.py:1002:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
x3003c0s37b0n0: [2024-03-28 12:50:29,531] [INFO] [config.py:1002:print]   curriculum_enabled_legacy .... False
x3003c0s37b0n0: [2024-03-28 12:50:29,531] [INFO] [config.py:1002:print]   curriculum_params_legacy ..... False
x3003c0s37b0n0: [2024-03-28 12:50:29,531] [INFO] [config.py:1002:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
x3003c0s37b0n0: [2024-03-28 12:50:29,531] [INFO] [config.py:1002:print]   data_efficiency_enabled ...... False
x3003c0s37b0n0: [2024-03-28 12:50:29,531] [INFO] [config.py:1002:print]   dataloader_drop_last ......... False
x3003c0s37b0n0: [2024-03-28 12:50:29,531] [INFO] [config.py:1002:print]   datastates_config ............ {
x3003c0s37b0n0:     "enabled": null, 
x3003c0s37b0n0:     "config": {
x3003c0s37b0n0:     }
x3003c0s37b0n0: }
x3003c0s37b0n0: [2024-03-28 12:50:29,531] [INFO] [config.py:1002:print]   disable_allgather ............ False
x3003c0s37b0n0: [2024-03-28 12:50:29,531] [INFO] [config.py:1002:print]   dump_state ................... False
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   dynamic_loss_scale_args ...... None
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   eigenvalue_enabled ........... False
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   eigenvalue_gas_boundary_resolution  1
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   eigenvalue_layer_name ........ bert.encoder.layer
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   eigenvalue_layer_num ......... 0
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   eigenvalue_max_iter .......... 100
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   eigenvalue_stability ......... 1e-06
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   eigenvalue_tol ............... 0.01
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   eigenvalue_verbose ........... False
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   elasticity_enabled ........... False
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   flops_profiler_config ........ {
x3003c0s37b0n0:     "enabled": false, 
x3003c0s37b0n0:     "recompute_fwd_factor": 0.0, 
x3003c0s37b0n0:     "profile_step": 1, 
x3003c0s37b0n0:     "module_depth": -1, 
x3003c0s37b0n0:     "top_modules": 1, 
x3003c0s37b0n0:     "detailed": true, 
x3003c0s37b0n0:     "output_file": null
x3003c0s37b0n0: }
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   fp16_auto_cast ............... None
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   fp16_enabled ................. False
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   fp16_master_weights_and_gradients  False
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   global_rank .................. 0
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   grad_accum_dtype ............. bf16
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   gradient_accumulation_steps .. 1
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   gradient_clipping ............ 0.0
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   gradient_predivide_factor .... 1.0
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   graph_harvesting ............. False
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   initial_dynamic_scale ........ 1
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   load_universal_checkpoint .... False
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   loss_scale ................... 1.0
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   memory_breakdown ............. True
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   mics_hierarchial_params_gather  False
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   mics_shard_size .............. -1
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   nebula_config ................ {
x3003c0s37b0n0:     "enabled": false, 
x3003c0s37b0n0:     "persistent_storage_path": null, 
x3003c0s37b0n0:     "persistent_time_interval": 100, 
x3003c0s37b0n0:     "num_of_version_in_retention": 2, 
x3003c0s37b0n0:     "enable_nebula_load": true, 
x3003c0s37b0n0:     "load_path": null
x3003c0s37b0n0: }
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   optimizer_legacy_fusion ...... False
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   optimizer_name ............... None
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   optimizer_params ............. None
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   pld_enabled .................. False
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   pld_params ................... False
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   prescale_gradients ........... False
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   scheduler_name ............... None
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   scheduler_params ............. None
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   seq_parallel_communication_data_type  torch.float32
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   sparse_attention ............. None
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   sparse_gradients_enabled ..... False
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   steps_per_print .............. 1
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   train_batch_size ............. 64
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   train_micro_batch_size_per_gpu  4
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   use_data_before_expert_parallel_  False
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   use_node_local_storage ....... False
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   wall_clock_breakdown ......... True
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   weight_quantization_config ... None
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   world_size ................... 16
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   zero_allow_untested_optimizer  False
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0, prefetch_optimizer=True, part_grads_async=True, prefetch_optimizer_gap=5) sub_group_size=100000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
x3003c0s37b0n0: [2024-03-28 12:50:29,532] [INFO] [config.py:1002:print]   zero_enabled ................. True
x3003c0s37b0n0: [2024-03-28 12:50:29,533] [INFO] [config.py:1002:print]   zero_force_ds_cpu_optimizer .. True
x3003c0s37b0n0: [2024-03-28 12:50:29,533] [INFO] [config.py:1002:print]   zero_optimization_stage ...... 3
x3003c0s37b0n0: [2024-03-28 12:50:29,533] [INFO] [config.py:988:print_user_config]   json = {
x3003c0s37b0n0:     "train_batch_size": 64, 
x3003c0s37b0n0:     "train_micro_batch_size_per_gpu": 4, 
x3003c0s37b0n0:     "steps_per_print": 1, 
x3003c0s37b0n0:     "zero_optimization": {
x3003c0s37b0n0:         "stage": 3, 
x3003c0s37b0n0:         "offload_optimizer": {
x3003c0s37b0n0:             "device": "cpu", 
x3003c0s37b0n0:             "ratio": 1, 
x3003c0s37b0n0:             "pin_memory": true, 
x3003c0s37b0n0:             "prefetch_optimizer": 1, 
x3003c0s37b0n0:             "part_grads_async": 1, 
x3003c0s37b0n0:             "prefetch_optimizer_gap": 5
x3003c0s37b0n0:         }, 
x3003c0s37b0n0:         "sub_group_size": 1.000000e+08
x3003c0s37b0n0:     }, 
x3003c0s37b0n0:     "bf16": {
x3003c0s37b0n0:         "enabled": true
x3003c0s37b0n0:     }, 
x3003c0s37b0n0:     "data_types": {
x3003c0s37b0n0:         "grad_accum_dtype": "bf16"
x3003c0s37b0n0:     }, 
x3003c0s37b0n0:     "wall_clock_breakdown": true, 
x3003c0s37b0n0:     "memory_breakdown": true, 
x3003c0s37b0n0:     "flops_profiler": {
x3003c0s37b0n0:         "enabled": false
x3003c0s37b0n0:     }
x3003c0s37b0n0: }
x3003c0s7b1n0: <TIMER:model-and-optimizer-setup,65.97259092330933>
x3003c0s7b1n0: <TIMER:model-and-optimizer-setup,65.97290468215942>
x3003c0s7b1n0: <TIMER:model-and-optimizer-setup,65.97315645217896>
x3003c0s7b0n0: <TIMER:model-and-optimizer-setup,65.97336435317993>
x3003c0s37b1n0: <TIMER:model-and-optimizer-setup,65.97336602210999>
x3003c0s37b1n0: <TIMER:model-and-optimizer-setup,65.9733989238739><TIMER:model-and-optimizer-setup,65.97339677810669>
x3003c0s37b1n0: 
x3003c0s7b1n0: <TIMER:model-and-optimizer-setup,65.97347474098206>
x3003c0s37b0n0: <TIMER:model-and-optimizer-setup,65.97362756729126>
x3003c0s7b0n0: <TIMER:model-and-optimizer-setup,65.97354865074158>
x3003c0s7b0n0: <TIMER:model-and-optimizer-setup,65.97356224060059>
x3003c0s7b0n0: <TIMER:model-and-optimizer-setup,65.97358250617981>
x3003c0s37b1n0: <TIMER:model-and-optimizer-setup,65.97367215156555>
x3003c0s37b0n0: <TIMER:model-and-optimizer-setup,65.97377586364746>
x3003c0s37b0n0: <TIMER:model-and-optimizer-setup,65.97387051582336>
x3003c0s37b0n0: <TIMER:model-and-optimizer-setup,65.97395944595337>
x3003c0s37b0n0: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-28 12:50:30 
x3003c0s37b0n0: > building train, validation, and test datasets ...
x3003c0s37b0n0:  > datasets target sizes (minimum size):
x3003c0s37b0n0:     train:      640
x3003c0s37b0n0:     validation: 0
x3003c0s37b0n0:     test:       0
x3003c0s37b0n0: > building train, validation, and test datasets for GPT ...
x3003c0s37b0n0: Single data path provided for train, valid & test
x3003c0s37b0n0:  > building dataset index ...
x3003c0s37b0n0:     reading sizes...
x3003c0s37b0n0:     reading pointers...
x3003c0s37b0n0:     reading document index...
x3003c0s37b0n0:     creating numpy buffer of mmap...
x3003c0s37b0n0:     creating memory view of numpy buffer...
x3003c0s37b0n0:  > finished creating indexed dataset in 0.002000 seconds
x3003c0s37b0n0:     number of documents: 79000
x3003c0s37b0n0:  > dataset split:
x3003c0s37b0n0:     train:
x3003c0s37b0n0:      document indices in [0, 74971) total of 74971 documents
x3003c0s37b0n0:     validation:
x3003c0s37b0n0:      document indices in [74971, 78921) total of 3950 documents
x3003c0s37b0n0:     test:
x3003c0s37b0n0:      document indices in [78921, 79000) total of 79 documents
x3003c0s37b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/d5d1daec41eb416469c3827ed48205ed_doc_idx.npy
x3003c0s37b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/d5d1daec41eb416469c3827ed48205ed_sample_idx.npy
x3003c0s37b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/d5d1daec41eb416469c3827ed48205ed_shuffle_idx.npy
x3003c0s37b0n0:     loaded indexed file in 0.002 seconds
x3003c0s37b0n0:     total number of samples: 108448
x3003c0s37b0n0:     total number of epochs: 1
x3003c0s37b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_doc_idx.npy
x3003c0s37b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_sample_idx.npy
x3003c0s37b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_shuffle_idx.npy
x3003c0s37b0n0:     loaded indexed file in 0.004 seconds
x3003c0s37b0n0:     total number of samples: 5792
x3003c0s37b0n0:     total number of epochs: 1
x3003c0s37b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_doc_idx.npy
x3003c0s37b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_sample_idx.npy
x3003c0s37b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_shuffle_idx.npy
x3003c0s37b0n0:     loaded indexed file in 0.037 seconds
x3003c0s37b0n0:     total number of samples: 185
x3003c0s37b0n0:     total number of epochs: 1
x3003c0s37b0n0: > finished creating GPT datasets ...
x3003c0s37b0n0: <TIMER:train/valid/test-data-iterators-setup,0.49840545654296875>
x3003c0s7b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5079472064971924>
x3003c0s7b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5150256156921387>
x3003c0s37b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5178878307342529>
x3003c0s37b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5188946723937988>
x3003c0s37b0n0: <TIMER:train/valid/test-data-iterators-setup,0.524571418762207>
x3003c0s37b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5307052135467529>
x3003c0s7b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5318317413330078>
x3003c0s7b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5344560146331787>
x3003c0s37b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5384218692779541>
x3003c0s37b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5434186458587646>
x3003c0s37b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5438053607940674>
x3003c0s7b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5555460453033447>
x3003c0s7b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5622591972351074>
x3003c0s7b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5764963626861572>
x3003c0s7b1n0: <TIMER:train/valid/test-data-iterators-setup,0.6506438255310059>
x3003c0s37b0n0: [after dataloaders are built] datetime: 2024-03-28 12:50:31 
x3003c0s37b0n0: done with setup ...
x3003c0s37b0n0: training ...
x3003c0s7b1n0: (min, max) time across ranks (ms):
x3003c0s7b1n0:     model-and-optimizer-setup ......................: (65972.59, 65973.96)
x3003c0s7b1n0:     train/valid/test-data-iterators-setup ..........: (498.41, 650.64)
x3003c0s37b0n0: [before training begins] datetime: 2024-03-28 12:50:31 
x3003c0s37b0n0: [before the start of training step] datetime: 2024-03-28 12:50:31 
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 12:50:31,456] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 12:50:31,456] [INFO] [utils.py:801:see_memory_usage] MA 7.36 GB         Max_MA 7.36 GB         CA 8.02 GB         Max_CA 21 GB 
x3003c0s37b0n0: [2024-03-28 12:50:31,456] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 329.0 GB, percent = 65.4%
x3003c0s37b0n0: [2024-03-28 12:50:31,576] [INFO] [checkpointing.py:539:forward] Activation Checkpointing Information
x3003c0s37b0n0: [2024-03-28 12:50:31,576] [INFO] [checkpointing.py:540:forward] ----Partition Activations False, CPU CHECKPOINTING False
x3003c0s37b0n0: [2024-03-28 12:50:31,576] [INFO] [checkpointing.py:541:forward] ----contiguous Memory Checkpointing False with 80 total layers
x3003c0s37b0n0: [2024-03-28 12:50:31,576] [INFO] [checkpointing.py:543:forward] ----Synchronization False
x3003c0s37b0n0: [2024-03-28 12:50:31,576] [INFO] [checkpointing.py:544:forward] ----Profiling time in checkpointing False
x3003c0s37b0n0: [2024-03-28 12:50:48,246] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 12:50:48,246] [INFO] [utils.py:801:see_memory_usage] MA 21.15 GB         Max_MA 24.22 GB         CA 28.51 GB         Max_CA 29 GB 
x3003c0s37b0n0: [2024-03-28 12:50:48,247] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 329.31 GB, percent = 65.4%
x3003c0s37b0n0: [2024-03-28 12:50:48,520] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 12:50:48,521] [INFO] [utils.py:801:see_memory_usage] MA 21.15 GB         Max_MA 21.15 GB         CA 21.43 GB         Max_CA 29 GB 
x3003c0s37b0n0: [2024-03-28 12:50:48,521] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 329.32 GB, percent = 65.4%
x3003c0s37b0n0: [2024-03-28 12:51:38,941] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 12:51:38,941] [INFO] [utils.py:801:see_memory_usage] MA 12.09 GB         Max_MA 26.34 GB         CA 14.14 GB         Max_CA 36 GB 
x3003c0s37b0n0: [2024-03-28 12:51:38,941] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 329.41 GB, percent = 65.5%
x3003c0s37b0n0: [2024-03-28 12:51:39,020] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 12:51:39,021] [INFO] [utils.py:801:see_memory_usage] MA 12.09 GB         Max_MA 12.09 GB         CA 14.14 GB         Max_CA 14 GB 
x3003c0s37b0n0: [2024-03-28 12:51:39,021] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 329.42 GB, percent = 65.5%
x3003c0s37b1n0: [2024-03-28 12:51:45,721] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.601599931716919
x3003c0s37b1n0: [2024-03-28 12:51:45,721] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.602372169494629
x3003c0s37b1n0: [2024-03-28 12:51:45,726] [INFO] [stage3.py:2251:step] Full outer step loop took 6.606910467147827
x3003c0s37b1n0: [2024-03-28 12:51:45,726] [INFO] [stage3.py:2251:step] Full outer step loop took 6.607255697250366
x3003c0s37b1n0: [2024-03-28 12:51:45,833] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.713456630706787
x3003c0s37b1n0: [2024-03-28 12:51:45,837] [INFO] [stage3.py:2251:step] Full outer step loop took 6.718440771102905
x3003c0s37b1n0: [2024-03-28 12:51:45,856] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.737360954284668
x3003c0s37b1n0: [2024-03-28 12:51:45,861] [INFO] [stage3.py:2251:step] Full outer step loop took 6.742475509643555
x3003c0s7b0n0: [2024-03-28 12:51:45,946] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.826687812805176
x3003c0s7b0n0: [2024-03-28 12:51:45,963] [INFO] [stage3.py:2251:step] Full outer step loop took 6.8433685302734375
x3003c0s7b0n0: [2024-03-28 12:51:45,972] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.853104829788208
x3003c0s7b0n0: [2024-03-28 12:51:45,982] [INFO] [stage3.py:2251:step] Full outer step loop took 6.862287282943726
x3003c0s7b0n0: [2024-03-28 12:51:46,000] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.880482912063599
x3003c0s7b0n0: [2024-03-28 12:51:46,005] [INFO] [stage3.py:2251:step] Full outer step loop took 6.885819673538208
x3003c0s7b0n0: [2024-03-28 12:51:46,019] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.899958610534668
x3003c0s7b0n0: [2024-03-28 12:51:46,024] [INFO] [stage3.py:2251:step] Full outer step loop took 6.904877662658691
x3003c0s37b0n0: [2024-03-28 12:51:46,026] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.907291412353516
x3003c0s37b0n0: [2024-03-28 12:51:46,032] [INFO] [stage3.py:2251:step] Full outer step loop took 6.91252064704895
x3003c0s37b0n0: [2024-03-28 12:51:46,060] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.940158128738403
x3003c0s37b0n0: [2024-03-28 12:51:46,065] [INFO] [stage3.py:2251:step] Full outer step loop took 6.945962190628052
x3003c0s37b0n0: [2024-03-28 12:51:46,083] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.964265584945679
x3003c0s37b0n0: [2024-03-28 12:51:46,083] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.964266300201416
x3003c0s37b0n0: [2024-03-28 12:51:46,088] [INFO] [stage3.py:2251:step] Full outer step loop took 6.9692206382751465
x3003c0s37b0n0: [2024-03-28 12:51:46,088] [INFO] [stage3.py:2251:step] Full outer step loop took 6.969472169876099
x3003c0s7b1n0: [2024-03-28 12:51:46,345] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.225669860839844
x3003c0s7b1n0: [2024-03-28 12:51:46,350] [INFO] [stage3.py:2251:step] Full outer step loop took 7.230729818344116
x3003c0s7b1n0: [2024-03-28 12:51:46,468] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.349366664886475
x3003c0s7b1n0: [2024-03-28 12:51:46,473] [INFO] [stage3.py:2251:step] Full outer step loop took 7.354265451431274
x3003c0s7b1n0: [2024-03-28 12:51:46,480] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.361453294754028
x3003c0s7b1n0: [2024-03-28 12:51:46,488] [INFO] [stage3.py:2251:step] Full outer step loop took 7.369128704071045
x3003c0s7b1n0: [2024-03-28 12:51:46,489] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.370187282562256
x3003c0s7b1n0: [2024-03-28 12:51:46,494] [INFO] [stage3.py:2251:step] Full outer step loop took 7.375045537948608
x3003c0s37b1n0: [2024-03-28 12:51:46,518] [INFO] [stage3.py:2277:step] End to end step took 7.398518085479736
x3003c0s37b0n0: [2024-03-28 12:51:46,518] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6969.47
x3003c0s7b1n0: [2024-03-28 12:51:46,518] [INFO] [stage3.py:2277:step] End to end step took 7.3989222049713135
x3003c0s7b1n0: [2024-03-28 12:51:46,518] [INFO] [stage3.py:2277:step] End to end step took 7.398919343948364
x3003c0s7b1n0: [2024-03-28 12:51:46,518] [INFO] [stage3.py:2277:step] End to end step took 7.3989784717559814
x3003c0s37b0n0: [2024-03-28 12:51:46,518] [INFO] [stage3.py:2277:step] End to end step took 7.398993253707886
x3003c0s37b0n0: [2024-03-28 12:51:46,518] [INFO] [stage3.py:2277:step] End to end step took 7.399002552032471
x3003c0s37b0n0: [2024-03-28 12:51:46,518] [WARNING] [stage3.py:2267:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3003c0s37b0n0: [2024-03-28 12:51:46,518] [INFO] [stage3.py:2277:step] End to end step took 7.399142503738403
x3003c0s7b1n0: [2024-03-28 12:51:46,518] [INFO] [stage3.py:2277:step] End to end step took 7.399080991744995
x3003c0s37b0n0: [2024-03-28 12:51:46,518] [INFO] [stage3.py:2277:step] End to end step took 7.39909291267395
x3003c0s37b1n0: [2024-03-28 12:51:46,518] [INFO] [stage3.py:2277:step] End to end step took 7.399031162261963
x3003c0s7b0n0: [2024-03-28 12:51:46,518] [INFO] [stage3.py:2277:step] End to end step took 7.398898124694824
x3003c0s7b0n0: [2024-03-28 12:51:46,518] [INFO] [stage3.py:2277:step] End to end step took 7.398925304412842
x3003c0s37b1n0: [2024-03-28 12:51:46,518] [INFO] [stage3.py:2277:step] End to end step took 7.399180889129639
x3003c0s7b0n0: [2024-03-28 12:51:46,518] [INFO] [stage3.py:2277:step] End to end step took 7.398970127105713
x3003c0s7b0n0: [2024-03-28 12:51:46,518] [INFO] [stage3.py:2277:step] End to end step took 7.399026155471802
x3003c0s37b1n0: [2024-03-28 12:51:46,518] [INFO] [stage3.py:2277:step] End to end step took 7.3993213176727295
x3003c0s37b0n0: [2024-03-28 12:51:46,519] [INFO] [logging.py:96:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 12:51:46,519] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 16951.06 | bwd_microstep: 50223.53 | bwd_inner_microstep: 50099.55 | bwd_allreduce_microstep: 123.88 | step_microstep: 7497.89
x3003c0s37b0n0: [2024-03-28 12:51:46,519] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 16951.06 | bwd: 50223.53 | bwd_inner: 50099.55 | bwd_allreduce: 123.88 | step: 7497.89
x3003c0s37b0n0: [2024-03-28 12:51:46,642] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 12:51:46,643] [INFO] [utils.py:801:see_memory_usage] MA 10.62 GB         Max_MA 13.28 GB         CA 11.28 GB         Max_CA 19 GB 
x3003c0s37b0n0: [2024-03-28 12:51:46,643] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.38 GB, percent = 82.1%
x3003c0s37b0n0: <TIMER:interval-time,75.38375520706177><TIMER:interval-time,75.38376450538635>
x3003c0s37b0n0: <TIMER:interval-time,75.38376641273499>
x3003c0s37b0n0: 
x3003c0s7b1n0: <TIMER:interval-time,75.38373517990112><TIMER:interval-time,75.38374137878418><TIMER:interval-time,75.38374710083008>
x3003c0s7b1n0: 
x3003c0s7b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,75.38371467590332><TIMER:interval-time,75.38371276855469><TIMER:interval-time,75.38371920585632>
x3003c0s37b1n0: 
x3003c0s37b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,75.38372278213501>
x3003c0s37b0n0: <TIMER:interval-time,75.38387370109558>
x3003c0s7b1n0: <TIMER:interval-time,75.3838677406311>
x3003c0s7b0n0: <TIMER:interval-time,75.38366436958313><TIMER:interval-time,75.38366913795471>
x3003c0s7b0n0: 
x3003c0s7b0n0: <TIMER:interval-time,75.38368844985962>
x3003c0s7b0n0: <TIMER:interval-time,75.38367629051208>
x3003c0s7b1n0:  elapsed_time 75.383735 | consumed samples:           64 | consumed tokens:       131072 | elapsed time per iteration (ms): 75383.7 | learning rate: 3.000E-04 | global batch size:    64 | lm loss: 1.246584E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.849 | TFLOPs: 58.61 |
x3003c0s37b0n0: [Rank 0] (after 1 iterations) memory (MB) | allocated: 10878.7685546875 | max allocated: 10878.76904296875 | reserved: 11546.0 | max reserved: 11546.0
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 12:51:46,799] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 12:51:46,799] [INFO] [utils.py:801:see_memory_usage] MA 10.63 GB         Max_MA 10.63 GB         CA 11.28 GB         Max_CA 11 GB 
x3003c0s37b0n0: [2024-03-28 12:51:46,799] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.48 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:52:00,520] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 12:52:00,520] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 28.96 GB         CA 32.87 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 12:52:00,520] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.47 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:52:00,605] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 12:52:00,605] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 25.84 GB         CA 26.54 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 12:52:00,605] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.47 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:52:35,693] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 12:52:35,694] [INFO] [utils.py:801:see_memory_usage] MA 13.35 GB         Max_MA 29.91 GB         CA 14.79 GB         Max_CA 37 GB 
x3003c0s37b0n0: [2024-03-28 12:52:35,694] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.49 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:52:35,772] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 12:52:35,773] [INFO] [utils.py:801:see_memory_usage] MA 13.35 GB         Max_MA 13.35 GB         CA 14.79 GB         Max_CA 15 GB 
x3003c0s37b0n0: [2024-03-28 12:52:35,773] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.49 GB, percent = 82.2%
x3003c0s37b1n0: [2024-03-28 12:52:40,131] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.327871322631836
x3003c0s37b1n0: [2024-03-28 12:52:40,135] [INFO] [stage3.py:2251:step] Full outer step loop took 4.332239389419556
x3003c0s37b1n0: [2024-03-28 12:52:40,415] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.612522602081299
x3003c0s37b1n0: [2024-03-28 12:52:40,424] [INFO] [stage3.py:2251:step] Full outer step loop took 4.621164798736572
x3003c0s37b1n0: [2024-03-28 12:52:40,436] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.632858037948608
x3003c0s37b1n0: [2024-03-28 12:52:40,441] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.637999773025513
x3003c0s37b1n0: [2024-03-28 12:52:40,443] [INFO] [stage3.py:2251:step] Full outer step loop took 4.640255451202393
x3003c0s37b1n0: [2024-03-28 12:52:40,446] [INFO] [stage3.py:2251:step] Full outer step loop took 4.642849445343018
x3003c0s7b1n0: [2024-03-28 12:52:40,514] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.711346864700317
x3003c0s7b1n0: [2024-03-28 12:52:40,526] [INFO] [stage3.py:2251:step] Full outer step loop took 4.723555564880371
x3003c0s7b0n0: [2024-03-28 12:52:40,519] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.715916156768799
x3003c0s7b0n0: [2024-03-28 12:52:40,524] [INFO] [stage3.py:2251:step] Full outer step loop took 4.720865249633789
x3003c0s37b0n0: [2024-03-28 12:52:40,556] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.753517150878906
x3003c0s37b0n0: [2024-03-28 12:52:40,571] [INFO] [stage3.py:2251:step] Full outer step loop took 4.768383741378784
x3003c0s37b0n0: [2024-03-28 12:52:40,600] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.7973902225494385
x3003c0s37b0n0: [2024-03-28 12:52:40,609] [INFO] [stage3.py:2251:step] Full outer step loop took 4.806359529495239
x3003c0s37b0n0: [2024-03-28 12:52:40,640] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.837160348892212
x3003c0s37b0n0: [2024-03-28 12:52:40,644] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.840864896774292
x3003c0s37b0n0: [2024-03-28 12:52:40,646] [INFO] [stage3.py:2251:step] Full outer step loop took 4.843315601348877
x3003c0s37b0n0: [2024-03-28 12:52:40,649] [INFO] [stage3.py:2251:step] Full outer step loop took 4.8457489013671875
x3003c0s7b0n0: [2024-03-28 12:52:40,687] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.883946418762207
x3003c0s7b0n0: [2024-03-28 12:52:40,714] [INFO] [stage3.py:2251:step] Full outer step loop took 4.9106059074401855
x3003c0s7b1n0: [2024-03-28 12:52:40,781] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.978243589401245
x3003c0s7b1n0: [2024-03-28 12:52:40,786] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.983131170272827
x3003c0s7b1n0: [2024-03-28 12:52:40,787] [INFO] [stage3.py:2251:step] Full outer step loop took 4.983839750289917
x3003c0s7b1n0: [2024-03-28 12:52:40,792] [INFO] [stage3.py:2251:step] Full outer step loop took 4.988816738128662
x3003c0s7b0n0: [2024-03-28 12:52:40,802] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.9994964599609375
x3003c0s7b0n0: [2024-03-28 12:52:40,810] [INFO] [stage3.py:2251:step] Full outer step loop took 5.007437467575073
x3003c0s7b0n0: [2024-03-28 12:52:40,814] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.0113136768341064
x3003c0s7b1n0: [2024-03-28 12:52:40,817] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.014434814453125
x3003c0s7b0n0: [2024-03-28 12:52:40,819] [INFO] [stage3.py:2251:step] Full outer step loop took 5.016189098358154
x3003c0s7b1n0: [2024-03-28 12:52:40,822] [INFO] [stage3.py:2251:step] Full outer step loop took 5.019274711608887
x3003c0s7b0n0: [2024-03-28 12:52:40,846] [INFO] [stage3.py:2277:step] End to end step took 5.0429816246032715
x3003c0s37b1n0: [2024-03-28 12:52:40,846] [INFO] [stage3.py:2277:step] End to end step took 5.0429558753967285
x3003c0s37b0n0: [2024-03-28 12:52:40,846] [INFO] [stage3.py:2277:step] End to end step took 5.043058395385742
x3003c0s37b0n0: [2024-03-28 12:52:40,846] [INFO] [stage3.py:2277:step] End to end step took 5.043087720870972
x3003c0s37b1n0: [2024-03-28 12:52:40,846] [INFO] [stage3.py:2277:step] End to end step took 5.043041706085205
x3003c0s7b0n0: [2024-03-28 12:52:40,846] [INFO] [stage3.py:2277:step] End to end step took 5.043003559112549
x3003c0s7b1n0: [2024-03-28 12:52:40,846] [INFO] [stage3.py:2277:step] End to end step took 5.043048143386841
x3003c0s7b1n0: [2024-03-28 12:52:40,846] [INFO] [stage3.py:2277:step] End to end step took 5.043243646621704
x3003c0s7b1n0: [2024-03-28 12:52:40,846] [INFO] [stage3.py:2277:step] End to end step took 5.043280601501465
x3003c0s7b0n0: [2024-03-28 12:52:40,846] [INFO] [stage3.py:2277:step] End to end step took 5.0435497760772705
x3003c0s37b0n0: [2024-03-28 12:52:40,846] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4807.23
x3003c0s37b0n0: [2024-03-28 12:52:40,847] [INFO] [stage3.py:2277:step] End to end step took 5.043715953826904
x3003c0s7b0n0: [2024-03-28 12:52:40,846] [INFO] [stage3.py:2277:step] End to end step took 5.0437538623809814
x3003c0s37b1n0: [2024-03-28 12:52:40,847] [INFO] [stage3.py:2277:step] End to end step took 5.04379415512085
x3003c0s37b1n0: [2024-03-28 12:52:40,847] [INFO] [stage3.py:2277:step] End to end step took 5.043774366378784
x3003c0s37b0n0: [2024-03-28 12:52:40,847] [WARNING] [stage3.py:2267:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3003c0s37b0n0: [2024-03-28 12:52:40,847] [INFO] [stage3.py:2277:step] End to end step took 5.0440733432769775
x3003c0s7b1n0: [2024-03-28 12:52:40,847] [INFO] [stage3.py:2277:step] End to end step took 5.043898105621338
x3003c0s37b0n0: [2024-03-28 12:52:40,847] [INFO] [logging.py:96:log_dist] [Rank 0] step=2, skipped=0, lr=[0.0002918585038060976], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 12:52:40,848] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 13694.56 | bwd_microstep: 34939.16 | bwd_inner_microstep: 34805.12 | bwd_allreduce_microstep: 133.97 | step_microstep: 5074.58
x3003c0s37b0n0: [2024-03-28 12:52:40,848] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 13694.55 | bwd: 34939.16 | bwd_inner: 34805.11 | bwd_allreduce: 133.99 | step: 5074.59
x3003c0s37b0n0: [2024-03-28 12:52:40,971] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 12:52:40,971] [INFO] [utils.py:801:see_memory_usage] MA 10.62 GB         Max_MA 13.58 GB         CA 11.28 GB         Max_CA 18 GB 
x3003c0s37b0n0: [2024-03-28 12:52:40,972] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.49 GB, percent = 82.2%
x3003c0s37b0n0: <TIMER:interval-time,54.32798409461975><TIMER:interval-time,54.327986001968384><TIMER:interval-time,54.32798647880554>
x3003c0s37b0n0: 
x3003c0s37b0n0: 
x3003c0s37b0n0: <TIMER:interval-time,54.32799768447876>
x3003c0s7b0n0: <TIMER:interval-time,54.32795834541321><TIMER:interval-time,54.32795977592468><TIMER:interval-time,54.32796359062195>
x3003c0s7b0n0: 
x3003c0s7b0n0: 
x3003c0s7b0n0: <TIMER:interval-time,54.327962160110474>
x3003c0s7b1n0: <TIMER:interval-time,54.328057050704956>
x3003c0s7b1n0: <TIMER:interval-time,54.328068017959595>
x3003c0s37b1n0: <TIMER:interval-time,54.327982902526855><TIMER:interval-time,54.32798409461975><TIMER:interval-time,54.32798719406128>
x3003c0s37b1n0: 
x3003c0s37b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,54.327993869781494>
x3003c0s7b1n0: <TIMER:interval-time,54.328145027160645><TIMER:interval-time,54.328160524368286>
x3003c0s7b1n0: 
x3003c0s7b1n0:  elapsed_time 54.328057 | consumed samples:          128 | consumed tokens:       262144 | elapsed time per iteration (ms): 54328.1 | learning rate: 2.919E-04 | global batch size:    64 | lm loss: 1.082584E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.178 | TFLOPs: 81.33 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 12:52:41,121] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 12:52:41,122] [INFO] [utils.py:801:see_memory_usage] MA 10.63 GB         Max_MA 10.63 GB         CA 11.28 GB         Max_CA 11 GB 
x3003c0s37b0n0: [2024-03-28 12:52:41,122] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.51 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:52:54,624] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 12:52:54,625] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 28.96 GB         CA 32.87 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 12:52:54,625] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.5 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:52:54,715] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 12:52:54,716] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 25.84 GB         CA 26.54 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 12:52:54,716] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.5 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:53:29,590] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 12:53:29,590] [INFO] [utils.py:801:see_memory_usage] MA 13.35 GB         Max_MA 29.91 GB         CA 14.79 GB         Max_CA 37 GB 
x3003c0s37b0n0: [2024-03-28 12:53:29,590] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.51 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:53:29,672] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 12:53:29,672] [INFO] [utils.py:801:see_memory_usage] MA 13.35 GB         Max_MA 13.35 GB         CA 14.79 GB         Max_CA 15 GB 
x3003c0s37b0n0: [2024-03-28 12:53:29,673] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.51 GB, percent = 82.2%
x3003c0s37b1n0: [2024-03-28 12:53:34,160] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.457891225814819
x3003c0s37b1n0: [2024-03-28 12:53:34,178] [INFO] [stage3.py:2251:step] Full outer step loop took 4.476771831512451
x3003c0s37b0n0: [2024-03-28 12:53:34,277] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.575751066207886
x3003c0s37b0n0: [2024-03-28 12:53:34,307] [INFO] [stage3.py:2251:step] Full outer step loop took 4.6050848960876465
x3003c0s37b1n0: [2024-03-28 12:53:34,379] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.677316904067993
x3003c0s37b1n0: [2024-03-28 12:53:34,395] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.6933369636535645
x3003c0s37b1n0: [2024-03-28 12:53:34,400] [INFO] [stage3.py:2251:step] Full outer step loop took 4.697868824005127
x3003c0s37b1n0: [2024-03-28 12:53:34,403] [INFO] [stage3.py:2251:step] Full outer step loop took 4.701691389083862
x3003c0s37b1n0: [2024-03-28 12:53:34,433] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.7318432331085205
x3003c0s37b1n0: [2024-03-28 12:53:34,438] [INFO] [stage3.py:2251:step] Full outer step loop took 4.736177444458008
x3003c0s37b0n0: [2024-03-28 12:53:34,545] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.8433849811553955
x3003c0s37b0n0: [2024-03-28 12:53:34,546] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.844617128372192
x3003c0s37b0n0: [2024-03-28 12:53:34,551] [INFO] [stage3.py:2251:step] Full outer step loop took 4.849507093429565
x3003c0s37b0n0: [2024-03-28 12:53:34,551] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.849658012390137
x3003c0s37b0n0: [2024-03-28 12:53:34,553] [INFO] [stage3.py:2251:step] Full outer step loop took 4.85134482383728
x3003c0s7b1n0: [2024-03-28 12:53:34,553] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.851101636886597
x3003c0s37b0n0: [2024-03-28 12:53:34,557] [INFO] [stage3.py:2251:step] Full outer step loop took 4.855323553085327
x3003c0s7b1n0: [2024-03-28 12:53:34,585] [INFO] [stage3.py:2251:step] Full outer step loop took 4.882765769958496
x3003c0s7b0n0: [2024-03-28 12:53:34,641] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.939790487289429
x3003c0s7b0n0: [2024-03-28 12:53:34,646] [INFO] [stage3.py:2251:step] Full outer step loop took 4.9446375370025635
x3003c0s7b1n0: [2024-03-28 12:53:34,682] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.980054616928101
x3003c0s7b1n0: [2024-03-28 12:53:34,718] [INFO] [stage3.py:2251:step] Full outer step loop took 5.015740871429443
x3003c0s7b0n0: [2024-03-28 12:53:34,729] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.02696681022644
x3003c0s7b0n0: [2024-03-28 12:53:34,734] [INFO] [stage3.py:2251:step] Full outer step loop took 5.032100677490234
x3003c0s7b1n0: [2024-03-28 12:53:34,752] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.050359487533569
x3003c0s7b0n0: [2024-03-28 12:53:34,756] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.054569244384766
x3003c0s7b1n0: [2024-03-28 12:53:34,757] [INFO] [stage3.py:2251:step] Full outer step loop took 5.055221319198608
x3003c0s7b0n0: [2024-03-28 12:53:34,761] [INFO] [stage3.py:2251:step] Full outer step loop took 5.059436321258545
x3003c0s7b1n0: [2024-03-28 12:53:34,773] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.071032285690308
x3003c0s7b0n0: [2024-03-28 12:53:34,774] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.072380065917969
x3003c0s7b1n0: [2024-03-28 12:53:34,778] [INFO] [stage3.py:2251:step] Full outer step loop took 5.076168775558472
x3003c0s7b0n0: [2024-03-28 12:53:34,779] [INFO] [stage3.py:2251:step] Full outer step loop took 5.077215671539307
x3003c0s7b1n0: [2024-03-28 12:53:34,801] [INFO] [stage3.py:2277:step] End to end step took 5.099860906600952
x3003c0s7b0n0: [2024-03-28 12:53:34,801] [INFO] [stage3.py:2277:step] End to end step took 5.099953651428223
x3003c0s7b0n0: [2024-03-28 12:53:34,801] [INFO] [stage3.py:2277:step] End to end step took 5.099965810775757
x3003c0s37b0n0: [2024-03-28 12:53:34,802] [INFO] [stage3.py:2277:step] End to end step took 5.10015344619751
x3003c0s37b0n0: [2024-03-28 12:53:34,802] [INFO] [stage3.py:2277:step] End to end step took 5.10016942024231
x3003c0s7b1n0: [2024-03-28 12:53:34,802] [INFO] [stage3.py:2277:step] End to end step took 5.100155591964722
x3003c0s7b0n0: [2024-03-28 12:53:34,802] [INFO] [stage3.py:2277:step] End to end step took 5.100365161895752
x3003c0s37b1n0: [2024-03-28 12:53:34,801] [INFO] [stage3.py:2277:step] End to end step took 5.099974155426025
x3003c0s37b1n0: [2024-03-28 12:53:34,802] [INFO] [stage3.py:2277:step] End to end step took 5.100131988525391
x3003c0s37b1n0: [2024-03-28 12:53:34,802] [INFO] [stage3.py:2277:step] End to end step took 5.1002867221832275
x3003c0s7b1n0: [2024-03-28 12:53:34,802] [INFO] [stage3.py:2277:step] End to end step took 5.1006247997283936
x3003c0s7b1n0: [2024-03-28 12:53:34,802] [INFO] [stage3.py:2277:step] End to end step took 5.100663423538208
x3003c0s37b1n0: [2024-03-28 12:53:34,802] [INFO] [stage3.py:2277:step] End to end step took 5.100712060928345
x3003c0s7b0n0: [2024-03-28 12:53:34,802] [INFO] [stage3.py:2277:step] End to end step took 5.100813150405884
x3003c0s37b0n0: [2024-03-28 12:53:34,802] [INFO] [stage3.py:2277:step] End to end step took 5.100803375244141
x3003c0s37b0n0: [2024-03-28 12:53:34,802] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4851.71
x3003c0s37b0n0: [2024-03-28 12:53:34,803] [WARNING] [stage3.py:2267:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3003c0s37b0n0: [2024-03-28 12:53:34,803] [INFO] [stage3.py:2277:step] End to end step took 5.101256847381592
x3003c0s37b0n0: [2024-03-28 12:53:34,803] [INFO] [logging.py:96:log_dist] [Rank 0] step=3, skipped=0, lr=[0.00026841599982106197], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 12:53:34,804] [INFO] [timer.py:260:stop] epoch=0/micro_step=3/global_step=3, RunningAvgSamplesPerSec=1.1922461824847082, CurrSamplesPerSec=1.1922461824847082, MemAllocated=10.62GB, MaxMemAllocated=13.58GB
x3003c0s37b0n0: [2024-03-28 12:53:34,804] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 13467.98 | bwd_microstep: 34725.04 | bwd_inner_microstep: 34593.63 | bwd_allreduce_microstep: 131.33 | step_microstep: 5131.25
x3003c0s37b0n0: [2024-03-28 12:53:34,804] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 13467.97 | bwd: 34725.03 | bwd_inner: 34593.62 | bwd_allreduce: 131.35 | step: 5131.26
x3003c0s37b0n0: [2024-03-28 12:53:34,934] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 12:53:34,935] [INFO] [utils.py:801:see_memory_usage] MA 10.62 GB         Max_MA 13.58 GB         CA 11.28 GB         Max_CA 18 GB 
x3003c0s37b0n0: [2024-03-28 12:53:34,935] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.51 GB, percent = 82.2%
x3003c0s37b0n0: <TIMER:interval-time,53.962841749191284><TIMER:interval-time,53.962843894958496><TIMER:interval-time,53.962843894958496>
x3003c0s37b0n0: 
x3003c0s37b0n0: 
x3003c0s37b0n0: <TIMER:interval-time,53.96284747123718>
x3003c0s7b0n0: <TIMER:interval-time,53.962876081466675><TIMER:interval-time,53.96287751197815><TIMER:interval-time,53.96288180351257>
x3003c0s7b1n0: <TIMER:interval-time,53.96290469169617><TIMER:interval-time,53.96291756629944><TIMER:interval-time,53.9629180431366>
x3003c0s7b1n0: <TIMER:interval-time,53.96291184425354>
x3003c0s7b0n0: 
x3003c0s7b0n0: <TIMER:interval-time,53.96286749839783>
x3003c0s7b1n0: 
x3003c0s7b0n0: 
x3003c0s7b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,53.96283292770386><TIMER:interval-time,53.96283507347107><TIMER:interval-time,53.96283769607544><TIMER:interval-time,53.962839126586914>
x3003c0s37b1n0: 
x3003c0s37b1n0: 
x3003c0s37b1n0: 
x3003c0s7b1n0:  elapsed_time 53.962918 | consumed samples:          192 | consumed tokens:       393216 | elapsed time per iteration (ms): 53962.9 | learning rate: 2.684E-04 | global batch size:    64 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.186 | TFLOPs: 81.88 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 12:53:35,085] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 12:53:35,086] [INFO] [utils.py:801:see_memory_usage] MA 10.63 GB         Max_MA 10.63 GB         CA 11.28 GB         Max_CA 11 GB 
x3003c0s37b0n0: [2024-03-28 12:53:35,086] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.53 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:53:48,557] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 12:53:48,558] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 28.96 GB         CA 32.87 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 12:53:48,558] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.53 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:53:48,645] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 12:53:48,646] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 25.84 GB         CA 26.54 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 12:53:48,646] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.53 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:54:25,414] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 12:54:25,415] [INFO] [utils.py:801:see_memory_usage] MA 13.35 GB         Max_MA 29.91 GB         CA 14.79 GB         Max_CA 37 GB 
x3003c0s37b0n0: [2024-03-28 12:54:25,415] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.53 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:54:25,493] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 12:54:25,494] [INFO] [utils.py:801:see_memory_usage] MA 13.35 GB         Max_MA 13.35 GB         CA 14.79 GB         Max_CA 15 GB 
x3003c0s37b0n0: [2024-03-28 12:54:25,494] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.53 GB, percent = 82.2%
x3003c0s7b0n0: [2024-03-28 12:54:30,195] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.672531366348267
x3003c0s37b1n0: [2024-03-28 12:54:30,203] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.680541753768921
x3003c0s37b1n0: [2024-03-28 12:54:30,208] [INFO] [stage3.py:2251:step] Full outer step loop took 4.685399532318115
x3003c0s7b0n0: [2024-03-28 12:54:30,203] [INFO] [stage3.py:2251:step] Full outer step loop took 4.680809736251831
x3003c0s37b0n0: [2024-03-28 12:54:30,242] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.719566345214844
x3003c0s37b0n0: [2024-03-28 12:54:30,249] [INFO] [stage3.py:2251:step] Full outer step loop took 4.726262092590332
x3003c0s37b1n0: [2024-03-28 12:54:30,267] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.744248151779175
x3003c0s37b1n0: [2024-03-28 12:54:30,274] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.750872611999512
x3003c0s37b1n0: [2024-03-28 12:54:30,275] [INFO] [stage3.py:2251:step] Full outer step loop took 4.752697467803955
x3003c0s37b1n0: [2024-03-28 12:54:30,282] [INFO] [stage3.py:2251:step] Full outer step loop took 4.759577989578247
x3003c0s37b1n0: [2024-03-28 12:54:30,294] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.771246433258057
x3003c0s37b1n0: [2024-03-28 12:54:30,298] [INFO] [stage3.py:2251:step] Full outer step loop took 4.7756264209747314
x3003c0s37b0n0: [2024-03-28 12:54:30,370] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.846982955932617
x3003c0s37b0n0: [2024-03-28 12:54:30,377] [INFO] [stage3.py:2251:step] Full outer step loop took 4.854295492172241
x3003c0s37b0n0: [2024-03-28 12:54:30,409] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.886625289916992
x3003c0s37b0n0: [2024-03-28 12:54:30,415] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.892480373382568
x3003c0s37b0n0: [2024-03-28 12:54:30,417] [INFO] [stage3.py:2251:step] Full outer step loop took 4.893872022628784
x3003c0s37b0n0: [2024-03-28 12:54:30,420] [INFO] [stage3.py:2251:step] Full outer step loop took 4.897360801696777
x3003c0s7b1n0: [2024-03-28 12:54:30,442] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.918442487716675
x3003c0s7b0n0: [2024-03-28 12:54:30,454] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.9312028884887695
x3003c0s7b0n0: [2024-03-28 12:54:30,463] [INFO] [stage3.py:2251:step] Full outer step loop took 4.940436363220215
x3003c0s7b1n0: [2024-03-28 12:54:30,478] [INFO] [stage3.py:2251:step] Full outer step loop took 4.955283164978027
x3003c0s7b1n0: [2024-03-28 12:54:30,485] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.962045669555664
x3003c0s7b0n0: [2024-03-28 12:54:30,488] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.965044736862183
x3003c0s7b1n0: [2024-03-28 12:54:30,494] [INFO] [stage3.py:2251:step] Full outer step loop took 4.9717020988464355
x3003c0s7b0n0: [2024-03-28 12:54:30,503] [INFO] [stage3.py:2251:step] Full outer step loop took 4.980767011642456
x3003c0s7b0n0: [2024-03-28 12:54:30,504] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.9812469482421875
x3003c0s7b0n0: [2024-03-28 12:54:30,508] [INFO] [stage3.py:2251:step] Full outer step loop took 4.9855732917785645
x3003c0s7b1n0: [2024-03-28 12:54:30,513] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.990124225616455
x3003c0s7b1n0: [2024-03-28 12:54:30,527] [INFO] [stage3.py:2251:step] Full outer step loop took 5.004000663757324
x3003c0s7b1n0: [2024-03-28 12:54:30,530] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.0073041915893555
x3003c0s7b1n0: [2024-03-28 12:54:30,534] [INFO] [stage3.py:2251:step] Full outer step loop took 5.011658668518066
x3003c0s37b0n0: [2024-03-28 12:54:30,558] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4894.15
x3003c0s7b0n0: [2024-03-28 12:54:30,558] [INFO] [stage3.py:2277:step] End to end step took 5.035322427749634
x3003c0s37b1n0: [2024-03-28 12:54:30,558] [INFO] [stage3.py:2277:step] End to end step took 5.0354392528533936
x3003c0s37b1n0: [2024-03-28 12:54:30,558] [INFO] [stage3.py:2277:step] End to end step took 5.035409688949585
x3003c0s7b1n0: [2024-03-28 12:54:30,558] [INFO] [stage3.py:2277:step] End to end step took 5.0355048179626465
x3003c0s7b0n0: [2024-03-28 12:54:30,558] [INFO] [stage3.py:2277:step] End to end step took 5.035512208938599
x3003c0s7b1n0: [2024-03-28 12:54:30,558] [INFO] [stage3.py:2277:step] End to end step took 5.035537004470825
x3003c0s37b0n0: [2024-03-28 12:54:30,558] [INFO] [stage3.py:2277:step] End to end step took 5.0354743003845215
x3003c0s37b0n0: [2024-03-28 12:54:30,558] [WARNING] [stage3.py:2267:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3003c0s37b0n0: [2024-03-28 12:54:30,558] [INFO] [stage3.py:2277:step] End to end step took 5.035632133483887
x3003c0s37b1n0: [2024-03-28 12:54:30,558] [INFO] [stage3.py:2277:step] End to end step took 5.0355610847473145
x3003c0s7b0n0: [2024-03-28 12:54:30,558] [INFO] [stage3.py:2277:step] End to end step took 5.035637378692627
x3003c0s7b1n0: [2024-03-28 12:54:30,558] [INFO] [stage3.py:2277:step] End to end step took 5.0356645584106445
x3003c0s7b1n0: [2024-03-28 12:54:30,558] [INFO] [stage3.py:2277:step] End to end step took 5.035706043243408
x3003c0s37b0n0: [2024-03-28 12:54:30,558] [INFO] [stage3.py:2277:step] End to end step took 5.035599708557129
x3003c0s37b0n0: [2024-03-28 12:54:30,559] [INFO] [logging.py:96:log_dist] [Rank 0] step=4, skipped=0, lr=[0.00023249999999999996], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 12:54:30,559] [INFO] [timer.py:260:stop] epoch=0/micro_step=4/global_step=4, RunningAvgSamplesPerSec=1.1726806795016724, CurrSamplesPerSec=1.1537469725502643, MemAllocated=10.62GB, MaxMemAllocated=13.58GB
x3003c0s37b0n0: [2024-03-28 12:54:30,559] [INFO] [stage3.py:2277:step] End to end step took 5.036189079284668
x3003c0s37b1n0: [2024-03-28 12:54:30,559] [INFO] [stage3.py:2277:step] End to end step took 5.036173105239868
x3003c0s7b0n0: [2024-03-28 12:54:30,559] [INFO] [stage3.py:2277:step] End to end step took 5.036281585693359
x3003c0s37b0n0: [2024-03-28 12:54:30,559] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 13439.70 | bwd_microstep: 36623.22 | bwd_inner_microstep: 36474.17 | bwd_allreduce_microstep: 148.99 | step_microstep: 5065.21
x3003c0s37b0n0: [2024-03-28 12:54:30,559] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 13439.68 | bwd: 36623.21 | bwd_inner: 36474.16 | bwd_allreduce: 149.00 | step: 5065.21
x3003c0s37b0n0: [2024-03-28 12:54:30,670] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 12:54:30,670] [INFO] [utils.py:801:see_memory_usage] MA 10.62 GB         Max_MA 13.58 GB         CA 11.28 GB         Max_CA 18 GB 
x3003c0s37b0n0: [2024-03-28 12:54:30,670] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.53 GB, percent = 82.2%
x3003c0s37b0n0: <TIMER:interval-time,55.735058307647705>
x3003c0s37b0n0: <TIMER:interval-time,55.73505139350891><TIMER:interval-time,55.735069274902344>
x3003c0s37b0n0: 
x3003c0s7b0n0: <TIMER:interval-time,55.73510432243347><TIMER:interval-time,55.735105752944946><TIMER:interval-time,55.73510670661926><TIMER:interval-time,55.735108613967896>
x3003c0s7b0n0: 
x3003c0s7b0n0: 
x3003c0s7b0n0: 
x3003c0s7b1n0: <TIMER:interval-time,55.73504447937012><TIMER:interval-time,55.73504447937012>
x3003c0s7b1n0: 
x3003c0s7b1n0: <TIMER:interval-time,55.73505783081055>
x3003c0s37b1n0: <TIMER:interval-time,55.73516869544983><TIMER:interval-time,55.7351713180542><TIMER:interval-time,55.73517465591431><TIMER:interval-time,55.73517346382141>
x3003c0s37b1n0: 
x3003c0s37b1n0: 
x3003c0s37b1n0: 
x3003c0s37b0n0: <TIMER:interval-time,55.735180616378784>
x3003c0s7b1n0: <TIMER:interval-time,55.73515868186951>
x3003c0s7b1n0:  elapsed_time 55.735058 | consumed samples:          256 | consumed tokens:       524288 | elapsed time per iteration (ms): 55735.1 | learning rate: 2.325E-04 | global batch size:    64 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.148 | TFLOPs: 79.27 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 12:54:30,807] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 12:54:30,808] [INFO] [utils.py:801:see_memory_usage] MA 10.63 GB         Max_MA 10.63 GB         CA 11.28 GB         Max_CA 11 GB 
x3003c0s37b0n0: [2024-03-28 12:54:30,808] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.53 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:54:46,863] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 12:54:46,864] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 28.96 GB         CA 32.87 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 12:54:46,864] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.52 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:54:46,948] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 12:54:46,949] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 25.84 GB         CA 26.54 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 12:54:46,949] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.52 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:55:25,713] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 12:55:25,714] [INFO] [utils.py:801:see_memory_usage] MA 13.35 GB         Max_MA 29.91 GB         CA 14.79 GB         Max_CA 37 GB 
x3003c0s37b0n0: [2024-03-28 12:55:25,714] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.52 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:55:25,791] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 12:55:25,792] [INFO] [utils.py:801:see_memory_usage] MA 13.35 GB         Max_MA 13.35 GB         CA 14.79 GB         Max_CA 15 GB 
x3003c0s37b0n0: [2024-03-28 12:55:25,792] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.52 GB, percent = 82.2%
x3003c0s37b1n0: [2024-03-28 12:55:30,376] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.552799463272095
x3003c0s37b1n0: [2024-03-28 12:55:30,388] [INFO] [stage3.py:2251:step] Full outer step loop took 4.565617561340332
x3003c0s37b1n0: [2024-03-28 12:55:30,475] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.65251088142395
x3003c0s37b1n0: [2024-03-28 12:55:30,480] [INFO] [stage3.py:2251:step] Full outer step loop took 4.658049821853638
x3003c0s37b1n0: [2024-03-28 12:55:30,519] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.696682929992676
x3003c0s37b1n0: [2024-03-28 12:55:30,519] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.696810007095337
x3003c0s37b1n0: [2024-03-28 12:55:30,524] [INFO] [stage3.py:2251:step] Full outer step loop took 4.7015345096588135
x3003c0s37b1n0: [2024-03-28 12:55:30,524] [INFO] [stage3.py:2251:step] Full outer step loop took 4.701665639877319
x3003c0s37b0n0: [2024-03-28 12:55:30,586] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.76338005065918
x3003c0s37b0n0: [2024-03-28 12:55:30,595] [INFO] [stage3.py:2251:step] Full outer step loop took 4.772567510604858
x3003c0s7b0n0: [2024-03-28 12:55:30,649] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.826538562774658
x3003c0s7b0n0: [2024-03-28 12:55:30,654] [INFO] [stage3.py:2251:step] Full outer step loop took 4.832003355026245
x3003c0s37b0n0: [2024-03-28 12:55:30,662] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.839780330657959
x3003c0s37b0n0: [2024-03-28 12:55:30,663] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.841172456741333
x3003c0s37b0n0: [2024-03-28 12:55:30,667] [INFO] [stage3.py:2251:step] Full outer step loop took 4.844846725463867
x3003c0s37b0n0: [2024-03-28 12:55:30,668] [INFO] [stage3.py:2251:step] Full outer step loop took 4.846108675003052
x3003c0s37b0n0: [2024-03-28 12:55:30,685] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.862922191619873
x3003c0s37b0n0: [2024-03-28 12:55:30,690] [INFO] [stage3.py:2251:step] Full outer step loop took 4.867806673049927
x3003c0s7b1n0: [2024-03-28 12:55:30,795] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.972388982772827
x3003c0s7b1n0: [2024-03-28 12:55:30,814] [INFO] [stage3.py:2251:step] Full outer step loop took 4.99168848991394
x3003c0s7b1n0: [2024-03-28 12:55:30,843] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.020342588424683
x3003c0s7b1n0: [2024-03-28 12:55:30,852] [INFO] [stage3.py:2251:step] Full outer step loop took 5.029173374176025
x3003c0s7b1n0: [2024-03-28 12:55:30,852] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.030002117156982
x3003c0s7b1n0: [2024-03-28 12:55:30,870] [INFO] [stage3.py:2251:step] Full outer step loop took 5.047794818878174
x3003c0s7b1n0: [2024-03-28 12:55:30,897] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.074299335479736
x3003c0s7b1n0: [2024-03-28 12:55:30,901] [INFO] [stage3.py:2251:step] Full outer step loop took 5.07915472984314
x3003c0s7b0n0: [2024-03-28 12:55:30,930] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.107346057891846
x3003c0s7b0n0: [2024-03-28 12:55:30,938] [INFO] [stage3.py:2251:step] Full outer step loop took 5.115790843963623
x3003c0s7b0n0: [2024-03-28 12:55:30,954] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.132213592529297
x3003c0s7b0n0: [2024-03-28 12:55:30,955] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.132232189178467
x3003c0s7b0n0: [2024-03-28 12:55:30,959] [INFO] [stage3.py:2251:step] Full outer step loop took 5.137064456939697
x3003c0s7b0n0: [2024-03-28 12:55:30,959] [INFO] [stage3.py:2251:step] Full outer step loop took 5.1371009349823
x3003c0s7b0n0: [2024-03-28 12:55:30,983] [INFO] [stage3.py:2277:step] End to end step took 5.160307884216309
x3003c0s7b0n0: [2024-03-28 12:55:30,983] [INFO] [stage3.py:2277:step] End to end step took 5.160440683364868
x3003c0s7b1n0: [2024-03-28 12:55:30,983] [INFO] [stage3.py:2277:step] End to end step took 5.160423040390015
x3003c0s7b0n0: [2024-03-28 12:55:30,983] [INFO] [stage3.py:2277:step] End to end step took 5.160611152648926
x3003c0s37b1n0: [2024-03-28 12:55:30,983] [INFO] [stage3.py:2277:step] End to end step took 5.16059947013855
x3003c0s37b0n0: [2024-03-28 12:55:30,983] [INFO] [stage3.py:2277:step] End to end step took 5.160660982131958
x3003c0s7b1n0: [2024-03-28 12:55:30,983] [INFO] [stage3.py:2277:step] End to end step took 5.160828113555908
x3003c0s7b1n0: [2024-03-28 12:55:30,983] [INFO] [stage3.py:2277:step] End to end step took 5.160897493362427
x3003c0s37b0n0: [2024-03-28 12:55:30,983] [INFO] [stage3.py:2277:step] End to end step took 5.160970687866211
x3003c0s37b0n0: [2024-03-28 12:55:30,983] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4846.37
x3003c0s7b1n0: [2024-03-28 12:55:30,983] [INFO] [stage3.py:2277:step] End to end step took 5.160990953445435
x3003c0s37b1n0: [2024-03-28 12:55:30,983] [INFO] [stage3.py:2277:step] End to end step took 5.1611199378967285
x3003c0s7b0n0: [2024-03-28 12:55:30,983] [INFO] [stage3.py:2277:step] End to end step took 5.161093235015869
x3003c0s37b1n0: [2024-03-28 12:55:30,983] [INFO] [stage3.py:2277:step] End to end step took 5.161179780960083
x3003c0s37b0n0: [2024-03-28 12:55:30,983] [WARNING] [stage3.py:2267:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3003c0s37b0n0: [2024-03-28 12:55:30,984] [INFO] [stage3.py:2277:step] End to end step took 5.161300420761108
x3003c0s37b1n0: [2024-03-28 12:55:30,984] [INFO] [stage3.py:2277:step] End to end step took 5.16138219833374
x3003c0s37b0n0: [2024-03-28 12:55:30,984] [INFO] [stage3.py:2277:step] End to end step took 5.161510467529297
x3003c0s37b0n0: [2024-03-28 12:55:30,984] [INFO] [logging.py:96:log_dist] [Rank 0] step=5, skipped=0, lr=[0.0001884425039850356], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 12:55:30,984] [INFO] [timer.py:260:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=1.1339041825284453, CurrSamplesPerSec=1.063567199873869, MemAllocated=10.62GB, MaxMemAllocated=13.58GB
x3003c0s37b0n0: [2024-03-28 12:55:30,985] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 16028.57 | bwd_microstep: 38622.73 | bwd_inner_microstep: 38473.20 | bwd_allreduce_microstep: 149.46 | step_microstep: 5192.14
x3003c0s37b0n0: [2024-03-28 12:55:30,985] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 16028.56 | bwd: 38622.73 | bwd_inner: 38473.20 | bwd_allreduce: 149.47 | step: 5192.14
x3003c0s37b0n0: [2024-03-28 12:55:31,095] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 12:55:31,096] [INFO] [utils.py:801:see_memory_usage] MA 10.62 GB         Max_MA 13.58 GB         CA 11.28 GB         Max_CA 18 GB 
x3003c0s37b0n0: [2024-03-28 12:55:31,096] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.52 GB, percent = 82.2%
x3003c0s37b0n0: <TIMER:interval-time,60.42506003379822><TIMER:interval-time,60.42506504058838><TIMER:interval-time,60.42504596710205>
x3003c0s37b0n0: 
x3003c0s37b0n0: 
x3003c0s37b0n0: <TIMER:interval-time,60.42506814002991>
x3003c0s7b0n0: <TIMER:interval-time,60.42521953582764><TIMER:interval-time,60.42522192001343><TIMER:interval-time,60.42522192001343>
x3003c0s7b0n0: 
x3003c0s7b0n0: 
x3003c0s7b0n0: <TIMER:interval-time,60.42522668838501>
x3003c0s7b1n0: <TIMER:interval-time,60.425212144851685><TIMER:interval-time,60.42521643638611><TIMER:interval-time,60.425204277038574>
x3003c0s7b1n0: <TIMER:interval-time,60.42521810531616>
x3003c0s7b1n0: 
x3003c0s7b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,60.42522644996643><TIMER:interval-time,60.42522883415222>
x3003c0s37b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,60.42523455619812><TIMER:interval-time,60.42523670196533>
x3003c0s37b1n0: 
x3003c0s7b1n0:  elapsed_time 60.425212 | consumed samples:          320 | consumed tokens:       655360 | elapsed time per iteration (ms): 60425.2 | learning rate: 1.884E-04 | global batch size:    64 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.059 | TFLOPs: 73.12 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 12:55:31,236] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 12:55:31,236] [INFO] [utils.py:801:see_memory_usage] MA 10.63 GB         Max_MA 10.63 GB         CA 11.28 GB         Max_CA 11 GB 
x3003c0s37b0n0: [2024-03-28 12:55:31,237] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.54 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:55:46,698] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 12:55:46,698] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 28.96 GB         CA 32.87 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 12:55:46,698] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.54 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:55:46,783] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 12:55:46,783] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 25.84 GB         CA 26.54 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 12:55:46,783] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.54 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:56:25,082] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 12:56:25,083] [INFO] [utils.py:801:see_memory_usage] MA 13.35 GB         Max_MA 29.91 GB         CA 14.79 GB         Max_CA 37 GB 
x3003c0s37b0n0: [2024-03-28 12:56:25,083] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.54 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:56:25,161] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 12:56:25,162] [INFO] [utils.py:801:see_memory_usage] MA 13.35 GB         Max_MA 13.35 GB         CA 14.79 GB         Max_CA 15 GB 
x3003c0s37b0n0: [2024-03-28 12:56:25,162] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.54 GB, percent = 82.2%
x3003c0s37b1n0: [2024-03-28 12:56:29,646] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.4536566734313965
x3003c0s37b1n0: [2024-03-28 12:56:29,668] [INFO] [stage3.py:2251:step] Full outer step loop took 4.4757232666015625
x3003c0s37b1n0: [2024-03-28 12:56:29,845] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.6526734828948975
x3003c0s37b1n0: [2024-03-28 12:56:29,850] [INFO] [stage3.py:2251:step] Full outer step loop took 4.657918214797974
x3003c0s37b1n0: [2024-03-28 12:56:29,876] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.684186935424805
x3003c0s37b1n0: [2024-03-28 12:56:29,881] [INFO] [stage3.py:2251:step] Full outer step loop took 4.6890552043914795
x3003c0s37b1n0: [2024-03-28 12:56:29,895] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.703394412994385
x3003c0s37b1n0: [2024-03-28 12:56:29,900] [INFO] [stage3.py:2251:step] Full outer step loop took 4.707722902297974
x3003c0s7b0n0: [2024-03-28 12:56:29,953] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.761311769485474
x3003c0s7b0n0: [2024-03-28 12:56:29,958] [INFO] [stage3.py:2251:step] Full outer step loop took 4.7665863037109375
x3003c0s7b1n0: [2024-03-28 12:56:29,974] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.781448841094971
x3003c0s7b1n0: [2024-03-28 12:56:29,980] [INFO] [stage3.py:2251:step] Full outer step loop took 4.7875988483428955
x3003c0s37b0n0: [2024-03-28 12:56:30,065] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.873241424560547
x3003c0s37b0n0: [2024-03-28 12:56:30,066] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.874604225158691
x3003c0s37b0n0: [2024-03-28 12:56:30,071] [INFO] [stage3.py:2251:step] Full outer step loop took 4.879635572433472
x3003c0s37b0n0: [2024-03-28 12:56:30,074] [INFO] [stage3.py:2251:step] Full outer step loop took 4.882295608520508
x3003c0s7b1n0: [2024-03-28 12:56:30,093] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.900803565979004
x3003c0s7b1n0: [2024-03-28 12:56:30,098] [INFO] [stage3.py:2251:step] Full outer step loop took 4.90566349029541
x3003c0s37b0n0: [2024-03-28 12:56:30,101] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.908611297607422
x3003c0s37b0n0: [2024-03-28 12:56:30,101] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.9093945026397705
x3003c0s37b0n0: [2024-03-28 12:56:30,105] [INFO] [stage3.py:2251:step] Full outer step loop took 4.913587808609009
x3003c0s37b0n0: [2024-03-28 12:56:30,106] [INFO] [stage3.py:2251:step] Full outer step loop took 4.914111375808716
x3003c0s7b1n0: [2024-03-28 12:56:30,136] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.9441142082214355
x3003c0s7b1n0: [2024-03-28 12:56:30,136] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.944120645523071
x3003c0s7b1n0: [2024-03-28 12:56:30,141] [INFO] [stage3.py:2251:step] Full outer step loop took 4.949100017547607
x3003c0s7b1n0: [2024-03-28 12:56:30,141] [INFO] [stage3.py:2251:step] Full outer step loop took 4.949211120605469
x3003c0s7b0n0: [2024-03-28 12:56:30,234] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.0423784255981445
x3003c0s7b0n0: [2024-03-28 12:56:30,240] [INFO] [stage3.py:2251:step] Full outer step loop took 5.048404932022095
x3003c0s7b0n0: [2024-03-28 12:56:30,241] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.048964023590088
x3003c0s7b0n0: [2024-03-28 12:56:30,241] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.0489091873168945
x3003c0s7b0n0: [2024-03-28 12:56:30,246] [INFO] [stage3.py:2251:step] Full outer step loop took 5.053746700286865
x3003c0s7b0n0: [2024-03-28 12:56:30,246] [INFO] [stage3.py:2251:step] Full outer step loop took 5.053849458694458
x3003c0s7b0n0: [2024-03-28 12:56:30,269] [INFO] [stage3.py:2277:step] End to end step took 5.077167749404907
x3003c0s7b0n0: [2024-03-28 12:56:30,269] [INFO] [stage3.py:2277:step] End to end step took 5.077295541763306
x3003c0s7b1n0: [2024-03-28 12:56:30,269] [INFO] [stage3.py:2277:step] End to end step took 5.077260971069336
x3003c0s37b0n0: [2024-03-28 12:56:30,269] [INFO] [stage3.py:2277:step] End to end step took 5.077327728271484
x3003c0s7b1n0: [2024-03-28 12:56:30,269] [INFO] [stage3.py:2277:step] End to end step took 5.077274560928345
x3003c0s37b1n0: [2024-03-28 12:56:30,269] [INFO] [stage3.py:2277:step] End to end step took 5.077321529388428
x3003c0s37b1n0: [2024-03-28 12:56:30,269] [INFO] [stage3.py:2277:step] End to end step took 5.077340602874756
x3003c0s7b0n0: [2024-03-28 12:56:30,269] [INFO] [stage3.py:2277:step] End to end step took 5.077353000640869
x3003c0s37b0n0: [2024-03-28 12:56:30,269] [INFO] [stage3.py:2277:step] End to end step took 5.077428817749023
x3003c0s37b0n0: [2024-03-28 12:56:30,269] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4879.84
x3003c0s7b1n0: [2024-03-28 12:56:30,269] [INFO] [stage3.py:2277:step] End to end step took 5.077528238296509
x3003c0s37b0n0: [2024-03-28 12:56:30,270] [WARNING] [stage3.py:2267:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3003c0s37b0n0: [2024-03-28 12:56:30,270] [INFO] [stage3.py:2277:step] End to end step took 5.077836513519287
x3003c0s37b1n0: [2024-03-28 12:56:30,270] [INFO] [stage3.py:2277:step] End to end step took 5.077960252761841
x3003c0s7b0n0: [2024-03-28 12:56:30,270] [INFO] [stage3.py:2277:step] End to end step took 5.078062057495117
x3003c0s7b1n0: [2024-03-28 12:56:30,270] [INFO] [stage3.py:2277:step] End to end step took 5.078018665313721
x3003c0s37b0n0: [2024-03-28 12:56:30,270] [INFO] [stage3.py:2277:step] End to end step took 5.078100204467773
x3003c0s37b0n0: [2024-03-28 12:56:30,270] [INFO] [logging.py:96:log_dist] [Rank 0] step=6, skipped=0, lr=[0.0001415574960149644], mom=[(0.9, 0.95)]
x3003c0s37b1n0: [2024-03-28 12:56:30,270] [INFO] [stage3.py:2277:step] End to end step took 5.078106164932251
x3003c0s37b0n0: [2024-03-28 12:56:30,270] [INFO] [timer.py:260:stop] epoch=0/micro_step=6/global_step=6, RunningAvgSamplesPerSec=1.1210441648251603, CurrSamplesPerSec=1.0841567235250205, MemAllocated=10.62GB, MaxMemAllocated=13.58GB
x3003c0s37b0n0: [2024-03-28 12:56:30,271] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 15434.95 | bwd_microstep: 38151.21 | bwd_inner_microstep: 38005.58 | bwd_allreduce_microstep: 145.56 | step_microstep: 5108.24
x3003c0s37b0n0: [2024-03-28 12:56:30,271] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 15434.93 | bwd: 38151.21 | bwd_inner: 38005.57 | bwd_allreduce: 145.58 | step: 5108.24
x3003c0s37b0n0: [2024-03-28 12:56:30,389] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 12:56:30,389] [INFO] [utils.py:801:see_memory_usage] MA 10.62 GB         Max_MA 13.58 GB         CA 11.28 GB         Max_CA 18 GB 
x3003c0s37b0n0: [2024-03-28 12:56:30,389] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.54 GB, percent = 82.2%
x3003c0s37b0n0: <TIMER:interval-time,59.29307007789612><TIMER:interval-time,59.29306244850159><TIMER:interval-time,59.293076038360596><TIMER:interval-time,59.29307436943054>
x3003c0s37b0n0: 
x3003c0s37b0n0: 
x3003c0s37b0n0: 
x3003c0s7b0n0: <TIMER:interval-time,59.293198108673096><TIMER:interval-time,59.293195962905884><TIMER:interval-time,59.293198347091675>
x3003c0s7b0n0: <TIMER:interval-time,59.29320168495178>
x3003c0s7b0n0: 
x3003c0s7b0n0: 
x3003c0s37b1n0: <TIMER:interval-time,59.29315519332886><TIMER:interval-time,59.29315710067749><TIMER:interval-time,59.29315805435181><TIMER:interval-time,59.29316020011902>
x3003c0s37b1n0: 
x3003c0s37b1n0: 
x3003c0s37b1n0: 
x3003c0s7b1n0: <TIMER:interval-time,59.29357314109802><TIMER:interval-time,59.29357576370239>
x3003c0s7b1n0: 
x3003c0s7b1n0: <TIMER:interval-time,59.293577909469604><TIMER:interval-time,59.29358148574829>
x3003c0s7b1n0: 
x3003c0s7b1n0:  elapsed_time 59.293578 | consumed samples:          384 | consumed tokens:       786432 | elapsed time per iteration (ms): 59293.6 | learning rate: 1.416E-04 | global batch size:    64 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.079 | TFLOPs: 74.52 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 12:56:30,519] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 12:56:30,520] [INFO] [utils.py:801:see_memory_usage] MA 10.63 GB         Max_MA 10.63 GB         CA 11.28 GB         Max_CA 11 GB 
x3003c0s37b0n0: [2024-03-28 12:56:30,520] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.51 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:56:44,336] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 12:56:44,337] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 28.96 GB         CA 32.87 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 12:56:44,337] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.5 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:56:44,425] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 12:56:44,426] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 25.84 GB         CA 26.54 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 12:56:44,426] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.5 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:57:19,919] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 12:57:19,920] [INFO] [utils.py:801:see_memory_usage] MA 13.35 GB         Max_MA 29.91 GB         CA 14.79 GB         Max_CA 37 GB 
x3003c0s37b0n0: [2024-03-28 12:57:19,920] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.51 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:57:20,000] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 12:57:20,000] [INFO] [utils.py:801:see_memory_usage] MA 13.35 GB         Max_MA 13.35 GB         CA 14.79 GB         Max_CA 15 GB 
x3003c0s37b0n0: [2024-03-28 12:57:20,000] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.51 GB, percent = 82.2%
x3003c0s37b1n0: [2024-03-28 12:57:24,611] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.581108570098877
x3003c0s37b1n0: [2024-03-28 12:57:24,616] [INFO] [stage3.py:2251:step] Full outer step loop took 4.586498260498047
x3003c0s37b1n0: [2024-03-28 12:57:24,770] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.740685224533081
x3003c0s37b1n0: [2024-03-28 12:57:24,776] [INFO] [stage3.py:2251:step] Full outer step loop took 4.7465291023254395
x3003c0s37b0n0: [2024-03-28 12:57:24,787] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.7577900886535645
x3003c0s37b1n0: [2024-03-28 12:57:24,791] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.761019468307495
x3003c0s37b1n0: [2024-03-28 12:57:24,804] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.774813413619995
x3003c0s37b1n0: [2024-03-28 12:57:24,805] [INFO] [stage3.py:2251:step] Full outer step loop took 4.775289058685303
x3003c0s37b1n0: [2024-03-28 12:57:24,809] [INFO] [stage3.py:2251:step] Full outer step loop took 4.779900074005127
x3003c0s37b0n0: [2024-03-28 12:57:24,809] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.779685020446777
x3003c0s37b0n0: [2024-03-28 12:57:24,812] [INFO] [stage3.py:2251:step] Full outer step loop took 4.782510995864868
x3003c0s37b0n0: [2024-03-28 12:57:24,814] [INFO] [stage3.py:2251:step] Full outer step loop took 4.784872055053711
x3003c0s7b1n0: [2024-03-28 12:57:24,840] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.809958457946777
x3003c0s7b1n0: [2024-03-28 12:57:24,851] [INFO] [stage3.py:2251:step] Full outer step loop took 4.821716785430908
x3003c0s37b0n0: [2024-03-28 12:57:24,864] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.834796190261841
x3003c0s37b0n0: [2024-03-28 12:57:24,869] [INFO] [stage3.py:2251:step] Full outer step loop took 4.839770317077637
x3003c0s37b0n0: [2024-03-28 12:57:24,893] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.86329984664917
x3003c0s37b0n0: [2024-03-28 12:57:24,898] [INFO] [stage3.py:2251:step] Full outer step loop took 4.868155241012573
x3003c0s7b1n0: [2024-03-28 12:57:24,982] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.952021837234497
x3003c0s7b0n0: [2024-03-28 12:57:24,978] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.9487175941467285
x3003c0s7b0n0: [2024-03-28 12:57:24,983] [INFO] [stage3.py:2251:step] Full outer step loop took 4.953601360321045
x3003c0s7b0n0: [2024-03-28 12:57:24,984] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.954658269882202
x3003c0s7b0n0: [2024-03-28 12:57:24,998] [INFO] [stage3.py:2251:step] Full outer step loop took 4.967941045761108
x3003c0s7b1n0: [2024-03-28 12:57:25,015] [INFO] [stage3.py:2251:step] Full outer step loop took 4.9854655265808105
x3003c0s7b1n0: [2024-03-28 12:57:25,023] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.993603229522705
x3003c0s7b1n0: [2024-03-28 12:57:25,033] [INFO] [stage3.py:2251:step] Full outer step loop took 5.002960205078125
x3003c0s7b0n0: [2024-03-28 12:57:25,061] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.030909776687622
x3003c0s7b1n0: [2024-03-28 12:57:25,069] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.039538145065308
x3003c0s7b0n0: [2024-03-28 12:57:25,069] [INFO] [stage3.py:2251:step] Full outer step loop took 5.039541006088257
x3003c0s7b0n0: [2024-03-28 12:57:25,072] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.042163372039795
x3003c0s7b1n0: [2024-03-28 12:57:25,074] [INFO] [stage3.py:2251:step] Full outer step loop took 5.044373273849487
x3003c0s7b0n0: [2024-03-28 12:57:25,076] [INFO] [stage3.py:2251:step] Full outer step loop took 5.046494483947754
x3003c0s37b0n0: [2024-03-28 12:57:25,100] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4868.37
x3003c0s37b1n0: [2024-03-28 12:57:25,101] [INFO] [stage3.py:2277:step] End to end step took 5.071101427078247
x3003c0s7b0n0: [2024-03-28 12:57:25,101] [INFO] [stage3.py:2277:step] End to end step took 5.071084022521973
x3003c0s37b0n0: [2024-03-28 12:57:25,101] [INFO] [stage3.py:2277:step] End to end step took 5.071284294128418
x3003c0s37b0n0: [2024-03-28 12:57:25,101] [WARNING] [stage3.py:2267:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3003c0s37b0n0: [2024-03-28 12:57:25,101] [INFO] [stage3.py:2277:step] End to end step took 5.071384429931641
x3003c0s37b1n0: [2024-03-28 12:57:25,101] [INFO] [stage3.py:2277:step] End to end step took 5.071365118026733
x3003c0s7b0n0: [2024-03-28 12:57:25,101] [INFO] [stage3.py:2277:step] End to end step took 5.071445465087891
x3003c0s7b1n0: [2024-03-28 12:57:25,101] [INFO] [stage3.py:2277:step] End to end step took 5.071464776992798
x3003c0s7b1n0: [2024-03-28 12:57:25,101] [INFO] [stage3.py:2277:step] End to end step took 5.071615695953369
x3003c0s37b0n0: [2024-03-28 12:57:25,101] [INFO] [logging.py:96:log_dist] [Rank 0] step=7, skipped=0, lr=[9.750000000000001e-05], mom=[(0.9, 0.95)]
x3003c0s7b0n0: [2024-03-28 12:57:25,101] [INFO] [stage3.py:2277:step] End to end step took 5.071673393249512
x3003c0s37b0n0: [2024-03-28 12:57:25,101] [INFO] [stage3.py:2277:step] End to end step took 5.071718215942383
x3003c0s7b0n0: [2024-03-28 12:57:25,101] [INFO] [stage3.py:2277:step] End to end step took 5.071854591369629
x3003c0s37b0n0: [2024-03-28 12:57:25,101] [INFO] [stage3.py:2277:step] End to end step took 5.071804523468018
x3003c0s37b1n0: [2024-03-28 12:57:25,101] [INFO] [stage3.py:2277:step] End to end step took 5.0718674659729
x3003c0s37b1n0: [2024-03-28 12:57:25,101] [INFO] [stage3.py:2277:step] End to end step took 5.0719313621521
x3003c0s7b1n0: [2024-03-28 12:57:25,101] [INFO] [stage3.py:2277:step] End to end step took 5.07184624671936
x3003c0s37b0n0: [2024-03-28 12:57:25,101] [INFO] [timer.py:260:stop] epoch=0/micro_step=7/global_step=7, RunningAvgSamplesPerSec=1.1309886293920206, CurrSamplesPerSec=1.172595683204014, MemAllocated=10.62GB, MaxMemAllocated=13.58GB
x3003c0s7b1n0: [2024-03-28 12:57:25,102] [INFO] [stage3.py:2277:step] End to end step took 5.071949481964111
x3003c0s37b0n0: [2024-03-28 12:57:25,102] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 13781.38 | bwd_microstep: 35344.73 | bwd_inner_microstep: 35212.04 | bwd_allreduce_microstep: 132.61 | step_microstep: 5101.03
x3003c0s37b0n0: [2024-03-28 12:57:25,102] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 13781.37 | bwd: 35344.73 | bwd_inner: 35212.03 | bwd_allreduce: 132.63 | step: 5101.03
x3003c0s37b0n0: [2024-03-28 12:57:25,222] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 12:57:25,222] [INFO] [utils.py:801:see_memory_usage] MA 10.62 GB         Max_MA 13.58 GB         CA 11.28 GB         Max_CA 18 GB 
x3003c0s37b0n0: [2024-03-28 12:57:25,223] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.51 GB, percent = 82.2%
x3003c0s37b0n0: <TIMER:interval-time,54.832536935806274>
x3003c0s37b0n0: <TIMER:interval-time,54.83253359794617><TIMER:interval-time,54.832539796829224>
x3003c0s37b0n0: 
x3003c0s37b0n0: <TIMER:interval-time,54.83253192901611>
x3003c0s7b1n0: <TIMER:interval-time,54.83254861831665><TIMER:interval-time,54.83255219459534><TIMER:interval-time,54.83254861831665><TIMER:interval-time,54.83255219459534>
x3003c0s7b1n0: 
x3003c0s7b1n0: 
x3003c0s7b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,54.83260726928711><TIMER:interval-time,54.83261442184448>
x3003c0s37b1n0: <TIMER:interval-time,54.83261156082153>
x3003c0s7b0n0: <TIMER:interval-time,54.8325834274292><TIMER:interval-time,54.83258509635925><TIMER:interval-time,54.8325879573822>
x3003c0s37b1n0: 
x3003c0s7b0n0: 
x3003c0s7b0n0: 
x3003c0s7b0n0: <TIMER:interval-time,54.832592248916626>
x3003c0s37b1n0: <TIMER:interval-time,54.83271813392639>
x3003c0s7b1n0:  elapsed_time 54.832549 | consumed samples:          448 | consumed tokens:       917504 | elapsed time per iteration (ms): 54832.5 | learning rate: 9.750E-05 | global batch size:    64 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.167 | TFLOPs: 80.58 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 12:57:25,374] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 12:57:25,374] [INFO] [utils.py:801:see_memory_usage] MA 10.63 GB         Max_MA 10.63 GB         CA 11.28 GB         Max_CA 11 GB 
x3003c0s37b0n0: [2024-03-28 12:57:25,374] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.53 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:57:38,724] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 12:57:38,724] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 28.96 GB         CA 32.87 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 12:57:38,725] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.52 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:57:38,813] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 12:57:38,814] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 25.84 GB         CA 26.54 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 12:57:38,814] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.52 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:58:13,369] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 12:58:13,370] [INFO] [utils.py:801:see_memory_usage] MA 13.35 GB         Max_MA 29.91 GB         CA 14.79 GB         Max_CA 37 GB 
x3003c0s37b0n0: [2024-03-28 12:58:13,370] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.53 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:58:13,449] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 12:58:13,450] [INFO] [utils.py:801:see_memory_usage] MA 13.35 GB         Max_MA 13.35 GB         CA 14.79 GB         Max_CA 15 GB 
x3003c0s37b0n0: [2024-03-28 12:58:13,450] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.53 GB, percent = 82.2%
x3003c0s37b1n0: [2024-03-28 12:58:18,042] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.561576843261719
x3003c0s37b1n0: [2024-03-28 12:58:18,067] [INFO] [stage3.py:2251:step] Full outer step loop took 4.586797475814819
x3003c0s7b1n0: [2024-03-28 12:58:18,163] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.682900667190552
x3003c0s37b0n0: [2024-03-28 12:58:18,163] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.68335747718811
x3003c0s37b0n0: [2024-03-28 12:58:18,171] [INFO] [stage3.py:2251:step] Full outer step loop took 4.6915178298950195
x3003c0s7b1n0: [2024-03-28 12:58:18,178] [INFO] [stage3.py:2251:step] Full outer step loop took 4.698270320892334
x3003c0s37b1n0: [2024-03-28 12:58:18,198] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.717456579208374
x3003c0s37b1n0: [2024-03-28 12:58:18,220] [INFO] [stage3.py:2251:step] Full outer step loop took 4.740476608276367
x3003c0s37b1n0: [2024-03-28 12:58:18,231] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.751134157180786
x3003c0s37b1n0: [2024-03-28 12:58:18,236] [INFO] [stage3.py:2251:step] Full outer step loop took 4.756000518798828
x3003c0s37b1n0: [2024-03-28 12:58:18,257] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.7766032218933105
x3003c0s37b1n0: [2024-03-28 12:58:18,261] [INFO] [stage3.py:2251:step] Full outer step loop took 4.7809739112854
x3003c0s7b0n0: [2024-03-28 12:58:18,302] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.822244644165039
x3003c0s7b0n0: [2024-03-28 12:58:18,316] [INFO] [stage3.py:2251:step] Full outer step loop took 4.835604429244995
x3003c0s37b0n0: [2024-03-28 12:58:18,357] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.876629114151001
x3003c0s37b0n0: [2024-03-28 12:58:18,364] [INFO] [stage3.py:2251:step] Full outer step loop took 4.884158611297607
x3003c0s37b0n0: [2024-03-28 12:58:18,366] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.88575005531311
x3003c0s37b0n0: [2024-03-28 12:58:18,371] [INFO] [stage3.py:2251:step] Full outer step loop took 4.890742063522339
x3003c0s7b1n0: [2024-03-28 12:58:18,380] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.900079727172852
x3003c0s37b0n0: [2024-03-28 12:58:18,386] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.906574249267578
x3003c0s37b0n0: [2024-03-28 12:58:18,391] [INFO] [stage3.py:2251:step] Full outer step loop took 4.911452054977417
x3003c0s7b1n0: [2024-03-28 12:58:18,400] [INFO] [stage3.py:2251:step] Full outer step loop took 4.91960072517395
x3003c0s7b1n0: [2024-03-28 12:58:18,453] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.972709655761719
x3003c0s7b1n0: [2024-03-28 12:58:18,461] [INFO] [stage3.py:2251:step] Full outer step loop took 4.980597257614136
x3003c0s7b1n0: [2024-03-28 12:58:18,464] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.984009027481079
x3003c0s7b1n0: [2024-03-28 12:58:18,469] [INFO] [stage3.py:2251:step] Full outer step loop took 4.989178657531738
x3003c0s7b0n0: [2024-03-28 12:58:18,472] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.992383718490601
x3003c0s7b0n0: [2024-03-28 12:58:18,477] [INFO] [stage3.py:2251:step] Full outer step loop took 4.9973180294036865
x3003c0s7b0n0: [2024-03-28 12:58:18,481] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.0010435581207275
x3003c0s7b0n0: [2024-03-28 12:58:18,486] [INFO] [stage3.py:2251:step] Full outer step loop took 5.0063722133636475
x3003c0s7b0n0: [2024-03-28 12:58:18,504] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.024193048477173
x3003c0s7b0n0: [2024-03-28 12:58:18,509] [INFO] [stage3.py:2251:step] Full outer step loop took 5.028565406799316
x3003c0s7b0n0: [2024-03-28 12:58:18,530] [INFO] [stage3.py:2277:step] End to end step took 5.050013542175293
x3003c0s7b0n0: [2024-03-28 12:58:18,530] [INFO] [stage3.py:2277:step] End to end step took 5.050203323364258
x3003c0s37b0n0: [2024-03-28 12:58:18,530] [INFO] [stage3.py:2277:step] End to end step took 5.050212621688843
x3003c0s37b1n0: [2024-03-28 12:58:18,530] [INFO] [stage3.py:2277:step] End to end step took 5.050241231918335
x3003c0s7b1n0: [2024-03-28 12:58:18,530] [INFO] [stage3.py:2277:step] End to end step took 5.050272703170776
x3003c0s37b0n0: [2024-03-28 12:58:18,530] [INFO] [stage3.py:2277:step] End to end step took 5.0504844188690186
x3003c0s7b0n0: [2024-03-28 12:58:18,530] [INFO] [stage3.py:2277:step] End to end step took 5.050499200820923
x3003c0s7b1n0: [2024-03-28 12:58:18,531] [INFO] [stage3.py:2277:step] End to end step took 5.050696849822998
x3003c0s37b0n0: [2024-03-28 12:58:18,530] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4890.96
x3003c0s7b0n0: [2024-03-28 12:58:18,531] [INFO] [stage3.py:2277:step] End to end step took 5.0508131980896
x3003c0s37b1n0: [2024-03-28 12:58:18,531] [INFO] [stage3.py:2277:step] End to end step took 5.050741672515869
x3003c0s37b1n0: [2024-03-28 12:58:18,531] [INFO] [stage3.py:2277:step] End to end step took 5.0507471561431885
x3003c0s37b0n0: [2024-03-28 12:58:18,531] [WARNING] [stage3.py:2267:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3003c0s37b0n0: [2024-03-28 12:58:18,531] [INFO] [stage3.py:2277:step] End to end step took 5.051091909408569
x3003c0s37b0n0: [2024-03-28 12:58:18,531] [INFO] [stage3.py:2277:step] End to end step took 5.051004409790039
x3003c0s7b1n0: [2024-03-28 12:58:18,531] [INFO] [stage3.py:2277:step] End to end step took 5.050926923751831
x3003c0s7b1n0: [2024-03-28 12:58:18,531] [INFO] [stage3.py:2277:step] End to end step took 5.051025390625
x3003c0s37b1n0: [2024-03-28 12:58:18,531] [INFO] [stage3.py:2277:step] End to end step took 5.051170587539673
x3003c0s37b0n0: [2024-03-28 12:58:18,531] [INFO] [logging.py:96:log_dist] [Rank 0] step=8, skipped=0, lr=[6.158400017893797e-05], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 12:58:18,532] [INFO] [timer.py:260:stop] epoch=0/micro_step=8/global_step=8, RunningAvgSamplesPerSec=1.1425378720093953, CurrSamplesPerSec=1.2040125532319448, MemAllocated=10.62GB, MaxMemAllocated=13.58GB
x3003c0s37b0n0: [2024-03-28 12:58:18,532] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 13315.99 | bwd_microstep: 34401.24 | bwd_inner_microstep: 34270.69 | bwd_allreduce_microstep: 130.48 | step_microstep: 5081.58
x3003c0s37b0n0: [2024-03-28 12:58:18,532] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 13315.98 | bwd: 34401.23 | bwd_inner: 34270.68 | bwd_allreduce: 130.49 | step: 5081.59
x3003c0s37b0n0: [2024-03-28 12:58:18,649] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 12:58:18,649] [INFO] [utils.py:801:see_memory_usage] MA 10.62 GB         Max_MA 13.58 GB         CA 11.28 GB         Max_CA 18 GB 
x3003c0s37b0n0: [2024-03-28 12:58:18,649] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.52 GB, percent = 82.2%
x3003c0s37b0n0: <TIMER:interval-time,53.42634177207947><TIMER:interval-time,53.42635726928711><TIMER:interval-time,53.42636203765869>
x3003c0s37b0n0: 
x3003c0s37b0n0: 
x3003c0s37b0n0: <TIMER:interval-time,53.42636489868164>
x3003c0s7b0n0: <TIMER:interval-time,53.426382541656494><TIMER:interval-time,53.426382541656494><TIMER:interval-time,53.42638278007507><TIMER:interval-time,53.426384925842285>
x3003c0s7b0n0: 
x3003c0s7b0n0: 
x3003c0s7b0n0: 
x3003c0s37b1n0: <TIMER:interval-time,53.42640042304993><TIMER:interval-time,53.42640233039856><TIMER:interval-time,53.4263973236084>
x3003c0s37b1n0: 
x3003c0s37b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,53.42641091346741>
x3003c0s7b1n0: <TIMER:interval-time,53.426385164260864><TIMER:interval-time,53.42638802528381>
x3003c0s7b1n0: 
x3003c0s7b1n0: <TIMER:interval-time,53.42639231681824><TIMER:interval-time,53.42639088630676>
x3003c0s7b1n0: 
x3003c0s7b1n0:  elapsed_time 53.426385 | consumed samples:          512 | consumed tokens:      1048576 | elapsed time per iteration (ms): 53426.4 | learning rate: 6.158E-05 | global batch size:    64 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.198 | TFLOPs: 82.70 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 12:58:18,805] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 12:58:18,805] [INFO] [utils.py:801:see_memory_usage] MA 10.63 GB         Max_MA 10.63 GB         CA 11.28 GB         Max_CA 11 GB 
x3003c0s37b0n0: [2024-03-28 12:58:18,805] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.54 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:58:34,873] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 12:58:34,874] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 28.96 GB         CA 32.87 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 12:58:34,874] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.52 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:58:34,968] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 12:58:34,968] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 25.84 GB         CA 26.54 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 12:58:34,969] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.52 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:59:14,641] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 12:59:14,642] [INFO] [utils.py:801:see_memory_usage] MA 13.35 GB         Max_MA 29.91 GB         CA 14.79 GB         Max_CA 37 GB 
x3003c0s37b0n0: [2024-03-28 12:59:14,642] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.53 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:59:14,724] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 12:59:14,725] [INFO] [utils.py:801:see_memory_usage] MA 13.35 GB         Max_MA 13.35 GB         CA 14.79 GB         Max_CA 15 GB 
x3003c0s37b0n0: [2024-03-28 12:59:14,725] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.53 GB, percent = 82.2%
x3003c0s37b1n0: [2024-03-28 12:59:19,331] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.575456619262695
x3003c0s37b1n0: [2024-03-28 12:59:19,336] [INFO] [stage3.py:2251:step] Full outer step loop took 4.580381155014038
x3003c0s37b1n0: [2024-03-28 12:59:19,351] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.596280336380005
x3003c0s37b1n0: [2024-03-28 12:59:19,357] [INFO] [stage3.py:2251:step] Full outer step loop took 4.60185980796814
x3003c0s37b1n0: [2024-03-28 12:59:19,377] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.621484756469727
x3003c0s37b1n0: [2024-03-28 12:59:19,377] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.621592044830322
x3003c0s37b1n0: [2024-03-28 12:59:19,381] [INFO] [stage3.py:2251:step] Full outer step loop took 4.626327991485596
x3003c0s37b1n0: [2024-03-28 12:59:19,382] [INFO] [stage3.py:2251:step] Full outer step loop took 4.626409292221069
x3003c0s37b0n0: [2024-03-28 12:59:19,417] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.661499977111816
x3003c0s37b0n0: [2024-03-28 12:59:19,430] [INFO] [stage3.py:2251:step] Full outer step loop took 4.674879312515259
x3003c0s7b1n0: [2024-03-28 12:59:19,521] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.766278028488159
x3003c0s7b1n0: [2024-03-28 12:59:19,526] [INFO] [stage3.py:2251:step] Full outer step loop took 4.7713398933410645
x3003c0s7b1n0: [2024-03-28 12:59:19,560] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.8052191734313965
x3003c0s7b1n0: [2024-03-28 12:59:19,560] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.805130958557129
x3003c0s7b1n0: [2024-03-28 12:59:19,565] [INFO] [stage3.py:2251:step] Full outer step loop took 4.810060501098633
x3003c0s7b1n0: [2024-03-28 12:59:19,565] [INFO] [stage3.py:2251:step] Full outer step loop took 4.810226917266846
x3003c0s7b0n0: [2024-03-28 12:59:19,574] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.819106817245483
x3003c0s7b0n0: [2024-03-28 12:59:19,594] [INFO] [stage3.py:2251:step] Full outer step loop took 4.83918023109436
x3003c0s37b0n0: [2024-03-28 12:59:19,640] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.885096549987793
x3003c0s37b0n0: [2024-03-28 12:59:19,645] [INFO] [stage3.py:2251:step] Full outer step loop took 4.890123128890991
x3003c0s37b0n0: [2024-03-28 12:59:19,650] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.89413046836853
x3003c0s37b0n0: [2024-03-28 12:59:19,661] [INFO] [stage3.py:2251:step] Full outer step loop took 4.905954837799072
x3003c0s37b0n0: [2024-03-28 12:59:19,664] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.908626556396484
x3003c0s37b0n0: [2024-03-28 12:59:19,668] [INFO] [stage3.py:2251:step] Full outer step loop took 4.912988185882568
x3003c0s7b1n0: [2024-03-28 12:59:19,738] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.982929468154907
x3003c0s7b1n0: [2024-03-28 12:59:19,742] [INFO] [stage3.py:2251:step] Full outer step loop took 4.987269639968872
x3003c0s7b0n0: [2024-03-28 12:59:19,785] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.030335426330566
x3003c0s7b0n0: [2024-03-28 12:59:19,790] [INFO] [stage3.py:2251:step] Full outer step loop took 5.035233020782471
x3003c0s7b0n0: [2024-03-28 12:59:19,797] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.042217493057251
x3003c0s7b0n0: [2024-03-28 12:59:19,802] [INFO] [stage3.py:2251:step] Full outer step loop took 5.047316312789917
x3003c0s7b0n0: [2024-03-28 12:59:19,803] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.047662973403931
x3003c0s7b0n0: [2024-03-28 12:59:19,807] [INFO] [stage3.py:2251:step] Full outer step loop took 5.052029371261597
x3003c0s7b0n0: [2024-03-28 12:59:19,830] [INFO] [stage3.py:2277:step] End to end step took 5.074808359146118
x3003c0s37b0n0: [2024-03-28 12:59:19,830] [INFO] [stage3.py:2277:step] End to end step took 5.074896335601807
x3003c0s37b0n0: [2024-03-28 12:59:19,830] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4890.43
x3003c0s7b1n0: [2024-03-28 12:59:19,830] [INFO] [stage3.py:2277:step] End to end step took 5.074943542480469
x3003c0s7b0n0: [2024-03-28 12:59:19,830] [INFO] [stage3.py:2277:step] End to end step took 5.07498025894165
x3003c0s7b0n0: [2024-03-28 12:59:19,830] [INFO] [stage3.py:2277:step] End to end step took 5.074923038482666
x3003c0s37b1n0: [2024-03-28 12:59:19,830] [INFO] [stage3.py:2277:step] End to end step took 5.074918031692505
x3003c0s37b1n0: [2024-03-28 12:59:19,830] [INFO] [stage3.py:2277:step] End to end step took 5.074962377548218
x3003c0s37b0n0: [2024-03-28 12:59:19,830] [WARNING] [stage3.py:2267:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3003c0s37b0n0: [2024-03-28 12:59:19,830] [INFO] [stage3.py:2277:step] End to end step took 5.075260877609253
x3003c0s37b0n0: [2024-03-28 12:59:19,830] [INFO] [stage3.py:2277:step] End to end step took 5.075410604476929
x3003c0s37b0n0: [2024-03-28 12:59:19,831] [INFO] [logging.py:96:log_dist] [Rank 0] step=9, skipped=0, lr=[3.814149619390238e-05], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 12:59:19,831] [INFO] [stage3.py:2277:step] End to end step took 5.075586795806885
x3003c0s7b1n0: [2024-03-28 12:59:19,831] [INFO] [stage3.py:2277:step] End to end step took 5.075576066970825
x3003c0s7b1n0: [2024-03-28 12:59:19,831] [INFO] [stage3.py:2277:step] End to end step took 5.075637102127075
x3003c0s7b0n0: [2024-03-28 12:59:19,831] [INFO] [stage3.py:2277:step] End to end step took 5.075659990310669
x3003c0s7b1n0: [2024-03-28 12:59:19,831] [INFO] [stage3.py:2277:step] End to end step took 5.075582504272461
x3003c0s37b1n0: [2024-03-28 12:59:19,831] [INFO] [stage3.py:2277:step] End to end step took 5.075663089752197
x3003c0s37b1n0: [2024-03-28 12:59:19,831] [INFO] [stage3.py:2277:step] End to end step took 5.07568883895874
x3003c0s37b0n0: [2024-03-28 12:59:19,831] [INFO] [timer.py:260:stop] epoch=0/micro_step=9/global_step=9, RunningAvgSamplesPerSec=1.1281291968864504, CurrSamplesPerSec=1.0487720917037868, MemAllocated=10.62GB, MaxMemAllocated=13.58GB
x3003c0s37b0n0: [2024-03-28 12:59:19,831] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 16029.44 | bwd_microstep: 39524.02 | bwd_inner_microstep: 39373.91 | bwd_allreduce_microstep: 150.02 | step_microstep: 5105.90
x3003c0s37b0n0: [2024-03-28 12:59:19,831] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 16029.43 | bwd: 39524.00 | bwd_inner: 39373.90 | bwd_allreduce: 150.04 | step: 5105.89
x3003c0s37b0n0: [2024-03-28 12:59:19,944] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 12:59:19,945] [INFO] [utils.py:801:see_memory_usage] MA 10.62 GB         Max_MA 13.58 GB         CA 11.28 GB         Max_CA 18 GB 
x3003c0s37b0n0: [2024-03-28 12:59:19,945] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.53 GB, percent = 82.2%
x3003c0s37b0n0: <TIMER:interval-time,61.295328855514526><TIMER:interval-time,61.29533267021179><TIMER:interval-time,61.2953360080719>
x3003c0s37b0n0: 
x3003c0s37b0n0: 
x3003c0s7b0n0: <TIMER:interval-time,61.29531955718994><TIMER:interval-time,61.29532289505005><TIMER:interval-time,61.2953245639801>
x3003c0s7b0n0: 
x3003c0s7b0n0: 
x3003c0s7b0n0: <TIMER:interval-time,61.295334815979004>
x3003c0s37b0n0: <TIMER:interval-time,61.2954306602478>
x3003c0s7b1n0: <TIMER:interval-time,61.29534935951233><TIMER:interval-time,61.29535174369812>
x3003c0s7b1n0: 
x3003c0s7b1n0: <TIMER:interval-time,61.29535722732544>
x3003c0s7b1n0: <TIMER:interval-time,61.2953622341156>
x3003c0s37b1n0: <TIMER:interval-time,61.295372009277344><TIMER:interval-time,61.29537391662598>
x3003c0s37b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,61.29537749290466>
x3003c0s37b1n0: <TIMER:interval-time,61.29537844657898>
x3003c0s7b1n0:  elapsed_time 61.295349 | consumed samples:          576 | consumed tokens:      1179648 | elapsed time per iteration (ms): 61295.3 | learning rate: 3.814E-05 | global batch size:    64 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.044 | TFLOPs: 72.08 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 12:59:20,049] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 12:59:20,050] [INFO] [utils.py:801:see_memory_usage] MA 10.63 GB         Max_MA 10.63 GB         CA 11.28 GB         Max_CA 11 GB 
x3003c0s37b0n0: [2024-03-28 12:59:20,050] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.53 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:59:37,097] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 12:59:37,098] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 28.96 GB         CA 32.87 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 12:59:37,098] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.52 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 12:59:37,185] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 12:59:37,185] [INFO] [utils.py:801:see_memory_usage] MA 25.84 GB         Max_MA 25.84 GB         CA 26.54 GB         Max_CA 33 GB 
x3003c0s37b0n0: [2024-03-28 12:59:37,186] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.52 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 13:00:16,739] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 13:00:16,740] [INFO] [utils.py:801:see_memory_usage] MA 13.35 GB         Max_MA 29.91 GB         CA 14.79 GB         Max_CA 37 GB 
x3003c0s37b0n0: [2024-03-28 13:00:16,740] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.54 GB, percent = 82.2%
x3003c0s37b0n0: [2024-03-28 13:00:16,817] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 13:00:16,818] [INFO] [utils.py:801:see_memory_usage] MA 13.35 GB         Max_MA 13.35 GB         CA 14.79 GB         Max_CA 15 GB 
x3003c0s37b0n0: [2024-03-28 13:00:16,818] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.54 GB, percent = 82.2%
x3003c0s37b1n0: [2024-03-28 13:00:21,485] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.638014793395996
x3003c0s37b1n0: [2024-03-28 13:00:21,502] [INFO] [stage3.py:2251:step] Full outer step loop took 4.650948762893677
x3003c0s37b1n0: [2024-03-28 13:00:21,509] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.6612138748168945
x3003c0s37b1n0: [2024-03-28 13:00:21,513] [INFO] [stage3.py:2251:step] Full outer step loop took 4.666042327880859
x3003c0s37b1n0: [2024-03-28 13:00:21,535] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.687549352645874
x3003c0s37b1n0: [2024-03-28 13:00:21,535] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.687600374221802
x3003c0s37b1n0: [2024-03-28 13:00:21,540] [INFO] [stage3.py:2251:step] Full outer step loop took 4.692367792129517
x3003c0s37b1n0: [2024-03-28 13:00:21,540] [INFO] [stage3.py:2251:step] Full outer step loop took 4.692470550537109
x3003c0s7b0n0: [2024-03-28 13:00:21,549] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.7018585205078125
x3003c0s7b0n0: [2024-03-28 13:00:21,556] [INFO] [stage3.py:2251:step] Full outer step loop took 4.708638429641724
x3003c0s37b0n0: [2024-03-28 13:00:21,695] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.847788095474243
x3003c0s37b0n0: [2024-03-28 13:00:21,703] [INFO] [stage3.py:2251:step] Full outer step loop took 4.8555097579956055
x3003c0s37b0n0: [2024-03-28 13:00:21,756] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.908768892288208
x3003c0s7b0n0: [2024-03-28 13:00:21,758] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.911055088043213
x3003c0s7b0n0: [2024-03-28 13:00:21,761] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.914124250411987
x3003c0s7b0n0: [2024-03-28 13:00:21,763] [INFO] [stage3.py:2251:step] Full outer step loop took 4.915938377380371
x3003c0s7b0n0: [2024-03-28 13:00:21,766] [INFO] [stage3.py:2251:step] Full outer step loop took 4.919007778167725
x3003c0s37b0n0: [2024-03-28 13:00:21,776] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.928831338882446
x3003c0s37b0n0: [2024-03-28 13:00:21,781] [INFO] [stage3.py:2251:step] Full outer step loop took 4.933707237243652
x3003c0s37b0n0: [2024-03-28 13:00:21,781] [INFO] [stage3.py:2251:step] Full outer step loop took 4.933906316757202
x3003c0s7b0n0: [2024-03-28 13:00:21,788] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.940364599227905
x3003c0s7b0n0: [2024-03-28 13:00:21,792] [INFO] [stage3.py:2251:step] Full outer step loop took 4.9447221755981445
x3003c0s7b1n0: [2024-03-28 13:00:21,796] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.948814153671265
x3003c0s37b0n0: [2024-03-28 13:00:21,797] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.9502081871032715
x3003c0s37b0n0: [2024-03-28 13:00:21,803] [INFO] [stage3.py:2251:step] Full outer step loop took 4.956005811691284
x3003c0s7b1n0: [2024-03-28 13:00:21,803] [INFO] [stage3.py:2251:step] Full outer step loop took 4.9554123878479
x3003c0s7b1n0: [2024-03-28 13:00:21,834] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.986292362213135
x3003c0s7b1n0: [2024-03-28 13:00:21,839] [INFO] [stage3.py:2251:step] Full outer step loop took 4.991497993469238
x3003c0s7b1n0: [2024-03-28 13:00:21,844] [INFO] [stage3.py:2243:step] With missing steps outer loop took 4.995972633361816
x3003c0s7b1n0: [2024-03-28 13:00:21,851] [INFO] [stage3.py:2251:step] Full outer step loop took 5.004180908203125
x3003c0s7b1n0: [2024-03-28 13:00:21,851] [INFO] [stage3.py:2243:step] With missing steps outer loop took 5.004219055175781
x3003c0s7b1n0: [2024-03-28 13:00:21,856] [INFO] [stage3.py:2251:step] Full outer step loop took 5.00856351852417
x3003c0s7b0n0: [2024-03-28 13:00:21,879] [INFO] [stage3.py:2277:step] End to end step took 5.031627416610718
x3003c0s37b0n0: [2024-03-28 13:00:21,879] [INFO] [stage3.py:2277:step] End to end step took 5.03174614906311
x3003c0s7b1n0: [2024-03-28 13:00:21,879] [INFO] [stage3.py:2277:step] End to end step took 5.031670808792114
x3003c0s7b1n0: [2024-03-28 13:00:21,879] [INFO] [stage3.py:2277:step] End to end step took 5.031663179397583
x3003c0s7b1n0: [2024-03-28 13:00:21,879] [INFO] [stage3.py:2277:step] End to end step took 5.031707763671875
x3003c0s37b0n0: [2024-03-28 13:00:21,879] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 4933.94
x3003c0s7b0n0: [2024-03-28 13:00:21,879] [INFO] [stage3.py:2277:step] End to end step took 5.031751871109009
x3003c0s37b1n0: [2024-03-28 13:00:21,879] [INFO] [stage3.py:2277:step] End to end step took 5.031743288040161
x3003c0s7b1n0: [2024-03-28 13:00:21,879] [INFO] [stage3.py:2277:step] End to end step took 5.0317981243133545
x3003c0s37b1n0: [2024-03-28 13:00:21,879] [INFO] [stage3.py:2277:step] End to end step took 5.031744003295898
x3003c0s7b0n0: [2024-03-28 13:00:21,879] [INFO] [stage3.py:2277:step] End to end step took 5.031948804855347
x3003c0s37b0n0: [2024-03-28 13:00:21,879] [INFO] [stage3.py:2277:step] End to end step took 5.031980514526367
x3003c0s37b0n0: [2024-03-28 13:00:21,879] [WARNING] [stage3.py:2267:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3003c0s37b0n0: [2024-03-28 13:00:21,879] [INFO] [stage3.py:2277:step] End to end step took 5.032100439071655
x3003c0s37b1n0: [2024-03-28 13:00:21,879] [INFO] [stage3.py:2277:step] End to end step took 5.031980276107788
x3003c0s7b0n0: [2024-03-28 13:00:21,879] [INFO] [stage3.py:2277:step] End to end step took 5.032258987426758
x3003c0s37b0n0: [2024-03-28 13:00:21,880] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[3e-05], mom=[(0.9, 0.95)]
x3003c0s37b1n0: [2024-03-28 13:00:21,880] [INFO] [stage3.py:2277:step] End to end step took 5.032329559326172
x3003c0s37b0n0: [2024-03-28 13:00:21,880] [INFO] [stage3.py:2277:step] End to end step took 5.032434701919556
x3003c0s37b0n0: [2024-03-28 13:00:21,880] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=1.1155995195908768, CurrSamplesPerSec=1.0351226803483276, MemAllocated=10.62GB, MaxMemAllocated=13.58GB
x3003c0s37b0n0: [2024-03-28 13:00:21,880] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 17016.92 | bwd_microstep: 39409.13 | bwd_inner_microstep: 39259.95 | bwd_allreduce_microstep: 149.11 | step_microstep: 5061.96
x3003c0s37b0n0: [2024-03-28 13:00:21,880] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 17016.91 | bwd: 39409.12 | bwd_inner: 39259.94 | bwd_allreduce: 149.13 | step: 5061.96
x3003c0s37b0n0: [2024-03-28 13:00:21,993] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 13:00:21,994] [INFO] [utils.py:801:see_memory_usage] MA 10.62 GB         Max_MA 13.58 GB         CA 11.28 GB         Max_CA 18 GB 
x3003c0s37b0n0: [2024-03-28 13:00:21,994] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 413.54 GB, percent = 82.2%
x3003c0s37b0n0: <TIMER:interval-time,62.048429012298584>
x3003c0s37b0n0: <TIMER:interval-time,62.048429012298584>
x3003c0s37b0n0: <TIMER:interval-time,62.04844641685486><TIMER:interval-time,62.048450231552124>
x3003c0s37b0n0: 
x3003c0s37b1n0: <TIMER:interval-time,62.04842281341553><TIMER:interval-time,62.04842138290405>
x3003c0s37b1n0: 
x3003c0s7b0n0: <TIMER:interval-time,62.0484344959259><TIMER:interval-time,62.04842758178711><TIMER:interval-time,62.048439502716064>
x3003c0s7b0n0: <TIMER:interval-time,62.04842805862427>
x3003c0s7b0n0: 
x3003c0s7b0n0: 
x3003c0s7b1n0: <TIMER:interval-time,62.0484459400177><TIMER:interval-time,62.0484459400177><TIMER:interval-time,62.0484459400177><TIMER:interval-time,62.04844880104065>
x3003c0s7b1n0: 
x3003c0s7b1n0: 
x3003c0s7b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,62.048505544662476>
x3003c0s37b1n0: <TIMER:interval-time,62.04852223396301>
x3003c0s7b1n0:  elapsed_time 62.048446 | consumed samples:          640 | consumed tokens:      1310720 | elapsed time per iteration (ms): 62048.4 | learning rate: 3.000E-05 | global batch size:    64 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.031 | TFLOPs: 71.21 |
x3003c0s37b1n0: <<<only_train:590.7448251247406>>>
x3003c0s37b1n0: <<<only_train:590.74480509758>>>
x3003c0s37b1n0: <<<only_train:590.7448580265045>>>
x3003c0s7b0n0: <<<only_train:590.7448370456696>>>
x3003c0s7b0n0: <<<only_train:590.7448523044586>>>
x3003c0s37b0n0: <<<only_train:590.7448492050171>>>
x3003c0s37b0n0: <<<only_train:590.7448375225067>>><<<only_train:590.7448446750641>>>
x3003c0s37b0n0: 
x3003c0s7b0n0: <<<only_train:590.744838476181>>>
x3003c0s7b1n0: <<<only_train:590.7446222305298>>>
x3003c0s7b1n0: <<<only_train:590.7449123859406>>>
x3003c0s7b1n0: <<<only_train:590.7449300289154>>>
x3003c0s37b1n0: <<<only_train:590.7449469566345>>>
x3003c0s7b0n0: <<<only_train:590.7450041770935>>>
x3003c0s37b0n0: <<<only_train:590.7446975708008>>>
x3003c0s7b1n0: <<<only_train:590.7447695732117>>>
x3003c0s37b0n0: [after training ends] datetime: 2024-03-28 13:00:22 
x3003c0s37b0n0: <<<full_time:590.7451438903809>>><<<full_time:590.7451536655426>>>
x3003c0s37b0n0: 
x3003c0s37b0n0: <<<full_time:590.7451894283295>>><<<full_time:590.7449626922607>>>
x3003c0s37b0n0: 
x3003c0s37b1n0: <<<full_time:590.745267868042>>><<<full_time:590.7453308105469>>>
x3003c0s37b1n0: 
x3003c0s37b1n0: <<<full_time:590.7453088760376>>>
x3003c0s37b1n0: <<<full_time:590.7453303337097>>>
x3003c0s7b0n0: <<<full_time:590.7452068328857>>><<<full_time:590.7451808452606>>><<<full_time:590.7453179359436>>>
x3003c0s7b0n0: 
x3003c0s7b0n0: <<<full_time:590.7452239990234>>>
x3003c0s7b0n0: 
x3003c0s7b1n0: <<<full_time:590.7449979782104>>><<<full_time:590.7452712059021>>><<<full_time:590.74507188797>>><<<full_time:590.7452874183655>>>
x3003c0s7b1n0: 
x3003c0s7b1n0: 
x3003c0s7b1n0: 
x3003c0s7b0n0: [2024-03-28 13:00:30,584] [INFO] [launch.py:348:main] Process 44075 exits successfully.
x3003c0s37b1n0: [2024-03-28 13:00:30,818] [INFO] [launch.py:348:main] Process 32405 exits successfully.
x3003c0s37b0n0: [2024-03-28 13:00:31,172] [INFO] [launch.py:348:main] Process 38678 exits successfully.
x3003c0s7b1n0: [2024-03-28 13:00:31,633] [INFO] [launch.py:348:main] Process 51859 exits successfully.
x3003c0s37b1n0: [2024-03-28 13:00:33,821] [INFO] [launch.py:348:main] Process 32403 exits successfully.
x3003c0s7b0n0: [2024-03-28 13:00:39,594] [INFO] [launch.py:348:main] Process 44073 exits successfully.
x3003c0s7b0n0: [2024-03-28 13:00:39,594] [INFO] [launch.py:348:main] Process 44076 exits successfully.
x3003c0s7b0n0: [2024-03-28 13:00:39,594] [INFO] [launch.py:348:main] Process 44074 exits successfully.
x3003c0s7b1n0: [2024-03-28 13:00:39,642] [INFO] [launch.py:348:main] Process 51861 exits successfully.
x3003c0s7b1n0: [2024-03-28 13:00:39,642] [INFO] [launch.py:348:main] Process 51862 exits successfully.
x3003c0s7b1n0: [2024-03-28 13:00:39,642] [INFO] [launch.py:348:main] Process 51860 exits successfully.
x3003c0s37b1n0: [2024-03-28 13:00:39,827] [INFO] [launch.py:348:main] Process 32404 exits successfully.
x3003c0s37b1n0: [2024-03-28 13:00:39,828] [INFO] [launch.py:348:main] Process 32402 exits successfully.
x3003c0s37b0n0: [2024-03-28 13:00:40,182] [INFO] [launch.py:348:main] Process 38676 exits successfully.
x3003c0s37b0n0: [2024-03-28 13:00:40,182] [INFO] [launch.py:348:main] Process 38677 exits successfully.
x3003c0s37b0n0: [2024-03-28 13:00:40,182] [INFO] [launch.py:348:main] Process 38675 exits successfully.

[2024-03-28 13:11:40,396] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-28 13:11:43,465] [INFO] [runner.py:463:main] Using IP address of 10.140.57.42 for node x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-28 13:11:43,467] [INFO] [runner.py:557:main] deepspeed_env file = ./.deepspeed_env
[2024-03-28 13:11:43,467] [INFO] [multinode_runner.py:80:get_cmd] Running on the following workers: x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov,x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov,x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov,x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov
[2024-03-28 13:11:43,468] [INFO] [runner.py:568:main] cmd = pdsh -S -f 1024 -w x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov,x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov,x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov,x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov export PYTHONUSERBASE=/home/am6429/.local/polaris/conda/2023-10-04; export PYTHONPATH=/home/am6429/dl-io/Megatron-DeepSpeed; export PATH=/home/am6429/.conda/envs/dspeed_env/bin:/soft/datascience/conda/2023-10-04/mconda3/condabin:/soft/compilers/cudatoolkit/cuda-11.8.0/bin:/soft/buildtools/cmake/cmake-3.23.2/cmake-3.23.2-linux-x86_64/bin:/opt/cray/pe/gcc/11.2.0/bin:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/include:/opt/cray/pe/pals/1.2.11/bin:/opt/cray/pe/craype/2.7.20/bin:/opt/cray/libfabric/1.15.2.0/bin:/home/am6429/.conda/envs/dspeed_env/bin:/opt/cray/pe/perftools/23.03.0/bin:/opt/cray/pe/papi/7.0.0.1/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/am6429/.local/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/am6429/veloc-build/include:/home/am6429/veloc-build/bin:/soft/datascience/conda/2023-01-10/mconda3/include:/opt/cray/pe/bin:/soft/datascience/conda/2023-01-10/mconda3/include; export LD_LIBRARY_PATH=/usr/lib64/:/soft/compilers/cudatoolkit/cuda-11.8.0/lib64:/opt/cray/pe/gcc/11.2.0/snos/lib64:/soft/compilers/cudatoolkit/cuda-11.8.0/extras/CUPTI/lib64:/soft/libraries/trt/TensorRT-8.5.3.1.Linux.x86_64-gnu.cuda-11.8.cudnn8.6/lib:/soft/libraries/nccl/nccl_2.18.3-1+cuda11.8_x86_64/lib:/soft/libraries/cudnn/cudnn-11-linux-x64-v8.6.0.163/lib:/opt/cray/libfabric/1.15.2.0/lib64:/usr/lib64:/opt/cray/pe/papi/7.0.0.1/lib64:/home/am6429/veloc-build/lib:/home/am6429/veloc-build/lib64:/home/am6429/nvcomp/lib:/soft/datascience/conda/2023-01-10/mconda3/lib:/soft/datascience/conda/2023-01-10/mconda3/lib/; export http_proxy=http://proxy.alcf.anl.gov:3128; export https_proxy=http://proxy.alcf.anl.gov:3128; export CC=gcc; export CXX=g++; export IBV_FORK_SAFE=1; export CFLAGS=-I/soft/datascience/conda/2023-01-10/mconda3/include/; export LDFLAGS=-L/soft/datascience/conda/2023-01-10/mconda3/lib/; export CUDA_DEVICE_MAX_CONNECTIONS=1; export TORCHSNAPSHOT_PER_RANK_MEMORY_BUDGET_BYTES=34359738368; export _DEFAULT_MAX_PER_RANK_IO_CONCURRENCY=1; export _MAX_PER_RANK_IO_CONCURRENCY=1; export NSYS_REPORT_DIR=/home/am6429/dl-io/dl-io-outputs/act-output-70B//rep-70B-tp1-dp16-l80-h8192-a64-sl2048-gbs64-mbs4-ratio1-subg100000000-prefetch0-flush_async1-opt_gaps5-%n;  cd /home/am6429/dl-io/Megatron-DeepSpeed; /home/am6429/.conda/envs/dspeed_env/bin/python -u -m deepspeed.launcher.launch --world_info=eyJ4MzAwM2MwczM3YjBuMC5oc24uY20ucG9sYXJpcy5hbGNmLmFubC5nb3YiOiBbMCwgMSwgMiwgM10sICJ4MzAwM2MwczM3YjFuMC5oc24uY20ucG9sYXJpcy5hbGNmLmFubC5nb3YiOiBbMCwgMSwgMiwgM10sICJ4MzAwM2MwczdiMG4wLmhzbi5jbS5wb2xhcmlzLmFsY2YuYW5sLmdvdiI6IFswLCAxLCAyLCAzXSwgIngzMDAzYzBzN2IxbjAuaHNuLmNtLnBvbGFyaXMuYWxjZi5hbmwuZ292IjogWzAsIDEsIDIsIDNdfQ== --node_rank=%n --master_addr=10.140.57.42 --master_port=29700 /home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py --tensor-model-parallel-size 1 --num-layers 80 --hidden-size 8192 --num-attention-heads 64 --micro-batch-size 4 --global-batch-size 64 --seq-length 2048 --max-position-embeddings 2048 --train-iters 10 --save /local/scratch/llama2/zero3-tp1}_dp16 --data-path /home/am6429/dl-io/datasets/meg-gpt2_text_document --vocab-file /home/am6429/dl-io/datasets/gpt2-vocab.json --merge-file /home/am6429/dl-io/datasets/gpt2-merges.txt --data-impl mmap --tokenizer-type GPTSentencePieceTokenizer --tokenizer-model /home/am6429/dl-io/datasets/tokenizer.model --split 949,50,1 --distributed-backend nccl --lr 3e-4 --lr-decay-style cosine --min-lr 3e-5 --weight-decay 0.1 --clip-grad 1 --lr-warmup-iters 1 --optimizer adam --adam-beta1 0.9 --adam-beta2 0.95 --log-interval 1 --save-interval 1000 --eval-interval 1000 --eval-iters 0 --bf16 --no-query-key-layer-scaling --attention-dropout 0 --hidden-dropout 0 --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization rmsnorm --disable-bias-linear --num-key-value-heads 4 --deepspeed --exit-interval 20 --deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json --zero-stage=3 --no-pipeline-parallel --cpu-optimizer --checkpoint-activations --deepspeed-activation-checkpointing
x3003c0s37b1n0: [2024-03-28 13:11:45,199] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b0n0: [2024-03-28 13:11:45,477] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b1n0: [2024-03-28 13:11:45,489] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b0n0: [2024-03-28 13:11:45,500] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b1n0: [2024-03-28 13:11:46,994] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3003c0s37b1n0: [2024-03-28 13:11:46,995] [INFO] [launch.py:151:main] nnodes=4, num_local_procs=4, node_rank=1
x3003c0s37b1n0: [2024-03-28 13:11:46,995] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [8, 9, 10, 11], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [12, 13, 14, 15]})
x3003c0s37b1n0: [2024-03-28 13:11:46,995] [INFO] [launch.py:163:main] dist_world_size=16
x3003c0s37b1n0: [2024-03-28 13:11:46,995] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3003c0s37b1n0: [2024-03-28 13:11:46,995] [INFO] [launch.py:253:main] process 38816 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b1n0: [2024-03-28 13:11:46,996] [INFO] [launch.py:253:main] process 38817 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b1n0: [2024-03-28 13:11:46,996] [INFO] [launch.py:253:main] process 38818 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b1n0: [2024-03-28 13:11:46,997] [INFO] [launch.py:253:main] process 38819 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b0n0: [2024-03-28 13:11:47,340] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3003c0s37b0n0: [2024-03-28 13:11:47,340] [INFO] [launch.py:151:main] nnodes=4, num_local_procs=4, node_rank=0
x3003c0s37b0n0: [2024-03-28 13:11:47,340] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [8, 9, 10, 11], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [12, 13, 14, 15]})
x3003c0s37b0n0: [2024-03-28 13:11:47,340] [INFO] [launch.py:163:main] dist_world_size=16
x3003c0s37b0n0: [2024-03-28 13:11:47,340] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3003c0s37b0n0: [2024-03-28 13:11:47,340] [INFO] [launch.py:253:main] process 46193 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b0n0: [2024-03-28 13:11:47,341] [INFO] [launch.py:253:main] process 46194 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b0n0: [2024-03-28 13:11:47,342] [INFO] [launch.py:253:main] process 46195 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b0n0: [2024-03-28 13:11:47,342] [INFO] [launch.py:253:main] process 46196 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b1n0: [2024-03-28 13:11:47,845] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3003c0s7b1n0: [2024-03-28 13:11:47,845] [INFO] [launch.py:151:main] nnodes=4, num_local_procs=4, node_rank=3
x3003c0s7b1n0: [2024-03-28 13:11:47,845] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [8, 9, 10, 11], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [12, 13, 14, 15]})
x3003c0s7b1n0: [2024-03-28 13:11:47,845] [INFO] [launch.py:163:main] dist_world_size=16
x3003c0s7b1n0: [2024-03-28 13:11:47,845] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3003c0s7b1n0: [2024-03-28 13:11:47,846] [INFO] [launch.py:253:main] process 58257 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b1n0: [2024-03-28 13:11:47,846] [INFO] [launch.py:253:main] process 58258 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b1n0: [2024-03-28 13:11:47,847] [INFO] [launch.py:253:main] process 58259 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b1n0: [2024-03-28 13:11:47,847] [INFO] [launch.py:253:main] process 58260 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b0n0: [2024-03-28 13:11:47,864] [INFO] [launch.py:145:main] WORLD INFO DICT: {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3]}
x3003c0s7b0n0: [2024-03-28 13:11:47,864] [INFO] [launch.py:151:main] nnodes=4, num_local_procs=4, node_rank=2
x3003c0s7b0n0: [2024-03-28 13:11:47,865] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov': [0, 1, 2, 3], 'x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov': [4, 5, 6, 7], 'x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov': [8, 9, 10, 11], 'x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov': [12, 13, 14, 15]})
x3003c0s7b0n0: [2024-03-28 13:11:47,865] [INFO] [launch.py:163:main] dist_world_size=16
x3003c0s7b0n0: [2024-03-28 13:11:47,865] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
x3003c0s7b0n0: [2024-03-28 13:11:47,865] [INFO] [launch.py:253:main] process 50485 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=0', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b0n0: [2024-03-28 13:11:47,866] [INFO] [launch.py:253:main] process 50486 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=1', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b0n0: [2024-03-28 13:11:47,867] [INFO] [launch.py:253:main] process 50487 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=2', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s7b0n0: [2024-03-28 13:11:47,867] [INFO] [launch.py:253:main] process 50488 spawned with command: ['/home/am6429/.conda/envs/dspeed_env/bin/python', '-u', '/home/am6429/dl-io/Megatron-DeepSpeed//pretrain_gpt.py', '--local_rank=3', '--tensor-model-parallel-size', '1', '--num-layers', '80', '--hidden-size', '8192', '--num-attention-heads', '64', '--micro-batch-size', '4', '--global-batch-size', '64', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-iters', '10', '--save', '/local/scratch/llama2/zero3-tp1}_dp16', '--data-path', '/home/am6429/dl-io/datasets/meg-gpt2_text_document', '--vocab-file', '/home/am6429/dl-io/datasets/gpt2-vocab.json', '--merge-file', '/home/am6429/dl-io/datasets/gpt2-merges.txt', '--data-impl', 'mmap', '--tokenizer-type', 'GPTSentencePieceTokenizer', '--tokenizer-model', '/home/am6429/dl-io/datasets/tokenizer.model', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr', '3e-4', '--lr-decay-style', 'cosine', '--min-lr', '3e-5', '--weight-decay', '0.1', '--clip-grad', '1', '--lr-warmup-iters', '1', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--log-interval', '1', '--save-interval', '1000', '--eval-interval', '1000', '--eval-iters', '0', '--bf16', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--num-key-value-heads', '4', '--deepspeed', '--exit-interval', '20', '--deepspeed_config=/home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json', '--zero-stage=3', '--no-pipeline-parallel', '--cpu-optimizer', '--checkpoint-activations', '--deepspeed-activation-checkpointing']
x3003c0s37b1n0: [2024-03-28 13:11:48,435] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b1n0: [2024-03-28 13:11:48,479] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b1n0: [2024-03-28 13:11:48,488] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b1n0: [2024-03-28 13:11:48,491] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b0n0: [2024-03-28 13:11:49,125] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b0n0: [2024-03-28 13:11:49,144] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b0n0: [2024-03-28 13:11:49,147] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b0n0: [2024-03-28 13:11:49,162] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b1n0: [2024-03-28 13:11:49,632] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b1n0: [2024-03-28 13:11:49,653] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b0n0: [2024-03-28 13:11:49,669] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b0n0: [2024-03-28 13:11:49,680] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b1n0: [2024-03-28 13:11:49,691] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b0n0: [2024-03-28 13:11:49,692] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b1n0: [2024-03-28 13:11:49,694] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s7b0n0: [2024-03-28 13:11:49,699] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b1n0:       meet the required dependencies to JIT install the op.
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: JIT compiled ops requires ninja
x3003c0s37b1n0: ninja .................. [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: op name ................ installed .. compatible
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: DeepSpeed general environment info:
x3003c0s37b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b1n0: torch version .................... 2.0.1+cu118
x3003c0s37b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b1n0: torch cuda version ............... 11.8
x3003c0s37b1n0: torch hip version ................ None
x3003c0s37b1n0: nvcc version ..................... 11.8
x3003c0s37b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b1n0: [2024-03-28 13:11:50,504] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b1n0:       meet the required dependencies to JIT install the op.
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: JIT compiled ops requires ninja
x3003c0s37b1n0: ninja .................. [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: op name ................ installed .. compatible
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b1n0:       meet the required dependencies to JIT install the op.
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: JIT compiled ops requires ninja
x3003c0s37b1n0: ninja .................. [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: op name ................ installed .. compatible
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b1n0:       meet the required dependencies to JIT install the op.
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: JIT compiled ops requires ninja
x3003c0s37b1n0: ninja .................. [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: op name ................ installed .. compatible
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: DeepSpeed general environment info:
x3003c0s37b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b1n0: torch version .................... 2.0.1+cu118
x3003c0s37b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b1n0: torch cuda version ............... 11.8
x3003c0s37b1n0: torch hip version ................ None
x3003c0s37b1n0: nvcc version ..................... 11.8
x3003c0s37b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b1n0: DeepSpeed general environment info:
x3003c0s37b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b1n0: torch version .................... 2.0.1+cu118
x3003c0s37b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b1n0: torch cuda version ............... 11.8
x3003c0s37b1n0: torch hip version ................ None
x3003c0s37b1n0: nvcc version ..................... 11.8
x3003c0s37b1n0: spatial_inferencedeepspeed wheel compiled w.  ............  torch 2.0, cuda 11.8[93m[NO][0m
x3003c0s37b1n0:  .......shared memory (/dev/shm) size [92m[OKAY][0m 
x3003c0s37b1n0: .... 251.61 GB
x3003c0s37b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b1n0: --------------------------------------------------
x3003c0s37b1n0: DeepSpeed general environment info:
x3003c0s37b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b1n0: torch version .................... 2.0.1+cu118
x3003c0s37b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b1n0: torch cuda version ............... 11.8
x3003c0s37b1n0: torch hip version ................ None
x3003c0s37b1n0: nvcc version ..................... 11.8
x3003c0s37b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b1n0: [2024-03-28 13:11:50,685] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b1n0: [2024-03-28 13:11:50,708] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b1n0: [2024-03-28 13:11:50,709] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b0n0:       meet the required dependencies to JIT install the op.
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: JIT compiled ops requires ninja
x3003c0s37b0n0: ninja .................. [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: op name ................ installed .. compatible
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b0n0:       meet the required dependencies to JIT install the op.
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: JIT compiled ops requires ninja
x3003c0s37b0n0: ninja .................. [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: op name ................ installed .. compatible
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b0n0:       meet the required dependencies to JIT install the op.
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: JIT compiled ops requires ninja
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s37b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s37b0n0:       meet the required dependencies to JIT install the op.
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: JIT compiled ops requires ninja
x3003c0s37b0n0: ninja .................. [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: op name ................ installed .. compatible
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b0n0: ninja .................. [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: op name ................ installed .. compatible
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s37b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s37b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: DeepSpeed general environment info:
x3003c0s37b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b0n0: torch version .................... 2.0.1+cu118
x3003c0s37b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b0n0: torch cuda version ............... 11.8
x3003c0s37b0n0: torch hip version ................ None
x3003c0s37b0n0: nvcc version ..................... 11.8
x3003c0s37b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s37b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s37b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s37b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s37b0n0: --------------------------------------------------
x3003c0s37b0n0: DeepSpeed general environment info:
x3003c0s37b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b0n0: torch version .................... 2.0.1+cu118
x3003c0s37b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b0n0: torch cuda version ............... 11.8
x3003c0s37b0n0: torch hip version ................ None
x3003c0s37b0n0: nvcc version ..................... 11.8
x3003c0s37b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b0n0: INFO: overriding default arguments for tokenizer_type:GPTSentencePieceTokenizer                    with tokenizer_type:GPT2BPETokenizer
x3003c0s37b0n0: using world size: 16, data-parallel-size: 16, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
x3003c0s37b0n0: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
x3003c0s37b0n0: using torch.bfloat16 for parameters ...
x3003c0s37b0n0: ------------------------ arguments ------------------------
x3003c0s37b0n0:   accumulate_allreduce_grads_in_fp32 .............. True
x3003c0s37b0n0:   adam_beta1 ...................................... 0.9
x3003c0s37b0n0:   adam_beta2 ...................................... 0.95
x3003c0s37b0n0:   adam_eps ........................................ 1e-08
x3003c0s37b0n0:   add_bias_linear ................................. False
x3003c0s37b0n0:   add_position_embedding .......................... False
x3003c0s37b0n0:   adlr_autoresume ................................. False
x3003c0s37b0n0:   adlr_autoresume_interval ........................ 1000
x3003c0s37b0n0:   aml_data_download_path .......................... None
x3003c0s37b0n0:   apply_layernorm_1p .............................. False
x3003c0s37b0n0:   apply_query_key_layer_scaling ................... False
x3003c0s37b0n0:   apply_residual_connection_post_layernorm ........ False
x3003c0s37b0n0:   async_tensor_model_parallel_allreduce ........... False
x3003c0s37b0n0:   attention_dropout ............................... 0.0
x3003c0s37b0n0:   attention_softmax_in_fp32 ....................... False
x3003c0s37b0n0:   barrier_with_L1_time ............................ True
x3003c0s37b0n0:   bert_binary_head ................................ True
x3003c0s37b0n0:   bert_embedder_type .............................. megatron
x3003c0s37b0n0:   bert_load ....................................... None
x3003c0s37b0n0:   bf16 ............................................ True
x3003c0s37b0n0:   bias_dropout_fusion ............................. True
x3003c0s37b0n0:   bias_gelu_fusion ................................ False
x3003c0s37b0n0:   biencoder_projection_dim ........................ 0
x3003c0s37b0n0:   biencoder_shared_query_context_model ............ False
x3003c0s37b0n0:   block_data_path ................................. None
x3003c0s37b0n0:   checkpoint_activations .......................... True
x3003c0s37b0n0:   checkpoint_in_cpu ............................... False
x3003c0s37b0n0:   checkpoint_num_layers ........................... 1
x3003c0s37b0n0:   classes_fraction ................................ 1.0
x3003c0s37b0n0:   clip_grad ....................................... 1.0
x3003c0s37b0n0:   compression_training ............................ False
x3003c0s37b0n0:   consumed_train_samples .......................... 0
x3003c0s37b0n0:   consumed_train_tokens ........................... 0
x3003c0s37b0n0:   consumed_valid_samples .......................... 0
x3003c0s37b0n0:   contigious_checkpointing ........................ False
x3003c0s37b0n0:   cpu_optimizer ................................... True
x3003c0s37b0n0:   cpu_torch_adam .................................. False
x3003c0s37b0n0:   create_moe_param_group .......................... False
x3003c0s37b0n0:   curriculum_learning_legacy ...................... False
x3003c0s37b0n0:   data_cache_path ................................. None
x3003c0s37b0n0:   data_efficiency_curriculum_learning ............. False
x3003c0s37b0n0:   data_impl ....................................... mmap
x3003c0s37b0n0:   data_parallel_random_init ....................... False
x3003c0s37b0n0:   data_parallel_size .............................. 16
x3003c0s37b0n0:   data_path ....................................... ['/home/am6429/dl-io/datasets/meg-gpt2_text_document']
x3003c0s37b0n0:   data_per_class_fraction ......................... 1.0
x3003c0s37b0n0:   data_sharding ................................... True
x3003c0s37b0n0:   dataloader_type ................................. single
x3003c0s37b0n0:   DDP_impl ........................................ local
x3003c0s37b0n0:   decoder_num_layers .............................. None
x3003c0s37b0n0:   decoder_seq_length .............................. None
x3003c0s37b0n0:   deepscale ....................................... False
x3003c0s37b0n0:   deepscale_config ................................ None
x3003c0s37b0n0:   deepspeed ....................................... True
x3003c0s37b0n0:   deepspeed_activation_checkpointing .............. True
x3003c0s37b0n0:   deepspeed_config ................................ /home/am6429/dl-io/dl-io-outputs/act-output-70B//ds_config.json
x3003c0s37b0n0:   dino_bottleneck_size ............................ 256
x3003c0s37b0n0:   dino_freeze_last_layer .......................... 1
x3003c0s37b0n0:   dino_head_hidden_size ........................... 2048
x3003c0s37b0n0:   dino_local_crops_number ......................... 10
x3003c0s37b0n0:   dino_local_img_size ............................. 96
x3003c0s37b0n0:   dino_norm_last_layer ............................ False
x3003c0s37b0n0:   dino_teacher_temp ............................... 0.07
x3003c0s37b0n0:   dino_warmup_teacher_temp ........................ 0.04
x3003c0s37b0n0:   dino_warmup_teacher_temp_epochs ................. 30
x3003c0s37b0n0:   distribute_checkpointed_activations ............. False
x3003c0s37b0n0:   distribute_saved_activations .................... False
x3003c0s37b0n0:   distributed_backend ............................. nccl
x3003c0s37b0n0:   distributed_timeout_minutes ..................... 10
x3003c0s37b0n0:   ds_inference .................................... False
x3003c0s37b0n0:   ds_pipeline_enabled ............................. False
x3003c0s37b0n0:   ds_sequence_parallel_size ....................... 1
x3003c0s37b0n0:   embedding_path .................................. None
x3003c0s37b0n0:   embedding_weights_in_fp32 ....................... False
x3003c0s37b0n0:   empty_unused_memory_level ....................... 0
x3003c0s37b0n0:   enable_expert_tensor_parallelism ................ False
x3003c0s37b0n0:   encoder_num_layers .............................. 80
x3003c0s37b0n0:   encoder_seq_length .............................. 2048
x3003c0s37b0n0:   end_weight_decay ................................ 0.1
x3003c0s37b0n0:   eod_mask_loss ................................... False
x3003c0s37b0n0:   eval_interval ................................... 1000
x3003c0s37b0n0:   eval_iters ...................................... 0
x3003c0s37b0n0:   evidence_data_path .............................. None
x3003c0s37b0n0:   exit_duration_in_mins ........................... None
x3003c0s37b0n0:   exit_interval ................................... 20
x3003c0s37b0n0:   exit_on_missing_checkpoint ...................... False
x3003c0s37b0n0:   exit_signal_handler ............................. Falsespatial_inference
x3003c0s37b0n0:   expert_interval ................................. 2
x3003c0s37b0n0:    ffn_hidden_size ................................. 21824
x3003c0s37b0n0: ......  finetune ........................................ False 
x3003c0s37b0n0: [93m[NO][0m   force_ds_sequence_parallel ...................... False.......
x3003c0s37b0n0:    fp16 ............................................ False[92m[OKAY][0m
x3003c0s37b0n0:   fp16_lm_cross_entropy ........................... False
x3003c0s37b0n0: 
x3003c0s37b0n0:   fp32_residual_connection ........................ False
x3003c0s37b0n0:   fp8_amax_compute_algo ........................... most_recent
x3003c0s37b0n0:   fp8_amax_history_len ............................ 1
x3003c0s37b0n0:   fp8_e4m3 ........................................ Falsetransformer
x3003c0s37b0n0:    fp8_hybrid ...................................... False............
x3003c0s37b0n0:    fp8_interval .................................... 1[93m[NO][0m
x3003c0s37b0n0:    fp8_margin ...................................... 0.......
x3003c0s37b0n0:    fp8_wgrad ....................................... True[92m[OKAY][0m
x3003c0s37b0n0: 
x3003c0s37b0n0:   global_batch_size ............................... 64
x3003c0s37b0n0:   gradient_accumulation_fusion .................... True
x3003c0s37b0n0:   head_lr_mult .................................... 1.0
x3003c0s37b0n0: stochastic_transformer  hidden_dropout .................................. 0.0 
x3003c0s37b0n0: .  hidden_size ..................................... 8192 
x3003c0s37b0n0: [93m[NO][0m  hidden_size_teacher ............................. None
x3003c0s37b0n0:    hysteresis ...................................... 2.......
x3003c0s37b0n0:    ict_head_size ................................... None[92m[OKAY][0m
x3003c0s37b0n0: 
x3003c0s37b0n0:   ict_load ........................................ None
x3003c0s37b0n0:   img_h ........................................... 224
x3003c0s37b0n0: --------------------------------------------------  img_w ........................................... 224
x3003c0s37b0n0: 
x3003c0s37b0n0:   indexer_batch_size .............................. 128
x3003c0s37b0n0:   indexer_log_interval ............................ 1000
x3003c0s37b0n0:   inference ....................................... False
x3003c0s37b0n0:   inference_batch_times_seqlen_threshold .......... 512
x3003c0s37b0n0:   init_method_std ................................. 0.02
x3003c0s37b0n0:   init_method_xavier_uniform ...................... False
x3003c0s37b0n0:   initial_loss_scale .............................. 4294967296
x3003c0s37b0n0:   iter_per_epoch .................................. 1250
x3003c0s37b0n0:   kd .............................................. False
x3003c0s37b0n0:   kd_alpha_ce ..................................... 1
x3003c0s37b0n0:   kd_beta_ce ...................................... 1
x3003c0s37b0n0:   kd_temp ......................................... 1.0
x3003c0s37b0n0:   kv_channels ..................................... 128
x3003c0s37b0n0:   layernorm_epsilon ............................... 1e-05
x3003c0s37b0n0:   lazy_mpu_init ................................... None
x3003c0s37b0n0:   load ............................................ None
x3003c0s37b0n0:   load_teacher .................................... None
x3003c0s37b0n0:   local_rank ...................................... 0
x3003c0s37b0n0:   log_batch_size_to_tensorboard ................... False
x3003c0s37b0n0:   log_interval .................................... 1
x3003c0s37b0n0:   log_learning_rate_to_tensorboard ................ True
x3003c0s37b0n0:   log_loss_scale_to_tensorboard ................... True
x3003c0s37b0n0:   log_memory_to_tensorboard ....................... False
x3003c0s37b0n0:   log_num_zeros_in_grad ........................... False
x3003c0s37b0n0:   log_optimizer_states_to_tensorboard ............. False
x3003c0s37b0n0:   log_params_norm ................................. False
x3003c0s37b0n0:   log_timers_to_tensorboard ....................... False
x3003c0s37b0n0:   log_validation_ppl_to_tensorboard ............... False
x3003c0s37b0n0:   log_world_size_to_tensorboard ................... False
x3003c0s37b0n0:   loss_scale ...................................... None
x3003c0s37b0n0:   loss_scale_window ............................... 1000
x3003c0s37b0n0:   lr .............................................. 0.0003
x3003c0s37b0n0:   lr_decay_iters .................................. None
x3003c0s37b0n0:   lr_decay_samples ................................ None
x3003c0s37b0n0:   lr_decay_style .................................. cosine
x3003c0s37b0n0:   lr_decay_tokens ................................. None
x3003c0s37b0n0:   lr_warmup_fraction .............................. None
x3003c0s37b0n0:   lr_warmup_iters ................................. 1
x3003c0s37b0n0:   lr_warmup_samples ............................... 0
x3003c0s37b0n0:   lr_warmup_tokens ................................ None
x3003c0s37b0n0:   make_vocab_size_divisible_by .................... 128
x3003c0s37b0n0:   mask_factor ..................................... 1.0
x3003c0s37b0n0:   mask_prob ....................................... 0.15
x3003c0s37b0n0:   mask_type ....................................... random
x3003c0s37b0n0:   masked_softmax_fusion ........................... True
x3003c0s37b0n0:   max_position_embeddings ......................... 2048
x3003c0s37b0n0:   max_tokens_to_oom ............................... 12000
x3003c0s37b0n0:   memory_centric_tiled_linear ..................... False
x3003c0s37b0n0:   merge_file ...................................... /home/am6429/dl-io/datasets/gpt2-merges.txt
x3003c0s37b0n0:   micro_batch_size ................................ 4
x3003c0s37b0n0:   min_loss_scale .................................. 1.0
x3003c0s37b0n0:   min_lr .......................................... 3e-05
x3003c0s37b0n0:   mlp_type ........................................ standard
x3003c0s37b0n0:   mmap_warmup ..................................... False
x3003c0s37b0n0:   moe_eval_capacity_factor ........................ 1.0
x3003c0s37b0n0:   moe_expert_parallel_size ........................ 1
x3003c0s37b0n0:   moe_loss_coeff .................................. 0.1
x3003c0s37b0n0:   moe_min_capacity ................................ 4
x3003c0s37b0n0:   moe_token_dropping .............................. True
x3003c0s37b0n0:   moe_train_capacity_factor ....................... 1.0
x3003c0s37b0n0:   mos ............................................. False
x3003c0s37b0n0:   no_load_lr_state ................................ False
x3003c0s37b0n0:   no_load_optim ................................... None
x3003c0s37b0n0:   no_load_rng ..................................... None
x3003c0s37b0n0:   no_persist_layer_norm ........................... False
x3003c0s37b0n0:   no_pipeline_parallel ............................ True
x3003c0s37b0n0:   no_save_optim ................................... None
x3003c0s37b0n0:   no_save_rng ..................................... None
x3003c0s37b0n0:   normalization ................................... rmsnorm
x3003c0s37b0n0:   num_attention_heads ............................. 64
x3003c0s37b0n0:   num_attention_heads_teacher ..................... None
x3003c0s37b0n0:   num_channels .................................... 3
x3003c0s37b0n0:   num_classes ..................................... 1000
x3003c0s37b0n0:   num_experts ..................................... [1]
x3003c0s37b0n0:   num_experts_switch .............................. None
x3003c0s37b0n0:   num_experts_teacher ............................. [1]
x3003c0s37b0n0:   num_key_value_heads ............................. 4
x3003c0s37b0n0:   num_layers ...................................... 80
x3003c0s37b0n0:   num_layers_per_virtual_pipeline_stage ........... None
x3003c0s37b0n0:   num_layers_teacher .............................. None
x3003c0s37b0n0:   num_workers ..................................... 2
x3003c0s37b0n0:   onnx_safe ....................................... None
x3003c0s37b0n0:   openai_gelu ..................................... False
x3003c0s37b0n0:   optimizer ....................................... adam
x3003c0s37b0n0:   output_bert_embeddings .......................... False
x3003c0s37b0n0:   overlap_p2p_comm ................................ False
x3003c0s37b0n0:   override_opt_param_scheduler .................... False
x3003c0s37b0n0:   params_dtype .................................... torch.bfloat16
x3003c0s37b0n0:   partition_activations ........................... False
x3003c0s37b0n0:   patch_dim ....................................... 16
x3003c0s37b0n0:   perform_initialization .......................... True
x3003c0s37b0n0:   pipeline_model_parallel_size .................... 1
x3003c0s37b0n0:   pipeline_model_parallel_split_rank .............. None
x3003c0s37b0n0:   profile_backward ................................ False
x3003c0s37b0n0:   query_in_block_prob ............................. 0.1
x3003c0s37b0n0:   rampup_batch_size ............................... None
x3003c0s37b0n0:   random_ltd ...................................... False
x3003c0s37b0n0:   rank ............................................ 0
x3003c0s37b0n0:   recompute_granularity ........................... None
x3003c0s37b0n0:   recompute_method ................................ None
x3003c0s37b0n0:   recompute_num_layers ............................ 1
x3003c0s37b0n0:   remote_device ................................... none
x3003c0s37b0n0:   reset_attention_mask ............................ False
x3003c0s37b0n0:   reset_iteration ................................. False
x3003c0s37b0n0:   reset_position_ids .............................. False
x3003c0s37b0n0:   retriever_report_topk_accuracies ................ []
x3003c0s37b0n0:   retriever_score_scaling ......................... False
x3003c0s37b0n0:   retriever_seq_length ............................ 256
x3003c0s37b0n0:   retro_add_retriever ............................. False
x3003c0s37b0n0:   retro_cyclic_train_iters ........................ None
x3003c0s37b0n0:   retro_encoder_attention_dropout ................. 0.1
x3003c0s37b0n0:   retro_encoder_hidden_dropout .................... 0.1
x3003c0s37b0n0:   retro_encoder_layers ............................ 2
x3003c0s37b0n0:   retro_num_neighbors ............................. 2
x3003c0s37b0n0:   retro_num_retrieved_chunks ...................... 2
x3003c0s37b0n0:   retro_return_doc_ids ............................ False
x3003c0s37b0n0:   retro_workdir ................................... None
x3003c0s37b0n0:   return_data_index ............................... False
x3003c0s37b0n0:   rotary_percent .................................. 1.0
x3003c0s37b0n0:   sample_rate ..................................... 1.0
x3003c0s37b0n0:   save ............................................ /local/scratch/llama2/zero3-tp1}_dp16
x3003c0s37b0n0:   save_interval ................................... 1000
x3003c0s37b0n0:   scatter_gather_tensors_in_pipeline .............. True
x3003c0s37b0n0:   scattered_embeddings ............................ False
x3003c0s37b0n0:   seed ............................................ 1234
x3003c0s37b0n0:   seq_length ...................................... 2048
x3003c0s37b0n0:   sequence_parallel ............................... False
x3003c0s37b0n0:   sgd_momentum .................................... 0.9
x3003c0s37b0n0:   short_seq_prob .................................. 0.1
x3003c0s37b0n0:   skip_train ...................................... False
x3003c0s37b0n0:   split ........................................... 949,50,1
x3003c0s37b0n0:   split_transformers .............................. False
x3003c0s37b0n0:   squared_relu .................................... False
x3003c0s37b0n0:   standalone_embedding_stage ...................... False
x3003c0s37b0n0:   start_weight_decay .............................. 0.1
x3003c0s37b0n0:   swiglu .......................................... True
x3003c0s37b0n0:   swin_backbone_type .............................. tiny
x3003c0s37b0n0:   synchronize_each_layer .......................... False
x3003c0s37b0n0:   tensor_model_parallel_size ...................... 1
x3003c0s37b0n0:   tensorboard_dir ................................. None
x3003c0s37b0n0:   tensorboard_log_interval ........................ 1
x3003c0s37b0n0:   tensorboard_queue_size .......................... 1000
x3003c0s37b0n0:   test_data_path .................................. None
x3003c0s37b0n0:   tile_factor ..................................... 1
x3003c0s37b0n0:   timing_log_level ................................ 0
x3003c0s37b0n0:   timing_log_option ............................... minmax
x3003c0s37b0n0:   titles_data_path ................................ None
x3003c0s37b0n0:   tokenizer_model ................................. /home/am6429/dl-io/datasets/tokenizer.model
x3003c0s37b0n0:   tokenizer_type .................................. GPT2BPETokenizer
x3003c0s37b0n0:   topk ............................................ 1
x3003c0s37b0n0:   train_data_exact_num_epochs ..................... None
x3003c0s37b0n0:   train_data_path ................................. None
x3003c0s37b0n0:   train_desc_path ................................. None
x3003c0s37b0n0:   train_doc_idx_path .............................. None
x3003c0s37b0n0:   train_idx_path .................................. None
x3003c0s37b0n0:   train_iters ..................................... 10
x3003c0s37b0n0:   train_sample_idx_path ........................... None
x3003c0s37b0n0:   train_samples ................................... None
x3003c0s37b0n0:   train_shuffle_idx_path .......................... None
x3003c0s37b0n0:   train_tokens .................................... None
x3003c0s37b0n0:   transformer_impl ................................ local
x3003c0s37b0n0:   transformer_pipeline_model_parallel_size ........ 1
x3003c0s37b0n0:   untie_embeddings_and_output_weights ............. True
x3003c0s37b0n0:   use_checkpoint_args ............................. False
x3003c0s37b0n0:   use_checkpoint_opt_param_scheduler .............. False
x3003c0s37b0n0:   use_contiguous_buffers_in_local_ddp ............. True
x3003c0s37b0n0:   use_cpu_initialization .......................... None
x3003c0s37b0n0:   use_dataset_only ................................ False
x3003c0s37b0n0:   use_distributed_optimizer ....................... False
x3003c0s37b0n0:   use_flash_attn .................................. False
x3003c0s37b0n0:   use_flash_attn_triton ........................... False
x3003c0s37b0n0:   use_flash_attn_v1 ............................... False
x3003c0s37b0n0:   use_flash_attn_v2 ............................... False
x3003c0s37b0n0:   use_one_sent_docs ............................... False
x3003c0s37b0n0:   use_pin_memory .................................. False
x3003c0s37b0n0:   use_ring_exchange_p2p ........................... False
x3003c0s37b0n0:   use_rotary_position_embeddings .................. True
x3003c0s37b0n0:   use_tutel ....................................... False
x3003c0s37b0n0:   valid_data_path ................................. None
x3003c0s37b0n0:   variable_seq_lengths ............................ False
x3003c0s37b0n0:   virtual_pipeline_model_parallel_size ............ None
x3003c0s37b0n0:   vision_backbone_type ............................ vit
x3003c0s37b0n0:   vision_pretraining .............................. False
x3003c0s37b0n0:   vision_pretraining_type ......................... classify
x3003c0s37b0n0:   vocab_extra_ids ................................. 0
x3003c0s37b0n0:   vocab_file ...................................... /home/am6429/dl-io/datasets/gpt2-vocab.json
x3003c0s37b0n0:   vocab_size ...................................... None
x3003c0s37b0n0:   weight_decay .................................... 0.1
x3003c0s37b0n0:   weight_decay_incr_style ......................... constant
x3003c0s37b0n0:   world_size ...................................... 16
x3003c0s37b0n0:   zero_allgather_bucket_size ...................... 0.0
x3003c0s37b0n0:   zero_contigious_gradients ....................... False
x3003c0s37b0n0:   zero_reduce_bucket_size ......................... 0.0
x3003c0s37b0n0:   zero_reduce_scatter ............................. False
x3003c0s37b0n0:   zero_stage ...................................... 3
x3003c0s37b0n0: -------------------- end of arguments ---------------------
x3003c0s37b0n0: setting number of micro-batches to constant 1
x3003c0s37b0n0: > building GPT2BPETokenizer tokenizer ...
x3003c0s37b0n0: DeepSpeed general environment info:
x3003c0s37b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b0n0: torch version .................... 2.0.1+cu118
x3003c0s37b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b0n0: torch cuda version ............... 11.8
x3003c0s37b0n0: torch hip version ................ None
x3003c0s37b0n0: nvcc version ..................... 11.8
x3003c0s37b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b0n0: DeepSpeed general environment info:
x3003c0s37b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s37b0n0: torch version .................... 2.0.1+cu118
x3003c0s37b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s37b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s37b0n0: torch cuda version ............... 11.8
x3003c0s37b0n0: torch hip version ................ None
x3003c0s37b0n0: nvcc version ..................... 11.8
x3003c0s37b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s37b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s37b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s37b0n0:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
x3003c0s37b0n0: > initializing torch distributed ...
x3003c0s37b0n0: [2024-03-28 13:11:52,136] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b0n0: [2024-03-28 13:11:52,136] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
x3003c0s37b0n0: [2024-03-28 13:11:52,162] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b0n0: [2024-03-28 13:11:52,164] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b0n0: [2024-03-28 13:11:52,169] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b0n0:       meet the required dependencies to JIT install the op.
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: JIT compiled ops requires ninja
x3003c0s7b0n0: ninja .................. [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: op name ................ installed .. compatible
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b1n0:       meet the required dependencies to JIT install the op.
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: JIT compiled ops requires ninja
x3003c0s7b1n0: ninja .................. [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: op name ................ installed .. compatible
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b1n0:       meet the required dependencies to JIT install the op.
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: JIT compiled ops requires ninja
x3003c0s7b1n0: ninja .................. [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: op name ................ installed .. compatible
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b1n0:       meet the required dependencies to JIT install the op.
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: JIT compiled ops requires ninja
x3003c0s7b1n0: ninja .................. [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: op name ................ installed .. compatible
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b1n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b1n0:       meet the required dependencies to JIT install the op.
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: JIT compiled ops requires ninja
x3003c0s7b1n0: ninja .................. [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: op name ................ installed .. compatible
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b0n0:       meet the required dependencies to JIT install the op.
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: JIT compiled ops requires ninja
x3003c0s7b0n0: ninja .................. [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: op name ................ installed .. compatible
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b0n0:       meet the required dependencies to JIT install the op.
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: JIT compiled ops requires ninja
x3003c0s7b0n0: ninja .................. [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: op name ................ installed .. compatible
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: DeepSpeed C++/CUDA extension op report
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: NOTE: Ops not installed will be just-in-time (JIT) compiled at
x3003c0s7b0n0:       runtime if needed. Op compatibility means that your system
x3003c0s7b0n0:       meet the required dependencies to JIT install the op.
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: JIT compiled ops requires ninja
x3003c0s7b0n0: ninja .................. [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: op name ................ installed .. compatible
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: async_copier ........... [92m[YES][0m ...... [92m[OKAY][0m
x3003c0s7b1n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b1n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: DeepSpeed general environment info:
x3003c0s7b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b0n0: torch version .................... 2.0.1+cu118
x3003c0s7b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b0n0: torch cuda version ............... 11.8
x3003c0s7b0n0: torch hip version ................ None
x3003c0s7b0n0: nvcc version ..................... 11.8
x3003c0s7b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: DeepSpeed general environment info:
x3003c0s7b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b1n0: torch version .................... 2.0.1+cu118
x3003c0s7b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b1n0: torch cuda version ............... 11.8
x3003c0s7b1n0: torch hip version ................ None
x3003c0s7b1n0: nvcc version ..................... 11.8
x3003c0s7b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: DeepSpeed general environment info:
x3003c0s7b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b1n0: torch version .................... 2.0.1+cu118
x3003c0s7b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b1n0: torch cuda version ............... 11.8
x3003c0s7b1n0: torch hip version ................ None
x3003c0s7b1n0: nvcc version ..................... 11.8
x3003c0s7b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: DeepSpeed general environment info:
x3003c0s7b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b1n0: torch version .................... 2.0.1+cu118
x3003c0s7b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b1n0: torch cuda version ............... 11.8
x3003c0s7b1n0: torch hip version ................ None
x3003c0s7b1n0: nvcc version ..................... 11.8
x3003c0s7b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b1n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b1n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b1n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b1n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: --------------------------------------------------
x3003c0s7b1n0: DeepSpeed general environment info:
x3003c0s7b1n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b1n0: torch version .................... 2.0.1+cu118
x3003c0s7b1n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b1n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b1n0: torch cuda version ............... 11.8
x3003c0s7b1n0: torch hip version ................ None
x3003c0s7b1n0: nvcc version ..................... 11.8
x3003c0s7b1n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b1n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b0n0: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
x3003c0s7b0n0: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b0n0: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [2024-03-28 13:11:52,607] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [2024-03-28 13:11:52,618] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b1n0: [2024-03-28 13:11:52,630] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: DeepSpeed general environment info:
x3003c0s7b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b0n0: torch version .................... 2.0.1+cu118
x3003c0s7b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b0n0: torch cuda version ............... 11.8
x3003c0s7b0n0: torch hip version ................ None
x3003c0s7b0n0: nvcc version ..................... 11.8
x3003c0s7b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b1n0: [2024-03-28 13:11:52,650] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b0n0: DeepSpeed general environment info:
x3003c0s7b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b0n0: torch version .................... 2.0.1+cu118
x3003c0s7b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b0n0: torch cuda version ............... 11.8
x3003c0s7b0n0: torch hip version ................ None
x3003c0s7b0n0: nvcc version ..................... 11.8
x3003c0s7b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b0n0: inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b1n0: [2024-03-28 13:11:52,668] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b0n0: ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
x3003c0s7b0n0: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
x3003c0s7b0n0: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
x3003c0s7b0n0: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
x3003c0s7b0n0: --------------------------------------------------
x3003c0s7b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b0n0: DeepSpeed general environment info:
x3003c0s7b0n0: torch install path ............... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/torch']
x3003c0s7b0n0: torch version .................... 2.0.1+cu118
x3003c0s7b0n0: deepspeed install path ........... ['/home/am6429/.conda/envs/dspeed_env/lib/python3.10/site-packages/deepspeed']
x3003c0s7b0n0: deepspeed info ................... 0.13.3+8074cd62, 8074cd62, hybrid_opt_offload
x3003c0s7b0n0: torch cuda version ............... 11.8
x3003c0s7b0n0: torch hip version ................ None
x3003c0s7b0n0: nvcc version ..................... 11.8
x3003c0s7b0n0: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
x3003c0s7b0n0: shared memory (/dev/shm) size .... 251.61 GB
x3003c0s7b0n0: **** Git info for Megatron: git_hash=248aa6f git_branch=HEAD ****
x3003c0s7b0n0: [2024-03-28 13:11:52,709] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b0n0: [2024-03-28 13:11:52,718] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s7b0n0: [2024-03-28 13:11:52,742] [INFO] [comm.py:637:init_distributed] cdb=None
x3003c0s37b0n0: > initialized tensor model parallel with size 1
x3003c0s37b0n0: > initialized pipeline model parallel with size 1
x3003c0s37b0n0: > setting random seeds to 1234 ...
x3003c0s37b0n0: [2024-03-28 13:11:53,693] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
x3003c0s37b0n0: > compiling dataset index builder ...
x3003c0s37b0n0: make: Entering directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3003c0s37b0n0: make: Nothing to be done for 'default'.
x3003c0s37b0n0: make: Leaving directory '/home/am6429/dl-io/Megatron-DeepSpeed/megatron/data'
x3003c0s37b0n0: >>> done with dataset index builder. Compilation time: 0.085 seconds
x3003c0s37b0n0: > compiling and loading fused kernels ...
x3003c0s37b0n0: ninja: no work to do.
x3003c0s37b0n0: ninja: no work to do.
x3003c0s37b0n0: ninja: no work to do.
x3003c0s37b0n0: >>> done with compiling and loading fused kernels. Compilation time: 3.913 seconds
x3003c0s37b1n0: <<<<<<<<<<< 4
x3003c0s37b1n0: <<<<<<<<<<< 5
x3003c0s37b1n0: <<<<<<<<<<< 7
x3003c0s37b1n0: <<<<<<<<<<< 6
x3003c0s7b1n0: <<<<<<<<<<< 12
x3003c0s7b0n0: <<<<<<<<<<< 10
x3003c0s7b0n0: <<<<<<<<<<< 11
x3003c0s7b1n0: <<<<<<<<<<< 14
x3003c0s7b1n0: <<<<<<<<<<< 15
x3003c0s7b1n0: <<<<<<<<<<< 13
x3003c0s7b0n0: <<<<<<<<<<< 9
x3003c0s37b0n0: initialize_megatron took 6.403801918029785
x3003c0s37b0n0: <<<<<<<<<<< 0
x3003c0s7b0n0: <<<<<<<<<<< 8
x3003c0s37b0n0: <<<<<<<<<<< 3
x3003c0s37b0n0: <<<<<<<<<<< 1
x3003c0s37b0n0: <<<<<<<<<<< 2
x3003c0s37b0n0: time to initialize megatron (seconds): 8.219
x3003c0s37b0n0: [after megatron is initialized] datetime: 2024-03-28 13:11:58 
x3003c0s37b0n0: get_accelerator and all_reduce  took 0.0032935142517089844
x3003c0s37b0n0: building GPT model ...
x3003c0s37b0n0: [2024-03-28 13:11:58,570] [INFO] [utils.py:800:see_memory_usage] Before Building Model
x3003c0s37b0n0: [2024-03-28 13:11:58,571] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 5.68 GB         CA 0.0 GB         Max_CA 6 GB 
x3003c0s37b0n0: [2024-03-28 13:11:58,571] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 19.86 GB, percent = 3.9%
x3003c0s37b0n0: [2024-03-28 13:12:12,027] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 563, num_elems = 55.14B
x3003c0s37b0n0: [2024-03-28 13:12:12,098] [INFO] [utils.py:800:see_memory_usage] After Building Model
x3003c0s37b0n0: [2024-03-28 13:12:12,098] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 7.22 GB         CA 35.12 GB         Max_CA 38 GB 
x3003c0s37b0n0: [2024-03-28 13:12:12,099] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 20.28 GB, percent = 4.0%
x3003c0s37b0n0:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 55141736448
x3003c0s37b1n0: ninja: no work to do.
x3003c0s37b1n0: Time to load cpu_adam op: 2.324786901473999 seconds
x3003c0s37b1n0: Time to load cpu_adam op: 2.34582781791687 seconds
x3003c0s37b1n0: Time to load cpu_adam op: 2.3536605834960938 seconds
x3003c0s37b1n0: Time to load cpu_adam op: 2.3657569885253906 seconds
x3003c0s37b0n0: ninja: no work to do.
x3003c0s37b0n0: Time to load cpu_adam op: 2.4474246501922607 seconds
x3003c0s7b0n0: Time to load cpu_adam op: 2.4672250747680664 seconds
x3003c0s7b0n0: Time to load cpu_adam op: 2.47269606590271 seconds
x3003c0s7b0n0: Time to load cpu_adam op: 2.4897351264953613 seconds
x3003c0s7b1n0: Time to load cpu_adam op: 2.502697467803955 seconds
x3003c0s7b1n0: Time to load cpu_adam op: 2.5103402137756348 seconds
x3003c0s7b0n0: Time to load cpu_adam op: 2.5233421325683594 seconds
x3003c0s7b1n0: Time to load cpu_adam op: 2.548830986022949 seconds
x3003c0s7b1n0: Time to load cpu_adam op: 2.5777745246887207 seconds
x3003c0s37b0n0: ninja: no work to do.
x3003c0s37b0n0: Time to load cpu_adam op: 2.4475486278533936 seconds
x3003c0s37b0n0: ninja: no work to do.
x3003c0s37b0n0: Time to load cpu_adam op: 2.5439293384552 seconds
x3003c0s37b0n0: Time to load cpu_adam op: 2.6397931575775146 seconds
x3003c0s37b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b0n0: > learning rate decay style: cosine
x3003c0s37b0n0: DeepSpeed is enabled.
x3003c0s37b0n0: [2024-03-28 13:12:17,358] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.3+8074cd62, git-hash=8074cd62, git-branch=hybrid_opt_offload
x3003c0s37b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s37b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s7b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s7b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s7b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s7b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s7b1n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b1n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s7b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s7b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s7b0n0: Adam Optimizer #0 is created with AVX2 arithmetic capability.
x3003c0s7b0n0: Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
x3003c0s37b0n0: [2024-03-28 13:12:17,431] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After args sanity test
x3003c0s37b0n0: [2024-03-28 13:12:17,432] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 6.46 GB         CA 35.12 GB         Max_CA 35 GB 
x3003c0s37b0n0: [2024-03-28 13:12:17,432] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.71 GB, percent = 5.5%
x3003c0s37b0n0: [2024-03-28 13:12:17,492] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure distributed model
x3003c0s37b0n0: [2024-03-28 13:12:17,492] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 6.46 GB         CA 35.12 GB         Max_CA 35 GB 
x3003c0s37b0n0: [2024-03-28 13:12:17,492] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.71 GB, percent = 5.5%
x3003c0s37b0n0: [2024-03-28 13:12:17,558] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After configure distributed model
x3003c0s37b0n0: [2024-03-28 13:12:17,559] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 6.46 GB         CA 35.12 GB         Max_CA 35 GB 
x3003c0s37b0n0: [2024-03-28 13:12:17,559] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.71 GB, percent = 5.5%
x3003c0s37b0n0: [2024-03-28 13:12:17,559] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
x3003c0s37b0n0: [2024-03-28 13:12:17,614] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: After setting model parameters
x3003c0s37b0n0: [2024-03-28 13:12:17,615] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 6.46 GB         CA 35.12 GB         Max_CA 35 GB 
x3003c0s37b0n0: [2024-03-28 13:12:17,615] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.71 GB, percent = 5.5%
x3003c0s37b0n0: [2024-03-28 13:12:17,672] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure optimizer
x3003c0s37b0n0: [2024-03-28 13:12:17,673] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 6.46 GB         CA 35.12 GB         Max_CA 35 GB 
x3003c0s37b0n0: [2024-03-28 13:12:17,673] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.71 GB, percent = 5.5%
x3003c0s37b0n0: [2024-03-28 13:12:17,673] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
x3003c0s37b0n0: [2024-03-28 13:12:17,673] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
x3003c0s37b0n0: [2024-03-28 13:12:17,704] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
x3003c0s37b0n0: [2024-03-28 13:12:17,704] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
x3003c0s37b0n0: [2024-03-28 13:12:17,704] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
x3003c0s37b0n0: [2024-03-28 13:12:17,704] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
x3003c0s37b0n0: [2024-03-28 13:12:17,759] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
x3003c0s37b0n0: [2024-03-28 13:12:17,760] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 6.46 GB         CA 35.12 GB         Max_CA 35 GB 
x3003c0s37b0n0: [2024-03-28 13:12:17,760] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.71 GB, percent = 5.5%
x3003c0s37b0n0: [2024-03-28 13:12:17,762] [INFO] [stage3.py:137:__init__] Reduce bucket size 500,000,000
x3003c0s37b0n0: [2024-03-28 13:12:17,762] [INFO] [stage3.py:138:__init__] Prefetch bucket size 50,000,000
x3003c0s37b0n0: [2024-03-28 13:12:17,819] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
x3003c0s37b0n0: [2024-03-28 13:12:17,819] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 6.46 GB         CA 35.12 GB         Max_CA 35 GB 
x3003c0s37b0n0: [2024-03-28 13:12:17,819] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.71 GB, percent = 5.5%
x3003c0s37b0n0: Parameter Offload: Total persistent parameters: 1318912 in 161 params
x3003c0s37b0n0: [2024-03-28 13:12:17,909] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
x3003c0s37b0n0: [2024-03-28 13:12:17,909] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 6.46 GB         CA 35.12 GB         Max_CA 35 GB 
x3003c0s37b0n0: [2024-03-28 13:12:17,909] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.72 GB, percent = 5.5%
x3003c0s37b0n0: [2024-03-28 13:12:17,969] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
x3003c0s37b0n0: [2024-03-28 13:12:17,970] [INFO] [utils.py:801:see_memory_usage] MA 6.46 GB         Max_MA 6.46 GB         CA 35.12 GB         Max_CA 35 GB 
x3003c0s37b0n0: [2024-03-28 13:12:17,970] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.72 GB, percent = 5.5%
x3003c0s37b0n0: [2024-03-28 13:12:17,971] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b0n0: [2024-03-28 13:12:17,971] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b0n0: [2024-03-28 13:12:17,971] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b0n0: [2024-03-28 13:12:17,971] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b0n0: [2024-03-28 13:12:17,971] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b0n0: [2024-03-28 13:12:17,971] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b0n0: [2024-03-28 13:12:17,971] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b0n0: [2024-03-28 13:12:17,971] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b1n0: [2024-03-28 13:12:17,971] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b1n0: [2024-03-28 13:12:17,971] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b1n0: [2024-03-28 13:12:17,971] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b1n0: [2024-03-28 13:12:17,971] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b1n0: [2024-03-28 13:12:17,971] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b1n0: [2024-03-28 13:12:17,971] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s37b1n0: [2024-03-28 13:12:17,971] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b1n0: [2024-03-28 13:12:17,971] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 0, numel: 110626816
x3003c0s7b0n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b0n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b0n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b0n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b0n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b0n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b0n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b0n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b1n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b1n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b1n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b1n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b1n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b1n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b1n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s7b1n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 1, numel: 116132864
x3003c0s37b0n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 2, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,972] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 3, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 4, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 5, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,973] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 6, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 7, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 8, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 9, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,974] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 10, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 11, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 12, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 13, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 14, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,975] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 15, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 16, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 17, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 18, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,976] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 19, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 20, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 21, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 22, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 23, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,977] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 24, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 25, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 26, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 27, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,978] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 28, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 29, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s7b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 30, numel: 100239872
x3003c0s37b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s7b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 31, numel: 111938048
x3003c0s37b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 3] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 0] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 2] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 1] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 13] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 6] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 10] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 12] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 9] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 14] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 5] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 15] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 4] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 8] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b1n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 7] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s7b0n0: [2024-03-28 13:12:17,979] [INFO] [stage3.py:706:_create_fp16_partitions_with_defragmentation] [Rank 11] ========== Param group 0, Subgroup 32, numel: 36930048
x3003c0s37b0n0: [2024-03-28 13:12:20,403] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 33
x3003c0s37b0n0: [2024-03-28 13:12:20,404] [INFO] [utils.py:801:see_memory_usage] MA 6.42 GB         Max_MA 6.46 GB         CA 6.42 GB         Max_CA 35 GB 
x3003c0s37b0n0: [2024-03-28 13:12:20,404] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 46.99 GB, percent = 9.3%
x3003c0s37b0n0: [2024-03-28 13:12:20,467] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
x3003c0s37b0n0: [2024-03-28 13:12:20,467] [INFO] [utils.py:801:see_memory_usage] MA 6.42 GB         Max_MA 6.42 GB         CA 6.42 GB         Max_CA 6 GB 
x3003c0s37b0n0: [2024-03-28 13:12:20,467] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 47.0 GB, percent = 9.3%
x3003c0s37b0n0: [2024-03-28 13:12:22,123] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
x3003c0s37b0n0: [2024-03-28 13:12:22,124] [INFO] [utils.py:801:see_memory_usage] MA 6.42 GB         Max_MA 6.42 GB         CA 6.42 GB         Max_CA 6 GB 
x3003c0s37b0n0: [2024-03-28 13:12:22,124] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 53.54 GB, percent = 10.6%
x3003c0s37b0n0: [2024-03-28 13:12:25,189] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
x3003c0s37b0n0: [2024-03-28 13:12:25,190] [INFO] [utils.py:801:see_memory_usage] MA 6.42 GB         Max_MA 6.42 GB         CA 6.42 GB         Max_CA 6 GB 
x3003c0s37b0n0: [2024-03-28 13:12:25,190] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 82.61 GB, percent = 16.4%
x3003c0s37b0n0: [2024-03-28 13:12:31,851] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | init_optimizer_state: 6648.75
x3003c0s37b0n0: [2024-03-28 13:12:31,926] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
x3003c0s37b0n0: [2024-03-28 13:12:31,927] [INFO] [utils.py:801:see_memory_usage] MA 6.42 GB         Max_MA 6.42 GB         CA 6.42 GB         Max_CA 6 GB 
x3003c0s37b0n0: [2024-03-28 13:12:31,927] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 145.72 GB, percent = 29.0%
x3003c0s37b0n0: [2024-03-28 13:12:32,489] [INFO] [stage3.py:520:_setup_for_real_optimizer] optimizer state initialized
x3003c0s37b0n0: [2024-03-28 13:12:48,400] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
x3003c0s37b0n0: [2024-03-28 13:12:48,401] [INFO] [utils.py:801:see_memory_usage] MA 7.35 GB         Max_MA 8.89 GB         CA 20.98 GB         Max_CA 21 GB 
x3003c0s37b0n0: [2024-03-28 13:12:48,401] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 178.45 GB, percent = 35.5%
x3003c0s37b0n0: [2024-03-28 13:12:48,401] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
x3003c0s37b0n0: [2024-03-28 13:12:48,465] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure LR scheduler
x3003c0s37b0n0: [2024-03-28 13:12:48,466] [INFO] [utils.py:801:see_memory_usage] MA 7.35 GB         Max_MA 7.35 GB         CA 20.98 GB         Max_CA 21 GB 
x3003c0s37b0n0: [2024-03-28 13:12:48,466] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 178.44 GB, percent = 35.5%
x3003c0s37b0n0: [2024-03-28 13:12:48,466] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
x3003c0s37b0n0: [2024-03-28 13:12:48,466] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7fbef24d82b0>
x3003c0s37b0n0: [2024-03-28 13:12:48,466] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 13:12:48,529] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before rewriting optimizer step
x3003c0s37b0n0: [2024-03-28 13:12:48,529] [INFO] [utils.py:801:see_memory_usage] MA 7.35 GB         Max_MA 7.35 GB         CA 20.98 GB         Max_CA 21 GB 
x3003c0s37b0n0: [2024-03-28 13:12:48,529] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 178.45 GB, percent = 35.5%
x3003c0s37b0n0: [2024-03-28 13:12:48,594] [INFO] [utils.py:800:see_memory_usage] DeepSpeed Engine: Before configure checkpointing
x3003c0s37b0n0: [2024-03-28 13:12:48,594] [INFO] [utils.py:801:see_memory_usage] MA 7.35 GB         Max_MA 7.35 GB         CA 20.98 GB         Max_CA 21 GB 
x3003c0s37b0n0: [2024-03-28 13:12:48,594] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 178.43 GB, percent = 35.5%
x3003c0s37b0n0: [2024-03-28 13:12:48,595] [INFO] [config.py:998:print] DeepSpeedEngine configuration:
x3003c0s37b0n0: [2024-03-28 13:12:48,595] [INFO] [config.py:1002:print]   activation_checkpointing_config  {
x3003c0s37b0n0:     "partition_activations": false, 
x3003c0s37b0n0:     "contiguous_memory_optimization": false, 
x3003c0s37b0n0:     "cpu_checkpointing": false, 
x3003c0s37b0n0:     "number_checkpoints": null, 
x3003c0s37b0n0:     "synchronize_checkpoint_boundary": false, 
x3003c0s37b0n0:     "profile": false
x3003c0s37b0n0: }
x3003c0s37b0n0: [2024-03-28 13:12:48,595] [INFO] [config.py:1002:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
x3003c0s37b0n0: [2024-03-28 13:12:48,595] [INFO] [config.py:1002:print]   amp_enabled .................. False
x3003c0s37b0n0: [2024-03-28 13:12:48,595] [INFO] [config.py:1002:print]   amp_params ................... False
x3003c0s37b0n0: [2024-03-28 13:12:48,595] [INFO] [config.py:1002:print]   autotuning_config ............ {
x3003c0s37b0n0:     "enabled": false, 
x3003c0s37b0n0:     "start_step": null, 
x3003c0s37b0n0:     "end_step": null, 
x3003c0s37b0n0:     "metric_path": null, 
x3003c0s37b0n0:     "arg_mappings": null, 
x3003c0s37b0n0:     "metric": "throughput", 
x3003c0s37b0n0:     "model_info": null, 
x3003c0s37b0n0:     "results_dir": "autotuning_results", 
x3003c0s37b0n0:     "exps_dir": "autotuning_exps", 
x3003c0s37b0n0:     "overwrite": true, 
x3003c0s37b0n0:     "fast": true, 
x3003c0s37b0n0:     "start_profile_step": 3, 
x3003c0s37b0n0:     "end_profile_step": 5, 
x3003c0s37b0n0:     "tuner_type": "gridsearch", 
x3003c0s37b0n0:     "tuner_early_stopping": 5, 
x3003c0s37b0n0:     "tuner_num_trials": 50, 
x3003c0s37b0n0:     "model_info_path": null, 
x3003c0s37b0n0:     "mp_size": 1, 
x3003c0s37b0n0:     "max_train_batch_size": null, 
x3003c0s37b0n0:     "min_train_batch_size": 1, 
x3003c0s37b0n0:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
x3003c0s37b0n0:     "min_train_micro_batch_size_per_gpu": 1, 
x3003c0s37b0n0:     "num_tuning_micro_batch_sizes": 3
x3003c0s37b0n0: }
x3003c0s37b0n0: [2024-03-28 13:12:48,595] [INFO] [config.py:1002:print]   bfloat16_enabled ............. True
x3003c0s37b0n0: [2024-03-28 13:12:48,595] [INFO] [config.py:1002:print]   bfloat16_immediate_grad_update  False
x3003c0s37b0n0: [2024-03-28 13:12:48,595] [INFO] [config.py:1002:print]   checkpoint_parallel_write_pipeline  False
x3003c0s37b0n0: [2024-03-28 13:12:48,595] [INFO] [config.py:1002:print]   checkpoint_tag_validation_enabled  True
x3003c0s37b0n0: [2024-03-28 13:12:48,595] [INFO] [config.py:1002:print]   checkpoint_tag_validation_fail  False
x3003c0s37b0n0: [2024-03-28 13:12:48,595] [INFO] [config.py:1002:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fbef24d8c10>
x3003c0s37b0n0: [2024-03-28 13:12:48,595] [INFO] [config.py:1002:print]   communication_data_type ...... None
x3003c0s37b0n0: [2024-03-28 13:12:48,595] [INFO] [config.py:1002:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
x3003c0s37b0n0: [2024-03-28 13:12:48,595] [INFO] [config.py:1002:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
x3003c0s37b0n0: [2024-03-28 13:12:48,595] [INFO] [config.py:1002:print]   curriculum_enabled_legacy .... False
x3003c0s37b0n0: [2024-03-28 13:12:48,595] [INFO] [config.py:1002:print]   curriculum_params_legacy ..... False
x3003c0s37b0n0: [2024-03-28 13:12:48,595] [INFO] [config.py:1002:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
x3003c0s37b0n0: [2024-03-28 13:12:48,595] [INFO] [config.py:1002:print]   data_efficiency_enabled ...... False
x3003c0s37b0n0: [2024-03-28 13:12:48,595] [INFO] [config.py:1002:print]   dataloader_drop_last ......... False
x3003c0s37b0n0: [2024-03-28 13:12:48,595] [INFO] [config.py:1002:print]   datastates_config ............ {
x3003c0s37b0n0:     "enabled": null, 
x3003c0s37b0n0:     "config": {
x3003c0s37b0n0:     }
x3003c0s37b0n0: }
x3003c0s37b0n0: [2024-03-28 13:12:48,595] [INFO] [config.py:1002:print]   disable_allgather ............ False
x3003c0s37b0n0: [2024-03-28 13:12:48,595] [INFO] [config.py:1002:print]   dump_state ................... False
x3003c0s37b0n0: [2024-03-28 13:12:48,595] [INFO] [config.py:1002:print]   dynamic_loss_scale_args ...... None
x3003c0s37b0n0: [2024-03-28 13:12:48,595] [INFO] [config.py:1002:print]   eigenvalue_enabled ........... False
x3003c0s37b0n0: [2024-03-28 13:12:48,595] [INFO] [config.py:1002:print]   eigenvalue_gas_boundary_resolution  1
x3003c0s37b0n0: [2024-03-28 13:12:48,595] [INFO] [config.py:1002:print]   eigenvalue_layer_name ........ bert.encoder.layer
x3003c0s37b0n0: [2024-03-28 13:12:48,595] [INFO] [config.py:1002:print]   eigenvalue_layer_num ......... 0
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   eigenvalue_max_iter .......... 100
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   eigenvalue_stability ......... 1e-06
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   eigenvalue_tol ............... 0.01
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   eigenvalue_verbose ........... False
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   elasticity_enabled ........... False
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   flops_profiler_config ........ {
x3003c0s37b0n0:     "enabled": false, 
x3003c0s37b0n0:     "recompute_fwd_factor": 0.0, 
x3003c0s37b0n0:     "profile_step": 1, 
x3003c0s37b0n0:     "module_depth": -1, 
x3003c0s37b0n0:     "top_modules": 1, 
x3003c0s37b0n0:     "detailed": true, 
x3003c0s37b0n0:     "output_file": null
x3003c0s37b0n0: }
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   fp16_auto_cast ............... None
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   fp16_enabled ................. False
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   fp16_master_weights_and_gradients  False
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   global_rank .................. 0
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   grad_accum_dtype ............. bf16
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   gradient_accumulation_steps .. 1
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   gradient_clipping ............ 0.0
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   gradient_predivide_factor .... 1.0
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   graph_harvesting ............. False
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   initial_dynamic_scale ........ 1
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   load_universal_checkpoint .... False
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   loss_scale ................... 1.0
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   memory_breakdown ............. True
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   mics_hierarchial_params_gather  False
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   mics_shard_size .............. -1
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   nebula_config ................ {
x3003c0s37b0n0:     "enabled": false, 
x3003c0s37b0n0:     "persistent_storage_path": null, 
x3003c0s37b0n0:     "persistent_time_interval": 100, 
x3003c0s37b0n0:     "num_of_version_in_retention": 2, 
x3003c0s37b0n0:     "enable_nebula_load": true, 
x3003c0s37b0n0:     "load_path": null
x3003c0s37b0n0: }
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   optimizer_legacy_fusion ...... False
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   optimizer_name ............... None
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   optimizer_params ............. None
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   pld_enabled .................. False
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   pld_params ................... False
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   prescale_gradients ........... False
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   scheduler_name ............... None
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   scheduler_params ............. None
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   seq_parallel_communication_data_type  torch.float32
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   sparse_attention ............. None
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   sparse_gradients_enabled ..... False
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   steps_per_print .............. 1
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   train_batch_size ............. 64
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   train_micro_batch_size_per_gpu  4
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   use_data_before_expert_parallel_  False
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   use_node_local_storage ....... False
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   wall_clock_breakdown ......... True
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   weight_quantization_config ... None
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   world_size ................... 16
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   zero_allow_untested_optimizer  False
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0, prefetch_optimizer=False, part_grads_async=True, prefetch_optimizer_gap=5) sub_group_size=100000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   zero_enabled ................. True
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   zero_force_ds_cpu_optimizer .. True
x3003c0s37b0n0: [2024-03-28 13:12:48,596] [INFO] [config.py:1002:print]   zero_optimization_stage ...... 3
x3003c0s37b0n0: [2024-03-28 13:12:48,597] [INFO] [config.py:988:print_user_config]   json = {
x3003c0s37b0n0:     "train_batch_size": 64, 
x3003c0s37b0n0:     "train_micro_batch_size_per_gpu": 4, 
x3003c0s37b0n0:     "steps_per_print": 1, 
x3003c0s37b0n0:     "zero_optimization": {
x3003c0s37b0n0:         "stage": 3, 
x3003c0s37b0n0:         "offload_optimizer": {
x3003c0s37b0n0:             "device": "cpu", 
x3003c0s37b0n0:             "ratio": 1, 
x3003c0s37b0n0:             "pin_memory": true, 
x3003c0s37b0n0:             "prefetch_optimizer": 0, 
x3003c0s37b0n0:             "part_grads_async": 1, 
x3003c0s37b0n0:             "prefetch_optimizer_gap": 5
x3003c0s37b0n0:         }, 
x3003c0s37b0n0:         "sub_group_size": 1.000000e+08
x3003c0s37b0n0:     }, 
x3003c0s37b0n0:     "bf16": {
x3003c0s37b0n0:         "enabled": true
x3003c0s37b0n0:     }, 
x3003c0s37b0n0:     "data_types": {
x3003c0s37b0n0:         "grad_accum_dtype": "bf16"
x3003c0s37b0n0:     }, 
x3003c0s37b0n0:     "wall_clock_breakdown": true, 
x3003c0s37b0n0:     "memory_breakdown": true, 
x3003c0s37b0n0:     "flops_profiler": {
x3003c0s37b0n0:         "enabled": false
x3003c0s37b0n0:     }
x3003c0s37b0n0: }
x3003c0s37b1n0: <TIMER:model-and-optimizer-setup,51.24509859085083>
x3003c0s37b1n0: <TIMER:model-and-optimizer-setup,51.245349407196045>
x3003c0s37b1n0: <TIMER:model-and-optimizer-setup,51.24566864967346>
x3003c0s37b0n0: <TIMER:model-and-optimizer-setup,51.24613332748413>
x3003c0s37b0n0: <TIMER:model-and-optimizer-setup,51.24617338180542>
x3003c0s37b1n0: <TIMER:model-and-optimizer-setup,51.24615240097046>
x3003c0s7b1n0: <TIMER:model-and-optimizer-setup,51.2462100982666>
x3003c0s7b1n0: <TIMER:model-and-optimizer-setup,51.24620771408081>
x3003c0s37b0n0: <TIMER:model-and-optimizer-setup,51.24634528160095>
x3003c0s37b0n0: <TIMER:model-and-optimizer-setup,51.24634623527527>
x3003c0s7b0n0: <TIMER:model-and-optimizer-setup,51.246349573135376><TIMER:model-and-optimizer-setup,51.246339082717896>
x3003c0s7b0n0: 
x3003c0s7b1n0: <TIMER:model-and-optimizer-setup,51.24640965461731>
x3003c0s7b1n0: <TIMER:model-and-optimizer-setup,51.246410608291626>
x3003c0s7b0n0: <TIMER:model-and-optimizer-setup,51.246578216552734>
x3003c0s7b0n0: <TIMER:model-and-optimizer-setup,51.24658203125>
x3003c0s37b0n0: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-28 13:12:49 
x3003c0s37b0n0: > building train, validation, and test datasets ...
x3003c0s37b0n0:  > datasets target sizes (minimum size):
x3003c0s37b0n0:     train:      640
x3003c0s37b0n0:     validation: 0
x3003c0s37b0n0:     test:       0
x3003c0s37b0n0: > building train, validation, and test datasets for GPT ...
x3003c0s37b0n0: Single data path provided for train, valid & test
x3003c0s37b0n0:  > building dataset index ...
x3003c0s37b0n0:     reading sizes...
x3003c0s37b0n0:     reading pointers...
x3003c0s37b0n0:     reading document index...
x3003c0s37b0n0:     creating numpy buffer of mmap...
x3003c0s37b0n0:     creating memory view of numpy buffer...
x3003c0s37b0n0:  > finished creating indexed dataset in 0.002151 seconds
x3003c0s37b0n0:     number of documents: 79000
x3003c0s37b0n0:  > dataset split:
x3003c0s37b0n0:     train:
x3003c0s37b0n0:      document indices in [0, 74971) total of 74971 documents
x3003c0s37b0n0:     validation:
x3003c0s37b0n0:      document indices in [74971, 78921) total of 3950 documents
x3003c0s37b0n0:     test:
x3003c0s37b0n0:      document indices in [78921, 79000) total of 79 documents
x3003c0s37b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/d5d1daec41eb416469c3827ed48205ed_doc_idx.npy
x3003c0s37b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/d5d1daec41eb416469c3827ed48205ed_sample_idx.npy
x3003c0s37b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/d5d1daec41eb416469c3827ed48205ed_shuffle_idx.npy
x3003c0s37b0n0:     loaded indexed file in 0.004 seconds
x3003c0s37b0n0:     total number of samples: 108448
x3003c0s37b0n0:     total number of epochs: 1
x3003c0s37b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_doc_idx.npy
x3003c0s37b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_sample_idx.npy
x3003c0s37b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/4ea6d225cc7d60d779e46cebdb4c487e_shuffle_idx.npy
x3003c0s37b0n0:     loaded indexed file in 0.003 seconds
x3003c0s37b0n0:     total number of samples: 5792
x3003c0s37b0n0:     total number of epochs: 1
x3003c0s37b0n0:  > loading doc-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_doc_idx.npy
x3003c0s37b0n0:  > loading sample-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_sample_idx.npy
x3003c0s37b0n0:  > loading shuffle-idx mapping from /home/am6429/dl-io/datasets/index-cache/14bf7f3c9438c6348d40db0f3af62a29_shuffle_idx.npy
x3003c0s37b0n0:     loaded indexed file in 0.041 seconds
x3003c0s37b0n0:     total number of samples: 185
x3003c0s37b0n0:     total number of epochs: 1
x3003c0s37b0n0: > finished creating GPT datasets ...
x3003c0s7b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5170924663543701>
x3003c0s7b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5249509811401367>
x3003c0s37b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5467908382415771>
x3003c0s37b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5469760894775391>
x3003c0s37b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5479562282562256>
x3003c0s37b0n0: <TIMER:train/valid/test-data-iterators-setup,0.552290678024292>
x3003c0s37b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5550854206085205>
x3003c0s37b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5559682846069336>
x3003c0s37b1n0: <TIMER:train/valid/test-data-iterators-setup,0.5562458038330078>
x3003c0s7b0n0: <TIMER:train/valid/test-data-iterators-setup,0.557636022567749>
x3003c0s7b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5598719120025635>
x3003c0s7b0n0: <TIMER:train/valid/test-data-iterators-setup,0.5836896896362305>
x3003c0s7b0n0: <TIMER:train/valid/test-data-iterators-setup,0.583712100982666>
x3003c0s37b0n0: <TIMER:train/valid/test-data-iterators-setup,0.588115930557251>
x3003c0s7b1n0: <TIMER:train/valid/test-data-iterators-setup,0.600776195526123>
x3003c0s7b1n0: <TIMER:train/valid/test-data-iterators-setup,0.6939001083374023>
x3003c0s37b0n0: [after dataloaders are built] datetime: 2024-03-28 13:12:50 
x3003c0s37b0n0: done with setup ...
x3003c0s7b1n0: (min, max) time across ranks (ms):
x3003c0s7b1n0:     model-and-optimizer-setup ......................: (51245.10, 51246.58)
x3003c0s7b1n0:     train/valid/test-data-iterators-setup ..........: (517.09, 693.90)
x3003c0s37b0n0: training ...
x3003c0s37b0n0: [before training begins] datetime: 2024-03-28 13:12:50 
x3003c0s37b0n0: [before the start of training step] datetime: 2024-03-28 13:12:50 
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 13:12:50,633] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 13:12:50,633] [INFO] [utils.py:801:see_memory_usage] MA 7.36 GB         Max_MA 7.36 GB         CA 8.02 GB         Max_CA 21 GB 
x3003c0s37b0n0: [2024-03-28 13:12:50,634] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 181.2 GB, percent = 36.0%
x3003c0s37b0n0: [2024-03-28 13:12:50,768] [INFO] [checkpointing.py:539:forward] Activation Checkpointing Information
x3003c0s37b0n0: [2024-03-28 13:12:50,768] [INFO] [checkpointing.py:540:forward] ----Partition Activations False, CPU CHECKPOINTING False
x3003c0s37b0n0: [2024-03-28 13:12:50,768] [INFO] [checkpointing.py:541:forward] ----contiguous Memory Checkpointing False with 80 total layers
x3003c0s37b0n0: [2024-03-28 13:12:50,768] [INFO] [checkpointing.py:543:forward] ----Synchronization False
x3003c0s37b0n0: [2024-03-28 13:12:50,768] [INFO] [checkpointing.py:544:forward] ----Profiling time in checkpointing False
x3003c0s37b0n0: [2024-03-28 13:13:07,262] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 13:13:07,263] [INFO] [utils.py:801:see_memory_usage] MA 21.15 GB         Max_MA 24.22 GB         CA 28.51 GB         Max_CA 29 GB 
x3003c0s37b0n0: [2024-03-28 13:13:07,263] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 181.5 GB, percent = 36.1%
x3003c0s37b0n0: [2024-03-28 13:13:07,534] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 13:13:07,534] [INFO] [utils.py:801:see_memory_usage] MA 21.15 GB         Max_MA 21.15 GB         CA 21.43 GB         Max_CA 29 GB 
x3003c0s37b0n0: [2024-03-28 13:13:07,534] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 181.51 GB, percent = 36.1%
x3003c0s37b0n0: [2024-03-28 13:14:00,579] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 13:14:00,580] [INFO] [utils.py:801:see_memory_usage] MA 10.9 GB         Max_MA 26.34 GB         CA 11.56 GB         Max_CA 35 GB 
x3003c0s37b0n0: [2024-03-28 13:14:00,580] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 181.54 GB, percent = 36.1%
x3003c0s37b0n0: [2024-03-28 13:14:00,667] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 13:14:00,668] [INFO] [utils.py:801:see_memory_usage] MA 10.9 GB         Max_MA 10.9 GB         CA 11.56 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:14:00,668] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 181.62 GB, percent = 36.1%
x3003c0s7b0n0: [2024-03-28 13:14:10,025] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.286000967025757
x3003c0s7b0n0: [2024-03-28 13:14:10,026] [INFO] [stage3.py:2251:step] Full outer step loop took 9.286813497543335
x3003c0s37b1n0: [2024-03-28 13:14:10,042] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.303446769714355
x3003c0s37b1n0: [2024-03-28 13:14:10,043] [INFO] [stage3.py:2251:step] Full outer step loop took 9.303828239440918
x3003c0s7b1n0: [2024-03-28 13:14:10,128] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.389323711395264
x3003c0s7b1n0: [2024-03-28 13:14:10,128] [INFO] [stage3.py:2251:step] Full outer step loop took 9.389607667922974
x3003c0s37b0n0: [2024-03-28 13:14:10,167] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.428602457046509
x3003c0s37b0n0: [2024-03-28 13:14:10,168] [INFO] [stage3.py:2251:step] Full outer step loop took 9.42903447151184
x3003c0s37b1n0: [2024-03-28 13:14:10,166] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.426712036132812
x3003c0s37b1n0: [2024-03-28 13:14:10,166] [INFO] [stage3.py:2251:step] Full outer step loop took 9.427601099014282
x3003c0s7b1n0: [2024-03-28 13:14:10,173] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.434483766555786
x3003c0s7b1n0: [2024-03-28 13:14:10,174] [INFO] [stage3.py:2251:step] Full outer step loop took 9.434807777404785
x3003c0s7b1n0: [2024-03-28 13:14:10,198] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.458930730819702
x3003c0s7b1n0: [2024-03-28 13:14:10,198] [INFO] [stage3.py:2251:step] Full outer step loop took 9.459519624710083
x3003c0s7b1n0: [2024-03-28 13:14:10,207] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.468342304229736
x3003c0s7b1n0: [2024-03-28 13:14:10,207] [INFO] [stage3.py:2251:step] Full outer step loop took 9.46865439414978
x3003c0s37b1n0: [2024-03-28 13:14:10,209] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.469829797744751
x3003c0s37b1n0: [2024-03-28 13:14:10,210] [INFO] [stage3.py:2251:step] Full outer step loop took 9.471317052841187
x3003c0s37b0n0: [2024-03-28 13:14:10,211] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.472533226013184
x3003c0s37b0n0: [2024-03-28 13:14:10,212] [INFO] [stage3.py:2251:step] Full outer step loop took 9.473723888397217
x3003c0s37b0n0: [2024-03-28 13:14:10,237] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.498362064361572
x3003c0s37b0n0: [2024-03-28 13:14:10,238] [INFO] [stage3.py:2251:step] Full outer step loop took 9.499081134796143
x3003c0s37b1n0: [2024-03-28 13:14:10,239] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.499780654907227
x3003c0s37b1n0: [2024-03-28 13:14:10,239] [INFO] [stage3.py:2251:step] Full outer step loop took 9.50010633468628
x3003c0s37b0n0: [2024-03-28 13:14:10,256] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.517334699630737
x3003c0s37b0n0: [2024-03-28 13:14:10,256] [INFO] [stage3.py:2251:step] Full outer step loop took 9.517554521560669
x3003c0s7b0n0: [2024-03-28 13:14:10,286] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.547648191452026
x3003c0s7b0n0: [2024-03-28 13:14:10,287] [INFO] [stage3.py:2251:step] Full outer step loop took 9.547877788543701
x3003c0s7b0n0: [2024-03-28 13:14:10,324] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.584911584854126
x3003c0s7b0n0: [2024-03-28 13:14:10,324] [INFO] [stage3.py:2251:step] Full outer step loop took 9.585194826126099
x3003c0s7b0n0: [2024-03-28 13:14:10,349] [INFO] [stage3.py:2243:step] With missing steps outer loop took 9.609139442443848
x3003c0s7b0n0: [2024-03-28 13:14:10,350] [INFO] [stage3.py:2251:step] Full outer step loop took 9.60947871208191
x3003c0s7b0n0: [2024-03-28 13:14:10,382] [INFO] [stage3.py:2277:step] End to end step took 9.643350839614868
x3003c0s37b0n0: [2024-03-28 13:14:10,382] [INFO] [stage3.py:2277:step] End to end step took 9.643351078033447
x3003c0s37b1n0: [2024-03-28 13:14:10,382] [INFO] [stage3.py:2277:step] End to end step took 9.64334487915039
x3003c0s7b0n0: [2024-03-28 13:14:10,382] [INFO] [stage3.py:2277:step] End to end step took 9.642054557800293
x3003c0s7b1n0: [2024-03-28 13:14:10,382] [INFO] [stage3.py:2277:step] End to end step took 9.643451452255249
x3003c0s37b0n0: [2024-03-28 13:14:10,382] [INFO] [stage3.py:2277:step] End to end step took 9.643682479858398
x3003c0s7b0n0: [2024-03-28 13:14:10,383] [INFO] [stage3.py:2277:step] End to end step took 9.643858194351196
x3003c0s7b1n0: [2024-03-28 13:14:10,383] [INFO] [stage3.py:2277:step] End to end step took 9.64384937286377
x3003c0s7b0n0: [2024-03-28 13:14:10,383] [INFO] [stage3.py:2277:step] End to end step took 9.643962383270264
x3003c0s37b1n0: [2024-03-28 13:14:10,383] [INFO] [stage3.py:2277:step] End to end step took 9.643980503082275
x3003c0s37b0n0: [2024-03-28 13:14:10,383] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 9429.32
x3003c0s37b0n0: [2024-03-28 13:14:10,383] [INFO] [stage3.py:2277:step] End to end step took 9.644033193588257
x3003c0s37b1n0: [2024-03-28 13:14:10,383] [INFO] [stage3.py:2277:step] End to end step took 9.644049406051636
x3003c0s37b1n0: [2024-03-28 13:14:10,383] [INFO] [stage3.py:2277:step] End to end step took 9.6440269947052
x3003c0s7b1n0: [2024-03-28 13:14:10,383] [INFO] [stage3.py:2277:step] End to end step took 9.64411211013794
x3003c0s7b1n0: [2024-03-28 13:14:10,383] [INFO] [stage3.py:2277:step] End to end step took 9.644037008285522
x3003c0s37b0n0: [2024-03-28 13:14:10,383] [WARNING] [stage3.py:2267:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
x3003c0s37b0n0: [2024-03-28 13:14:10,383] [INFO] [stage3.py:2277:step] End to end step took 9.644466638565063
x3003c0s37b0n0: [2024-03-28 13:14:10,384] [INFO] [logging.py:96:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 13:14:10,384] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 16786.64 | bwd_microstep: 52845.06 | bwd_inner_microstep: 52718.38 | bwd_allreduce_microstep: 126.55 | step_microstep: 9715.65
x3003c0s37b0n0: [2024-03-28 13:14:10,384] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 16786.64 | bwd: 52845.05 | bwd_inner: 52718.40 | bwd_allreduce: 126.56 | step: 9715.64
x3003c0s37b0n0: [2024-03-28 13:14:10,493] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 13:14:10,494] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 10.9 GB         CA 10.02 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:14:10,494] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.01 GB, percent = 66.8%
x3003c0s37b0n0: <TIMER:interval-time,80.0401840209961>
x3003c0s37b0n0: <TIMER:interval-time,80.04018902778625>
x3003c0s37b0n0: <TIMER:interval-time,80.04020357131958>
x3003c0s7b0n0: <TIMER:interval-time,80.04014134407043><TIMER:interval-time,80.04013133049011>
x3003c0s7b0n0: 
x3003c0s7b0n0: <TIMER:interval-time,80.04012894630432>
x3003c0s37b1n0: <TIMER:interval-time,80.04018473625183><TIMER:interval-time,80.04018568992615>
x3003c0s37b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,80.04019832611084>
x3003c0s37b1n0: <TIMER:interval-time,80.0401918888092>
x3003c0s7b1n0: <TIMER:interval-time,80.04023289680481><TIMER:interval-time,80.04023623466492>
x3003c0s7b1n0: 
x3003c0s7b1n0: <TIMER:interval-time,80.04025030136108><TIMER:interval-time,80.04024481773376>
x3003c0s7b1n0: 
x3003c0s7b0n0: <TIMER:interval-time,80.04024338722229>
x3003c0s37b0n0: <TIMER:interval-time,80.04029774665833>
x3003c0s37b0n0: [Rank 0] (after 1 iterations) memory (MB) | allocated: 9594.7685546875 | max allocated: 9594.76904296875 | reserved: 10262.0 | max reserved: 10262.0
x3003c0s7b1n0:  elapsed_time 80.040245 | consumed samples:           64 | consumed tokens:       131072 | elapsed time per iteration (ms): 80040.2 | learning rate: 3.000E-04 | global batch size:    64 | lm loss: 1.246584E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.800 | TFLOPs: 55.20 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 13:14:10,637] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 13:14:10,637] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 9.37 GB         CA 10.02 GB         Max_CA 10 GB 
x3003c0s37b0n0: [2024-03-28 13:14:10,637] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.09 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:14:23,513] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 13:14:23,513] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 27.7 GB         CA 31.61 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:14:23,513] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.06 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:14:23,601] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 13:14:23,602] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 24.58 GB         CA 25.29 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:14:23,602] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.06 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:14:57,030] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 13:14:57,030] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 28.66 GB         CA 11.56 GB         Max_CA 37 GB 
x3003c0s37b0n0: [2024-03-28 13:14:57,031] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.06 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:14:57,111] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 13:14:57,112] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 10.91 GB         CA 11.56 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:14:57,112] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.06 GB, percent = 66.8%
x3003c0s37b1n0: [2024-03-28 13:15:04,135] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.99357271194458
x3003c0s37b1n0: [2024-03-28 13:15:04,136] [INFO] [stage3.py:2251:step] Full outer step loop took 6.9938740730285645
x3003c0s7b1n0: [2024-03-28 13:15:04,195] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.05283260345459
x3003c0s7b1n0: [2024-03-28 13:15:04,195] [INFO] [stage3.py:2251:step] Full outer step loop took 7.053231477737427
x3003c0s7b0n0: [2024-03-28 13:15:04,250] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.108285903930664
x3003c0s7b0n0: [2024-03-28 13:15:04,250] [INFO] [stage3.py:2251:step] Full outer step loop took 7.108687162399292
x3003c0s37b0n0: [2024-03-28 13:15:04,255] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.113080739974976
x3003c0s37b0n0: [2024-03-28 13:15:04,255] [INFO] [stage3.py:2251:step] Full outer step loop took 7.113613605499268
x3003c0s37b0n0: [2024-03-28 13:15:04,257] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.114802598953247
x3003c0s37b0n0: [2024-03-28 13:15:04,257] [INFO] [stage3.py:2251:step] Full outer step loop took 7.115114688873291
x3003c0s7b0n0: [2024-03-28 13:15:04,270] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.128515720367432
x3003c0s7b0n0: [2024-03-28 13:15:04,271] [INFO] [stage3.py:2251:step] Full outer step loop took 7.128775358200073
x3003c0s7b0n0: [2024-03-28 13:15:04,281] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.139312982559204
x3003c0s7b0n0: [2024-03-28 13:15:04,281] [INFO] [stage3.py:2251:step] Full outer step loop took 7.139484405517578
x3003c0s7b0n0: [2024-03-28 13:15:04,313] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.154576063156128
x3003c0s7b0n0: [2024-03-28 13:15:04,313] [INFO] [stage3.py:2251:step] Full outer step loop took 7.154760360717773
x3003c0s37b0n0: [2024-03-28 13:15:04,317] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.175342798233032
x3003c0s37b0n0: [2024-03-28 13:15:04,317] [INFO] [stage3.py:2251:step] Full outer step loop took 7.175512075424194
x3003c0s7b1n0: [2024-03-28 13:15:04,321] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.1794140338897705
x3003c0s7b1n0: [2024-03-28 13:15:04,322] [INFO] [stage3.py:2251:step] Full outer step loop took 7.1800758838653564
x3003c0s7b1n0: [2024-03-28 13:15:04,365] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.223251581192017
x3003c0s7b1n0: [2024-03-28 13:15:04,365] [INFO] [stage3.py:2251:step] Full outer step loop took 7.223406553268433
x3003c0s37b1n0: [2024-03-28 13:15:04,367] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.225017309188843
x3003c0s37b1n0: [2024-03-28 13:15:04,367] [INFO] [stage3.py:2251:step] Full outer step loop took 7.225220203399658
x3003c0s37b0n0: [2024-03-28 13:15:04,368] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.2258710861206055
x3003c0s37b0n0: [2024-03-28 13:15:04,368] [INFO] [stage3.py:2251:step] Full outer step loop took 7.226024150848389
x3003c0s37b1n0: [2024-03-28 13:15:04,372] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.230339765548706
x3003c0s37b1n0: [2024-03-28 13:15:04,372] [INFO] [stage3.py:2251:step] Full outer step loop took 7.230542182922363
x3003c0s7b1n0: [2024-03-28 13:15:04,393] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.250917434692383
x3003c0s7b1n0: [2024-03-28 13:15:04,393] [INFO] [stage3.py:2251:step] Full outer step loop took 7.2510826587677
x3003c0s37b1n0: [2024-03-28 13:15:04,403] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.261174917221069
x3003c0s37b1n0: [2024-03-28 13:15:04,403] [INFO] [stage3.py:2251:step] Full outer step loop took 7.261345624923706
x3003c0s37b1n0: [2024-03-28 13:15:04,429] [INFO] [stage3.py:2277:step] End to end step took 7.287119150161743
x3003c0s37b0n0: [2024-03-28 13:15:04,429] [INFO] [stage3.py:2277:step] End to end step took 7.287179231643677
x3003c0s7b1n0: [2024-03-28 13:15:04,429] [INFO] [stage3.py:2277:step] End to end step took 7.287187814712524
x3003c0s7b0n0: [2024-03-28 13:15:04,429] [INFO] [stage3.py:2277:step] End to end step took 7.270845651626587
x3003c0s37b1n0: [2024-03-28 13:15:04,429] [INFO] [stage3.py:2277:step] End to end step took 7.287442922592163
x3003c0s7b0n0: [2024-03-28 13:15:04,429] [INFO] [stage3.py:2277:step] End to end step took 7.287477254867554
x3003c0s37b1n0: [2024-03-28 13:15:04,429] [INFO] [stage3.py:2277:step] End to end step took 7.287472486495972
x3003c0s37b0n0: [2024-03-28 13:15:04,429] [INFO] [stage3.py:2277:step] End to end step took 7.287444353103638
x3003c0s7b1n0: [2024-03-28 13:15:04,429] [INFO] [stage3.py:2277:step] End to end step took 7.2876129150390625
x3003c0s37b1n0: [2024-03-28 13:15:04,430] [INFO] [stage3.py:2277:step] End to end step took 7.287684679031372
x3003c0s37b0n0: [2024-03-28 13:15:04,429] [INFO] [stage3.py:2277:step] End to end step took 7.287681579589844
x3003c0s7b1n0: [2024-03-28 13:15:04,430] [INFO] [stage3.py:2277:step] End to end step took 7.287743330001831
x3003c0s7b0n0: [2024-03-28 13:15:04,430] [INFO] [stage3.py:2277:step] End to end step took 7.287776947021484
x3003c0s7b0n0: [2024-03-28 13:15:04,430] [INFO] [stage3.py:2277:step] End to end step took 7.287818193435669
x3003c0s7b1n0: [2024-03-28 13:15:04,430] [INFO] [stage3.py:2277:step] End to end step took 7.287889242172241
x3003c0s37b0n0: [2024-03-28 13:15:04,430] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 7175.70
x3003c0s37b0n0: [2024-03-28 13:15:04,430] [INFO] [stage3.py:2277:step] End to end step took 7.288120269775391
x3003c0s37b0n0: [2024-03-28 13:15:04,430] [INFO] [logging.py:96:log_dist] [Rank 0] step=2, skipped=0, lr=[0.0002918585038060976], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 13:15:04,431] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 12842.86 | bwd_microstep: 33210.49 | bwd_inner_microstep: 33085.13 | bwd_allreduce_microstep: 125.29 | step_microstep: 7318.38
x3003c0s37b0n0: [2024-03-28 13:15:04,431] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 12842.85 | bwd: 33210.48 | bwd_inner: 33085.12 | bwd_allreduce: 125.30 | step: 7318.37
x3003c0s37b0n0: [2024-03-28 13:15:04,580] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 13:15:04,580] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 10.91 GB         CA 10.02 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:15:04,581] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.11 GB, percent = 66.8%
x3003c0s37b0n0: <TIMER:interval-time,54.08600211143494><TIMER:interval-time,54.086002826690674>
x3003c0s37b0n0: 
x3003c0s37b0n0: <TIMER:interval-time,54.08601427078247>
x3003c0s37b0n0: <TIMER:interval-time,54.086008071899414>
x3003c0s7b0n0: <TIMER:interval-time,54.08599805831909>
x3003c0s7b0n0: <TIMER:interval-time,54.08599805831909>
x3003c0s7b0n0: <TIMER:interval-time,54.08600091934204>
x3003c0s7b0n0: <TIMER:interval-time,54.086037158966064>
x3003c0s37b1n0: <TIMER:interval-time,54.08603382110596><TIMER:interval-time,54.08603382110596><TIMER:interval-time,54.086037158966064>
x3003c0s37b1n0: 
x3003c0s7b1n0: <TIMER:interval-time,54.08602714538574><TIMER:interval-time,54.08602714538574>
x3003c0s37b1n0: <TIMER:interval-time,54.08603835105896>
x3003c0s7b1n0: 
x3003c0s7b1n0: <TIMER:interval-time,54.086034297943115>
x3003c0s37b1n0: 
x3003c0s7b1n0: <TIMER:interval-time,54.08613061904907>
x3003c0s7b1n0:  elapsed_time 54.086027 | consumed samples:          128 | consumed tokens:       262144 | elapsed time per iteration (ms): 54086.0 | learning rate: 2.919E-04 | global batch size:    64 | lm loss: 1.246764E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.183 | TFLOPs: 81.69 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 13:15:04,741] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 13:15:04,741] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 9.37 GB         CA 10.02 GB         Max_CA 10 GB 
x3003c0s37b0n0: [2024-03-28 13:15:04,742] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.13 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:15:16,944] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 13:15:16,945] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 27.7 GB         CA 31.61 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:15:16,945] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.09 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:15:17,038] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 13:15:17,039] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 24.58 GB         CA 25.29 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:15:17,039] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.09 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:15:51,412] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 13:15:51,413] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 28.66 GB         CA 11.56 GB         Max_CA 37 GB 
x3003c0s37b0n0: [2024-03-28 13:15:51,413] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.08 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:15:51,496] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 13:15:51,497] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 10.91 GB         CA 11.56 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:15:51,497] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.08 GB, percent = 66.8%
x3003c0s7b0n0: [2024-03-28 13:15:58,455] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.9078004360198975
x3003c0s7b0n0: [2024-03-28 13:15:58,455] [INFO] [stage3.py:2251:step] Full outer step loop took 6.907984972000122
x3003c0s7b0n0: [2024-03-28 13:15:58,470] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.942353248596191
x3003c0s7b0n0: [2024-03-28 13:15:58,470] [INFO] [stage3.py:2251:step] Full outer step loop took 6.942712068557739
x3003c0s37b1n0: [2024-03-28 13:15:58,571] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.043952941894531
x3003c0s37b1n0: [2024-03-28 13:15:58,572] [INFO] [stage3.py:2251:step] Full outer step loop took 7.044275522232056
x3003c0s37b1n0: [2024-03-28 13:15:58,583] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.0555195808410645
x3003c0s37b1n0: [2024-03-28 13:15:58,583] [INFO] [stage3.py:2251:step] Full outer step loop took 7.056105852127075
x3003c0s37b0n0: [2024-03-28 13:15:58,611] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.0832624435424805
x3003c0s37b0n0: [2024-03-28 13:15:58,611] [INFO] [stage3.py:2251:step] Full outer step loop took 7.08384895324707
x3003c0s37b0n0: [2024-03-28 13:15:58,627] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.100117921829224
x3003c0s37b0n0: [2024-03-28 13:15:58,628] [INFO] [stage3.py:2251:step] Full outer step loop took 7.10066556930542
x3003c0s7b1n0: [2024-03-28 13:15:58,657] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.129980564117432
x3003c0s7b1n0: [2024-03-28 13:15:58,657] [INFO] [stage3.py:2251:step] Full outer step loop took 7.130167722702026
x3003c0s7b0n0: [2024-03-28 13:15:58,666] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.138407945632935
x3003c0s7b0n0: [2024-03-28 13:15:58,666] [INFO] [stage3.py:2251:step] Full outer step loop took 7.138570785522461
x3003c0s7b1n0: [2024-03-28 13:15:58,670] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.142770051956177
x3003c0s7b1n0: [2024-03-28 13:15:58,670] [INFO] [stage3.py:2251:step] Full outer step loop took 7.142957448959351
x3003c0s7b1n0: [2024-03-28 13:15:58,670] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.143037796020508
x3003c0s7b1n0: [2024-03-28 13:15:58,671] [INFO] [stage3.py:2251:step] Full outer step loop took 7.143226623535156
x3003c0s37b1n0: [2024-03-28 13:15:58,676] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.149073123931885
x3003c0s37b1n0: [2024-03-28 13:15:58,677] [INFO] [stage3.py:2251:step] Full outer step loop took 7.149286270141602
x3003c0s7b1n0: [2024-03-28 13:15:58,690] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.162269115447998
x3003c0s7b1n0: [2024-03-28 13:15:58,690] [INFO] [stage3.py:2251:step] Full outer step loop took 7.162433862686157
x3003c0s7b0n0: [2024-03-28 13:15:58,692] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.16497802734375
x3003c0s7b0n0: [2024-03-28 13:15:58,692] [INFO] [stage3.py:2251:step] Full outer step loop took 7.165128469467163
x3003c0s37b1n0: [2024-03-28 13:15:58,716] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.188575267791748
x3003c0s37b1n0: [2024-03-28 13:15:58,716] [INFO] [stage3.py:2251:step] Full outer step loop took 7.18873405456543
x3003c0s37b0n0: [2024-03-28 13:15:58,753] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.225430488586426
x3003c0s37b0n0: [2024-03-28 13:15:58,753] [INFO] [stage3.py:2251:step] Full outer step loop took 7.2257044315338135
x3003c0s37b0n0: [2024-03-28 13:15:58,761] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.2333455085754395
x3003c0s37b0n0: [2024-03-28 13:15:58,761] [INFO] [stage3.py:2251:step] Full outer step loop took 7.233522653579712
x3003c0s37b0n0: [2024-03-28 13:15:58,786] [INFO] [stage3.py:2277:step] End to end step took 7.258708953857422
x3003c0s37b1n0: [2024-03-28 13:15:58,786] [INFO] [stage3.py:2277:step] End to end step took 7.258652925491333
x3003c0s7b1n0: [2024-03-28 13:15:58,786] [INFO] [stage3.py:2277:step] End to end step took 7.258716583251953
x3003c0s37b0n0: [2024-03-28 13:15:58,786] [INFO] [stage3.py:2277:step] End to end step took 7.258826732635498
x3003c0s7b1n0: [2024-03-28 13:15:58,786] [INFO] [stage3.py:2277:step] End to end step took 7.258866786956787
x3003c0s7b0n0: [2024-03-28 13:15:58,786] [INFO] [stage3.py:2277:step] End to end step took 7.258870363235474
x3003c0s7b0n0: [2024-03-28 13:15:58,786] [INFO] [stage3.py:2277:step] End to end step took 7.258919715881348
x3003c0s37b1n0: [2024-03-28 13:15:58,786] [INFO] [stage3.py:2277:step] End to end step took 7.2588794231414795
x3003c0s7b1n0: [2024-03-28 13:15:58,786] [INFO] [stage3.py:2277:step] End to end step took 7.259068012237549
x3003c0s7b0n0: [2024-03-28 13:15:58,786] [INFO] [stage3.py:2277:step] End to end step took 7.239109992980957
x3003c0s37b0n0: [2024-03-28 13:15:58,786] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 7084.18
x3003c0s37b1n0: [2024-03-28 13:15:58,786] [INFO] [stage3.py:2277:step] End to end step took 7.2591753005981445
x3003c0s7b0n0: [2024-03-28 13:15:58,786] [INFO] [stage3.py:2277:step] End to end step took 7.259258985519409
x3003c0s37b0n0: [2024-03-28 13:15:58,787] [INFO] [stage3.py:2277:step] End to end step took 7.259371042251587
x3003c0s37b0n0: [2024-03-28 13:15:58,787] [INFO] [stage3.py:2277:step] End to end step took 7.259561061859131
x3003c0s37b1n0: [2024-03-28 13:15:58,787] [INFO] [stage3.py:2277:step] End to end step took 7.259466171264648
x3003c0s7b1n0: [2024-03-28 13:15:58,787] [INFO] [stage3.py:2277:step] End to end step took 7.259579181671143
x3003c0s37b0n0: [2024-03-28 13:15:58,787] [INFO] [logging.py:96:log_dist] [Rank 0] step=3, skipped=0, lr=[0.00026841599982106197], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 13:15:58,788] [INFO] [timer.py:260:stop] epoch=0/micro_step=3/global_step=3, RunningAvgSamplesPerSec=1.1842157270657527, CurrSamplesPerSec=1.1842157270657527, MemAllocated=9.37GB, MaxMemAllocated=10.91GB
x3003c0s37b0n0: [2024-03-28 13:15:58,788] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 12172.41 | bwd_microstep: 34158.89 | bwd_inner_microstep: 34037.25 | bwd_allreduce_microstep: 121.57 | step_microstep: 7290.54
x3003c0s37b0n0: [2024-03-28 13:15:58,788] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 12172.40 | bwd: 34158.88 | bwd_inner: 34037.24 | bwd_allreduce: 121.58 | step: 7290.54
x3003c0s37b0n0: [2024-03-28 13:15:58,945] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 13:15:58,945] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 10.91 GB         CA 10.02 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:15:58,945] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.11 GB, percent = 66.8%
x3003c0s37b0n0: <TIMER:interval-time,54.36423301696777><TIMER:interval-time,54.36423063278198>
x3003c0s37b0n0: 
x3003c0s37b0n0: <TIMER:interval-time,54.36424899101257>
x3003c0s37b1n0: <TIMER:interval-time,54.364234924316406>
x3003c0s37b1n0: <TIMER:interval-time,54.36423969268799><TIMER:interval-time,54.36424160003662><TIMER:interval-time,54.36424255371094>
x3003c0s37b1n0: 
x3003c0s37b0n0: <TIMER:interval-time,54.36427307128906>
x3003c0s37b1n0: 
x3003c0s7b1n0: <TIMER:interval-time,54.36427426338196><TIMER:interval-time,54.364280700683594>
x3003c0s7b1n0: <TIMER:interval-time,54.36428165435791><TIMER:interval-time,54.36428165435791>
x3003c0s7b1n0: 
x3003c0s7b1n0: 
x3003c0s7b0n0: <TIMER:interval-time,54.3642201423645><TIMER:interval-time,54.364222049713135><TIMER:interval-time,54.36422300338745>
x3003c0s7b0n0: 
x3003c0s7b0n0: 
x3003c0s7b0n0: <TIMER:interval-time,54.36422920227051>
x3003c0s7b1n0:  elapsed_time 54.364282 | consumed samples:          192 | consumed tokens:       393216 | elapsed time per iteration (ms): 54364.3 | learning rate: 2.684E-04 | global batch size:    64 | lm loss: 4.076758E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.177 | TFLOPs: 81.27 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 13:15:59,105] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 13:15:59,106] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 9.37 GB         CA 10.02 GB         Max_CA 10 GB 
x3003c0s37b0n0: [2024-03-28 13:15:59,106] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.12 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:16:11,103] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 13:16:11,104] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 27.7 GB         CA 31.61 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:16:11,104] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.09 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:16:11,210] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 13:16:11,211] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 24.58 GB         CA 25.29 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:16:11,211] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.09 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:16:44,140] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 13:16:44,141] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 28.66 GB         CA 11.56 GB         Max_CA 37 GB 
x3003c0s37b0n0: [2024-03-28 13:16:44,141] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.08 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:16:44,223] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 13:16:44,223] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 10.91 GB         CA 11.56 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:16:44,223] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.08 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:16:51,043] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.7896647453308105
x3003c0s37b0n0: [2024-03-28 13:16:51,043] [INFO] [stage3.py:2251:step] Full outer step loop took 6.789857625961304
x3003c0s7b0n0: [2024-03-28 13:16:51,158] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.905341625213623
x3003c0s7b0n0: [2024-03-28 13:16:51,158] [INFO] [stage3.py:2251:step] Full outer step loop took 6.90552830696106
x3003c0s7b1n0: [2024-03-28 13:16:51,165] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.911512851715088
x3003c0s7b1n0: [2024-03-28 13:16:51,165] [INFO] [stage3.py:2251:step] Full outer step loop took 6.91219162940979
x3003c0s7b0n0: [2024-03-28 13:16:51,176] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.922826766967773
x3003c0s7b0n0: [2024-03-28 13:16:51,176] [INFO] [stage3.py:2251:step] Full outer step loop took 6.923061370849609
x3003c0s37b1n0: [2024-03-28 13:16:51,248] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.995471715927124
x3003c0s37b1n0: [2024-03-28 13:16:51,249] [INFO] [stage3.py:2251:step] Full outer step loop took 6.995674133300781
x3003c0s7b1n0: [2024-03-28 13:16:51,267] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.014283895492554
x3003c0s7b1n0: [2024-03-28 13:16:51,268] [INFO] [stage3.py:2251:step] Full outer step loop took 7.014740228652954
x3003c0s7b0n0: [2024-03-28 13:16:51,278] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.025054454803467
x3003c0s7b0n0: [2024-03-28 13:16:51,278] [INFO] [stage3.py:2251:step] Full outer step loop took 7.025314092636108
x3003c0s7b1n0: [2024-03-28 13:16:51,330] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.07696533203125
x3003c0s7b1n0: [2024-03-28 13:16:51,330] [INFO] [stage3.py:2251:step] Full outer step loop took 7.077144384384155
x3003c0s7b1n0: [2024-03-28 13:16:51,335] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.082124948501587
x3003c0s7b1n0: [2024-03-28 13:16:51,335] [INFO] [stage3.py:2251:step] Full outer step loop took 7.082296133041382
x3003c0s37b1n0: [2024-03-28 13:16:51,339] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.086413860321045
x3003c0s37b1n0: [2024-03-28 13:16:51,340] [INFO] [stage3.py:2251:step] Full outer step loop took 7.086616516113281
x3003c0s7b0n0: [2024-03-28 13:16:51,358] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.104860544204712
x3003c0s7b0n0: [2024-03-28 13:16:51,358] [INFO] [stage3.py:2251:step] Full outer step loop took 7.105019807815552
x3003c0s37b1n0: [2024-03-28 13:16:51,375] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.121301889419556
x3003c0s37b1n0: [2024-03-28 13:16:51,378] [INFO] [stage3.py:2251:step] Full outer step loop took 7.12535285949707
x3003c0s37b0n0: [2024-03-28 13:16:51,424] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.17150092124939
x3003c0s37b0n0: [2024-03-28 13:16:51,425] [INFO] [stage3.py:2251:step] Full outer step loop took 7.171809911727905
x3003c0s37b0n0: [2024-03-28 13:16:51,471] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.217625856399536
x3003c0s37b0n0: [2024-03-28 13:16:51,471] [INFO] [stage3.py:2251:step] Full outer step loop took 7.21783447265625
x3003c0s37b1n0: [2024-03-28 13:16:51,471] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.217776775360107
x3003c0s37b1n0: [2024-03-28 13:16:51,471] [INFO] [stage3.py:2251:step] Full outer step loop took 7.217939615249634
x3003c0s37b0n0: [2024-03-28 13:16:51,477] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.223588705062866
x3003c0s37b0n0: [2024-03-28 13:16:51,477] [INFO] [stage3.py:2251:step] Full outer step loop took 7.223764419555664
x3003c0s37b1n0: [2024-03-28 13:16:51,502] [INFO] [stage3.py:2277:step] End to end step took 7.249330282211304
x3003c0s7b1n0: [2024-03-28 13:16:51,502] [INFO] [stage3.py:2277:step] End to end step took 7.2493414878845215
x3003c0s37b0n0: [2024-03-28 13:16:51,502] [INFO] [stage3.py:2277:step] End to end step took 7.249419927597046
x3003c0s7b0n0: [2024-03-28 13:16:51,502] [INFO] [stage3.py:2277:step] End to end step took 7.249300003051758
x3003c0s7b1n0: [2024-03-28 13:16:51,502] [INFO] [stage3.py:2277:step] End to end step took 7.249452352523804
x3003c0s37b0n0: [2024-03-28 13:16:51,503] [INFO] [stage3.py:2277:step] End to end step took 7.249569654464722
x3003c0s37b1n0: [2024-03-28 13:16:51,503] [INFO] [stage3.py:2277:step] End to end step took 7.249727725982666
x3003c0s37b0n0: [2024-03-28 13:16:51,503] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6790.01
x3003c0s37b0n0: [2024-03-28 13:16:51,503] [INFO] [stage3.py:2277:step] End to end step took 7.249905586242676
x3003c0s37b0n0: [2024-03-28 13:16:51,503] [INFO] [stage3.py:2277:step] End to end step took 7.250014543533325
x3003c0s7b0n0: [2024-03-28 13:16:51,503] [INFO] [stage3.py:2277:step] End to end step took 7.250001668930054
x3003c0s7b1n0: [2024-03-28 13:16:51,503] [INFO] [stage3.py:2277:step] End to end step took 7.250016689300537
x3003c0s7b0n0: [2024-03-28 13:16:51,503] [INFO] [stage3.py:2277:step] End to end step took 7.250055313110352
x3003c0s7b1n0: [2024-03-28 13:16:51,503] [INFO] [stage3.py:2277:step] End to end step took 7.250080823898315
x3003c0s7b0n0: [2024-03-28 13:16:51,503] [INFO] [stage3.py:2277:step] End to end step took 7.250090837478638
x3003c0s37b1n0: [2024-03-28 13:16:51,503] [INFO] [stage3.py:2277:step] End to end step took 7.250124454498291
x3003c0s37b1n0: [2024-03-28 13:16:51,503] [INFO] [stage3.py:2277:step] End to end step took 7.250145196914673
x3003c0s37b0n0: [2024-03-28 13:16:51,503] [INFO] [logging.py:96:log_dist] [Rank 0] step=4, skipped=0, lr=[0.00023249999999999996], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 13:16:51,504] [INFO] [timer.py:260:stop] epoch=0/micro_step=4/global_step=4, RunningAvgSamplesPerSec=1.202553676082657, CurrSamplesPerSec=1.2214684958153053, MemAllocated=9.37GB, MaxMemAllocated=10.91GB
x3003c0s37b0n0: [2024-03-28 13:16:51,504] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 11964.00 | bwd_microstep: 32718.18 | bwd_inner_microstep: 32598.98 | bwd_allreduce_microstep: 119.12 | step_microstep: 7280.23
x3003c0s37b0n0: [2024-03-28 13:16:51,504] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 11963.99 | bwd: 32718.18 | bwd_inner: 32598.97 | bwd_allreduce: 119.14 | step: 7280.24
x3003c0s37b0n0: [2024-03-28 13:16:51,649] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 13:16:51,650] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 10.91 GB         CA 10.02 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:16:51,650] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.12 GB, percent = 66.8%
x3003c0s37b0n0: <TIMER:interval-time,52.70409893989563><TIMER:interval-time,52.704097270965576><TIMER:interval-time,52.70409893989563>
x3003c0s37b0n0: 
x3003c0s37b0n0: 
x3003c0s37b0n0: <TIMER:interval-time,52.70422053337097>
x3003c0s7b0n0: <TIMER:interval-time,52.70423150062561><TIMER:interval-time,52.70423150062561>
x3003c0s7b0n0: 
x3003c0s7b0n0: <TIMER:interval-time,52.704235792160034><TIMER:interval-time,52.70423102378845>
x3003c0s7b0n0: 
x3003c0s37b1n0: <TIMER:interval-time,52.70426869392395><TIMER:interval-time,52.704270124435425><TIMER:interval-time,52.70427131652832>
x3003c0s37b1n0: 
x3003c0s37b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,52.70427370071411>
x3003c0s7b1n0: <TIMER:interval-time,52.70420694351196>
x3003c0s7b1n0: <TIMER:interval-time,52.70420861244202><TIMER:interval-time,52.70421004295349><TIMER:interval-time,52.704211711883545>
x3003c0s7b1n0: 
x3003c0s7b1n0: 
x3003c0s7b1n0:  elapsed_time 52.704212 | consumed samples:          256 | consumed tokens:       524288 | elapsed time per iteration (ms): 52704.2 | learning rate: 2.325E-04 | global batch size:    64 | lm loss: 2.462902E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.214 | TFLOPs: 83.83 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 13:16:51,796] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 13:16:51,796] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 9.37 GB         CA 10.02 GB         Max_CA 10 GB 
x3003c0s37b0n0: [2024-03-28 13:16:51,796] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.13 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:17:03,928] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 13:17:03,929] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 27.7 GB         CA 31.61 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:17:03,929] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.09 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:17:04,015] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 13:17:04,016] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 24.58 GB         CA 25.29 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:17:04,016] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.09 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:17:36,945] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 13:17:36,945] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 28.66 GB         CA 11.56 GB         Max_CA 37 GB 
x3003c0s37b0n0: [2024-03-28 13:17:36,946] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.08 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:17:37,024] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 13:17:37,025] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 10.91 GB         CA 11.56 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:17:37,025] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.08 GB, percent = 66.8%
x3003c0s37b1n0: [2024-03-28 13:17:43,831] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.775272607803345
x3003c0s37b1n0: [2024-03-28 13:17:43,831] [INFO] [stage3.py:2251:step] Full outer step loop took 6.776074171066284
x3003c0s7b0n0: [2024-03-28 13:17:44,018] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.9627461433410645
x3003c0s7b0n0: [2024-03-28 13:17:44,020] [INFO] [stage3.py:2251:step] Full outer step loop took 6.964916944503784
x3003c0s37b0n0: [2024-03-28 13:17:44,023] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.967750549316406
x3003c0s37b0n0: [2024-03-28 13:17:44,024] [INFO] [stage3.py:2251:step] Full outer step loop took 6.968040227890015
x3003c0s7b1n0: [2024-03-28 13:17:44,082] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.026467323303223
x3003c0s7b1n0: [2024-03-28 13:17:44,082] [INFO] [stage3.py:2251:step] Full outer step loop took 7.026657342910767
x3003c0s37b0n0: [2024-03-28 13:17:44,088] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.0323591232299805
x3003c0s37b0n0: [2024-03-28 13:17:44,088] [INFO] [stage3.py:2251:step] Full outer step loop took 7.032664060592651
x3003c0s7b1n0: [2024-03-28 13:17:44,106] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.050395250320435
x3003c0s7b1n0: [2024-03-28 13:17:44,106] [INFO] [stage3.py:2251:step] Full outer step loop took 7.050638437271118
x3003c0s7b1n0: [2024-03-28 13:17:44,116] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.06094765663147
x3003c0s7b1n0: [2024-03-28 13:17:44,117] [INFO] [stage3.py:2251:step] Full outer step loop took 7.061432600021362
x3003c0s7b0n0: [2024-03-28 13:17:44,171] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.11341667175293
x3003c0s7b0n0: [2024-03-28 13:17:44,175] [INFO] [stage3.py:2251:step] Full outer step loop took 7.117891311645508
x3003c0s7b0n0: [2024-03-28 13:17:44,181] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.126095771789551
x3003c0s7b0n0: [2024-03-28 13:17:44,182] [INFO] [stage3.py:2251:step] Full outer step loop took 7.126305103302002
x3003c0s37b0n0: [2024-03-28 13:17:44,195] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.1400933265686035
x3003c0s37b0n0: [2024-03-28 13:17:44,196] [INFO] [stage3.py:2251:step] Full outer step loop took 7.14026141166687
x3003c0s37b0n0: [2024-03-28 13:17:44,223] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.167250156402588
x3003c0s37b0n0: [2024-03-28 13:17:44,223] [INFO] [stage3.py:2251:step] Full outer step loop took 7.167522192001343
x3003c0s37b1n0: [2024-03-28 13:17:44,228] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.172226190567017
x3003c0s37b1n0: [2024-03-28 13:17:44,228] [INFO] [stage3.py:2251:step] Full outer step loop took 7.17255973815918
x3003c0s7b0n0: [2024-03-28 13:17:44,263] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.207724571228027
x3003c0s7b0n0: [2024-03-28 13:17:44,263] [INFO] [stage3.py:2251:step] Full outer step loop took 7.207892179489136
x3003c0s7b1n0: [2024-03-28 13:17:44,273] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.2177653312683105
x3003c0s7b1n0: [2024-03-28 13:17:44,273] [INFO] [stage3.py:2251:step] Full outer step loop took 7.21791672706604
x3003c0s37b1n0: [2024-03-28 13:17:44,316] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.259778022766113
x3003c0s37b1n0: [2024-03-28 13:17:44,318] [INFO] [stage3.py:2251:step] Full outer step loop took 7.26263427734375
x3003c0s37b1n0: [2024-03-28 13:17:44,337] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.282176971435547
x3003c0s37b1n0: [2024-03-28 13:17:44,338] [INFO] [stage3.py:2251:step] Full outer step loop took 7.282329082489014
x3003c0s7b1n0: [2024-03-28 13:17:44,362] [INFO] [stage3.py:2277:step] End to end step took 7.306459188461304
x3003c0s37b1n0: [2024-03-28 13:17:44,362] [INFO] [stage3.py:2277:step] End to end step took 7.30646276473999
x3003c0s37b0n0: [2024-03-28 13:17:44,362] [INFO] [stage3.py:2277:step] End to end step took 7.306511163711548
x3003c0s37b0n0: [2024-03-28 13:17:44,362] [INFO] [stage3.py:2277:step] End to end step took 7.306528329849243
x3003c0s7b0n0: [2024-03-28 13:17:44,362] [INFO] [stage3.py:2277:step] End to end step took 7.3065385818481445
x3003c0s37b1n0: [2024-03-28 13:17:44,362] [INFO] [stage3.py:2277:step] End to end step took 7.306497812271118
x3003c0s7b0n0: [2024-03-28 13:17:44,362] [INFO] [stage3.py:2277:step] End to end step took 7.306927919387817
x3003c0s7b0n0: [2024-03-28 13:17:44,362] [INFO] [stage3.py:2277:step] End to end step took 7.305718421936035
x3003c0s7b1n0: [2024-03-28 13:17:44,362] [INFO] [stage3.py:2277:step] End to end step took 7.306987524032593
x3003c0s37b0n0: [2024-03-28 13:17:44,362] [INFO] [stage3.py:2277:step] End to end step took 7.30699348449707
x3003c0s37b1n0: [2024-03-28 13:17:44,362] [INFO] [stage3.py:2277:step] End to end step took 7.307031631469727
x3003c0s37b1n0: [2024-03-28 13:17:44,362] [INFO] [stage3.py:2277:step] End to end step took 7.307117700576782
x3003c0s7b0n0: [2024-03-28 13:17:44,362] [INFO] [stage3.py:2277:step] End to end step took 7.307140350341797
x3003c0s7b1n0: [2024-03-28 13:17:44,362] [INFO] [stage3.py:2277:step] End to end step took 7.307145357131958
x3003c0s37b0n0: [2024-03-28 13:17:44,362] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6968.26
x3003c0s7b1n0: [2024-03-28 13:17:44,363] [INFO] [stage3.py:2277:step] End to end step took 7.307257413864136
x3003c0s37b0n0: [2024-03-28 13:17:44,363] [INFO] [stage3.py:2277:step] End to end step took 7.307195663452148
x3003c0s37b0n0: [2024-03-28 13:17:44,363] [INFO] [logging.py:96:log_dist] [Rank 0] step=5, skipped=0, lr=[0.0001884425039850356], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 13:17:44,363] [INFO] [timer.py:260:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=1.2075048026725517, CurrSamplesPerSec=1.2175303793349865, MemAllocated=9.37GB, MaxMemAllocated=10.91GB
x3003c0s37b0n0: [2024-03-28 13:17:44,364] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 12103.60 | bwd_microstep: 32710.24 | bwd_inner_microstep: 32591.18 | bwd_allreduce_microstep: 118.98 | step_microstep: 7338.06
x3003c0s37b0n0: [2024-03-28 13:17:44,364] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 12103.59 | bwd: 32710.25 | bwd_inner: 32591.17 | bwd_allreduce: 118.99 | step: 7338.05
x3003c0s37b0n0: [2024-03-28 13:17:44,512] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 13:17:44,513] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 10.91 GB         CA 10.02 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:17:44,513] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.12 GB, percent = 66.8%
x3003c0s37b0n0: <TIMER:interval-time,52.86288666725159><TIMER:interval-time,52.86288666725159><TIMER:interval-time,52.86289143562317><TIMER:interval-time,52.862844944000244>
x3003c0s37b0n0: 
x3003c0s37b0n0: 
x3003c0s37b0n0: 
x3003c0s37b1n0: <TIMER:interval-time,52.86296343803406>
x3003c0s37b1n0: <TIMER:interval-time,52.86296534538269><TIMER:interval-time,52.86296534538269>
x3003c0s37b1n0: <TIMER:interval-time,52.862969160079956>
x3003c0s37b1n0: 
x3003c0s7b0n0: <TIMER:interval-time,52.862974643707275><TIMER:interval-time,52.86297559738159>
x3003c0s7b0n0: <TIMER:interval-time,52.86297798156738>
x3003c0s7b0n0: 
x3003c0s7b1n0: <TIMER:interval-time,52.86264610290527>
x3003c0s7b1n0: <TIMER:interval-time,52.86265158653259><TIMER:interval-time,52.862650871276855>
x3003c0s7b1n0: 
x3003c0s7b1n0: <TIMER:interval-time,52.862728118896484>
x3003c0s7b0n0: <TIMER:interval-time,52.86309266090393>
x3003c0s7b1n0:  elapsed_time 52.862651 | consumed samples:          320 | consumed tokens:       655360 | elapsed time per iteration (ms): 52862.7 | learning rate: 1.884E-04 | global batch size:    64 | lm loss: 1.722123E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.211 | TFLOPs: 83.58 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 13:17:44,642] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 13:17:44,643] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 9.37 GB         CA 10.02 GB         Max_CA 10 GB 
x3003c0s37b0n0: [2024-03-28 13:17:44,643] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.13 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:17:57,099] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 13:17:57,100] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 27.7 GB         CA 31.61 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:17:57,100] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.1 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:17:57,192] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 13:17:57,193] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 24.58 GB         CA 25.29 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:17:57,193] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.1 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:18:29,696] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 13:18:29,697] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 28.66 GB         CA 11.56 GB         Max_CA 37 GB 
x3003c0s37b0n0: [2024-03-28 13:18:29,697] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.08 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:18:29,780] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 13:18:29,781] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 10.91 GB         CA 11.56 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:18:29,781] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.08 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:18:36,448] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.637234449386597
x3003c0s37b0n0: [2024-03-28 13:18:36,448] [INFO] [stage3.py:2251:step] Full outer step loop took 6.6374382972717285
x3003c0s7b0n0: [2024-03-28 13:18:36,576] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.765226602554321
x3003c0s7b0n0: [2024-03-28 13:18:36,577] [INFO] [stage3.py:2251:step] Full outer step loop took 6.765652894973755
x3003c0s37b1n0: [2024-03-28 13:18:36,646] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.835037708282471
x3003c0s37b1n0: [2024-03-28 13:18:36,646] [INFO] [stage3.py:2251:step] Full outer step loop took 6.835266828536987
x3003c0s7b0n0: [2024-03-28 13:18:36,663] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.852354288101196
x3003c0s7b0n0: [2024-03-28 13:18:36,664] [INFO] [stage3.py:2251:step] Full outer step loop took 6.85296893119812
x3003c0s37b1n0: [2024-03-28 13:18:36,731] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.920161962509155
x3003c0s37b1n0: [2024-03-28 13:18:36,731] [INFO] [stage3.py:2251:step] Full outer step loop took 6.920334100723267
x3003c0s7b1n0: [2024-03-28 13:18:36,756] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.944916725158691
x3003c0s7b1n0: [2024-03-28 13:18:36,757] [INFO] [stage3.py:2251:step] Full outer step loop took 6.946123361587524
x3003c0s37b1n0: [2024-03-28 13:18:36,837] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.026033163070679
x3003c0s37b1n0: [2024-03-28 13:18:36,837] [INFO] [stage3.py:2251:step] Full outer step loop took 7.0261921882629395
x3003c0s37b1n0: [2024-03-28 13:18:36,889] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.07807993888855
x3003c0s37b1n0: [2024-03-28 13:18:36,889] [INFO] [stage3.py:2251:step] Full outer step loop took 7.078243970870972
x3003c0s7b0n0: [2024-03-28 13:18:36,928] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.117619752883911
x3003c0s7b0n0: [2024-03-28 13:18:36,929] [INFO] [stage3.py:2251:step] Full outer step loop took 7.117834568023682
x3003c0s37b0n0: [2024-03-28 13:18:36,941] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.129713296890259
x3003c0s37b0n0: [2024-03-28 13:18:36,942] [INFO] [stage3.py:2251:step] Full outer step loop took 7.130955934524536
x3003c0s7b0n0: [2024-03-28 13:18:36,965] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.1540210247039795
x3003c0s7b0n0: [2024-03-28 13:18:36,965] [INFO] [stage3.py:2251:step] Full outer step loop took 7.154186725616455
x3003c0s37b0n0: [2024-03-28 13:18:37,028] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.217309474945068
x3003c0s37b0n0: [2024-03-28 13:18:37,028] [INFO] [stage3.py:2251:step] Full outer step loop took 7.217586278915405
x3003c0s7b1n0: [2024-03-28 13:18:37,032] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.220815896987915
x3003c0s7b1n0: [2024-03-28 13:18:37,032] [INFO] [stage3.py:2251:step] Full outer step loop took 7.2210612297058105
x3003c0s7b1n0: [2024-03-28 13:18:37,041] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.2299535274505615
x3003c0s7b1n0: [2024-03-28 13:18:37,041] [INFO] [stage3.py:2251:step] Full outer step loop took 7.230123519897461
x3003c0s7b1n0: [2024-03-28 13:18:37,044] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.233614921569824
x3003c0s7b1n0: [2024-03-28 13:18:37,045] [INFO] [stage3.py:2251:step] Full outer step loop took 7.233762741088867
x3003c0s37b0n0: [2024-03-28 13:18:37,085] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.274244070053101
x3003c0s37b0n0: [2024-03-28 13:18:37,085] [INFO] [stage3.py:2251:step] Full outer step loop took 7.274413347244263
x3003c0s7b1n0: [2024-03-28 13:18:37,110] [INFO] [stage3.py:2277:step] End to end step took 7.29935097694397
x3003c0s7b0n0: [2024-03-28 13:18:37,110] [INFO] [stage3.py:2277:step] End to end step took 7.299372911453247
x3003c0s37b0n0: [2024-03-28 13:18:37,110] [INFO] [stage3.py:2277:step] End to end step took 7.2993998527526855
x3003c0s37b0n0: [2024-03-28 13:18:37,110] [INFO] [stage3.py:2277:step] End to end step took 7.299407958984375
x3003c0s7b1n0: [2024-03-28 13:18:37,110] [INFO] [stage3.py:2277:step] End to end step took 7.299414873123169
x3003c0s7b1n0: [2024-03-28 13:18:37,110] [INFO] [stage3.py:2277:step] End to end step took 7.299490690231323
x3003c0s7b0n0: [2024-03-28 13:18:37,110] [INFO] [stage3.py:2277:step] End to end step took 7.299637794494629
x3003c0s37b1n0: [2024-03-28 13:18:37,111] [INFO] [stage3.py:2277:step] End to end step took 7.299789667129517
x3003c0s37b0n0: [2024-03-28 13:18:37,111] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6637.60
x3003c0s37b1n0: [2024-03-28 13:18:37,111] [INFO] [stage3.py:2277:step] End to end step took 7.299975156784058
x3003c0s37b0n0: [2024-03-28 13:18:37,111] [INFO] [stage3.py:2277:step] End to end step took 7.300034523010254
x3003c0s37b1n0: [2024-03-28 13:18:37,111] [INFO] [stage3.py:2277:step] End to end step took 7.300069093704224
x3003c0s37b0n0: [2024-03-28 13:18:37,111] [INFO] [stage3.py:2277:step] End to end step took 7.300090074539185
x3003c0s7b0n0: [2024-03-28 13:18:37,111] [INFO] [stage3.py:2277:step] End to end step took 7.3000733852386475
x3003c0s7b1n0: [2024-03-28 13:18:37,111] [INFO] [stage3.py:2277:step] End to end step took 7.3000757694244385
x3003c0s7b0n0: [2024-03-28 13:18:37,111] [INFO] [stage3.py:2277:step] End to end step took 7.299843072891235
x3003c0s37b1n0: [2024-03-28 13:18:37,111] [INFO] [stage3.py:2277:step] End to end step took 7.300195932388306
x3003c0s37b0n0: [2024-03-28 13:18:37,111] [INFO] [logging.py:96:log_dist] [Rank 0] step=6, skipped=0, lr=[0.0001415574960149644], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 13:18:37,112] [INFO] [timer.py:260:stop] epoch=0/micro_step=6/global_step=6, RunningAvgSamplesPerSec=1.210557586077729, CurrSamplesPerSec=1.2198092597540997, MemAllocated=9.37GB, MaxMemAllocated=10.91GB
x3003c0s37b0n0: [2024-03-28 13:18:37,112] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 12423.31 | bwd_microstep: 32284.12 | bwd_inner_microstep: 32166.10 | bwd_allreduce_microstep: 117.95 | step_microstep: 7330.88
x3003c0s37b0n0: [2024-03-28 13:18:37,112] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 12423.30 | bwd: 32284.11 | bwd_inner: 32166.10 | bwd_allreduce: 117.96 | step: 7330.88
x3003c0s37b0n0: [2024-03-28 13:18:37,256] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 13:18:37,257] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 10.91 GB         CA 10.02 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:18:37,257] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.12 GB, percent = 66.8%
x3003c0s37b0n0: <TIMER:interval-time,52.74323320388794><TIMER:interval-time,52.74323654174805>
x3003c0s37b0n0: 
x3003c0s37b0n0: <TIMER:interval-time,52.743269205093384>
x3003c0s7b1n0: <TIMER:interval-time,52.74326205253601><TIMER:interval-time,52.74326205253601><TIMER:interval-time,52.743263244628906>
x3003c0s7b1n0: 
x3003c0s7b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,52.74326968193054><TIMER:interval-time,52.743268966674805><TIMER:interval-time,52.74327087402344>
x3003c0s7b1n0: <TIMER:interval-time,52.743268966674805>
x3003c0s37b1n0: 
x3003c0s7b0n0: <TIMER:interval-time,52.743316888809204><TIMER:interval-time,52.743316888809204><TIMER:interval-time,52.74331998825073>
x3003c0s37b1n0: <TIMER:interval-time,52.74327373504639>
x3003c0s37b1n0: 
x3003c0s7b0n0: 
x3003c0s7b0n0: 
x3003c0s7b0n0: <TIMER:interval-time,52.74332118034363>
x3003c0s37b0n0: <TIMER:interval-time,52.743350982666016>
x3003c0s7b1n0:  elapsed_time 52.743269 | consumed samples:          384 | consumed tokens:       786432 | elapsed time per iteration (ms): 52743.3 | learning rate: 1.416E-04 | global batch size:    64 | lm loss: 1.545124E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.213 | TFLOPs: 83.77 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 13:18:37,396] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 13:18:37,397] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 9.37 GB         CA 10.02 GB         Max_CA 10 GB 
x3003c0s37b0n0: [2024-03-28 13:18:37,397] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.13 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:18:50,304] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 13:18:50,304] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 27.7 GB         CA 31.61 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:18:50,305] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.1 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:18:50,395] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 13:18:50,396] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 24.58 GB         CA 25.29 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:18:50,396] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.1 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:19:24,254] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 13:19:24,255] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 28.66 GB         CA 11.56 GB         Max_CA 37 GB 
x3003c0s37b0n0: [2024-03-28 13:19:24,255] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.08 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:19:24,335] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 13:19:24,336] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 10.91 GB         CA 11.56 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:19:24,336] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.08 GB, percent = 66.8%
x3003c0s7b1n0: [2024-03-28 13:19:31,168] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.802480459213257
x3003c0s7b1n0: [2024-03-28 13:19:31,171] [INFO] [stage3.py:2251:step] Full outer step loop took 6.804407835006714
x3003c0s37b0n0: [2024-03-28 13:19:31,226] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.859828472137451
x3003c0s37b0n0: [2024-03-28 13:19:31,227] [INFO] [stage3.py:2251:step] Full outer step loop took 6.860655307769775
x3003c0s37b1n0: [2024-03-28 13:19:31,338] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.971606492996216
x3003c0s37b1n0: [2024-03-28 13:19:31,338] [INFO] [stage3.py:2251:step] Full outer step loop took 6.97214150428772
x3003c0s7b0n0: [2024-03-28 13:19:31,362] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.9954352378845215
x3003c0s7b0n0: [2024-03-28 13:19:31,362] [INFO] [stage3.py:2251:step] Full outer step loop took 6.996253728866577
x3003c0s7b0n0: [2024-03-28 13:19:31,410] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.043577194213867
x3003c0s7b0n0: [2024-03-28 13:19:31,410] [INFO] [stage3.py:2251:step] Full outer step loop took 7.043771982192993
x3003c0s7b0n0: [2024-03-28 13:19:31,414] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.048166751861572
x3003c0s7b0n0: [2024-03-28 13:19:31,415] [INFO] [stage3.py:2251:step] Full outer step loop took 7.048638820648193
x3003c0s37b1n0: [2024-03-28 13:19:31,414] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.047911643981934
x3003c0s37b1n0: [2024-03-28 13:19:31,415] [INFO] [stage3.py:2251:step] Full outer step loop took 7.04877495765686
x3003c0s37b1n0: [2024-03-28 13:19:31,433] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.066893815994263
x3003c0s37b1n0: [2024-03-28 13:19:31,433] [INFO] [stage3.py:2251:step] Full outer step loop took 7.067053318023682
x3003c0s37b1n0: [2024-03-28 13:19:31,457] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.091449022293091
x3003c0s37b1n0: [2024-03-28 13:19:31,458] [INFO] [stage3.py:2251:step] Full outer step loop took 7.091611385345459
x3003c0s37b0n0: [2024-03-28 13:19:31,469] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.102717638015747
x3003c0s37b0n0: [2024-03-28 13:19:31,469] [INFO] [stage3.py:2251:step] Full outer step loop took 7.10291051864624
x3003c0s7b1n0: [2024-03-28 13:19:31,484] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.118425130844116
x3003c0s7b1n0: [2024-03-28 13:19:31,485] [INFO] [stage3.py:2251:step] Full outer step loop took 7.118930816650391
x3003c0s7b0n0: [2024-03-28 13:19:31,489] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.122884035110474
x3003c0s7b0n0: [2024-03-28 13:19:31,489] [INFO] [stage3.py:2251:step] Full outer step loop took 7.123033761978149
x3003c0s7b1n0: [2024-03-28 13:19:31,509] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.142752170562744
x3003c0s7b1n0: [2024-03-28 13:19:31,509] [INFO] [stage3.py:2251:step] Full outer step loop took 7.142908096313477
x3003c0s37b0n0: [2024-03-28 13:19:31,509] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.143348693847656
x3003c0s37b0n0: [2024-03-28 13:19:31,509] [INFO] [stage3.py:2251:step] Full outer step loop took 7.143511056900024
x3003c0s7b1n0: [2024-03-28 13:19:31,541] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.174884557723999
x3003c0s7b1n0: [2024-03-28 13:19:31,541] [INFO] [stage3.py:2251:step] Full outer step loop took 7.175037622451782
x3003c0s37b0n0: [2024-03-28 13:19:31,610] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.2444117069244385
x3003c0s37b0n0: [2024-03-28 13:19:31,610] [INFO] [stage3.py:2251:step] Full outer step loop took 7.244569778442383
x3003c0s7b0n0: [2024-03-28 13:19:31,635] [INFO] [stage3.py:2277:step] End to end step took 7.269092082977295
x3003c0s37b0n0: [2024-03-28 13:19:31,635] [INFO] [stage3.py:2277:step] End to end step took 7.269287109375
x3003c0s7b1n0: [2024-03-28 13:19:31,635] [INFO] [stage3.py:2277:step] End to end step took 7.269343137741089
x3003c0s37b1n0: [2024-03-28 13:19:31,635] [INFO] [stage3.py:2277:step] End to end step took 7.269342422485352
x3003c0s37b1n0: [2024-03-28 13:19:31,635] [INFO] [stage3.py:2277:step] End to end step took 7.269381999969482
x3003c0s7b0n0: [2024-03-28 13:19:31,635] [INFO] [stage3.py:2277:step] End to end step took 7.2694244384765625
x3003c0s7b1n0: [2024-03-28 13:19:31,635] [INFO] [stage3.py:2277:step] End to end step took 7.269536256790161
x3003c0s7b0n0: [2024-03-28 13:19:31,635] [INFO] [stage3.py:2277:step] End to end step took 7.269506454467773
x3003c0s7b0n0: [2024-03-28 13:19:31,636] [INFO] [stage3.py:2277:step] End to end step took 7.26985502243042
x3003c0s7b1n0: [2024-03-28 13:19:31,636] [INFO] [stage3.py:2277:step] End to end step took 7.2698750495910645
x3003c0s37b0n0: [2024-03-28 13:19:31,636] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6861.83
x3003c0s37b0n0: [2024-03-28 13:19:31,636] [INFO] [stage3.py:2277:step] End to end step took 7.269852876663208
x3003c0s37b1n0: [2024-03-28 13:19:31,636] [INFO] [stage3.py:2277:step] End to end step took 7.269913673400879
x3003c0s37b1n0: [2024-03-28 13:19:31,636] [INFO] [stage3.py:2277:step] End to end step took 7.2698609828948975
x3003c0s37b0n0: [2024-03-28 13:19:31,636] [INFO] [stage3.py:2277:step] End to end step took 7.269897699356079
x3003c0s37b0n0: [2024-03-28 13:19:31,636] [INFO] [stage3.py:2277:step] End to end step took 7.270120620727539
x3003c0s7b1n0: [2024-03-28 13:19:31,636] [INFO] [stage3.py:2277:step] End to end step took 7.2700355052948
x3003c0s37b0n0: [2024-03-28 13:19:31,636] [INFO] [logging.py:96:log_dist] [Rank 0] step=7, skipped=0, lr=[9.750000000000001e-05], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 13:19:31,637] [INFO] [timer.py:260:stop] epoch=0/micro_step=7/global_step=7, RunningAvgSamplesPerSec=1.2043157675898435, CurrSamplesPerSec=1.179979161612636, MemAllocated=9.37GB, MaxMemAllocated=10.91GB
x3003c0s37b0n0: [2024-03-28 13:19:31,637] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 12877.69 | bwd_microstep: 33643.15 | bwd_inner_microstep: 33511.66 | bwd_allreduce_microstep: 131.42 | step_microstep: 7300.66
x3003c0s37b0n0: [2024-03-28 13:19:31,637] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 12877.68 | bwd: 33643.14 | bwd_inner: 33511.66 | bwd_allreduce: 131.43 | step: 7300.67
x3003c0s37b0n0: [2024-03-28 13:19:31,798] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 13:19:31,798] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 10.91 GB         CA 10.02 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:19:31,799] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.11 GB, percent = 66.8%
x3003c0s37b0n0: <TIMER:interval-time,54.54145884513855><TIMER:interval-time,54.54145669937134><TIMER:interval-time,54.541428327560425>
x3003c0s37b0n0: 
x3003c0s37b0n0: <TIMER:interval-time,54.54146146774292>
x3003c0s37b0n0: 
x3003c0s7b0n0: <TIMER:interval-time,54.541531801223755><TIMER:interval-time,54.54153275489807><TIMER:interval-time,54.54153275489807><TIMER:interval-time,54.54153347015381>
x3003c0s7b0n0: 
x3003c0s7b0n0: 
x3003c0s7b0n0: 
x3003c0s7b1n0: <TIMER:interval-time,54.54152059555054>
x3003c0s7b1n0: <TIMER:interval-time,54.54152512550354><TIMER:interval-time,54.54152512550354><TIMER:interval-time,54.541526317596436>
x3003c0s7b1n0: 
x3003c0s7b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,54.541916608810425>
x3003c0s37b1n0: <TIMER:interval-time,54.54192042350769><TIMER:interval-time,54.541921615600586>
x3003c0s37b1n0: <TIMER:interval-time,54.541924715042114>
x3003c0s37b1n0: 
x3003c0s7b1n0:  elapsed_time 54.541526 | consumed samples:          448 | consumed tokens:       917504 | elapsed time per iteration (ms): 54541.5 | learning rate: 9.750E-05 | global batch size:    64 | lm loss: 1.501596E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.173 | TFLOPs: 81.01 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 13:19:31,941] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 13:19:31,941] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 9.37 GB         CA 10.02 GB         Max_CA 10 GB 
x3003c0s37b0n0: [2024-03-28 13:19:31,941] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.12 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:19:44,025] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 13:19:44,026] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 27.7 GB         CA 31.61 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:19:44,026] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.09 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:19:44,117] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 13:19:44,118] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 24.58 GB         CA 25.29 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:19:44,118] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.09 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:20:18,276] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 13:20:18,277] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 28.66 GB         CA 11.56 GB         Max_CA 37 GB 
x3003c0s37b0n0: [2024-03-28 13:20:18,277] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.08 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:20:18,358] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 13:20:18,359] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 10.91 GB         CA 11.56 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:20:18,359] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.08 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:20:25,230] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.8412909507751465
x3003c0s37b0n0: [2024-03-28 13:20:25,231] [INFO] [stage3.py:2251:step] Full outer step loop took 6.842270851135254
x3003c0s7b1n0: [2024-03-28 13:20:25,318] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.928624153137207
x3003c0s7b1n0: [2024-03-28 13:20:25,318] [INFO] [stage3.py:2251:step] Full outer step loop took 6.9293060302734375
x3003c0s7b1n0: [2024-03-28 13:20:25,411] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.021994590759277
x3003c0s7b1n0: [2024-03-28 13:20:25,411] [INFO] [stage3.py:2251:step] Full outer step loop took 7.022207498550415
x3003c0s7b0n0: [2024-03-28 13:20:25,465] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.07596492767334
x3003c0s7b0n0: [2024-03-28 13:20:25,465] [INFO] [stage3.py:2251:step] Full outer step loop took 7.076196193695068
x3003c0s7b1n0: [2024-03-28 13:20:25,469] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.079808235168457
x3003c0s7b1n0: [2024-03-28 13:20:25,469] [INFO] [stage3.py:2251:step] Full outer step loop took 7.080039024353027
x3003c0s37b1n0: [2024-03-28 13:20:25,469] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.079560279846191
x3003c0s37b1n0: [2024-03-28 13:20:25,470] [INFO] [stage3.py:2251:step] Full outer step loop took 7.080628871917725
x3003c0s7b0n0: [2024-03-28 13:20:25,487] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.097529888153076
x3003c0s7b0n0: [2024-03-28 13:20:25,487] [INFO] [stage3.py:2251:step] Full outer step loop took 7.097727060317993
x3003c0s7b0n0: [2024-03-28 13:20:25,492] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.102517366409302
x3003c0s7b0n0: [2024-03-28 13:20:25,492] [INFO] [stage3.py:2251:step] Full outer step loop took 7.102745294570923
x3003c0s7b0n0: [2024-03-28 13:20:25,526] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.136496305465698
x3003c0s7b0n0: [2024-03-28 13:20:25,526] [INFO] [stage3.py:2251:step] Full outer step loop took 7.136647701263428
x3003c0s37b1n0: [2024-03-28 13:20:25,526] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.137023448944092
x3003c0s37b1n0: [2024-03-28 13:20:25,526] [INFO] [stage3.py:2251:step] Full outer step loop took 7.13729453086853
x3003c0s37b1n0: [2024-03-28 13:20:25,532] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.143425941467285
x3003c0s37b1n0: [2024-03-28 13:20:25,533] [INFO] [stage3.py:2251:step] Full outer step loop took 7.143606185913086
x3003c0s7b1n0: [2024-03-28 13:20:25,550] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.1608498096466064
x3003c0s7b1n0: [2024-03-28 13:20:25,550] [INFO] [stage3.py:2251:step] Full outer step loop took 7.161027431488037
x3003c0s37b0n0: [2024-03-28 13:20:25,559] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.170320272445679
x3003c0s37b0n0: [2024-03-28 13:20:25,560] [INFO] [stage3.py:2251:step] Full outer step loop took 7.170666217803955
x3003c0s37b0n0: [2024-03-28 13:20:25,593] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.204419374465942
x3003c0s37b0n0: [2024-03-28 13:20:25,594] [INFO] [stage3.py:2251:step] Full outer step loop took 7.204787015914917
x3003c0s37b0n0: [2024-03-28 13:20:25,599] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.210195302963257
x3003c0s37b0n0: [2024-03-28 13:20:25,599] [INFO] [stage3.py:2251:step] Full outer step loop took 7.210371255874634
x3003c0s37b1n0: [2024-03-28 13:20:25,630] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.240953207015991
x3003c0s37b1n0: [2024-03-28 13:20:25,630] [INFO] [stage3.py:2251:step] Full outer step loop took 7.241126298904419
x3003c0s37b0n0: [2024-03-28 13:20:25,656] [INFO] [stage3.py:2277:step] End to end step took 7.267061471939087
x3003c0s37b1n0: [2024-03-28 13:20:25,656] [INFO] [stage3.py:2277:step] End to end step took 7.267038822174072
x3003c0s37b0n0: [2024-03-28 13:20:25,656] [INFO] [stage3.py:2277:step] End to end step took 7.267133712768555
x3003c0s7b0n0: [2024-03-28 13:20:25,656] [INFO] [stage3.py:2277:step] End to end step took 7.26689887046814
x3003c0s7b0n0: [2024-03-28 13:20:25,657] [INFO] [stage3.py:2277:step] End to end step took 7.267438173294067
x3003c0s37b0n0: [2024-03-28 13:20:25,657] [INFO] [stage3.py:2277:step] End to end step took 7.267544269561768
x3003c0s7b0n0: [2024-03-28 13:20:25,657] [INFO] [stage3.py:2277:step] End to end step took 7.2675700187683105
x3003c0s7b1n0: [2024-03-28 13:20:25,657] [INFO] [stage3.py:2277:step] End to end step took 7.267580032348633
x3003c0s7b1n0: [2024-03-28 13:20:25,657] [INFO] [stage3.py:2277:step] End to end step took 7.267588138580322
x3003c0s7b1n0: [2024-03-28 13:20:25,657] [INFO] [stage3.py:2277:step] End to end step took 7.267634391784668
x3003c0s37b0n0: [2024-03-28 13:20:25,657] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6842.94
x3003c0s37b1n0: [2024-03-28 13:20:25,657] [INFO] [stage3.py:2277:step] End to end step took 7.267713308334351
x3003c0s37b1n0: [2024-03-28 13:20:25,657] [INFO] [stage3.py:2277:step] End to end step took 7.267723083496094
x3003c0s37b1n0: [2024-03-28 13:20:25,657] [INFO] [stage3.py:2277:step] End to end step took 7.267732858657837
x3003c0s37b0n0: [2024-03-28 13:20:25,657] [INFO] [stage3.py:2277:step] End to end step took 7.267890453338623
x3003c0s7b0n0: [2024-03-28 13:20:25,657] [INFO] [stage3.py:2277:step] End to end step took 7.267826557159424
x3003c0s7b1n0: [2024-03-28 13:20:25,657] [INFO] [stage3.py:2277:step] End to end step took 7.267820596694946
x3003c0s37b0n0: [2024-03-28 13:20:25,657] [INFO] [logging.py:96:log_dist] [Rank 0] step=8, skipped=0, lr=[6.158400017893797e-05], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 13:20:25,658] [INFO] [timer.py:260:stop] epoch=0/micro_step=8/global_step=8, RunningAvgSamplesPerSec=1.2021583442638801, CurrSamplesPerSec=1.1914861432218995, MemAllocated=9.37GB, MaxMemAllocated=10.91GB
x3003c0s37b0n0: [2024-03-28 13:20:25,658] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 12051.75 | bwd_microstep: 33943.07 | bwd_inner_microstep: 33814.16 | bwd_allreduce_microstep: 128.85 | step_microstep: 7298.37
x3003c0s37b0n0: [2024-03-28 13:20:25,658] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 12051.73 | bwd: 33943.07 | bwd_inner: 33814.15 | bwd_allreduce: 128.87 | step: 7298.37
x3003c0s37b0n0: [2024-03-28 13:20:25,799] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 13:20:25,800] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 10.91 GB         CA 10.02 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:20:25,800] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.11 GB, percent = 66.8%
x3003c0s37b0n0: <TIMER:interval-time,54.00056338310242><TIMER:interval-time,54.00056481361389><TIMER:interval-time,54.00056481361389>
x3003c0s37b0n0: 
x3003c0s37b0n0: <TIMER:interval-time,54.000564098358154>
x3003c0s37b0n0: 
x3003c0s37b1n0: <TIMER:interval-time,54.00059914588928><TIMER:interval-time,54.00059366226196><TIMER:interval-time,54.00060033798218>
x3003c0s37b1n0: 
x3003c0s37b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,54.00060558319092>
x3003c0s7b1n0: <TIMER:interval-time,54.000579595565796><TIMER:interval-time,54.00058078765869><TIMER:interval-time,54.0005829334259><TIMER:interval-time,54.0005829334259>
x3003c0s7b1n0: 
x3003c0s7b1n0: 
x3003c0s7b1n0: 
x3003c0s7b0n0: <TIMER:interval-time,54.00061011314392><TIMER:interval-time,54.000612020492554>
x3003c0s7b0n0: <TIMER:interval-time,54.000612020492554>
x3003c0s7b0n0: 
x3003c0s7b0n0: <TIMER:interval-time,54.00070929527283>
x3003c0s7b1n0:  elapsed_time 54.000581 | consumed samples:          512 | consumed tokens:      1048576 | elapsed time per iteration (ms): 54000.6 | learning rate: 6.158E-05 | global batch size:    64 | lm loss: 1.323451E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.185 | TFLOPs: 81.82 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 13:20:25,932] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 13:20:25,932] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 9.37 GB         CA 10.02 GB         Max_CA 10 GB 
x3003c0s37b0n0: [2024-03-28 13:20:25,932] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.12 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:20:37,292] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 13:20:37,293] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 27.7 GB         CA 31.61 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:20:37,293] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.09 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:20:37,379] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 13:20:37,379] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 24.58 GB         CA 25.29 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:20:37,379] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.09 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:21:11,428] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 13:21:11,428] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 28.66 GB         CA 11.56 GB         Max_CA 37 GB 
x3003c0s37b0n0: [2024-03-28 13:21:11,428] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.08 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:21:11,506] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 13:21:11,507] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 10.91 GB         CA 11.56 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:21:11,507] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.08 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:21:18,353] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.816308259963989
x3003c0s37b0n0: [2024-03-28 13:21:18,353] [INFO] [stage3.py:2251:step] Full outer step loop took 6.816650152206421
x3003c0s7b1n0: [2024-03-28 13:21:18,402] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.865393400192261
x3003c0s7b1n0: [2024-03-28 13:21:18,402] [INFO] [stage3.py:2251:step] Full outer step loop took 6.8656837940216064
x3003c0s7b0n0: [2024-03-28 13:21:18,416] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.8791823387146
x3003c0s7b0n0: [2024-03-28 13:21:18,419] [INFO] [stage3.py:2251:step] Full outer step loop took 6.882119655609131
x3003c0s7b0n0: [2024-03-28 13:21:18,583] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.046278238296509
x3003c0s7b0n0: [2024-03-28 13:21:18,583] [INFO] [stage3.py:2251:step] Full outer step loop took 7.046530723571777
x3003c0s7b1n0: [2024-03-28 13:21:18,606] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.070019006729126
x3003c0s7b1n0: [2024-03-28 13:21:18,607] [INFO] [stage3.py:2251:step] Full outer step loop took 7.070467948913574
x3003c0s37b0n0: [2024-03-28 13:21:18,635] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.098958253860474
x3003c0s37b0n0: [2024-03-28 13:21:18,636] [INFO] [stage3.py:2251:step] Full outer step loop took 7.099260568618774
x3003c0s37b0n0: [2024-03-28 13:21:18,654] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.118124723434448
x3003c0s37b0n0: [2024-03-28 13:21:18,655] [INFO] [stage3.py:2251:step] Full outer step loop took 7.118305921554565
x3003c0s7b1n0: [2024-03-28 13:21:18,701] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.16414213180542
x3003c0s7b1n0: [2024-03-28 13:21:18,701] [INFO] [stage3.py:2251:step] Full outer step loop took 7.164328336715698
x3003c0s7b1n0: [2024-03-28 13:21:18,733] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.196179151535034
x3003c0s7b1n0: [2024-03-28 13:21:18,733] [INFO] [stage3.py:2251:step] Full outer step loop took 7.1963419914245605
x3003c0s7b0n0: [2024-03-28 13:21:18,741] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.204832315444946
x3003c0s7b0n0: [2024-03-28 13:21:18,741] [INFO] [stage3.py:2251:step] Full outer step loop took 7.205007314682007
x3003c0s37b1n0: [2024-03-28 13:21:18,751] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.215019464492798
x3003c0s37b1n0: [2024-03-28 13:21:18,752] [INFO] [stage3.py:2251:step] Full outer step loop took 7.215295314788818
x3003c0s37b0n0: [2024-03-28 13:21:18,764] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.227777719497681
x3003c0s37b0n0: [2024-03-28 13:21:18,764] [INFO] [stage3.py:2251:step] Full outer step loop took 7.2279274463653564
x3003c0s7b0n0: [2024-03-28 13:21:18,778] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.241212368011475
x3003c0s7b0n0: [2024-03-28 13:21:18,778] [INFO] [stage3.py:2251:step] Full outer step loop took 7.2413599491119385
x3003c0s37b1n0: [2024-03-28 13:21:18,783] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.246468544006348
x3003c0s37b1n0: [2024-03-28 13:21:18,783] [INFO] [stage3.py:2251:step] Full outer step loop took 7.246671199798584
x3003c0s37b1n0: [2024-03-28 13:21:18,802] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.265533447265625
x3003c0s37b1n0: [2024-03-28 13:21:18,802] [INFO] [stage3.py:2251:step] Full outer step loop took 7.265730381011963
x3003c0s37b1n0: [2024-03-28 13:21:18,811] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.2747275829315186
x3003c0s37b1n0: [2024-03-28 13:21:18,811] [INFO] [stage3.py:2251:step] Full outer step loop took 7.27488899230957
x3003c0s37b1n0: [2024-03-28 13:21:18,836] [INFO] [stage3.py:2277:step] End to end step took 7.29975438117981
x3003c0s7b0n0: [2024-03-28 13:21:18,836] [INFO] [stage3.py:2277:step] End to end step took 7.299810409545898
x3003c0s7b1n0: [2024-03-28 13:21:18,836] [INFO] [stage3.py:2277:step] End to end step took 7.299792051315308
x3003c0s7b0n0: [2024-03-28 13:21:18,836] [INFO] [stage3.py:2277:step] End to end step took 7.299854040145874
x3003c0s37b1n0: [2024-03-28 13:21:18,836] [INFO] [stage3.py:2277:step] End to end step took 7.2998106479644775
x3003c0s37b1n0: [2024-03-28 13:21:18,836] [INFO] [stage3.py:2277:step] End to end step took 7.299937009811401
x3003c0s37b0n0: [2024-03-28 13:21:18,836] [INFO] [stage3.py:2277:step] End to end step took 7.300069332122803
x3003c0s7b1n0: [2024-03-28 13:21:18,836] [INFO] [stage3.py:2277:step] End to end step took 7.3001415729522705
x3003c0s7b0n0: [2024-03-28 13:21:18,837] [INFO] [stage3.py:2277:step] End to end step took 7.30026912689209
x3003c0s37b1n0: [2024-03-28 13:21:18,837] [INFO] [stage3.py:2277:step] End to end step took 7.300346374511719
x3003c0s7b0n0: [2024-03-28 13:21:18,837] [INFO] [stage3.py:2277:step] End to end step took 7.30039381980896
x3003c0s37b0n0: [2024-03-28 13:21:18,837] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 6816.88
x3003c0s37b0n0: [2024-03-28 13:21:18,837] [INFO] [stage3.py:2277:step] End to end step took 7.3004608154296875
x3003c0s37b0n0: [2024-03-28 13:21:18,837] [INFO] [stage3.py:2277:step] End to end step took 7.30050802230835
x3003c0s7b1n0: [2024-03-28 13:21:18,837] [INFO] [stage3.py:2277:step] End to end step took 7.300500392913818
x3003c0s7b1n0: [2024-03-28 13:21:18,837] [INFO] [stage3.py:2277:step] End to end step took 7.300518989562988
x3003c0s37b0n0: [2024-03-28 13:21:18,837] [INFO] [stage3.py:2277:step] End to end step took 7.300693988800049
x3003c0s37b0n0: [2024-03-28 13:21:18,837] [INFO] [logging.py:96:log_dist] [Rank 0] step=9, skipped=0, lr=[3.814149619390238e-05], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 13:21:18,838] [INFO] [timer.py:260:stop] epoch=0/micro_step=9/global_step=9, RunningAvgSamplesPerSec=1.2032361586462494, CurrSamplesPerSec=1.209743850384943, MemAllocated=9.37GB, MaxMemAllocated=10.91GB
x3003c0s37b0n0: [2024-03-28 13:21:18,838] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 11332.05 | bwd_microstep: 33837.02 | bwd_inner_microstep: 33703.47 | bwd_allreduce_microstep: 133.48 | step_microstep: 7330.90
x3003c0s37b0n0: [2024-03-28 13:21:18,838] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 11332.04 | bwd: 33837.01 | bwd_inner: 33703.46 | bwd_allreduce: 133.49 | step: 7330.90
x3003c0s37b0n0: [2024-03-28 13:21:18,987] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 13:21:18,988] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 10.91 GB         CA 10.02 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:21:18,988] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.11 GB, percent = 66.8%
x3003c0s37b0n0: <TIMER:interval-time,53.18725037574768><TIMER:interval-time,53.18724870681763>
x3003c0s37b0n0: 
x3003c0s37b0n0: <TIMER:interval-time,53.18726301193237><TIMER:interval-time,53.18725824356079>
x3003c0s37b0n0: 
x3003c0s7b0n0: <TIMER:interval-time,53.1872878074646><TIMER:interval-time,53.18729019165039>
x3003c0s7b0n0: <TIMER:interval-time,53.18729019165039>
x3003c0s7b0n0: 
x3003c0s37b1n0: <TIMER:interval-time,53.18729567527771><TIMER:interval-time,53.18729639053345>
x3003c0s37b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,53.18730044364929>
x3003c0s7b1n0: <TIMER:interval-time,53.187312841415405><TIMER:interval-time,53.1873140335083><TIMER:interval-time,53.1873140335083>
x3003c0s7b1n0: <TIMER:interval-time,53.187312841415405>
x3003c0s7b1n0: 
x3003c0s7b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,53.187376737594604>
x3003c0s7b0n0: <TIMER:interval-time,53.187397480010986>
x3003c0s7b1n0:  elapsed_time 53.187313 | consumed samples:          576 | consumed tokens:      1179648 | elapsed time per iteration (ms): 53187.3 | learning rate: 3.814E-05 | global batch size:    64 | lm loss: 1.256594E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.203 | TFLOPs: 83.07 |
x3003c0s37b0n0: In train_step in training.py!!!!!... True, False
x3003c0s37b0n0: [2024-03-28 13:21:19,138] [INFO] [utils.py:800:see_memory_usage] Engine before forward
x3003c0s37b0n0: [2024-03-28 13:21:19,138] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 9.37 GB         CA 10.02 GB         Max_CA 10 GB 
x3003c0s37b0n0: [2024-03-28 13:21:19,138] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.12 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:21:31,555] [INFO] [utils.py:800:see_memory_usage] Engine after forward
x3003c0s37b0n0: [2024-03-28 13:21:31,556] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 27.7 GB         CA 31.61 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:21:31,556] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.1 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:21:31,639] [INFO] [utils.py:800:see_memory_usage] Engine before backward
x3003c0s37b0n0: [2024-03-28 13:21:31,640] [INFO] [utils.py:801:see_memory_usage] MA 24.58 GB         Max_MA 24.58 GB         CA 25.29 GB         Max_CA 32 GB 
x3003c0s37b0n0: [2024-03-28 13:21:31,640] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.09 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:22:05,376] [INFO] [utils.py:800:see_memory_usage] Engine after backward
x3003c0s37b0n0: [2024-03-28 13:22:05,376] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 28.66 GB         CA 11.56 GB         Max_CA 37 GB 
x3003c0s37b0n0: [2024-03-28 13:22:05,377] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.09 GB, percent = 66.8%
x3003c0s37b0n0: [2024-03-28 13:22:05,450] [INFO] [utils.py:800:see_memory_usage] Engine before step
x3003c0s37b0n0: [2024-03-28 13:22:05,450] [INFO] [utils.py:801:see_memory_usage] MA 10.91 GB         Max_MA 10.91 GB         CA 11.56 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:22:05,450] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.09 GB, percent = 66.8%
x3003c0s7b0n0: [2024-03-28 13:22:12,266] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.785484790802002
x3003c0s7b0n0: [2024-03-28 13:22:12,266] [INFO] [stage3.py:2251:step] Full outer step loop took 6.785665035247803
x3003c0s37b1n0: [2024-03-28 13:22:12,290] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.809993505477905
x3003c0s37b1n0: [2024-03-28 13:22:12,291] [INFO] [stage3.py:2251:step] Full outer step loop took 6.810859441757202
x3003c0s37b1n0: [2024-03-28 13:22:12,425] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.945533037185669
x3003c0s37b1n0: [2024-03-28 13:22:12,426] [INFO] [stage3.py:2251:step] Full outer step loop took 6.946000814437866
x3003c0s7b1n0: [2024-03-28 13:22:12,457] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.976985454559326
x3003c0s7b1n0: [2024-03-28 13:22:12,457] [INFO] [stage3.py:2251:step] Full outer step loop took 6.97728157043457
x3003c0s37b1n0: [2024-03-28 13:22:12,460] [INFO] [stage3.py:2243:step] With missing steps outer loop took 6.980072021484375
x3003c0s37b1n0: [2024-03-28 13:22:12,460] [INFO] [stage3.py:2251:step] Full outer step loop took 6.9802422523498535
x3003c0s37b0n0: [2024-03-28 13:22:12,514] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.020162343978882
x3003c0s37b0n0: [2024-03-28 13:22:12,515] [INFO] [stage3.py:2251:step] Full outer step loop took 7.020493030548096
x3003c0s37b1n0: [2024-03-28 13:22:12,553] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.073297023773193
x3003c0s37b1n0: [2024-03-28 13:22:12,553] [INFO] [stage3.py:2251:step] Full outer step loop took 7.073445081710815
x3003c0s7b1n0: [2024-03-28 13:22:12,596] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.115903377532959
x3003c0s7b1n0: [2024-03-28 13:22:12,596] [INFO] [stage3.py:2251:step] Full outer step loop took 7.116267681121826
x3003c0s7b0n0: [2024-03-28 13:22:12,605] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.125357627868652
x3003c0s7b0n0: [2024-03-28 13:22:12,605] [INFO] [stage3.py:2251:step] Full outer step loop took 7.125545263290405
x3003c0s37b0n0: [2024-03-28 13:22:12,623] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.143124341964722
x3003c0s37b0n0: [2024-03-28 13:22:12,629] [INFO] [stage3.py:2251:step] Full outer step loop took 7.148910045623779
x3003c0s7b1n0: [2024-03-28 13:22:12,655] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.175502061843872
x3003c0s7b1n0: [2024-03-28 13:22:12,655] [INFO] [stage3.py:2251:step] Full outer step loop took 7.175710201263428
x3003c0s37b0n0: [2024-03-28 13:22:12,697] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.217719316482544
x3003c0s37b0n0: [2024-03-28 13:22:12,698] [INFO] [stage3.py:2251:step] Full outer step loop took 7.217901706695557
x3003c0s7b0n0: [2024-03-28 13:22:12,706] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.225708246231079
x3003c0s7b0n0: [2024-03-28 13:22:12,707] [INFO] [stage3.py:2251:step] Full outer step loop took 7.227216720581055
x3003c0s7b1n0: [2024-03-28 13:22:12,712] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.232385635375977
x3003c0s7b1n0: [2024-03-28 13:22:12,712] [INFO] [stage3.py:2251:step] Full outer step loop took 7.232534885406494
x3003c0s7b0n0: [2024-03-28 13:22:12,726] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.246540069580078
x3003c0s7b0n0: [2024-03-28 13:22:12,726] [INFO] [stage3.py:2251:step] Full outer step loop took 7.246688604354858
x3003c0s37b0n0: [2024-03-28 13:22:12,729] [INFO] [stage3.py:2243:step] With missing steps outer loop took 7.24889349937439
x3003c0s37b0n0: [2024-03-28 13:22:12,729] [INFO] [stage3.py:2251:step] Full outer step loop took 7.249056577682495
x3003c0s37b1n0: [2024-03-28 13:22:12,754] [INFO] [stage3.py:2277:step] End to end step took 7.274674892425537
x3003c0s37b0n0: [2024-03-28 13:22:12,754] [INFO] [stage3.py:2277:step] End to end step took 7.274748086929321
x3003c0s7b0n0: [2024-03-28 13:22:12,755] [INFO] [stage3.py:2277:step] End to end step took 7.274746894836426
x3003c0s7b1n0: [2024-03-28 13:22:12,754] [INFO] [stage3.py:2277:step] End to end step took 7.27473258972168
x3003c0s7b0n0: [2024-03-28 13:22:12,755] [INFO] [stage3.py:2277:step] End to end step took 7.274850368499756
x3003c0s37b0n0: [2024-03-28 13:22:12,755] [INFO] [stage3.py:2277:step] End to end step took 7.275181770324707
x3003c0s7b0n0: [2024-03-28 13:22:12,755] [INFO] [stage3.py:2277:step] End to end step took 7.275068283081055
x3003c0s37b0n0: [2024-03-28 13:22:12,755] [INFO] [stage3.py:2277:step] End to end step took 7.2753005027771
x3003c0s37b1n0: [2024-03-28 13:22:12,755] [INFO] [stage3.py:2277:step] End to end step took 7.27527117729187
x3003c0s7b1n0: [2024-03-28 13:22:12,755] [INFO] [stage3.py:2277:step] End to end step took 7.275381565093994
x3003c0s7b1n0: [2024-03-28 13:22:12,755] [INFO] [stage3.py:2277:step] End to end step took 7.275406122207642
x3003c0s37b1n0: [2024-03-28 13:22:12,755] [INFO] [stage3.py:2277:step] End to end step took 7.275402784347534
x3003c0s7b0n0: [2024-03-28 13:22:12,755] [INFO] [stage3.py:2277:step] End to end step took 7.275413990020752
x3003c0s37b0n0: [2024-03-28 13:22:12,755] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_step: 7020.77
x3003c0s7b1n0: [2024-03-28 13:22:12,755] [INFO] [stage3.py:2277:step] End to end step took 7.275441884994507
x3003c0s37b0n0: [2024-03-28 13:22:12,755] [INFO] [stage3.py:2277:step] End to end step took 7.261308908462524
x3003c0s37b1n0: [2024-03-28 13:22:12,755] [INFO] [stage3.py:2277:step] End to end step took 7.275589227676392
x3003c0s37b0n0: [2024-03-28 13:22:12,756] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[3e-05], mom=[(0.9, 0.95)]
x3003c0s37b0n0: [2024-03-28 13:22:12,756] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=1.2020322370551852, CurrSamplesPerSec=1.1936717747440089, MemAllocated=9.37GB, MaxMemAllocated=10.91GB
x3003c0s37b0n0: [2024-03-28 13:22:12,756] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd_microstep: 12391.52 | bwd_microstep: 33533.24 | bwd_inner_microstep: 33406.93 | bwd_allreduce_microstep: 126.25 | step_microstep: 7305.68
x3003c0s37b0n0: [2024-03-28 13:22:12,756] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 12391.51 | bwd: 33533.23 | bwd_inner: 33406.92 | bwd_allreduce: 126.26 | step: 7305.68
x3003c0s37b0n0: [2024-03-28 13:22:12,915] [INFO] [utils.py:800:see_memory_usage] Engine after step
x3003c0s37b0n0: [2024-03-28 13:22:12,916] [INFO] [utils.py:801:see_memory_usage] MA 9.37 GB         Max_MA 10.91 GB         CA 10.02 GB         Max_CA 12 GB 
x3003c0s37b0n0: [2024-03-28 13:22:12,916] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 336.11 GB, percent = 66.8%
x3003c0s37b0n0: <TIMER:interval-time,53.92765235900879><TIMER:interval-time,53.92765998840332><TIMER:interval-time,53.92765760421753>
x3003c0s37b0n0: 
x3003c0s37b0n0: 
x3003c0s7b1n0: <TIMER:interval-time,53.92764329910278><TIMER:interval-time,53.92764329910278><TIMER:interval-time,53.92764592170715>
x3003c0s7b1n0: 
x3003c0s7b1n0: 
x3003c0s7b0n0: <TIMER:interval-time,53.92766761779785><TIMER:interval-time,53.92766571044922><TIMER:interval-time,53.92765712738037><TIMER:interval-time,53.92767024040222>
x3003c0s7b0n0: 
x3003c0s7b0n0: 
x3003c0s7b0n0: 
x3003c0s37b1n0: <TIMER:interval-time,53.927695751190186><TIMER:interval-time,53.92770051956177><TIMER:interval-time,53.92770195007324>
x3003c0s37b1n0: 
x3003c0s37b1n0: 
x3003c0s37b1n0: <TIMER:interval-time,53.9277069568634>
x3003c0s37b0n0: <TIMER:interval-time,53.927764654159546>
x3003c0s7b1n0: <TIMER:interval-time,53.927756786346436>
x3003c0s7b1n0:  elapsed_time 53.927757 | consumed samples:          640 | consumed tokens:      1310720 | elapsed time per iteration (ms): 53927.8 | learning rate: 3.000E-05 | global batch size:    64 | lm loss: 1.172162E+01 | loss scale: 1.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.187 | TFLOPs: 81.93 |
x3003c0s37b1n0: <<<only_train:562.4718189239502>>>
x3003c0s37b1n0: <<<only_train:562.4720177650452>>>
x3003c0s37b1n0: <<<only_train:562.4720118045807>>>
x3003c0s37b1n0: <<<only_train:562.4720242023468>>>
x3003c0s7b1n0: <<<only_train:562.4718298912048>>>
x3003c0s7b1n0: <<<only_train:562.4719936847687>>>
x3003c0s7b1n0: <<<only_train:562.471976518631>>>
x3003c0s37b0n0: <<<only_train:562.4719247817993>>>
x3003c0s37b0n0: <<<only_train:562.4719636440277>>>
x3003c0s37b0n0: <<<only_train:562.4720106124878>>>
x3003c0s7b1n0: <<<only_train:562.472047328949>>>
x3003c0s7b0n0: <<<only_train:562.4719953536987>>>
x3003c0s7b0n0: <<<only_train:562.472003698349>>>
x3003c0s7b0n0: <<<only_train:562.471994638443>>>
x3003c0s7b0n0: <<<only_train:562.4721953868866>>>
x3003c0s37b0n0: <<<only_train:562.4720623493195>>>
x3003c0s37b0n0: [after training ends] datetime: 2024-03-28 13:22:12 
x3003c0s37b0n0: <<<full_time:562.4722325801849>>>
x3003c0s37b0n0: <<<full_time:562.4722757339478>>>
x3003c0s37b0n0: <<<full_time:562.4722771644592>>>
x3003c0s37b0n0: <<<full_time:562.4722743034363>>>
x3003c0s7b1n0: <<<full_time:562.4723670482635>>><<<full_time:562.47234582901>>>
x3003c0s7b1n0: 
x3003c0s7b1n0: <<<full_time:562.4722235202789>>>
x3003c0s7b1n0: <<<full_time:562.4723696708679>>>
x3003c0s7b0n0: <<<full_time:562.4723384380341>>><<<full_time:562.4724621772766>>>
x3003c0s7b0n0: 
x3003c0s7b0n0: <<<full_time:562.4723269939423>>><<<full_time:562.4722971916199>>>
x3003c0s7b0n0: 
x3003c0s37b1n0: <<<full_time:562.4724366664886>>><<<full_time:562.4724228382111>>>
x3003c0s37b1n0: 
x3003c0s37b1n0: <<<full_time:562.4724328517914>>><<<full_time:562.4722509384155>>>
x3003c0s37b1n0: 
x3003c0s7b1n0: [2024-03-28 13:22:16,612] [INFO] [launch.py:348:main] Process 58257 exits successfully.
x3003c0s7b0n0: [2024-03-28 13:22:16,614] [INFO] [launch.py:348:main] Process 50485 exits successfully.
x3003c0s37b1n0: [2024-03-28 13:22:16,722] [INFO] [launch.py:348:main] Process 38816 exits successfully.
x3003c0s37b0n0: [2024-03-28 13:22:17,079] [INFO] [launch.py:348:main] Process 46194 exits successfully.
x3003c0s7b0n0: [2024-03-28 13:22:18,616] [INFO] [launch.py:348:main] Process 50488 exits successfully.
x3003c0s7b1n0: [2024-03-28 13:22:19,615] [INFO] [launch.py:348:main] Process 58258 exits successfully.
x3003c0s7b1n0: [2024-03-28 13:22:19,615] [INFO] [launch.py:348:main] Process 58259 exits successfully.
x3003c0s7b1n0: [2024-03-28 13:22:19,615] [INFO] [launch.py:348:main] Process 58260 exits successfully.
x3003c0s7b0n0: [2024-03-28 13:22:19,617] [INFO] [launch.py:348:main] Process 50487 exits successfully.
x3003c0s7b0n0: [2024-03-28 13:22:19,618] [INFO] [launch.py:348:main] Process 50486 exits successfully.
x3003c0s37b1n0: [2024-03-28 13:22:19,726] [INFO] [launch.py:348:main] Process 38818 exits successfully.
x3003c0s37b1n0: [2024-03-28 13:22:19,726] [INFO] [launch.py:348:main] Process 38819 exits successfully.
x3003c0s37b1n0: [2024-03-28 13:22:19,726] [INFO] [launch.py:348:main] Process 38817 exits successfully.
x3003c0s37b0n0: [2024-03-28 13:22:20,083] [INFO] [launch.py:348:main] Process 46195 exits successfully.
x3003c0s37b0n0: [2024-03-28 13:22:20,083] [INFO] [launch.py:348:main] Process 46193 exits successfully.
x3003c0s37b0n0: [2024-03-28 13:22:20,083] [INFO] [launch.py:348:main] Process 46196 exits successfully.
